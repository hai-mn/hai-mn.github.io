---
title: "Latent Profile Analysis of Chicago Neighborhoods"
description: |
  American Census Survey data;    
  Factor Analysis;    
  Latent Profile Analysis: Gaussian Mixture modeling;    
author:
  - name: Hai Nguyen
date: "October 24, 2021"
categories:
  - Biostatistics
  - Psychology/Sociology
  - Factor Analysis
  - Mclust 
  - Tutorial
  - Mapping
base_url: https://hai-mn.github.io
output:
  distill::distill_article:
    toc: true
    toc_float: yes
    toc_depth: 3
    theme: journal
    highlight: haddock
    highlight_downlit: false
---

```{r setup, warning=FALSE, message=FALSE}
library(mclust)
library(psych)
library(tidyverse)
library(nFactors)
library(kableExtra)
```

## Factor Analysis

### ACS data

The summary of the data "Chicago Census Data by Tract_American Census Survey (ACS)". Data includes Census tract characteristics of 2010-2014 5-year ACS estimates that easy to download from https://data.census.gov/cedsci/.

We will use the following variables in the exploratory factor analysis:

* AgeDependencyRatio: _age dependency ratio_  
* LimitedEngProf5andUP: _limited English proficiency_ 
* LessThanHS: _less than high school in education level_  
* Unemployment  
* PctForeignBorn: _percentage of foreign born_  
* FemaleHHPct: _percentage of female per household_ 
* MedianIncomeHH: _median income per household_   
* AllOcc2009Pct: _percentage of all occupants in 2009_  
* PctVacHousing: _percentage of vacant housing_  
* PctBelowPoverty: _percentage below poverty_  
* PublicAssistancePct: _percentage of public assistance_  

```{r data manipulation 1}
census <- read.csv("data/Chicago Census Data by Tract_ACS 2015.csv", header = TRUE) %>% 
  rename(tract = ï..tract) %>% 
  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)

summary(census)
```

```{r data manipulation 2}  
# recode into % to scale to other variables
df <- census %>% mutate(
  PctForeignBorn = PctForeignBorn*100,
  FemaleHHPct = FemaleHHPct*100,
  # MedianIncomeHH1Kinv = (max(MedianIncomeHH, na.rm=TRUE) - MedianIncomeHH)/1000,
  # we would not use 1Kinv because of weird distribution (multi-mode)
  MedianIncomeHH1K = MedianIncomeHH/1000,
  AllOcc2009Pct = AllOcc2009Pct*100,
  PctVacHousing = PctVacHousing*100,
  PublicAssistancePct = PublicAssistancePct*100
) %>% 
  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH1K, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)

df.fa <- df[complete.cases(df),] %>% 
  select(-tract)# 1 missing case
``` 
 
```{r summary}
summary(df.fa)
```

### Determine Number of Factors to Extract

```{r scree plot, message=FALSE, warning=FALSE}
ev <- eigen(cor(df.fa)) # get eigenvalues
ap <- parallel(subject=nrow(df.fa),var=ncol(df.fa), rep=100,cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS) 
```


When we run a factor analysis, we need to decide on three things:

1. the number of factors
2. the method of estimation
3. the rotation

$\Rightarrow$ Maximum Likelihood Factor Analysis (bullet 2) entering raw data and extracting 3 factors (bullet 1), with varimax rotation (bullet 3)  

```{r fa}
fit.f <- factanal(x = df.fa, factors = 3, n.obs = 797, rotation="varimax")
print(fit.f, digits=2, cutoff=.3, sort=TRUE)
```

* In general, we’d like to see low uniquenesses or high communalities (subtract the uniquenesses from 1---The communality is the proportion of variance of the $i^{th}$ variable contributed by the $m$ (here's 3) common factors)

* At the conclusion of a factor analysis of census data, we might determine that the census measures 3 factors: (1) social poverty disparity strength, (2) education strength, (3) age disparity strength

* Factor analysis seeks to model the correlation matrix with fewer variables called factors. If we succeed with, said here, 3 factors, we are able to model the correlation matrix using only 3 variables instead of 11. Just remember these 3 variables, or factors, are unobserved. We give them names like "latent variables". They are not subsets of our original variables.

* Given the data matrix $X$, consider it's covariance matrix $cov(X)=\sum$ and obtain the matrix $V$ of eigenvectors from $\sum$. This matrix $V$ is what you call the loadings.

* This set of eigenvectors define an orthogonal change of basis matrix that maximize the variance from X. This means that, if I project X into the subspace generated by V I will obtain a matrix U by simply solving XV=U. This matrix U is what you call the scores (for each factor, we would have its own score).

* In our case, we would manipulate a little bit due to we use only variables in the first factor. Thus, if the absolute value of loadings less than or equal 0.5, it would not contribute to the score

```{r concentrated disadvantage--fa score}
df.fa$concdisadv <- as.matrix(df.fa) %*% (fit.f$loadings[,1]*(abs(fit.f$loadings[,1])>0.5))

# print out the scores
kable(df.fa[1:10,], caption = "FA scores -- The concentrated disadvantage index scores") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

## Latent Profile Analysis

### Data to run LPA

The latent profile analysis (normal mixture) will use variables loaded on factor 1 identified from the factor analysis:

* AgeDependencyRatio
* Unemployment
* FemaleHHPct
* MedianIncomeHH1K
* PctVacHousing
* PctBelowPoverty
* PublicAssistancePct
* LessThanHS

```{r data manipulation 3, echo=FALSE,  warning=FALSE}
df2 <- df[complete.cases(df),] %>% 
  select(tract, AgeDependencyRatio, LessThanHS, 
         Unemployment, FemaleHHPct, MedianIncomeHH1K,
         PctVacHousing, PctBelowPoverty, PublicAssistancePct)
```

~~~
    If we run Mclust for all 11 indicators, the data-driven would produce up to 
    8, 9 clusters. Indeed, I have checked the problem. That's the reason 
    we would use factor analysis to select covariates in the first components.
~~~

### Check how many clusters should create?

```{r overall mclust, out.width='1200px', out.hight='1200px'}
X <- as.matrix(df2[,-1])
mod <- Mclust(X)
summary(mod$BIC)
plot(mod, what="BIC", ylim=range(mod$BIC[,-(1:2)], na.rm=TRUE), legendArgs = list(x = "bottomleft")) 
summary(mod)
head(mod$classification)
head(mod$uncertainty)
mod$uncertainty[mod$classification==4]
plot(mod, what="classification")
```

* In the above Mclust() function call, the number of mixing components and the covariance parameterization are selected using the Bayesian Information Criterion (BIC). A summary showing the top-three models and a plot of the BIC traces (the first fig.) for all the models considered is then obtained.

* We would use the indication of a 4-component mixture with covariances having different shapes, volume and orientation (VVV). The use can be confirmed by the second plot, there are no clear regions of 7 or 6-component on the plot.

### Fitted a 4-components GMM with unconstrained covariance matrices

```{r 4-comp VVV, out.width='1200px', out.hight='1200px'}
mod.4 <- Mclust(df2[,-1], G=4) 
summary(mod.4$BIC) 

plot(mod.4, what="classification")
summary(mod.4, parameters = TRUE)

# compare classification between mod and mod.4
#table(mod$classification, mod.4$classification)

df2$ConcDisadv_cluster <- mod.4$classification
with(df2, table(ConcDisadv_cluster))
df2[,11:14] <- mod.4$z
names(df2)[11:14] <- c("ConcDisadv_prob1", "ConcDisadv_prob2", "ConcDisadv_prob3", "ConcDisadv_prob4")
kable(df2[20:50,], caption = "Concentrated Disadvantage Clustering Selection based on the highest probability") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

* Here is the interesting point. Even though, we would see the mismatch among the clustering with the indicators, for example, cluster 4 and 3 may have the contradict between income, less than HS and poverty level. But overall, we will apply on the real cases, the clustering reflexes the correct situations. We can see below 2 applications on mapping and on classifying on race-ethnicity.

## Applying the Clustering

### Mapping on Chicago Neighborhood map

With the level:  

* 1: not/mild disadvantage  
* 2, 3: medium disadvantage  
* 4: highly disadvantage  

We can map on Chicago Neighborhood map (using ArcGIS) as such  

![](pics/Mapping Concentrated Disadvantage-Family Violence.png)  

### Classification by race-ethnicity

Save for later on!!! 

## Refs

Pugach, Oksana. 2018. "Latent Profile Analysis of Chicago Neighborhoods" Unpublished Work. Chalk Talk - Institute for Health Resereach; Policy - Methodology Research Core.

University of Virginia Library, Research Data Services + Sciences. "Getting Started with Factor Analysis" - https://data.library.virginia.edu/getting-started-with-factor-analysis/

Stats Stackexchange. "How to calculate the loading matrix from the score matrix and a data matrix X (PCA)?" - https://stats.stackexchange.com/questions/447952/how-to-calculate-the-loading-matrix-from-the-score-matrix-and-a-data-matrix-x-p 

Luca Scrucca. 2020. "A quick tour of mclust" - https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html

Luca Scrucca, Michael Fop, T. Brendan Murphy,Adrian E. Raftery. "mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models." R J. 2016 August ; 8(1): 289-317