---
title: "Exploratory / Confirmatory Factor Analysis?"
description: |
     Correlation matrix;  
     Exploratory Factor Analysis vs. Confirmatory Factor Analysis;
     Run An Example with Categorical Data in R (psych package);  
author:
  - name: Hai Nguyen
date: September 19, 2022
categories:
  - Biostatistics  
  - Psychology/Sociology  
  - EFA 
  - Model Fit Statistics   
base_url: https://hai-mn.github.io
output:
  distill::distill_article:
    toc: true
    toc_float: yes
    toc_depth: 4
    theme: journal
    highlight: haddock
    highlight_downlit: false
---


## Why use factor analysis?

Factor analysis is a useful tool for investigating variable relationships for complex concepts such as socioeconomic status, dietary patterns, or psychological scales.  

It allows researchers to investigate concepts that are not easily measured directly by collapsing a large number of variables into a few interpretable underlying factors.   

## What is a factor?

In factor analysis, a factor is an latent (unmeasured) variable that expresses itself through its relationship with other measured variables.  

Take for example a variable like cognition. We may want to measure a person’s cognition, but this is the kind of construct that would be impossible to measure using a single variable. It’s just too abstract and multifaceted, although it does represent a single concept. So instead, you may have to develop the scale with many items, each of which measures some more measurable part of leadership. The idea would be that there is an underlying unmeasurable factor, cognition, that causes people to respond in certain patterns on the many items on the scale.  

The purpose of factor analysis is to analyze these patterns of response as a way of getting at this underlying factor. Factor analysis also allows you to use the weighted item responses to create what are called factor scores.  These represent a single score for each person on the factor. Factor scores are nice because they allow you to use a single variable as a measure of the factor in the other analyses, rather than a set of items.  

Factor analysis often applied for continuous variables, but for nominal variables we would have no problem when using `fa()` in library `psych`  

$\Rightarrow$ As demonstrated below, using ordinal or nominal/binary data for factor analysis in R is no more difficult than using continuous data for factor analysis in R.   

If variables include mixed variables. If that is the case use `cor="mixed"` in `fa()` of psych library  

## A case study of Depression data of ordinal variables

Now, we jump on an example: Depression data (NHANES, 2014-15) includes 10 ordinal variables. This approach would be better helping us see through the meaningfulness of FA.     

- `DPQ010`	Have little interest in doing things  
- `DPQ020*`	Feeling down, depressed, or hopeless
- `DPQ030`	Trouble sleeping or sleeping too much  
- `DPQ040`	Feeling tired or having little energy  
- `DPQ050`	Poor appetite or overeating  
- `DPQ060*`	Feeling bad about yourself  
- `DPQ070`	Trouble concentrating on things  
- `DPQ080`	Moving or speaking slowly or too fast  
- `DPQ090*`	Thought you would be better off dead  
- `DPQ100`	Difficulty these problems have caused  

Few more variables of different ordinal scales, which are not used in this example  
* more critical/serious variables  

Values of each variable has the same format of "ordinal" numerical values:   

  * 0	Not at all    
  * 1	Several days  
  * 2	More than half the days   
  * 3	Nearly every day  

Higher is the value $\Rightarrow$ "more" likely to be depressed ???

#### Read raw data into R 

```{r}
options(width = 300)
dep <- read.csv("data/dep.csv",header = TRUE, 
                sep = ",", na.strings = "NA")
# na.strings : a character vector of strings which are to be 
#             interpreted as NA values.
head(dep)
nrow(dep)

x <- dep[,2:11]
```

We first check our correlation matrix.
Visual analysis of the correlation matrix is done to reveal some similarities or correlations that can be found in the data between groups of questions.

```{r}
R <- cor(x)
library(corrplot)
corrplot(R,type="upper",order="hclust")
```

The blue structures are positively correlated groups and the red structure represents a negatively correlated group that emerges among the data.

#### FA using fa() form psych

```{r}
library(psych)
library(GPArotation)
```

<aside>
Note: `psych` package also has Tetrachoric, polychoric, biserial 
and polyserial correlations for various types of variables
</aside>

```{r}
head(x)
```

Important note for FA using psych lib of different types of variables

`fa(data,n.factors)` has an additional argument: `cor` with several options depending on input variable types  

  * `cor="cor"` 	Pearson correlation, for numeric variables with different scales  
  * `cor="cov"`   Covariance, for numeric variables with similar scales  
  * `cor="tet"`   tetrachoric, for dichotomous variables   
  * `cor="poly"`  polychoric,  for ordinal variables   
  * `cor="mixed"` mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate  

Remarks  

  * tetrachoric "correlation" for 2 x 2 table or two dichotomous variables.   
  * polychoric correlation for k1 x k2 ordinal variables.   
  * biserial correlation for continuous variable and dichotmous x variable.  
  * polyserial correlation for continuous variable and a ordered categorical Y.  

May convert nominal variables into dummy variables but that increases dimensionality very quickly

Assumption for ordinal variable, it has latenent normal variable  
Correlations between non-numeric variables is often related chi-square test stats  

Since our variables are all ordinal lets use polychoric correlations

```{r}
polychoric(x)
```

Output has   

  * rho="correlations" and  
  * tau="the normal equivalent of the cutpoints"   

`fa` for ordinal variables of more than two values.   
In this case, assuming two factors.  

```{r}
fa.out <- fa(dep[,2:11], nfactors=2, cor='poly', rotate = "oblimin")
```

<aside>
Note:   
- `oblimin` is default  
- If the variables are all 0/1 binary, use tetrachoric correlations i.e. cor="tet"
</aside>

```{r}
fa.out
names(fa.out)
plot(fa.out$values,type='b')
```

```{r fig2, fig.height = 12, fig.width = 12}
biplot(fa.out)
```

```{r}
head(fa.out$scores)
```

Let's check factor scores of subject 122, who is really depressed!  

```{r}
x[122,]
fa.out$scores[122,]
```

Really large numbers, look at the second score  
Let's get predicted values of `x[122,]` and they should be high  

```{r}
apply(x,2,mean) + fa.out$scores[122,]%*%t(as.matrix(fa.out$loadings[,1:2]))
```

Although 2nd factor is not significant according to Elbow rule, we still include it in analysis for its meaning!   
Interpret the output as  of numeric variables  
But, care must be taken while generalizing FA of non-numeric variables  

Interpretation:   

  * F1 or MR1: weighted average non-critical/non-clinical depression variables  
  * F2 or MR2: weighted average critical/clinical depression variables  


Alternative way of doing same FA using correlation matrix but less output(no scores)   
                   
```{r, eval=FALSE}
poly.R <- polychoric(x)$rho
fa.out <- fa(r=poly.R, nfactors=2, n.obs = nrow(dep[,2:11]))
```

## What are factor loadings?

```{r}
fa.diagram(fa.out, digits = 2)
```

The relationship of each variable to the underlying factor is expressed by the so-called factor loading. Let’s go through each part of the printed output.


```{r}
round(cbind(fa.out$loadings,"h2"=fa.out$communality,"u2"=fa.out$uniquenesses,"com"=fa.out$complexity),2)
```

  * What’s __MR, ML, PC__ etc.? These are factors, and the name merely reflects the fitting method, e.g. minimum residual, maximum likelihood, principal components. The default is minimum residual, so in this case __MR__.

  * Why are they __‘out of order’__? the number assigned is arbitrary, but this has to do with a rotated solution. See the help file for more details, otherwise they are numbered in terms of variance accounted for.  

  * **h2**: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. **communality**.  
  
  * **u2**: **1 - h2**. residual variance, a.k.a. **uniqueness**     
  
  * **com**: Item complexity. Specifically it is “Hoffman’s index of complexity for each item. This is just $(\sum \lambda_i^2)^2/\sum \lambda_i^4$, where $\lambda_i$ is the factor loading on the $i^{th}$ factor. Basically it tells you how much an item reflects a single construct. It will be lower for relatively lower loadings.
  
```{r}
fa.out$Vaccounted
```

The variance accounted for portion of the output can be explained as follows:  

  * **SS loadings**: These are the eigenvalues, the sum of the squared loadings. In this case where we are using a correlation matrix, summing across all factors would equal the number of variables used in the analysis.  
  * **Proportion Var**: tells us how much of the overall variance the factor accounts for out of all the variables.  
  * **Cumulative Var**: the cumulative sum of **Proportion Var**.  
  * **Proportion Explained**: The relative amount of variance explained- Proportion Var/sum(Proportion Var).  
  * **Cumulative Proportion**: the cumulative sum of **Proportion Explained**.  
These are contained in `model$Vaccounted`.
  
The variable with the strongest association to the underlying latent variable. Factor 1, is DPQ060 (Feeling bad about yourself), with a factor loading of 0.90.

Since factor loadings can be interpreted like standardized regression coefficients, one could also say that the variable DPQ060 has a correlation of 0.90 with Factor 1. This would be considered a strong association for a factor analysis in most research fields.

Two other variables, DPQ090 (Thought you would be better off dead) and DPQ020 (Feeling down, depressed, or hopeless), are also associated with Factor 1. Based on the variables loading highly onto Factor 1, we could call it "Low self-esteem individual awareness."

DPQ010, DPQ030, DPQ040, DPQ050, DPQ070, DPQ080 and DPQ100, however, have high factor loadings on the Factor 2. They seem to indicate the observed signs of depression, so we may want to call Factor 2 "Physical depression signs."

```{r}
fa.out$Phi
```

Factor correlations   
Whether you get this part of the analysis depends on whether or not these are estimated. You have to have multiple factors and a rotation that allows for the correlations.  

  * **factor correlations**: the correlation matrix for the factors. $\phi (phi)$  
  * **Mean item complexity**: the mean of **com**.  
These are contained in `model$Phi`   

Model test results and Fit Indices (already discuss in [SEM](https://hai-mn.github.io/posts/2022-06-20-SEM/))

## Rotation in EFA

An feature of EFA is that the axes of the factors can be rotated within the multidimensional variable space.    
What a factor analysis program does while determining the best fit between the variables and the latent factors. We have 10 variables that go into a factor analysis.  
The program looks first for the strongest correlations between variables and the latent factor, and makes that Factor 1. Visually, one can think of it as an axis (Axis 1).
The factor analysis program then looks for the second set of correlations and calls it Factor 2, and so on. Sometimes, the initial solution results in strong correlations of a variable with several factors or in a variable that has no strong correlations with any of the factors.  

```{r}
fa.dem <- factor.rotate(fa.out,0,plot=TRUE,xlim=c(-1,1),ylim=c(-1,1), title="oblimin rotation")
```

Rotations that allow for correlation are called oblique rotations; rotations that assume the factors are not correlated are called orthogonal rotations. Our graph shows an orthogonal rotation. We run the example above using the oblimin (oblique). Below, we test with varimax (orthogonal):

```{r}
fa.out <- fa(dep[,2:11], nfactors=2, cor='poly', rotate = "varimax")
fa.out
```

Sometimes, the orthogonal rotation did not work out. i.e. no variable is loading highly onto another factor.

```{r}
op <- par(mfrow=c(1,2))
cluster.plot(fa.out,xlim=c(-1,1),ylim=c(-1,1),title="Unrotated ")
f2r <- factor.rotate(fa.out,-65,plot=TRUE,xlim=c(-1,1),ylim=c(-1,1),title="rotated -65 degrees")
```
<aside>
`factor.rotate` is useful for those cases that require specific rotations that are not available in more advanced packages such as GPArotation
</aside>

## Comparison of EFA and CFA

|                   |EFA             |CFA            |
|:------------------|:---------------|:--------------|
|What gets analyzed |                |               |
|                   |Correlation matrix (of items = indicators)|Covariance matrix (of items = indicators)|  
|                   |➢ Only correlations among observed item responses are used <br>➢ Only a standardized solution is provided, so the original means and variances of the observed item responses are irrelevant|➢ Means, variances, and covariances of item responses are analyzed<br>➢ Item response means historically have been ignored<br>➢Output includes unstandardized AND standardized solutions|   
| Interpretation    |               |                 |  
|                   |Rotation       |Defining interpretation|
|                   |➢ All items load on all factors (latent traits)<br>➢ Goal is to pick a rotation that gives closest approximation to "simple structure"<br>➢ No way of distinguishing latent variables due to traits being measured from correlation|➢ CFA is theory-driven: any structure becomes a testable hypothesis<br>➢ You specify number of latent factors and their inter-correlations<br>➢You specify which items load on which latent factors <br>➢ You specify any additional relationships for method/other covariance<br>➢ You just need a clue; you don’t have to be right (misfit is informative)|
|Model Fit          |               |                   |
|                   |Eye-balls and Opinion|Inferential tests via ML|  
|                   |➢ #Factors? Scree plots, interpretability<br>➢ Which rotation? <br>➢ Which items load on each factor? Arbitrary cutoff of .3-.4ish<br>➢ Standard errors infrequently used)|➢ Global model fit test (and local)<br>➢Standard errors (and significance tests) of item loadings, error variances, and error covariances (all parameters)<br>➢ Ability to test appropriateness of model constraints or model additions via tests for change in model fit|
|Factor scores      |               |                   |
|                   |Don’t ever use factor scores from an EFA|Factor scores can be used, but only if necessary|
|                   |➢ Factor scores are indeterminate (especially due to rotation) <br>➢ Inconsistency in how factor models are applied to data|Best option: Test relations among latent factors directly through SEM<br>➢ Factor scores are less indeterminate in CFA, and could be used|


