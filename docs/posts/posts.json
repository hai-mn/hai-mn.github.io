[
  {
    "path": "posts/2022-02-28-Bayesian methods - Series 6 of 10/",
    "title": "Series 6 -- ANOVA & ANCOVA",
    "description": "A bit review ANOVA & ANCOVA in the Frequentist's view   \n\"ANOVA & ANCOVA\" in Bayesian Context   \nContrast Comparison",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-02-28",
    "categories": [
      "Biostatistics",
      "Bayesian methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nOne-way ANOVA\r\nSex and Death\r\nexample of section 19.3 in (Kruschke 2015)\r\nTraditional analysis\r\nBayesian\r\napproach\r\n\r\nTwo-way ANOVA\r\nModel\r\ndescription\r\nData of section 20.1 in (Kruschke\r\n2015)\r\nMCMC\r\nWorking with chains and\r\ncontrasts\r\nContrast for comparison\r\nof departments\r\nContrast for comparison of\r\npositions\r\nContrast for comparison of\r\nspreads\r\n\r\nUnderstanding\r\nthe effect of scaling and transformations on interactions\r\n\r\nANCOVA\r\nTraditional analysis\r\nBayesian\r\napproach\r\n\r\nHeterogeneous variances\r\nApply traditional ANOVA\r\nmethod.\r\nBayes’\r\n\r\nFurther\r\nReadings\r\n\r\nIf we pay a bit attention to the ANOVA and ANCOVA, we can see the\r\ninteresting point of views in these cases:\r\nthey greatly depend on assumptions (homoscedastic, normally\r\ndistributed)\r\n“Bayes” perhaps not call the terms ANOVA or ANCOVA. The analysis is\r\nbased on linear regression’s hierarchical modeling with the restriction\r\non some beta coefficient distributions. Thus, “Bayes” would call the\r\nmetric response with nominal predictor with/without metric\r\npredictor.\r\nOne-way ANOVA\r\nThis type of analysis actually is a model with metric output and one\r\nnominal predictor.\r\n\r\n\r\nA diagram for metric output and one nominal predictor shows parameters\r\ndenoted as \\(\\alpha\\) instead of\r\ntraditional \\(\\beta\\).\r\n\r\n\r\nWe represent the nominal predictor by a vector \\(\\vec{x}\\) = {\\(x_{1}, ..., x_{J}\\)}, where \\(J\\) is the number of categories that the\r\npredictor has. When an individual falls in group \\(j\\) of the nominal predictor, this is\r\nrepresented by setting \\(x_{j} = 1\\)\r\nand \\(x_{i\\neq j} = 0\\). The predicted\r\nvalue, denoted \\(\\mu\\), is the overall\r\nbaseline plus the deflection of a group:\r\n\\[\\begin{align*}  \r\n\\mu &= \\beta_0 + \\sum_j \\beta_{j} x_{j} \\\\  \r\n    &= \\beta_0 + \\vec{\\beta} \\cdot \\vec{x}  \r\n\\end{align*}\\]\r\nThe overall baseline is constrained so that the deflections sum to\r\n\\(0\\) across the categories:\\[ \\sum_j \\beta_{[j]} = 0 \\]\r\nIn order to satisfy this constraint, first, find unconstrained slopes\r\n\\(\\alpha_1, ..., \\alpha_J\\) and then\r\nadjust them using formulas:\r\n\\[\\begin{align*}  \r\n\\mu &= \\alpha_0 + \\sum_j \\alpha_{j} x_{j} \\\\  \r\n    &= (\\alpha_0 + \\bar{\\alpha}) + \\sum_j (\\alpha_{j} -\r\n\\bar{\\alpha}) \\cdot x_{j}\\\\  \r\n    &= \\beta_0 + \\sum_j \\beta_{j} x_{j}\r\n\\end{align*}\\]\r\nwhere \\(\\bar{\\alpha} = \\frac{1}{n} \\sum_j\r\n\\alpha_j\\)\r\nOn the diagram the within-group standard deviation \\(\\sigma\\) is given a broad uniform prior,\r\nthe intercept is given a broad normal prior centered at \\(0\\) and the group coefficients come from\r\nzero-centered normal distribution.\r\nInterpretation of intercept is the grand mean value.\r\nThe standard deviation of slopes \\(\\sigma_{\\beta}\\) may be either a constant\r\nor may have a prior of its own.\r\nIn case when \\(\\sigma_{\\beta}\\) =\r\nconst means of all groups are estimated separately, they do not inform\r\neach other through estimation of \\(\\sigma_{\\beta}\\). A large number for \\(\\sigma_{\\beta}\\) will make the results\r\nclose or equivalent to ANOVA.\r\nGiving \\(\\sigma_{\\beta}\\) a prior\r\naffects shrinkage rate. If all group means are close to the grand mean\r\nthen \\(\\sigma_{\\beta}\\) is estimated as\r\nsmall number. This makes shrinkage stronger.\r\nA key assumption behind estimating \\(\\sigma_{\\beta}\\) from the data is that all\r\ngroups can be meaningfully described as representatives of shared\r\nhigher-level distribution.\r\nExample. Giving \\(\\sigma_{\\beta}\\) a prior distribution is\r\nnot justified if, for example, in medical studies there are several\r\ncontrol groups and one treated group. In this case small difference\r\nbetween placebo groups may result in significant shrinkage (bias) of the\r\ntreated group.\r\nIn this example the treated group is an outlier. Heavy-tailed prior\r\ndistribution for \\(\\beta_j\\) may help\r\nto overcome excessive shrinkage.\r\nPossible priors for \\(\\sigma_{\\alpha}\\):\r\nNo prior, i.e. parameter is not estimated from the data. Estimated\r\ngroup means do not inform each other through \\(\\sigma_{\\alpha}\\)\r\nGamma distribution or any other distribution concentrated on\r\npositive part of real line with shape allowing large deviations from\r\nzero\r\nDistributions on non-negative part of real line allowing large\r\nvalues, but also giving positive probability to zero value: half-Cauchy,\r\nfolded t-distribution. Should be careful with this option since with\r\nsmall data set it may cause an implosive shrinkage.\r\nAnother interpretation of imploding shrinkage is: the model has to\r\nmake choice between assigning more variability to between the groups\r\n(\\(\\sigma_{\\alpha}\\)) or shrinking the\r\ngroups and assigning more variability to within the groups noise (\\(\\sigma_{y}\\)).\r\nWhenever it is possible the model will prefer the latter.\r\nShrinkage will not be very efficient in case of binary predictor.\r\nSex and Death\r\nexample of section 19.3 in (Kruschke 2015)\r\nThe fruit fly, Drosophila melanogaster, is known for this species\r\nthat newly inseminated females will not remate (for at least two days),\r\nand males will not actively court pregnant females. There were 25 male\r\nfruit flies in each of the five groups.\r\nCompanionNumber: group\r\nNone0: males with zero companions\r\nPregnant1: males with one pregnant female\r\nPregnant8: males accompanied by eight pregnant\r\nfemales\r\nVirgin1: males accompanied by one virgin female\r\nVirgin8: males accompanied by eight virgin females\r\nResearch Question: estimate the life spans and the\r\nmagnitude of differences between groups.\r\n\r\n#read the data, the file is available at [K].\r\ndta <- read.csv(\"data/Fruitfly.csv\")\r\nnames(dta)\r\n[1] \"Longevity\"       \"CompanionNumber\" \"Thorax\"         \r\ndatatable(dta)\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\"],[35,37,49,46,63,39,46,56,63,65,56,65,70,63,65,70,77,81,86,70,70,77,77,81,77,40,37,44,47,47,47,68,47,54,61,71,75,89,58,59,62,79,96,58,62,70,72,75,96,75,46,42,65,46,58,42,48,58,50,80,63,65,70,70,72,97,46,56,70,70,72,76,90,76,92,21,40,44,54,36,40,56,60,48,53,60,60,65,68,60,81,81,48,48,56,68,75,81,48,68,16,19,19,32,33,33,30,42,42,33,26,30,40,54,34,34,47,47,42,47,54,54,56,60,44],[\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\"],[0.64,0.68,0.68,0.72,0.72,0.76,0.76,0.76,0.76,0.76,0.8,0.8,0.8,0.84,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.92,0.92,0.92,0.94,0.64,0.7,0.72,0.72,0.72,0.76,0.78,0.8,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.88,0.88,0.92,0.92,0.92,0.92,0.92,0.92,0.94,0.64,0.68,0.72,0.76,0.76,0.8,0.8,0.8,0.82,0.82,0.84,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.88,0.88,0.88,0.88,0.92,0.92,0.68,0.68,0.72,0.76,0.78,0.8,0.8,0.8,0.84,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.9,0.9,0.9,0.9,0.9,0.9,0.92,0.92,0.64,0.64,0.68,0.72,0.72,0.74,0.76,0.76,0.76,0.78,0.8,0.8,0.82,0.82,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.88,0.88,0.88,0.92]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Longevity<\\/th>\\n      <th>CompanionNumber<\\/th>\\n      <th>Thorax<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nTraditional analysis\r\n\r\nlibrary(tidyverse)\r\ngrMeans<-group_by(dta, CompanionNumber) %>%\r\n  summarise(\r\n    count = n(),\r\n    mean = mean(Longevity, na.rm = TRUE),\r\n    sd = sd(Longevity, na.rm = TRUE)\r\n  )\r\nlevels(as.factor(dta$CompanionNumber))\r\n[1] \"None0\"     \"Pregnant1\" \"Pregnant8\" \"Virgin1\"   \"Virgin8\"  \r\ngrMeans\r\n# A tibble: 5 x 4\r\n  CompanionNumber count  mean    sd\r\n  <chr>           <int> <dbl> <dbl>\r\n1 None0              25  63.6  16.5\r\n2 Pregnant1          25  64.8  15.7\r\n3 Pregnant8          25  63.4  14.5\r\n4 Virgin1            25  56.8  14.9\r\n5 Virgin8            25  38.7  12.1\r\n\r\n\r\n# Box plots\r\n# ++++++++++++++++++++\r\n# Plot weight by group and color by group\r\nlibrary(\"ggpubr\")\r\nggboxplot(dta, x = \"CompanionNumber\", y = \"Longevity\", \r\n          color = \"CompanionNumber\", \r\n          order = c(\"None0\", \"Pregnant1\", \"Pregnant8\", \"Virgin1\", \"Virgin8\"),\r\n          ylab = \"Longevity\", xlab = \"Companion Number\")\r\n\r\nggline(dta, x = \"CompanionNumber\", y = \"Longevity\", \r\n       add = c(\"mean_se\", \"jitter\"), \r\n       order = c(\"None0\", \"Pregnant1\", \"Pregnant8\", \"Virgin1\", \"Virgin8\"),\r\n       ylab = \"Longevity\", xlab = \"Companion Number\")\r\n\r\n\r\n\r\n# Compute the analysis of variance\r\nlongevity.aov <- aov(Longevity ~ CompanionNumber, data = dta)\r\n# Summary of the analysis\r\nsummary(longevity.aov)\r\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \r\nCompanionNumber   4  11939  2984.8   13.61 3.52e-09 ***\r\nResiduals       120  26314   219.3                     \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nInterpret the result of one-way ANOVA tests:\r\nAs the p-value is less than the significance level \\(0.05\\), we can conclude that there are\r\nsignificant differences between the groups highlighted with “***” in the\r\nmodel summary.\r\nTukey multiple pairwise-comparisons\r\nAs the ANOVA test is significant, we can compute Tukey HSD (Tukey\r\nHonest Significant Differences, R function: TukeyHSD()) for performing\r\nmultiple pairwise-comparison between the means of groups.\r\nThe function TukeyHD() takes the fitted ANOVA as an argument.\r\n\r\nTukeyHSD(longevity.aov)\r\n  Tukey multiple comparisons of means\r\n    95% family-wise confidence level\r\n\r\nFit: aov(formula = Longevity ~ CompanionNumber, data = dta)\r\n\r\n$CompanionNumber\r\n                      diff       lwr        upr     p adj\r\nPregnant1-None0       1.24 -10.36047  12.840468 0.9983034\r\nPregnant8-None0      -0.20 -11.80047  11.400468 0.9999988\r\nVirgin1-None0        -6.80 -18.40047   4.800468 0.4854206\r\nVirgin8-None0       -24.84 -36.44047 -13.239532 0.0000003\r\nPregnant8-Pregnant1  -1.44 -13.04047  10.160468 0.9969591\r\nVirgin1-Pregnant1    -8.04 -19.64047   3.560468 0.3126549\r\nVirgin8-Pregnant1   -26.08 -37.68047 -14.479532 0.0000001\r\nVirgin1-Pregnant8    -6.60 -18.20047   5.000468 0.5157692\r\nVirgin8-Pregnant8   -24.64 -36.24047 -13.039532 0.0000004\r\nVirgin8-Virgin1     -18.04 -29.64047  -6.439532 0.0003240\r\n\r\nCheck ANOVA assumptions: test validity?\r\n\r\n# 1. Homogeneity of variances\r\nplot(longevity.aov, 1)\r\n\r\n# 2. Normality\r\nplot(longevity.aov, 2)\r\n\r\n# Extract the residuals\r\naov_residuals <- residuals(object = longevity.aov )\r\n# Run Shapiro-Wilk test\r\nshapiro.test(x = aov_residuals ) #Shapiro-Wilk test on the ANOVA residuals (W = 0.99, p = 0.4) which finds no indication that normality is violated.\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  aov_residuals\r\nW = 0.98887, p-value = 0.4088\r\n\r\nA Shapiro-Wilk test on the ANOVA residuals (W = 0.99, p = 0.4) which\r\nfinds no indication that normality is violated.\r\nBayesian approach\r\n\r\ndataList <- list(Ntotal = nrow(dta),\r\n                 y = dta$Longevity,\r\n                 x = as.integer(as.factor(dta$CompanionNumber)),\r\n                 NxLvl = nlevels(as.factor(dta$CompanionNumber)),\r\n                 agammaShRa=unlist(gammaShRaFromModeSD(mode = sd(dta$Longevity)/2,\r\n                                                       sd = 2*sd(dta$Longevity))))\r\n\r\nFunction gammaShRaFromModeSD() from the book calculates\r\nshape and rate parameters of gamma distribution from mode and standard\r\ndeviation.\r\n\r\n#view source code of function gammaShRaFromModeSD()\r\ngammaShRaFromModeSD\r\nfunction (mode, sd) \r\n{\r\n    if (mode <= 0) \r\n        stop(\"mode must be > 0\")\r\n    if (sd <= 0) \r\n        stop(\"sd must be > 0\")\r\n    rate = (mode + sqrt(mode^2 + 4 * sd^2))/(2 * sd^2)\r\n    shape = 1 + mode * rate\r\n    return(list(shape = shape, rate = rate))\r\n}\r\n(ShRa<-gammaShRaFromModeSD(mode=sd(dta$Longevity)/2,sd=2*sd(dta$Longevity)))\r\n$shape\r\n[1] 1.283196\r\n\r\n$rate\r\n[1] 0.03224747\r\n\r\nWith these parameters standard deviation of slopes is concentrated\r\naround a relatively small value 8.7819463, but has fat enough right\r\ntail.\r\n\r\nxAxis <- seq(from=0.001, to=100, by=1 )\r\nplot(xAxis, dgamma(xAxis, shape = ShRa$shape, rate = ShRa$rate), type='l',\r\n     ylab=\"Gamma Density\",xlab=\"Sigma_Alpha\", main=\"Prior for Sigma_Alpha: mode=8.78\")\r\nabline(v=sd(dta$Longevity)/2)\r\n\r\n\r\nPrepare the model description.\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    int<lower=2> NxLvl;\r\n    int<lower=1, upper=NxLvl> x[Ntotal];\r\n    real<lower=0> agammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n}\r\nparameters {\r\n    real a0;\r\n    real<lower=0> aSigma;\r\n    vector[NxLvl] a;\r\n    real<lower=0> ySigma;\r\n}\r\nmodel {\r\n    a0 ~ normal(meanY, 5*sdY);\r\n    aSigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a ~ normal(0, aSigma);\r\n    ySigma ~ uniform(sdY/100, sdY*10);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ normal(a0 + a[x[i]], ySigma);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a0,a[] to sum-to-zero b0,b[] :\r\n        real b0;\r\n    vector[NxLvl] b;\r\n    b0 = a0 + mean(a);\r\n    b = a - mean(a);\r\n}\"\r\n\r\nRun MCMC.\r\n\r\nstanDso <- stan_model(model_code = modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(stanDso,file=\"data/stanDSO.Rds\")\r\nstanDso <- readRDS(\"data/stanDSO.Rds\")\r\n\r\n\r\nfit <- sampling(stanDso, \r\n                data = dataList, \r\n                pars = c('b0', 'b', 'aSigma', 'ySigma'),\r\n                iter = 5000, chains = 2, cores = 4)\r\n\r\nAnalyze the chains using shinystan()\r\n\r\nlaunch_shinystan(fit)\r\n\r\n\r\nsummary(fit)$summary[,c(1,4,6,8,10)]\r\n               mean         2.5%          50%       97.5%      Rhat\r\nb0       57.4389405   54.7404583   57.4363366   60.030781 0.9996407\r\nb[1]      5.7391473    0.6721319    5.6702451   11.006805 0.9998268\r\nb[2]      6.8806784    1.9088332    6.9014198   11.919059 0.9997847\r\nb[3]      5.5798729    0.5277791    5.5732278   10.579570 0.9996462\r\nb[4]     -0.6248182   -5.7416889   -0.5962544    4.261992 1.0004579\r\nb[5]    -17.5748804  -22.9517493  -17.5560210  -12.145087 1.0004044\r\naSigma   15.1326002    6.3576135   13.0496293   35.363724 1.0011346\r\nySigma   14.9639337   13.2289575   14.9051123   17.058750 1.0000600\r\nlp__   -409.5749930 -414.6181727 -409.2509208 -406.448647 1.0008724\r\ncbind(GroupMeans=grMeans,\r\n      EstimatedMeans=summary(fit)$summary[2:6,1]+summary(fit)$summary[1,1])\r\n     GroupMeans.CompanionNumber GroupMeans.count GroupMeans.mean\r\nb[1]                      None0               25           63.56\r\nb[2]                  Pregnant1               25           64.80\r\nb[3]                  Pregnant8               25           63.36\r\nb[4]                    Virgin1               25           56.76\r\nb[5]                    Virgin8               25           38.72\r\n     GroupMeans.sd EstimatedMeans\r\nb[1]      16.45215       63.17809\r\nb[2]      15.65248       64.31962\r\nb[3]      14.53983       63.01881\r\nb[4]      14.92838       56.81412\r\nb[5]      12.10207       39.86406\r\n\r\nNote differences between estimated group means and also\r\nshrinkage.\r\n\r\nstan_ac(fit, separate_chains = T)\r\n\r\npairs(fit)\r\n\r\nplot(fit)\r\n\r\nplot(fit,pars=c(\"b\"))\r\n\r\nstan_dens(fit)\r\n\r\n\r\nFrom MCMC we see immediately that not all parameters \\(\\beta_j\\) are zeros (omnibus test).\r\nAs usual, the following question in ANOVA is: which group means are not\r\nequal.\r\nTo answer this question we use contrasts.\r\nIn FNP approach there is a constraint on how many pairs or subgroups\r\ncan be checked with contrasts with desired accuracy of inference.\r\nIn Bayesian approach testing of contrasts is straight forward and\r\ndoes not have restrictions: each step of chain returns a combination of\r\ngroup means. Thus we only need to compare frequency of these\r\ncombinations in the chain.\r\nCalculate contrasts for betas.\r\nExtract chains for parameters b.\r\n\r\nfit_ext <- rstan::extract(fit)\r\nhead(fit_ext$b)\r\n          \r\niterations     [,1]     [,2]       [,3]      [,4]      [,5]\r\n      [1,] 6.855855 2.182986  6.6738154  1.637767 -17.35042\r\n      [2,] 6.637835 5.211015 12.6860452 -3.791568 -20.74333\r\n      [3,] 9.755984 4.116482  4.8271676 -1.744606 -16.95503\r\n      [4,] 3.081568 9.927490 -0.3562593  2.993046 -15.64584\r\n      [5,] 7.769221 4.711784  7.6943528 -1.325809 -18.84955\r\n      [6,] 6.660574 7.111976 -0.9158100  5.244166 -18.10091\r\ndim(fit_ext$b)\r\n[1] 5000    5\r\n\r\nContrast between group 1 (‘None0’) and group 5 (‘Virgin8’)\r\n\r\ncontrast_1_5 <- fit_ext$b[,1] - fit_ext$b[,5]\r\nplot(contrast_1_5)\r\n\r\nhist(contrast_1_5)\r\n\r\n(hdiContrast_1_5 <- hdi(contrast_1_5))\r\n   lower    upper \r\n14.72735 31.74161 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast_1_5 <- sd(contrast_1_5))\r\n[1] 4.377288\r\nplot(rank(fit_ext$b[,1]), rank(fit_ext$b[,5]))\r\n\r\n\r\nContrast between average of groups 1-3 (None0, Pregnant1, Pregnant8)\r\nand average of groups 4-5 (Virgin1, Virgin8)\r\n\r\nComb1<-(fit_ext$b[,1] + fit_ext$b[,2] + fit_ext$b[,3])/3\r\nComb2<-(fit_ext$b[,4] + fit_ext$b[,5])/2\r\ncontrast_123_45 <- Comb1 - Comb2\r\nplot(contrast_123_45)\r\n\r\nhist(contrast_123_45)\r\n\r\n(hdiContrast_123_45<-hdi(contrast_123_45))\r\n    lower     upper \r\n 9.727677 20.505621 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast_123_45<-sd(contrast_123_45))\r\n[1] 2.77919\r\nhead(cbind(Comb1,Comb2))\r\n        Comb1      Comb2\r\n[1,] 5.237552  -7.856328\r\n[2,] 8.178299 -12.267448\r\n[3,] 6.233211  -9.349816\r\n[4,] 4.217599  -6.326399\r\n[5,] 6.725119 -10.087679\r\n[6,] 4.285580  -6.428370\r\nplot(Comb1[1:100],Comb2[1:100])\r\n\r\nplot(rank((fit_ext$b[,1] + fit_ext$b[,2] + fit_ext$b[,3])/3),rank((fit_ext$b[,4] + fit_ext$b[,5])/2))\r\n\r\n\r\nTwo-way ANOVA\r\nModel description\r\nDefine the model\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    vector[Ntotal] y;\r\n    int<lower=2> Nx1Lvl;\r\n    int<lower=2> Nx2Lvl;\r\n    int<lower=1, upper=Nx1Lvl> x1[Ntotal];\r\n    int<lower=1, upper=Nx2Lvl> x2[Ntotal];\r\n    real<lower=0> agammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    vector[Ntotal] zy;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    zy = (y - mean(y)) / sdY;  // center & normalize\r\n}\r\nparameters {\r\n    real a0;\r\n    real<lower=0> a1Sigma;\r\n    real<lower=0> a2Sigma;\r\n    real<lower=0> a1a2Sigma;\r\n    vector[Nx1Lvl] a1;\r\n    vector[Nx2Lvl] a2;\r\n    matrix[Nx1Lvl,Nx2Lvl] a1a2;\r\n    real<lower=0> zySigma;\r\n}\r\nmodel {\r\n    a0 ~ normal(0, 1);\r\n    a1Sigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a1 ~ normal(0, a1Sigma);\r\n    a2Sigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a2 ~ normal(0, a2Sigma);\r\n    a1a2Sigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    for (j1 in 1:Nx1Lvl) {\r\n        a1a2[j1,] ~ normal(0, a1a2Sigma);\r\n    }\r\n    zySigma ~ uniform(1.0/10, 10);\r\n    for ( i in 1:Ntotal ) {\r\n        zy[i] ~ normal(a0 + a1[x1[i]] + a2[x2[i]]+ a1a2[x1[i],x2[i]], zySigma);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a to sum-to-zero b :\r\n    real b0;\r\n    vector[Nx1Lvl] b1;\r\n    vector[Nx2Lvl] b2;\r\n    matrix[Nx1Lvl,Nx2Lvl] b1b2;\r\n    matrix[Nx1Lvl,Nx2Lvl] m;\r\n    real<lower=0> b1Sigma;\r\n    real<lower=0> b2Sigma;\r\n    real<lower=0> b1b2Sigma;\r\n    real<lower=0> ySigma;\r\n    for ( j1 in 1:Nx1Lvl ) { for ( j2 in 1:Nx2Lvl ) {\r\n        m[j1,j2] = a0 + a1[j1] + a2[j2] + a1a2[j1,j2]; // cell means \r\n    } }\r\n    b0 = mean(m);\r\n    for ( j1 in 1:Nx1Lvl ) { b1[j1] = mean( m[j1,] ) - b0; }\r\n    for ( j2 in 1:Nx2Lvl ) { b2[j2] = mean( m[,j2] ) - b0; }\r\n    for ( j1 in 1:Nx1Lvl ) { for ( j2 in 1:Nx2Lvl ) {\r\n        b1b2[j1,j2] = m[j1,j2] - ( b0 + b1[j1] + b2[j2] );  \r\n    } }\r\n    // transform to original scale:\r\n    b0 = meanY + sdY * b0;\r\n    b1 = sdY * b1;\r\n    b2 = sdY * b2;\r\n    b1b2 = sdY * b1b2;\r\n    b1Sigma = sdY * a1Sigma;\r\n    b2Sigma = sdY * a2Sigma;\r\n    b1b2Sigma = sdY * a1a2Sigma;\r\n    ySigma = sdY * zySigma;\r\n}\"\r\n\r\n\r\n#Create DSO.\r\nstanDsoANOVA2Way <- stan_model(model_code=modelString)\r\n\r\n\r\n#If saved DSO is used load it, then run the chains.\r\n# saveRDS(stanDsoANOVA2Way,file=\"data/stanDso2WayANOVA.Rds\")\r\nstanDsoANOVA2Way <- readRDS(file=\"data/stanDso2WayANOVA.Rds\")\r\n\r\nData of section 20.1 in (Kruschke 2015)\r\nRead the data.\r\nThe data show faculty salaries of a university.\r\nThe columns selected as predictors are:\r\nPosition (Assistant Professor, Associate Professor, Full Professor,\r\nFull Professor with endowment salary and Distinguished Professor)\r\nDepartment (total 60 departments)\r\n\r\n# 20_1: Metric Predicted Variable with Two Nominal Predictors\r\n\r\n# load data from 'Salary.csv' (see Kruschke)\r\ndf <- read.csv(\"data/Salary.csv\")\r\nmean(df$Salary)\r\n[1] 110452\r\ntail(df)\r\n      Org                                  OrgName Cla Pos ClaPos\r\n1075 OADT Business Operations & Decision Technolog  PT FT3 PT.FT3\r\n1076 CSCI                         Computer Science  PT FT3 PT.FT3\r\n1077 PHYS                                  Physics  PT FT3 PT.FT3\r\n1078 GEOL                      Geological Sciences  PR FT1 PR.FT1\r\n1079   BI                                  Biology  PR FT1 PR.FT1\r\n1080 MUST           Music School-String Technology  PR FT1 PR.FT1\r\n     Salary\r\n1075 147400\r\n1076  92500\r\n1077  76748\r\n1078 105786\r\n1079 116592\r\n1080 138022\r\ndim(df)\r\n[1] 1080    6\r\nnames(df)\r\n[1] \"Org\"     \"OrgName\" \"Cla\"     \"Pos\"     \"ClaPos\"  \"Salary\" \r\nwith(df, table(Pos))\r\nPos\r\nDST FT1 FT2 FT3 NDW \r\n 32 360 364 240  84 \r\nwith(df, table(Org))\r\nOrg\r\nACTG AFRO AMST ANTH APHS  AST BEPP BFIN   BI BLAN CEDP CEUS CHEM CMCL \r\n  19    7    7   23   18    7    9   21   51    7   25   11   35   17 \r\nCMLT CRIN CSCI EALC ECON ELPS  ENG FINH FINS FOLK FRIT GEOG GEOL GERM \r\n   8   23   30   14   20   16   39    9   14   11   14    8   19    9 \r\nHIST INFO JOUR KINE LAWS LGED LING MATH MGMT MKTG MUHI MUIN MUST MUTH \r\n  35   20   14   15   37   11    8   40   12   13    9   38   14    8 \r\nMUVO OADT  OPT PHYS   PL POLS  PSY  REL RPAD SLIS  SLS  SOC SPAN SPEA \r\n  10   14   11   32   14   22   41   13   10   13    7   20   21   40 \r\nSPHS STAT TELC THTR \r\n  15    7   12   13 \r\nlength(table(df$Org))\r\n[1] 60\r\ny <- df$Salary\r\nx1 <- as.factor(df$Pos)\r\nx2 <- as.factor(df$Org)\r\nstr(df)\r\n'data.frame':   1080 obs. of  6 variables:\r\n $ Org    : chr  \"PL\" \"MUTH\" \"ENG\" \"CMLT\" ...\r\n $ OrgName: chr  \"Philosophy\" \"Music Theory\" \"English\" \"Comparative Literature\" ...\r\n $ Cla    : chr  \"PC\" \"PC\" \"PC\" \"PC\" ...\r\n $ Pos    : chr  \"FT2\" \"FT2\" \"FT2\" \"FT2\" ...\r\n $ ClaPos : chr  \"PC.FT2\" \"PC.FT2\" \"PC.FT2\" \"PC.FT2\" ...\r\n $ Salary : int  72395 61017 82370 68805 63796 219600 98814 107745 114275 173302 ...\r\ndataListSalary <- list(Ntotal=length(y),\r\n                       y=y,\r\n                       x1=as.integer(x1),\r\n                       x2=as.integer(x2),\r\n                       Nx1Lvl=nlevels(x1),\r\n                       Nx2Lvl=nlevels(x2),\r\n                       agammaShRa=unlist(gammaShRaFromModeSD(mode=1/2, sd=2)))\r\n\r\nCreate names of variables and their interactions\r\n\r\n(namesPos<-names(table(df$Pos)))\r\n[1] \"DST\" \"FT1\" \"FT2\" \"FT3\" \"NDW\"\r\n(namesOrg<-names(table(df$Org)))\r\n [1] \"ACTG\" \"AFRO\" \"AMST\" \"ANTH\" \"APHS\" \"AST\"  \"BEPP\" \"BFIN\" \"BI\"  \r\n[10] \"BLAN\" \"CEDP\" \"CEUS\" \"CHEM\" \"CMCL\" \"CMLT\" \"CRIN\" \"CSCI\" \"EALC\"\r\n[19] \"ECON\" \"ELPS\" \"ENG\"  \"FINH\" \"FINS\" \"FOLK\" \"FRIT\" \"GEOG\" \"GEOL\"\r\n[28] \"GERM\" \"HIST\" \"INFO\" \"JOUR\" \"KINE\" \"LAWS\" \"LGED\" \"LING\" \"MATH\"\r\n[37] \"MGMT\" \"MKTG\" \"MUHI\" \"MUIN\" \"MUST\" \"MUTH\" \"MUVO\" \"OADT\" \"OPT\" \r\n[46] \"PHYS\" \"PL\"   \"POLS\" \"PSY\"  \"REL\"  \"RPAD\" \"SLIS\" \"SLS\"  \"SOC\" \r\n[55] \"SPAN\" \"SPEA\" \"SPHS\" \"STAT\" \"TELC\" \"THTR\"\r\n\r\nInteractions names:\r\n\r\n#use outer() to create names for interactions.\r\nas.vector(outer(1:4,1:2,paste,sep=\"-\"))\r\n[1] \"1-1\" \"2-1\" \"3-1\" \"4-1\" \"1-2\" \"2-2\" \"3-2\" \"4-2\"\r\n#apply to our case\r\n(namesInter<-as.vector(outer(namesOrg,namesPos,paste,sep=\"-\")))\r\n  [1] \"ACTG-DST\" \"AFRO-DST\" \"AMST-DST\" \"ANTH-DST\" \"APHS-DST\"\r\n  [6] \"AST-DST\"  \"BEPP-DST\" \"BFIN-DST\" \"BI-DST\"   \"BLAN-DST\"\r\n [11] \"CEDP-DST\" \"CEUS-DST\" \"CHEM-DST\" \"CMCL-DST\" \"CMLT-DST\"\r\n [16] \"CRIN-DST\" \"CSCI-DST\" \"EALC-DST\" \"ECON-DST\" \"ELPS-DST\"\r\n [21] \"ENG-DST\"  \"FINH-DST\" \"FINS-DST\" \"FOLK-DST\" \"FRIT-DST\"\r\n [26] \"GEOG-DST\" \"GEOL-DST\" \"GERM-DST\" \"HIST-DST\" \"INFO-DST\"\r\n [31] \"JOUR-DST\" \"KINE-DST\" \"LAWS-DST\" \"LGED-DST\" \"LING-DST\"\r\n [36] \"MATH-DST\" \"MGMT-DST\" \"MKTG-DST\" \"MUHI-DST\" \"MUIN-DST\"\r\n [41] \"MUST-DST\" \"MUTH-DST\" \"MUVO-DST\" \"OADT-DST\" \"OPT-DST\" \r\n [46] \"PHYS-DST\" \"PL-DST\"   \"POLS-DST\" \"PSY-DST\"  \"REL-DST\" \r\n [51] \"RPAD-DST\" \"SLIS-DST\" \"SLS-DST\"  \"SOC-DST\"  \"SPAN-DST\"\r\n [56] \"SPEA-DST\" \"SPHS-DST\" \"STAT-DST\" \"TELC-DST\" \"THTR-DST\"\r\n [61] \"ACTG-FT1\" \"AFRO-FT1\" \"AMST-FT1\" \"ANTH-FT1\" \"APHS-FT1\"\r\n [66] \"AST-FT1\"  \"BEPP-FT1\" \"BFIN-FT1\" \"BI-FT1\"   \"BLAN-FT1\"\r\n [71] \"CEDP-FT1\" \"CEUS-FT1\" \"CHEM-FT1\" \"CMCL-FT1\" \"CMLT-FT1\"\r\n [76] \"CRIN-FT1\" \"CSCI-FT1\" \"EALC-FT1\" \"ECON-FT1\" \"ELPS-FT1\"\r\n [81] \"ENG-FT1\"  \"FINH-FT1\" \"FINS-FT1\" \"FOLK-FT1\" \"FRIT-FT1\"\r\n [86] \"GEOG-FT1\" \"GEOL-FT1\" \"GERM-FT1\" \"HIST-FT1\" \"INFO-FT1\"\r\n [91] \"JOUR-FT1\" \"KINE-FT1\" \"LAWS-FT1\" \"LGED-FT1\" \"LING-FT1\"\r\n [96] \"MATH-FT1\" \"MGMT-FT1\" \"MKTG-FT1\" \"MUHI-FT1\" \"MUIN-FT1\"\r\n[101] \"MUST-FT1\" \"MUTH-FT1\" \"MUVO-FT1\" \"OADT-FT1\" \"OPT-FT1\" \r\n[106] \"PHYS-FT1\" \"PL-FT1\"   \"POLS-FT1\" \"PSY-FT1\"  \"REL-FT1\" \r\n[111] \"RPAD-FT1\" \"SLIS-FT1\" \"SLS-FT1\"  \"SOC-FT1\"  \"SPAN-FT1\"\r\n[116] \"SPEA-FT1\" \"SPHS-FT1\" \"STAT-FT1\" \"TELC-FT1\" \"THTR-FT1\"\r\n[121] \"ACTG-FT2\" \"AFRO-FT2\" \"AMST-FT2\" \"ANTH-FT2\" \"APHS-FT2\"\r\n[126] \"AST-FT2\"  \"BEPP-FT2\" \"BFIN-FT2\" \"BI-FT2\"   \"BLAN-FT2\"\r\n[131] \"CEDP-FT2\" \"CEUS-FT2\" \"CHEM-FT2\" \"CMCL-FT2\" \"CMLT-FT2\"\r\n[136] \"CRIN-FT2\" \"CSCI-FT2\" \"EALC-FT2\" \"ECON-FT2\" \"ELPS-FT2\"\r\n[141] \"ENG-FT2\"  \"FINH-FT2\" \"FINS-FT2\" \"FOLK-FT2\" \"FRIT-FT2\"\r\n[146] \"GEOG-FT2\" \"GEOL-FT2\" \"GERM-FT2\" \"HIST-FT2\" \"INFO-FT2\"\r\n[151] \"JOUR-FT2\" \"KINE-FT2\" \"LAWS-FT2\" \"LGED-FT2\" \"LING-FT2\"\r\n[156] \"MATH-FT2\" \"MGMT-FT2\" \"MKTG-FT2\" \"MUHI-FT2\" \"MUIN-FT2\"\r\n[161] \"MUST-FT2\" \"MUTH-FT2\" \"MUVO-FT2\" \"OADT-FT2\" \"OPT-FT2\" \r\n[166] \"PHYS-FT2\" \"PL-FT2\"   \"POLS-FT2\" \"PSY-FT2\"  \"REL-FT2\" \r\n[171] \"RPAD-FT2\" \"SLIS-FT2\" \"SLS-FT2\"  \"SOC-FT2\"  \"SPAN-FT2\"\r\n[176] \"SPEA-FT2\" \"SPHS-FT2\" \"STAT-FT2\" \"TELC-FT2\" \"THTR-FT2\"\r\n[181] \"ACTG-FT3\" \"AFRO-FT3\" \"AMST-FT3\" \"ANTH-FT3\" \"APHS-FT3\"\r\n[186] \"AST-FT3\"  \"BEPP-FT3\" \"BFIN-FT3\" \"BI-FT3\"   \"BLAN-FT3\"\r\n[191] \"CEDP-FT3\" \"CEUS-FT3\" \"CHEM-FT3\" \"CMCL-FT3\" \"CMLT-FT3\"\r\n[196] \"CRIN-FT3\" \"CSCI-FT3\" \"EALC-FT3\" \"ECON-FT3\" \"ELPS-FT3\"\r\n[201] \"ENG-FT3\"  \"FINH-FT3\" \"FINS-FT3\" \"FOLK-FT3\" \"FRIT-FT3\"\r\n[206] \"GEOG-FT3\" \"GEOL-FT3\" \"GERM-FT3\" \"HIST-FT3\" \"INFO-FT3\"\r\n[211] \"JOUR-FT3\" \"KINE-FT3\" \"LAWS-FT3\" \"LGED-FT3\" \"LING-FT3\"\r\n[216] \"MATH-FT3\" \"MGMT-FT3\" \"MKTG-FT3\" \"MUHI-FT3\" \"MUIN-FT3\"\r\n[221] \"MUST-FT3\" \"MUTH-FT3\" \"MUVO-FT3\" \"OADT-FT3\" \"OPT-FT3\" \r\n[226] \"PHYS-FT3\" \"PL-FT3\"   \"POLS-FT3\" \"PSY-FT3\"  \"REL-FT3\" \r\n[231] \"RPAD-FT3\" \"SLIS-FT3\" \"SLS-FT3\"  \"SOC-FT3\"  \"SPAN-FT3\"\r\n[236] \"SPEA-FT3\" \"SPHS-FT3\" \"STAT-FT3\" \"TELC-FT3\" \"THTR-FT3\"\r\n[241] \"ACTG-NDW\" \"AFRO-NDW\" \"AMST-NDW\" \"ANTH-NDW\" \"APHS-NDW\"\r\n[246] \"AST-NDW\"  \"BEPP-NDW\" \"BFIN-NDW\" \"BI-NDW\"   \"BLAN-NDW\"\r\n[251] \"CEDP-NDW\" \"CEUS-NDW\" \"CHEM-NDW\" \"CMCL-NDW\" \"CMLT-NDW\"\r\n[256] \"CRIN-NDW\" \"CSCI-NDW\" \"EALC-NDW\" \"ECON-NDW\" \"ELPS-NDW\"\r\n[261] \"ENG-NDW\"  \"FINH-NDW\" \"FINS-NDW\" \"FOLK-NDW\" \"FRIT-NDW\"\r\n[266] \"GEOG-NDW\" \"GEOL-NDW\" \"GERM-NDW\" \"HIST-NDW\" \"INFO-NDW\"\r\n[271] \"JOUR-NDW\" \"KINE-NDW\" \"LAWS-NDW\" \"LGED-NDW\" \"LING-NDW\"\r\n[276] \"MATH-NDW\" \"MGMT-NDW\" \"MKTG-NDW\" \"MUHI-NDW\" \"MUIN-NDW\"\r\n[281] \"MUST-NDW\" \"MUTH-NDW\" \"MUVO-NDW\" \"OADT-NDW\" \"OPT-NDW\" \r\n[286] \"PHYS-NDW\" \"PL-NDW\"   \"POLS-NDW\" \"PSY-NDW\"  \"REL-NDW\" \r\n[291] \"RPAD-NDW\" \"SLIS-NDW\" \"SLS-NDW\"  \"SOC-NDW\"  \"SPAN-NDW\"\r\n[296] \"SPEA-NDW\" \"SPHS-NDW\" \"STAT-NDW\" \"TELC-NDW\" \"THTR-NDW\"\r\n#all names:\r\nvarNames<-c(\"Intercept\", namesPos, namesOrg, namesInter, rep(\"Var\",5)) #why need to rep Var 5 times\r\n\r\nMCMC\r\n\r\n# fit model\r\nfit <- sampling(stanDsoANOVA2Way, \r\n                data=dataListSalary, \r\n                pars=c('b0',\r\n                       'b1', \r\n                       'b2', \r\n                       'b1b2',\r\n                       'b1Sigma', \r\n                       'b2Sigma',\r\n                       'b1b2Sigma',\r\n                       'ySigma'),\r\n                iter=5000, chains = 2, cores = 4)\r\n\r\n\r\n# save(fit, file = \"data/fitinstan1.Rdata\")\r\nload(\"data/fitinstan1.Rdata\")\r\n\r\n\r\nlaunch_shinystan(fit)\r\n\r\nCreate results including mean value, 2.5%, 50% and 97.5% quantiles.\r\nAdd variable names as row names.\r\n\r\nSalaryResults <- summary(fit)$summary[,c(1,4,6,8)]\r\nvarNames[nrow(SalaryResults)-(4:0)] <- rownames(SalaryResults)[nrow(SalaryResults)-(4:0)]\r\nrownames(SalaryResults) <- varNames\r\nhead(SalaryResults)\r\n                mean      2.5%        50%       97.5%\r\nIntercept 127109.022 124810.45 127121.025 129405.5751\r\nDST        55628.429  48475.65  55680.359  62649.8065\r\nFT1        -3149.964  -6004.94  -3183.797   -266.4279\r\nFT2       -33076.540 -35809.15 -33073.658 -30362.7908\r\nFT3       -46377.954 -49426.70 -46407.605 -43254.6686\r\nNDW        26976.029  22363.91  26959.878  31589.5023\r\n\r\nMake plots of mean values and HDIs.\r\n\r\nplot(fit,pars=c(\"b1\"))\r\n\r\nplot(fit,pars=c('b2'))\r\n\r\nplot(fit,pars=c(\"b1b2\"))\r\n\r\n\r\nPlots show that not all coefficients of the model equal zero. This\r\nanswers the question of utility test.\r\nWorking with chains and\r\ncontrasts\r\nExtract chains for the position variables.\r\n\r\nfit_ext <- rstan::extract(fit)\r\nnames(fit_ext)\r\n[1] \"b0\"        \"b1\"        \"b2\"        \"b1b2\"      \"b1Sigma\"  \r\n[6] \"b2Sigma\"   \"b1b2Sigma\" \"ySigma\"    \"lp__\"     \r\nfit_ext.b0<-fit_ext$b0\r\nfit_ext.b1<-fit_ext$b1\r\ncolnames(fit_ext.b1) <- namesPos\r\nhead(fit_ext.b1)\r\n          \r\niterations      DST       FT1       FT2       FT3      NDW\r\n      [1,] 55889.00 -2545.621 -32436.47 -45754.71 24847.80\r\n      [2,] 52218.80 -1291.582 -31308.32 -47600.26 27981.36\r\n      [3,] 55355.28 -2949.890 -32832.24 -48218.55 28645.39\r\n      [4,] 51433.99 -2765.288 -33308.70 -45642.99 30282.99\r\n      [5,] 61663.83 -5375.056 -33912.46 -47640.01 25263.69\r\n      [6,] 57716.42 -5850.982 -33587.85 -47903.39 29625.81\r\n\r\nExtract chains for the department variables.\r\n\r\nfit_ext.b2 <- fit_ext$b2\r\ncolnames(fit_ext.b2) <- namesOrg\r\nhead(fit_ext.b2)\r\n          \r\niterations     ACTG       AFRO       AMST      ANTH       APHS\r\n      [1,] 79943.14 -25879.637 -11222.498 -20352.95  -849.5822\r\n      [2,] 76080.15  -8138.556  -8954.764 -16600.86  7614.9022\r\n      [3,] 81468.05 -19605.644 -36019.249 -26707.29 -7308.5706\r\n      [4,] 88637.85 -19828.992 -13077.109 -15112.53 -5631.3437\r\n      [5,] 81424.36 -24718.757 -20790.370 -10446.33  9391.8464\r\n      [6,] 76862.94 -25473.262 -29516.959 -22377.00 -1396.5355\r\n          \r\niterations        AST     BEPP     BFIN         BI     BLAN      CEDP\r\n      [1,]  -111.0909 46002.00 106363.6  2158.4666 25751.61 -23633.85\r\n      [2,] -1489.9559 52279.55 115186.7   706.1020 15576.34 -20342.80\r\n      [3,] -6756.5728 42638.74 114828.7  -336.1857 21347.19 -12464.72\r\n      [4,]  -691.7461 47705.99 116199.9 -3387.2859 18939.08 -21129.49\r\n      [5,] -3884.2082 43883.36 102112.5 -5401.7509 23386.47 -13576.59\r\n      [6,]  9593.8691 56263.41 105435.6 -4580.9050 25136.37 -11130.68\r\n          \r\niterations      CEUS     CHEM      CMCL      CMLT      CRIN      CSCI\r\n      [1,] -31331.28 24795.45 -23382.07 -19971.23 -23668.60  7339.508\r\n      [2,] -26425.10 21304.96 -17850.53 -26247.63 -28435.82 13627.947\r\n      [3,] -16455.06 21199.79 -16383.86 -22590.31 -21606.54 12685.703\r\n      [4,] -19326.49 22096.80 -12893.23 -22814.69 -24052.79  8260.378\r\n      [5,] -19228.84 18722.48 -10776.11 -24544.06 -21415.45 15065.380\r\n      [6,] -16171.89 16461.72 -13605.67 -25592.45 -20051.80 17071.769\r\n          \r\niterations       EALC     ECON       ELPS       ENG       FINH\r\n      [1,]  -3787.812 57360.22 -16191.288 -15112.75 -24247.426\r\n      [2,] -25676.851 57589.34 -15671.106 -14747.60 -10753.914\r\n      [3,] -17638.158 49484.27  -1848.665 -23500.73 -27255.609\r\n      [4,] -25706.941 61869.13   3841.717 -16652.11   1377.585\r\n      [5,] -10615.279 60537.78  -4153.230 -21519.64 -18326.635\r\n      [6,] -20680.110 62151.29 -11615.176 -19390.36 -24159.456\r\n          \r\niterations       FINS      FOLK       FRIT        GEOG       GEOL\r\n      [1,] -18387.511 -28684.92 -11890.942 -7853.91867 -17399.626\r\n      [2,]  -7522.695 -20023.54 -22264.782   716.66268 -18534.101\r\n      [3,] -21722.023 -19341.44 -12241.881  7317.49467 -14328.742\r\n      [4,] -20850.540 -25557.29  -1629.553 -6443.08509 -21648.871\r\n      [5,] -18246.232 -27727.19  -2856.445    30.49148 -22427.052\r\n      [6,] -17214.883 -17585.87  -5709.320 -4897.56231  -8353.044\r\n          \r\niterations      GERM       HIST      INFO       JOUR       KINE\r\n      [1,] -39652.59 -10401.833  8648.805  -9788.303 -19147.745\r\n      [2,] -43367.13 -13670.380 13380.505 -16611.153  -9411.267\r\n      [3,] -33104.33  -5039.539  1102.208 -14645.829 -12508.362\r\n      [4,] -34731.75 -18002.584  1981.710 -10038.436 -14569.498\r\n      [5,] -38823.95  -5365.481  8824.972 -17303.011  -5417.512\r\n      [6,] -33617.63 -17595.166 11016.335 -10212.507   3251.580\r\n          \r\niterations     LAWS      LGED       LING       MATH     MGMT     MKTG\r\n      [1,] 30980.84 -19361.19 -12340.158 -6298.2286 58205.92 67811.17\r\n      [2,] 38196.64 -20110.28  -6473.704 -6338.8930 70805.55 70026.03\r\n      [3,] 38149.22 -18272.85  -9298.740   896.6368 67786.97 76289.45\r\n      [4,] 33725.69 -14370.04  -7059.491 -1930.6726 63322.70 64262.54\r\n      [5,] 40670.12 -24596.34 -21288.533 -3160.6917 63983.91 65816.62\r\n      [6,] 41209.28 -28417.38 -11572.264 -4291.1453 60815.29 75363.07\r\n          \r\niterations      MUHI      MUIN       MUST      MUTH      MUVO\r\n      [1,] -25393.19 -19257.61   5283.030 -21443.45 -21624.55\r\n      [2,] -28922.27 -26294.89  -2549.769 -32030.69 -37341.37\r\n      [3,] -44544.15 -25439.84   2995.413 -33936.56 -44836.06\r\n      [4,] -28982.82 -18540.86  -9002.239 -36269.65 -29226.47\r\n      [5,] -38929.81 -30073.70   2979.425 -34611.79 -16596.47\r\n      [6,] -34299.47 -26695.17 -11828.485 -27950.95 -40543.21\r\n          \r\niterations     OADT       OPT        PHYS         PL       POLS\r\n      [1,] 63681.19 -11354.93  -8874.2707  -6081.606   981.0707\r\n      [2,] 69603.41 -18102.97 -12213.3160 -12478.264  7634.5155\r\n      [3,] 70187.39 -22018.82    389.8068  -5042.052 11461.3939\r\n      [4,] 56608.33 -14985.36   1612.2422 -14106.521  6006.6247\r\n      [5,] 70622.84 -18764.18  -6001.6030  -5705.518  6683.9357\r\n      [6,] 53978.05 -12087.79  -5642.8848  -8087.877  7351.4053\r\n          \r\niterations      PSY        REL        RPAD      SLIS       SLS\r\n      [1,] 9637.909  -8355.072  -6655.3719 18053.849 -26494.91\r\n      [2,] 6301.373 -23607.650  -6200.1939 15323.967 -36799.31\r\n      [3,] 7291.383 -12384.840  -3677.8843 13463.402 -21244.41\r\n      [4,] 1844.440 -17085.096  -8354.0570  3242.021 -23751.68\r\n      [5,] 3063.510 -15165.778   -538.9977 10030.821 -24981.15\r\n      [6,] 9196.656 -13934.034 -13579.8620 -7302.095 -15579.96\r\n          \r\niterations        SOC      SPAN      SPEA      SPHS      STAT\r\n      [1,] -10668.593 -14095.58 21704.249  8757.033  1313.720\r\n      [2,] -13559.676 -11884.70 17092.785 12827.040  8087.050\r\n      [3,]  -1504.499 -13506.99 14739.827  6685.327  3822.699\r\n      [4,]  -1181.861 -19952.18 21490.894  7745.884 -2271.457\r\n      [5,]  -8235.645 -11685.73  9840.211  3882.267  3737.229\r\n      [6,]  -8070.830 -17617.54 20580.435  6534.513 11629.246\r\n          \r\niterations       TELC       THTR\r\n      [1,] -12559.968 -10964.621\r\n      [2,] -10246.183 -12066.897\r\n      [3,]  -9223.986 -11890.115\r\n      [4,] -15637.948 -14286.743\r\n      [5,] -11236.253 -25554.188\r\n      [6,] -14977.458  -6496.113\r\n\r\nExtract chains for interaction variables.\r\n\r\nfit_ext.b1.b2<-fit_ext$b1b2\r\ndim(fit_ext.b1.b2)\r\n[1] 5000    5   60\r\n\r\nInteraction chains make a cube.\r\n\r\ndimnames(fit_ext.b1.b2)[[2]]<-namesPos\r\ndimnames(fit_ext.b1.b2)[[3]]<-namesOrg\r\ndimnames(fit_ext.b1.b2)\r\n$iterations\r\nNULL\r\n\r\n[[2]]\r\n[1] \"DST\" \"FT1\" \"FT2\" \"FT3\" \"NDW\"\r\n\r\n[[3]]\r\n [1] \"ACTG\" \"AFRO\" \"AMST\" \"ANTH\" \"APHS\" \"AST\"  \"BEPP\" \"BFIN\" \"BI\"  \r\n[10] \"BLAN\" \"CEDP\" \"CEUS\" \"CHEM\" \"CMCL\" \"CMLT\" \"CRIN\" \"CSCI\" \"EALC\"\r\n[19] \"ECON\" \"ELPS\" \"ENG\"  \"FINH\" \"FINS\" \"FOLK\" \"FRIT\" \"GEOG\" \"GEOL\"\r\n[28] \"GERM\" \"HIST\" \"INFO\" \"JOUR\" \"KINE\" \"LAWS\" \"LGED\" \"LING\" \"MATH\"\r\n[37] \"MGMT\" \"MKTG\" \"MUHI\" \"MUIN\" \"MUST\" \"MUTH\" \"MUVO\" \"OADT\" \"OPT\" \r\n[46] \"PHYS\" \"PL\"   \"POLS\" \"PSY\"  \"REL\"  \"RPAD\" \"SLIS\" \"SLS\"  \"SOC\" \r\n[55] \"SPAN\" \"SPEA\" \"SPHS\" \"STAT\" \"TELC\" \"THTR\"\r\nfit_ext.b1.b2[1,,]\r\n     \r\n            ACTG       AFRO       AMST       ANTH       APHS\r\n  DST  -7584.115 -4049.2655 -2996.4522  5239.5295  10431.507\r\n  FT1 -15213.626  -844.8355  2573.3882   969.4782  11721.279\r\n  FT2  16942.365  6542.3560  -420.1959   227.7032 -20652.244\r\n  FT3  11818.817  8231.7049 -3270.2672 -2593.3647  -6066.473\r\n  NDW  -5963.441 -9879.9599  4113.5270 -3843.3462   4565.931\r\n     \r\n             AST      BEPP       BFIN        BI       BLAN      CEDP\r\n  DST  -3640.321  5200.292 -1537.9235  8568.545   7159.254 -5326.195\r\n  FT1 -14398.986 -9086.486  -259.0784  1283.842  -2363.879 -3843.261\r\n  FT2   2228.547  4659.597 10493.5599 -6856.983   2982.987 -2917.414\r\n  FT3   2308.513 -4667.465 -1920.9022 -5428.479   4722.395  4592.881\r\n  NDW  13502.247  3894.062 -6775.6559  2433.075 -12500.757  7493.988\r\n     \r\n            CEUS      CHEM      CMCL      CMLT         CRIN\r\n  DST -19190.123   5084.46 -6528.697 10384.960  10747.84139\r\n  FT1   3165.468  11719.98 15178.084 -6128.083  -4472.00704\r\n  FT2  26765.122 -14268.17  -804.769  2082.940     22.71676\r\n  FT3   3829.992 -13999.01  1663.957 -2352.980   4044.74726\r\n  NDW -14570.459  11462.74 -9508.575 -3986.836 -10343.29837\r\n     \r\n              CSCI      EALC        ECON       ELPS       ENG\r\n  DST  -3980.13416 -5372.913  -7501.9102 -1381.0623 -1163.043\r\n  FT1     66.94373 -9337.181   -416.1579  6519.6693  1863.654\r\n  FT2   7384.25618 -8752.227   4771.3023   697.8683  2691.566\r\n  FT3   8233.22390 -2451.378 -11073.2103 -1184.3765  4201.826\r\n  NDW -11704.28964 25913.699  14219.9761 -4652.0987 -7594.003\r\n     \r\n            FINH       FINS      FOLK       FRIT       GEOG\r\n  DST  -4992.985 -2690.6237  9230.347 -12617.275  8182.4028\r\n  FT1 -14827.962   472.9582 -3058.817   1348.381  9249.1692\r\n  FT2  12466.768   661.3206  8680.618 -16082.146 -6788.9418\r\n  FT3  -2565.441  2004.2694 -8005.837   9902.733  -742.6674\r\n  NDW   9919.620  -447.9245 -6846.311  17448.307 -9899.9628\r\n     \r\n            GEOL         GERM       HIST       INFO      JOUR\r\n  DST  -8516.774   9510.02572  6382.9809   2387.563 -3808.785\r\n  FT1  12394.500   7468.24834 -4646.6719   4354.928  8173.818\r\n  FT2   6572.406    401.15195  4007.6825 -10915.638 -6388.942\r\n  FT3  15299.843 -17350.50578  -377.3762   2993.020  3836.820\r\n  NDW -25749.976    -28.92023 -5366.6152   1180.128 -1812.912\r\n     \r\n            KINE       LAWS       LGED       LING        MATH\r\n  DST -17188.854  -8918.642  1249.1268 -10607.340  -8000.0226\r\n  FT1  15089.554   1963.490  -483.3230  -2729.075 -13327.6839\r\n  FT2   6651.482  17622.387 -3542.6007   3988.323   2060.3902\r\n  FT3  14635.773 -18932.957  -399.1769   2746.228    500.9943\r\n  NDW -19187.955   8265.722  3175.9738   6601.865  18766.3221\r\n     \r\n            MGMT        MKTG        MUHI       MUIN       MUST\r\n  DST  -6807.337  10012.7838 -10639.9268  1619.4404  5083.8184\r\n  FT1   2560.836 -15533.2774    -44.2382 -3712.3535 -2175.4010\r\n  FT2  10058.431   2405.6487  13264.4034 10223.7482  5844.0922\r\n  FT3 -10634.705   2396.7818  -6648.0248   620.4336  -244.0252\r\n  NDW   4822.775    718.0632   4067.7864 -8751.2686 -8508.4843\r\n     \r\n            MUTH       MUVO        OADT       OPT       PHYS\r\n  DST   4921.569 -18372.605   4361.7928  6556.251  -6020.450\r\n  FT1 -12458.118 -10812.744 -16472.3301 -2793.282   9570.858\r\n  FT2 -11260.520   8316.420  11481.3988 -2895.080  -1981.303\r\n  FT3   5423.393   1478.046   1013.7407  8841.246  15282.187\r\n  NDW  13373.676  19390.883   -384.6023 -9709.135 -16851.292\r\n     \r\n              PL       POLS        PSY       REL       RPAD\r\n  DST   1402.966  -1846.650   3813.639  3179.156  10580.993\r\n  FT1   2540.147   7969.566 -22460.016 13576.815   8111.708\r\n  FT2 -12241.977  -6844.831  -9032.855 -5792.517  -1910.718\r\n  FT3   1491.660 -11960.880   6577.039 -1588.535  -3731.482\r\n  NDW   6807.204  12682.795  21102.192 -9374.919 -13050.501\r\n     \r\n             SLIS       SLS          SOC      SPAN       SPEA\r\n  DST   7522.0663 -6414.986     47.31315  7504.810  29021.410\r\n  FT1  11807.8674 13962.615   -145.22853 -2329.022   5540.731\r\n  FT2  -5973.7208 -6624.259 -10857.78577 -7487.790 -21385.492\r\n  FT3 -12950.4652  8088.994   5935.49642 -6378.261  -1531.480\r\n  NDW   -405.7477 -9012.363   5020.20474  8690.263 -11645.169\r\n     \r\n              SPHS      STAT       TELC       THTR\r\n  DST 10538.036785 -4137.708 -2418.3351  -1673.425\r\n  FT1 -5464.604642 -7410.671  -701.8165  16732.240\r\n  FT2     9.459041 10190.669 -7733.5551 -12987.013\r\n  FT3  1940.953263 -5516.859  6761.3798  -6852.506\r\n  NDW -7023.844447  6874.569  4092.3268   4780.704\r\n\r\nRows correspond to iterations of MCMC.\r\nFor each iteration there is interactions table between schools and\r\npositions.\r\nSalary of a school across positions is base level beta, plus school\r\nbeta: \\(\\beta_0+\\beta_{school}\\).\r\nSalary of a position across schools is base level beta, plus position\r\nbeta: \\(\\beta_0+\\beta_{position}\\).\r\nSalary of a certain position of a certain school is predicted as base\r\nlevel beta, plus school beta, plus position beta, plus the interaction\r\nbeta: \\(\\beta_0+\\beta_{school}+\\beta_{position}+\\beta_{school\r\n\\ \\times \\ position}\\).\r\nTo answer each of the following 4 questions do the\r\nfollowing:\r\nCreate contrast for comparison\r\nMake histogram of the contrast\r\nCalculate mean of the contrast\r\nCalculate 95%-HDI for the contrast. To find HDI use\r\nhdi() from library HDInterval\r\nContrast for comparison\r\nof departments\r\nUse contrasts to compare salaries at Business and Finance (“BFIN”)\r\nwith Physics (“PHYS”) and with Chemistry (“CHEM”) departments.\r\nTo do that select columns of MCMC for departments to “BFIN” and\r\n“PHYS” and “BFIN” and “CHEM”, take their differences and look at the\r\nposterior distribution of the differences\r\n\r\ncontrast.BFIN.PHYS <- fit_ext.b2[,\"BFIN\"]-fit_ext.b2[,\"PHYS\"]\r\nhist(contrast.BFIN.PHYS)\r\n\r\nmean(contrast.BFIN.PHYS)\r\n[1] 111960.4\r\nhdi(contrast.BFIN.PHYS)\r\n    lower     upper \r\n 99848.48 125661.30 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nWhat do we conclude about the differences of\r\nsalaries?\r\nPlot histograms of salaries for both departments. Salaries\r\nBFINComp of Business and Finance and PHYSComp\r\nof Physics are calculate as described above.\r\n\r\nBFINComp <- fit_ext.b2[,\"BFIN\"]\r\nPHYSComp <- fit_ext.b2[,\"PHYS\"]\r\n\r\n\r\ndistrBFIN <- density(BFINComp)\r\ndistrPHYS <- density(PHYSComp)\r\n\r\nplot(distrPHYS,xlim=c(-20000,200000), lwd=2, main=\"Salaries Distributions\", col=\"blue\")\r\nlines(distrBFIN$x, distrPHYS$y, lwd=2, col=\"orange\")\r\nlegend(\"top\", legend = c(\"PHYS\",\"BFIN\"), col = c(\"blue\",\"orange\"), lty=1, lwd=2)\r\n\r\n\r\nDo the same comparison for finance professors and chemistry\r\nprofessors.\r\n\r\ncontrast.BFIN.CHEM<-fit_ext.b2[,\"BFIN\"]-fit_ext.b2[,\"CHEM\"]\r\nhist(contrast.BFIN.CHEM)\r\n\r\nmean(contrast.BFIN.CHEM)\r\n[1] 90074.79\r\nhdi(contrast.BFIN.CHEM)\r\n    lower     upper \r\n 78388.99 101324.94 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n\r\nBFINComp <- fit_ext.b2[,\"BFIN\"]\r\nCHEMComp <- fit_ext.b2[,\"CHEM\"]\r\n\r\n\r\ndistrBFIN <- density(BFINComp)\r\ndistrPHYS <- density(PHYSComp)\r\n\r\nplot(distrPHYS,xlim=c(-20000,200000), lwd=2, main=\"Salaries Distributions\", col=\"blue\")\r\nlines(distrBFIN$x, distrPHYS$y, lwd=2, col=\"orange\")\r\nlegend(\"top\", legend = c(\"PHYS\",\"BFIN\"), col = c(\"blue\",\"orange\"), lty=1, lwd=2)\r\n\r\n\r\nContrast for comparison of\r\npositions\r\nUse contrasts to compare salaries of Endowment Full Professor (“NDW”)\r\nand Distinguished Full Professor (“DST”). Compare salaries of Full\r\nProfessor (“FT1”) and Endowment Full Professor\r\n\r\ncontrast.NDW.DST <- fit_ext.b1[,\"NDW\"] - fit_ext.b1[,\"DST\"]\r\nhist(contrast.NDW.DST)\r\n\r\nmean(contrast.NDW.DST)\r\n[1] -28652.4\r\nhdi(contrast.NDW.DST)\r\n    lower     upper \r\n-38788.91 -18566.57 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\nDSTComp <- fit_ext.b1[,\"NDW\"]\r\nNDWComp <- fit_ext.b1[,\"DST\"]\r\n\r\ndistrDST<-density(DSTComp)\r\ndistrNDW<-density(NDWComp)\r\nplot(distrDST,xlim=c(14000,80000),lwd=2,main=\"Salaries Distributions\",col=\"blue\")\r\nlines(distrNDW$x,distrPHYS$y,lwd=2,col=\"orange\")\r\nlegend(\"top\",legend=c(\"DST\",\"NDW\"),col=c(\"blue\",\"orange\"),lty=1,lwd=2)\r\n\r\n\r\nSalary of a Distinguished Full Professor is significantly higher than\r\nsalary of Endowment Full Professor.\r\nAnalyze difference between salaries of Full Professor (“FT1”) and\r\nEndowment Full Professor\r\n\r\ncontrast.NDW.FT1<-fit_ext.b1[,\"NDW\"]-fit_ext.b1[,\"FT1\"]\r\nhist(contrast.NDW.FT1)\r\n\r\nmean(contrast.NDW.FT1)\r\n[1] 30125.99\r\nhdi(contrast.NDW.FT1)\r\n   lower    upper \r\n24192.40 35818.67 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nContrast for comparison of\r\nspreads\r\nUse contrasts to compare salaries spreads between Full Professor and\r\nAssistant Professor at Physics Department and at Chemistry\r\nDepartment.\r\n\r\nspreadPhys<-fit_ext.b1[,\"FT1\"]+fit_ext.b1.b2[,\"FT1\",\"PHYS\"]-\r\n  (fit_ext.b1[,\"FT3\"]+fit_ext.b1.b2[,\"FT3\",\"PHYS\"])\r\nspreadChem<-fit_ext.b1[,\"FT1\"]+fit_ext.b1.b2[,\"FT1\",\"CHEM\"]-\r\n  (fit_ext.b1[,\"FT3\"]+fit_ext.b1.b2[,\"FT3\",\"CHEM\"])\r\nspread<-spreadPhys-spreadChem\r\nhist(spread)\r\n\r\nmean(spread)\r\n[1] -15128.45\r\nhdi(spread)\r\n     lower      upper \r\n-35123.668   3520.849 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nFind the highest HDI level for which the spread of differences\r\nbetween “FT1” and “FT3” is significant.\r\n\r\nhdi(spread,.88)\r\n      lower       upper \r\n-31490.1393   -984.7276 \r\nattr(,\"credMass\")\r\n[1] 0.88\r\ntryLevels<-seq(from=.8,to=.95,by=.01)\r\nhdis<-sapply(tryLevels,function(z) hdi(spread,z))\r\nmaxLevel<-tail(tryLevels[hdis[2,]<0],1)\r\nhdi(spread,maxLevel)\r\n       lower        upper \r\n-32308.00023    -49.20156 \r\nattr(,\"credMass\")\r\n[1] 0.9\r\n\r\nUnderstanding\r\nthe effect of scaling and transformations on interactions\r\nNonlinear transformations may affect interactions very\r\nsignificantly.\r\nIllustrate it on a simple simulated example.\r\n\r\nmean00<-1\r\nmean10<-3\r\nmean01<-4\r\nmean11<-6\r\ny00<-rnorm(5,mean00,.1)\r\ny10<-rnorm(5,mean10,.1)\r\ny01<-rnorm(5,mean01,.1)\r\ny11<-rnorm(5,mean11,.1)\r\n\r\nPlot the effects. If the lines are parallel the effects are\r\nadditive.\r\n\r\nplot(c(0,1),c(mean(y00),mean(y10)),type=\"b\",ylim=c(1,8),col=\"darkgreen\",lwd=3,ylab=\"Response\",xlab=\"Predictor 1\")\r\nlines(c(0,1),c(mean(y01),mean(y11)),type=\"b\",col=\"lightblue\",lwd=3)\r\nlegend(\"topleft\",legend=c(\"Predictor2 at 0\",\"Predictor2 at 1\"),lty=1,lwd=3,col=c(\"darkgreen\",\"lightblue\"))\r\n\r\n\r\nTaking exponent of the same data introduces significant\r\ninteraction.\r\n\r\nplot(c(0,1),c(mean(exp(y00)),mean(exp(y10))),type=\"b\",ylim=c(1,400),col=\"darkgreen\",lwd=3,ylab=\"Response\",xlab=\"Predictor 1\")\r\nlines(c(0,1),c(mean(exp(y01)),mean(exp(y11))),type=\"b\",col=\"lightblue\",lwd=3)\r\nlegend(\"topleft\",legend=c(\"Predictor2 at 0\",\"Predictor2 at 1\"),lty=1,lwd=3,col=c(\"darkgreen\",\"lightblue\"))\r\n\r\n\r\nANCOVA\r\nTraditional analysis\r\nWe cannot use aov in ANCOVA because it uses Type I SS,\r\ninstead we should use Type II SS to get the correct results.\r\n\r\nlongevity.aov2 <- aov(Longevity ~ CompanionNumber + Thorax, data = dta)\r\n# car Anova command on our longevity.aov2 object,\r\n#summary(longevity.aov2) #produces incorrect results\r\nAnova(longevity.aov2, type=\"II\")\r\nAnova Table (Type II tests)\r\n\r\nResponse: Longevity\r\n                 Sum Sq  Df F value    Pr(>F)    \r\nCompanionNumber  9611.5   4  21.753 1.719e-13 ***\r\nThorax          13168.9   1 119.219 < 2.2e-16 ***\r\nResiduals       13144.7 119                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThe covariate, Thorax, was significantly related to the\r\nfly’s longevity, F(1,119)=119.22, p<.001. There was also a\r\nsignificant effect of the Companion Number on the Longevity after\r\ncontrolling for the effect of the Thorax, F(4,119)=21.753,\r\np<.001.\r\nBayesian approach\r\nData show significant variance within groups.\r\n\r\n\r\nAn additional metric predictor that can explain part of that variance.\r\n\r\n\r\nIn this example the subjects of the experiment were in different\r\nphysical conditions which contributed to large variation of life\r\nexpectancy.\r\nA metric predictor that is available is the size of the species.\r\nCreate data with both predictors.\r\n\r\ndataList2<-list(Ntotal=nrow(dta),\r\n                y=dta$Longevity,\r\n                xMet=dta$Thorax,\r\n                xNom=as.integer(as.factor(dta$CompanionNumber)),\r\n                NxLvl=nlevels(as.factor(dta$CompanionNumber)),\r\n                agammaShRa=unlist(gammaShRaFromModeSD(mode=sd(dta$Longevity)/2, \r\n                                                      sd=2*sd(dta$Longevity))))\r\n\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    int<lower=2> NxLvl;\r\n    int<lower=1, upper=NxLvl> xNom[Ntotal];\r\n    real xMet[Ntotal];\r\n    real<lower=0> agammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    real xMetMean;\r\n    real xMetSD;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    xMetMean = mean(xMet);\r\n    xMetSD = sd(xMet);\r\n}\r\nparameters {\r\n    real a0;\r\n    real<lower=0> aSigma;\r\n    vector[NxLvl] a;\r\n    real aMet;\r\n    real<lower=0> ySigma;\r\n}\r\nmodel {\r\n    a0 ~ normal(meanY, 5*sdY);\r\n    aSigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a ~ normal(0, aSigma);\r\n    aMet ~ normal(0, 2*sdY/xMetSD);\r\n    ySigma ~ uniform(sdY/100, sdY*10);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ normal(a0 + a[xNom[i]] + aMet*(xMet[i] - xMetMean), ySigma);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a0,a[] to sum-to-zero b0,b[] :\r\n        real b0;\r\n    vector[NxLvl] b;\r\n    b0 = a0 + mean(a) - aMet * xMetMean;\r\n    b = a - mean(a);\r\n}\r\n\"\r\n\r\n\r\nmodel2 <- stan_model(model_code=modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(model2, file=\"data/stanDSO2.Rds\")\r\nmodel2 <- readRDS(\"data/stanDSO2.Rds\")\r\n\r\nRun MCMC.\r\n\r\nfit2 <- sampling(model2, \r\n                data=dataList2, \r\n                pars=c('b0', 'b', 'aMet', 'aSigma', 'ySigma'),\r\n                iter=5000, chains = 2, cores = 4)\r\n\r\n\r\nlaunch_shinystan(fit2)\r\n\r\nCalculate same contrasts for betas:\r\nContrasts between group 1 (‘None0’) and 5 (‘Virgin8’).\r\n\r\nfit_ext2 <- rstan::extract(fit2)\r\nhead(fit_ext2$b)\r\n          \r\niterations     [,1]     [,2]      [,3]         [,4]      [,5]\r\n      [1,] 2.206979 6.764691  8.239671 -2.694472962 -14.51687\r\n      [2,] 2.787427 5.866173  9.047119 -1.874186849 -15.82653\r\n      [3,] 8.746136 6.816189  4.591818  0.009875567 -20.16402\r\n      [4,] 5.900714 3.768748  8.250167 -2.912498254 -15.00713\r\n      [5,] 4.067076 3.517638 10.195578 -1.823468995 -15.95682\r\n      [6,] 3.755893 6.918879  7.858666 -3.478249829 -15.05519\r\ncontrast2_1_5 <- fit_ext2$b[,1] - fit_ext2$b[,5]\r\nplot(contrast2_1_5)\r\n\r\nhist(contrast2_1_5)\r\n\r\n(hdiContrast2_1_5<-hdi(contrast2_1_5))\r\n   lower    upper \r\n13.47089 25.05462 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast2_1_5<-sd(contrast2_1_5))\r\n[1] 3.010371\r\nplot(rank(fit_ext2$b[,1]),rank(fit_ext2$b[,5]))\r\n\r\n\r\nContrast between average of groups 1-3 and average of groups\r\n4-5\r\n\r\nComb1<-(fit_ext2$b[,1] + fit_ext2$b[,2] + fit_ext2$b[,3])/3\r\nComb2<-(fit_ext2$b[,4] + fit_ext2$b[,5])/2\r\ncontrast2_123_45 <-Comb1-Comb2\r\nplot(contrast2_123_45)\r\n\r\nhist(contrast2_123_45)\r\n\r\n(hdiContrast2_123_45<-hdi(contrast2_123_45))\r\n   lower    upper \r\n11.24954 18.72515 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast2_123_45<-sd(contrast2_123_45))\r\n[1] 1.909079\r\nplot(rank(Comb1),rank(Comb2))\r\n\r\n\r\nCompare the models.\r\nUsing standard deviation:\r\n\r\nrbind(WithoutMetricPred=c(sd.contrast_1_5,sd.contrast_123_45),WithMetricPred=c(sd.contrast2_1_5,sd.contrast2_123_45))\r\n                      [,1]     [,2]\r\nWithoutMetricPred 4.377288 2.779190\r\nWithMetricPred    3.010371 1.909079\r\n\r\nUsing HDI:\r\n\r\nrbind(WithoutMetricPred=c(hdiContrast_1_5,hdiContrast_123_45),WithMetricPred=c(hdiContrast2_1_5,hdiContrast2_123_45))\r\n                     lower    upper     lower    upper\r\nWithoutMetricPred 14.72735 31.74161  9.727677 20.50562\r\nWithMetricPred    13.47089 25.05462 11.249538 18.72515\r\n\r\nUsing HDI width:\r\n\r\nrbind(WithoutMetricPred=c(Contrast_1_5=unname(diff(hdiContrast_1_5)),\r\n                          Contrast_123_45=unname(diff(hdiContrast_123_45))),\r\n      WithMetricPred=c(Contrast_1_5=diff(hdiContrast2_1_5),\r\n                       Contrast_123_45=diff(hdiContrast2_123_45)))\r\n                  Contrast_1_5 Contrast_123_45\r\nWithoutMetricPred     17.01425       10.777944\r\nWithMetricPred        11.58373        7.475614\r\n\r\nObviously, the width was shrinkage so with the metric predictor in\r\nthe model, we can explain more the variability of Longevity due to the\r\ndifference among groups and Thorax.\r\nHeterogeneous variances\r\nThe most restrictive assumption of ANOVA is equivalence of variances\r\nin all groups.\r\nLook how Bayesian approach can remove this assumption.\r\nOn the following model diagram there is an implementation of model\r\nwith heterogeneous variances.\r\nThe model uses t-distributed noise and allows each group to have its\r\nown standard deviation.\r\nData set InsectSprays from datasets shows\r\ntest of 6 different insect sprays. Column count contains numbers of\r\ninsects found in the field after each spraying. Column spray identifies\r\nthe type of spray.\r\nPrepare the data.\r\n\r\ndf <- InsectSprays\r\nhead(df)\r\n  count spray\r\n1    10     A\r\n2     7     A\r\n3    20     A\r\n4    14     A\r\n5    14     A\r\n6    12     A\r\nlevels(df$spray)\r\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\r\ndataListSprays <- list(Ntotal = nrow(df),\r\n                       y = df$count,\r\n                       x = as.integer(df$spray),\r\n                       NxLvl = nlevels(df$spray),\r\n                       aGammaShRa = unlist(gammaShRaFromModeSD(mode=sd(df$count)/2, \r\n                                                               sd=2*sd(df$count))))\r\n\r\nPlot the data.\r\n\r\nplot(df$count ~ df$spray)\r\n\r\n\r\nNote that variances within the groups are very different. If\r\nestimated overall the variance for groups “C”, “D” and “E” is\r\noverestimated and for “A” “B” and “F” - underestimated.\r\nApply traditional ANOVA\r\nmethod.\r\n\r\nm1 <- lm(count~spray, df)\r\nsummary(m1)\r\n\r\nCall:\r\nlm(formula = count ~ spray, data = df)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-8.333 -1.958 -0.500  1.667  9.333 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  14.5000     1.1322  12.807  < 2e-16 ***\r\nsprayB        0.8333     1.6011   0.520    0.604    \r\nsprayC      -12.4167     1.6011  -7.755 7.27e-11 ***\r\nsprayD       -9.5833     1.6011  -5.985 9.82e-08 ***\r\nsprayE      -11.0000     1.6011  -6.870 2.75e-09 ***\r\nsprayF        2.1667     1.6011   1.353    0.181    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.922 on 66 degrees of freedom\r\nMultiple R-squared:  0.7244,    Adjusted R-squared:  0.7036 \r\nF-statistic:  34.7 on 5 and 66 DF,  p-value: < 2.2e-16\r\nanova(m1)\r\nAnalysis of Variance Table\r\n\r\nResponse: count\r\n          Df Sum Sq Mean Sq F value    Pr(>F)    \r\nspray      5 2668.8  533.77  34.702 < 2.2e-16 ***\r\nResiduals 66 1015.2   15.38                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n#m.aov <- aov(count~spray, df)\r\n#summary(m.aov)\r\n\r\nThe categorical factor is significant, i.e. utility test fails.\r\nCreate contrasts to identify differences between combinations of\r\nsprays.\r\n\r\nA <- factor(df$spray)\r\ncontrasts(A)\r\n  B C D E F\r\nA 0 0 0 0 0\r\nB 1 0 0 0 0\r\nC 0 1 0 0 0\r\nD 0 0 1 0 0\r\nE 0 0 0 1 0\r\nF 0 0 0 0 1\r\ncontrasts(A)<-cbind(\"GA vs GB\"=c(1,-1,0,0,0,0),\r\n                    \"GAB vs GF\"=c(1,1,0,0,0,-2),\r\n                    \"GC vs GD\"=c(0,0,1,-1,0,0),\r\n                    \"GC vs GE\"=c(0,0,1,0,-1,0),\r\n                    \"GABF vs GCDE\"=c(1,1,-1,-1,-1,1))\r\nA\r\n [1] A A A A A A A A A A A A B B B B B B B B B B B B C C C C C C C C C\r\n[34] C C C D D D D D D D D D D D D E E E E E E E E E E E E F F F F F F\r\n[67] F F F F F F\r\nattr(,\"contrasts\")\r\n  GA vs GB GAB vs GF GC vs GD GC vs GE GABF vs GCDE\r\nA        1         1        0        0            1\r\nB       -1         1        0        0            1\r\nC        0         0        1        1           -1\r\nD        0         0       -1        0           -1\r\nE        0         0        0       -1           -1\r\nF        0        -2        0        0            1\r\nLevels: A B C D E F\r\naov(df$count ~ A)\r\nCall:\r\n   aov(formula = df$count ~ A)\r\n\r\nTerms:\r\n                       A Residuals\r\nSum of Squares  2668.833  1015.167\r\nDeg. of Freedom        5        66\r\n\r\nResidual standard error: 3.921902\r\nEstimated effects may be unbalanced\r\nsummary.lm(aov(df$count ~ A))\r\n\r\nCall:\r\naov(formula = df$count ~ A)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-8.333 -1.958 -0.500  1.667  9.333 \r\n\r\nCoefficients:\r\n                Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)    9.500e+00  4.622e-01  20.554   <2e-16 ***\r\nAGA vs GB     -4.167e-01  8.006e-01  -0.520    0.604    \r\nAGAB vs GF    -5.833e-01  4.622e-01  -1.262    0.211    \r\nAGC vs GD     -1.417e+00  9.244e-01  -1.533    0.130    \r\nAGC vs GE      8.374e-16  9.244e-01   0.000    1.000    \r\nAGABF vs GCDE  6.000e+00  4.622e-01  12.981   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.922 on 66 degrees of freedom\r\nMultiple R-squared:  0.7244,    Adjusted R-squared:  0.7036 \r\nF-statistic:  34.7 on 5 and 66 DF,  p-value: < 2.2e-16\r\n\r\nSummary shows that sprays A and B, C and D, C and E are\r\nindistinguishable as well as combined observations of sprays A and B\r\nvs. F, but combined observations for sprays A, B, F are significantly\r\ndifferent from C, D and E.\r\nHowever, separate t-test of C vs. D shows significant difference and\r\nequivalence of C and E is almost rejected by t-test.\r\n\r\nt.test(df$count[df$spray==\"C\"], df$count[df$spray==\"D\"])\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  df$count[df$spray == \"C\"] and df$count[df$spray == \"D\"]\r\nt = -3.0782, df = 20.872, p-value = 0.00573\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -4.7482230 -0.9184437\r\nsample estimates:\r\nmean of x mean of y \r\n 2.083333  4.916667 \r\nt.test(df$count[df$spray==\"C\"], df$count[df$spray==\"E\"])\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  df$count[df$spray == \"C\"] and df$count[df$spray == \"E\"]\r\nt = -1.868, df = 21.631, p-value = 0.07536\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -2.9909888  0.1576555\r\nsample estimates:\r\nmean of x mean of y \r\n 2.083333  3.500000 \r\n\r\nThese contradicting results are caused by the fact that for\r\ncomparison of C with D and E the assumption of homoscedasticity leads to\r\noverestimation of noise in general framework of ANOVA, but pairwise\r\nt-test is only accounting for variances of the compared pairs.\r\nBayes’\r\nRun MCMC.\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    int<lower=2> NxLvl;\r\n    int<lower=1, upper=NxLvl> x[Ntotal];\r\n    real<lower=0> aGammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n}\r\nparameters {\r\n    real<lower=0> nu;\r\n    real a0;\r\n    real<lower=0> aSigma;\r\n    vector[NxLvl] a;\r\n    real<lower=0> ySigma[NxLvl];\r\n    real<lower=0> ySigmaMode;\r\n    real<lower=0> ySigmaSD;\r\n}\r\ntransformed parameters{\r\n    real<lower=0> ySigmaSh;\r\n    real<lower=0> ySigmaRa;\r\n    ySigmaRa = ( ( ySigmaMode + sqrt( ySigmaMode^2 + 4*ySigmaSD^2 ) ) / ( 2*ySigmaSD^2 ) );\r\n    ySigmaSh = 1 + ySigmaMode * ySigmaRa;\r\n}\r\nmodel {\r\n    nu ~ exponential(1/30.0);\r\n    a0 ~ normal(meanY, 10*sdY);\r\n    aSigma ~ gamma(aGammaShRa[1], aGammaShRa[2]);\r\n    a ~ normal(0, aSigma);\r\n    ySigma ~ gamma(ySigmaSh, ySigmaRa);\r\n    ySigmaMode ~ gamma(aGammaShRa[1], aGammaShRa[2]);\r\n    ySigmaSD ~ gamma(aGammaShRa[1], aGammaShRa[2]);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ student_t(nu, a0 + a[x[i]], ySigma[x[i]]);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a0,a[] to sum-to-zero b0,b[] :\r\n        real b0;\r\n    vector[NxLvl] b;\r\n    b0 = a0 + mean(a);\r\n    b = a - mean(a);\r\n}\r\n\"\r\n\r\n\r\nmodel3 <- stan_model(model_code = modelString)\r\n\r\n\r\n# saveRDS(model3,file = \"data/stanDSO3.Rds\")\r\nmodel3 <- readRDS(file = \"data/stanDSO3.Rds\")\r\n\r\nRun chains.\r\n\r\nfit.sprays <- sampling(model3, \r\n                data=dataListSprays, \r\n                pars=c('b0', 'b', 'aSigma', 'ySigma', 'nu', 'ySigmaMode', 'ySigmaSD'),\r\n                iter=5000, chains = 2, cores = 4)\r\n\r\n\r\nlaunch_shinystan(fit)\r\n\r\nAnalyze estimates.\r\n\r\nplot(fit.sprays,pars=c(\"b0\",\"aSigma\"))\r\n\r\nplot(fit.sprays,pars=c(\"b\"))\r\n\r\nplot(fit.sprays,pars=c(\"ySigma\"))\r\n\r\n\r\nExtract chains.\r\n\r\nchains_sprays <- rstan::extract(fit.sprays)\r\nhead(chains_sprays$b)\r\n          \r\niterations     [,1]     [,2]      [,3]      [,4]      [,5]     [,6]\r\n      [1,] 4.857229 4.406569 -6.726589 -5.825001 -5.722536 9.010329\r\n      [2,] 4.772449 6.016272 -6.918228 -4.328620 -5.127054 5.585181\r\n      [3,] 6.249474 4.233752 -6.661174 -4.839482 -6.052489 7.069918\r\n      [4,] 4.683855 7.234533 -6.111765 -5.126468 -5.951647 5.271492\r\n      [5,] 5.383824 7.476675 -7.539103 -4.812495 -5.659039 5.150138\r\n      [6,] 6.056509 5.838986 -7.750583 -4.615163 -6.925910 7.396160\r\n\r\nLook at the same contrasts.\r\nA vs. B\r\n\r\ncontrast1_2 <- chains_sprays$b[,1] - chains_sprays$b[,2]\r\nplot(contrast1_2)\r\n\r\nhist(contrast1_2)\r\n\r\n(hdiContrast1_2<-hdi(contrast1_2))\r\n    lower     upper \r\n-4.709984  2.887591 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast1_2<-sd(contrast1_2))\r\n[1] 1.960466\r\nplot(rank(chains_sprays$b[,1]),rank(chains_sprays$b[,5]))\r\n\r\n\r\nThe contrast is not different from zero.\r\nA, B vs. F\r\n\r\nComb1<-chains_sprays$b[,1] + chains_sprays$b[,2]\r\nComb2<-2*chains_sprays$b[,6]\r\ncontrast12_6 <-Comb1  - Comb2\r\nplot(contrast12_6)\r\n\r\nhist(contrast12_6)\r\n\r\n(hdiContrast12_6<-hdi(contrast12_6))\r\n     lower      upper \r\n-10.668902   5.111966 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast12_6<-sd(contrast12_6))\r\n[1] 4.018199\r\nplot(rank(Comb1),rank(Comb2))\r\n\r\n\r\nThe contrast is not different from zero.\r\nC vs. D\r\n\r\ncontrast3_4 <- chains_sprays$b[,3] - chains_sprays$b[,4]\r\nplot(contrast3_4)\r\n\r\nhist(contrast3_4)\r\n\r\n(hdiContrast3_4<-hdi(contrast3_4))\r\n     lower      upper \r\n-4.6604902 -0.6528553 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast3_4<-sd(contrast3_4))\r\n[1] 1.006201\r\nplot(rank(chains_sprays$b[,3]),rank(chains_sprays$b[,4]))\r\n\r\n\r\nThis contrast is different from zero. ANOVA could not detect\r\nthat.\r\nC vs. E\r\n\r\ncontrast3_5 <- chains_sprays$b[,3] - chains_sprays$b[,5]\r\nplot(contrast3_5)\r\n\r\nhist(contrast3_5)\r\n\r\n(hdiContrast3_5<-hdi(contrast3_5))\r\n     lower      upper \r\n-3.3196751  0.2382747 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast3_5<-sd(contrast3_5))\r\n[1] 0.9001176\r\nplot(rank(chains_sprays$b[,3]),rank(chains_sprays$b[,5]))\r\n\r\n\r\nThe contrast is not different from zero.\r\nCombined observations A, B and F vs. C, D, E\r\n\r\nComb1<-chains_sprays$b[,1] + chains_sprays$b[,2] + chains_sprays$b[,6]\r\nComb2<-chains_sprays$b[,3] + chains_sprays$b[,4] + chains_sprays$b[,5]\r\ncontrast126_345 <-Comb1  - Comb2\r\nplot(contrast126_345)\r\n\r\nhist(contrast126_345)\r\n\r\n(hdiContrast126_345<-hdi(contrast126_345))\r\n   lower    upper \r\n29.17613 40.78201 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast126_345<-sd(contrast126_345))\r\n[1] 2.989119\r\nplot(rank(Comb1),rank(Comb2))\r\n\r\n\r\nThe contrast is significantly different from zero.\r\nFurther Readings\r\nAdapted from UC’s coursework\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial\r\nwith r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic\r\nPress is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-01T20:42:07-06:00",
    "input_file": "Series-6---ANOVA-and-ANCOVA.knit.md"
  },
  {
    "path": "posts/2022-02-14-Bayesian methods - Series 5 of 10/",
    "title": "Series 5 of 10 -- Introduction to Selection of Variables using Bayesian Approach",
    "description": "Gently Introduce to Variable Selection using Bayesian Approach",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-02-14",
    "categories": [
      "Biostatistics",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nRead data\r\nModeling in\r\nJAGS\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake\\)\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend\\)\r\nmodel \\(SATT = \\beta_0 + \\beta_2 \\\r\nPrcntTake\\)\r\nmodel\r\n\\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake + \\beta_3 \\ StuTeaRat + \\beta_4 \\ Salary\\)\r\n\r\nInclusion Probabilities\r\nConclusions\r\nReferences\r\n\r\nIntroduction\r\nThis example is based on section 18.4 in Kruschke, 2015.\r\nA Bayesian approach variable selection in regression analysis can be\r\ndone by adding a binary parameter for each slope:\\[y_i = \\beta_0 + \\sum_j \\delta_j \\ \\beta_j\r\nx_{i,j},\\]\r\nwith \\[\\delta_j = 0, \\ 1.\\]\r\nEvery combination of values \\(\\delta_1, \\\r\n..., \\delta_k\\) gives a submodel. Simple priors for\r\ndelta-indicators are independent Bernoulli distributions: \\(\\delta \\sim \\text{dbern}(0.5)\\). Then\r\nposterior probabilities \\(P(\\delta_j = 1\\mid\r\nD)\\) show importance of the corresponding predictors \\(x_i\\).\r\nThe diagram of such model is an easy generalization of regression\r\nmodel.\r\nSince parameters \\(\\delta_j\\) are\r\ndiscrete, we need to use JAGS: library(runjags) instead of\r\nStan\r\n\r\nlibrary(runjags)\r\n\r\nRead data\r\nThe SAT\r\ndata includes:\r\nState: 50 states,\r\nSATV: SAT verbal; SATM: SAT math;\r\nSATT: SAT total score\r\nSpend: average spending per pupil\r\nPrcntTake: percentage of students who took the\r\ntest\r\nStuTeaRat: the average student teacher ratio in each\r\nstate and\r\nSalary: the average salary of the teachers\r\nThese latter four variables are also plausible predictors of SAT\r\nscore.\r\n\r\ndta <- read.csv(\"data/Guber1999data.csv\")\r\nnames(dta)\r\n[1] \"State\"     \"Spend\"     \"StuTeaRat\" \"Salary\"    \"PrcntTake\"\r\n[6] \"SATV\"      \"SATM\"      \"SATT\"     \r\nDT::datatable(dta)\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"],[\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"],[4.405,8.963,4.778,4.459,4.992,5.443,8.817,7.03,5.718,5.193,6.078,4.21,6.136,5.826,5.483,5.817,5.217,4.761,6.428,7.245,7.287,6.994,6,4.08,5.383,5.692,5.935,5.16,5.859,9.774,4.586,9.623,5.077,4.775,6.162,4.845,6.436,7.109,7.469,4.797,4.775,4.388,5.222,3.656,6.75,5.327,5.906,6.107,6.93,6.16],[17.2,17.6,19.3,17.1,24,18.4,14.4,16.6,19.1,16.3,17.9,19.1,17.3,17.5,15.8,15.1,17,16.8,13.8,17,14.8,20.1,17.5,17.5,15.5,16.3,14.5,18.7,15.6,13.8,17.2,15.2,16.2,15.3,16.6,15.5,19.9,17.1,14.7,16.4,14.4,18.6,15.7,24.3,13.8,14.6,20.2,14.8,15.9,14.9],[31.144,47.951,32.175,28.934,41.078,34.571,50.045,39.076,32.588,32.291,38.518,29.783,39.431,36.785,31.511,34.652,32.257,26.461,31.972,40.661,40.795,41.895,35.948,26.818,31.189,28.785,30.922,34.836,34.72,46.087,28.493,47.612,30.793,26.327,36.802,28.172,38.555,44.51,40.729,30.279,25.994,32.477,31.223,29.082,35.406,33.987,36.151,31.944,37.746,31.285],[8,47,27,6,45,29,81,68,48,65,57,15,13,58,5,9,11,9,68,64,80,11,9,4,9,21,9,30,70,70,11,74,60,5,23,9,51,70,70,58,5,12,47,4,68,65,48,17,9,10],[491,445,448,482,417,462,431,429,420,406,407,468,488,415,516,503,477,486,427,430,430,484,506,496,495,473,494,434,444,420,485,419,411,515,460,491,448,419,425,401,505,497,419,513,429,428,443,448,501,476],[538,489,496,523,485,518,477,468,469,448,482,511,560,467,583,557,522,535,469,479,477,549,579,540,550,536,556,483,491,478,530,473,454,592,515,536,499,461,463,443,563,543,474,563,472,468,494,484,572,525],[1029,934,944,1005,902,980,908,897,889,854,889,979,1048,882,1099,1060,999,1021,896,909,907,1033,1085,1036,1045,1009,1050,917,935,898,1015,892,865,1107,975,1027,947,880,888,844,1068,1040,893,1076,901,896,937,932,1073,1001]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>State<\\/th>\\n      <th>Spend<\\/th>\\n      <th>StuTeaRat<\\/th>\\n      <th>Salary<\\/th>\\n      <th>PrcntTake<\\/th>\\n      <th>SATV<\\/th>\\n      <th>SATM<\\/th>\\n      <th>SATT<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nWe now let look at the correlation matrix among 4 predictors.\r\n\r\ndta %>%\r\n  select(Spend, PrcntTake, StuTeaRat, Salary) %>%\r\n  cor(.) %>%\r\n  round(., 3)\r\n           Spend PrcntTake StuTeaRat Salary\r\nSpend      1.000     0.593    -0.371  0.870\r\nPrcntTake  0.593     1.000    -0.213  0.617\r\nStuTeaRat -0.371    -0.213     1.000 -0.001\r\nSalary     0.870     0.617    -0.001  1.000\r\n\r\nThat Salary is strongly correlated with Spend, and therefore, a model\r\nthat includes both Salary and Spend will show a strong trade-off between\r\nthose predictors and consequently will show inflated uncertainty in the\r\nregression coefficients for either one. Should only one or the other be\r\nincluded, or both, or neither?\r\nOne more point, response variable is average high school SAT score\r\nper state. One of the predictors is amount of money spent by the state\r\non schools. Plotting amount of money spent against SAT scores shows\r\nnegative slope.\r\n\r\nplot(dta$Spend,dta$SATT)\r\n\r\n\r\nThus, should we cut funding for school? Because even with the\r\nfunding, SAT would be reduced disproportional with the budget\r\nspent!!\r\nModeling in JAGS\r\n\r\ny <- dta[,'SATT']\r\nx <- as.matrix(dta[ , c(\"Spend\",\"PrcntTake\",\"StuTeaRat\",\"Salary\")])\r\n\r\n#prepare data in JAGS\r\ndataList <- list(Ntotal = length(y),\r\n                 y = y,\r\n                 x = x,\r\n                 Nx = ncol(x))\r\n\r\nDescription of the model:\r\n\\[y_i = \\beta_0 + \\delta_j \\ \\beta_j\r\nx_{i,j} + \\epsilon_i \\\\\r\ny_i \\sim t(\\mu_i, \\sigma, \\nu) \\\\\r\n\\\\\r\n\\text{where mean, scale and degree of freedom, respectively, would be}\r\n\\\\\r\n\\mu_i = \\beta_0 + \\delta_j \\ \\beta_j x_{i,j} \\\\\r\n\\beta_0 \\sim N(0, \\frac{1}{1/2^2}) \\ ; \\ \\beta_j \\sim t(0,\r\n1/\\sigma_{\\beta}^2, \\nu = 1) \\\\\r\n\\sigma \\sim Unif(L, H) \\\\\r\n\\nu = \\gamma^{\\prime} + 1, \\ \\text{where} \\ \\gamma^{\\prime} \\sim\r\nexp(\\lambda) \\\\\r\n\\delta_j \\sim Bernoulli(0.5) \\ ; \\ \\sigma_{\\beta} \\sim\r\n\\text{Gamma(shape, rate)}\\]\r\n\r\nmodelString = \"\r\n    # Standardize the data:\r\n    data {\r\n        ym <- mean(y)\r\n        ysd <- sd(y)\r\n        for (i in 1:Ntotal) {\r\n            zy[i] <- (y[i] - ym) / ysd\r\n        }\r\n        for (j in 1:Nx) {\r\n            xm[j]  <- mean(x[,j])\r\n            xsd[j] <-   sd(x[,j])\r\n            for (i in 1:Ntotal) {\r\n                zx[i,j] <- (x[i,j] - xm[j]) / xsd[j]\r\n            }\r\n        }\r\n    }\r\n    # Specify the model for standardized data:\r\n    model {\r\n        for (i in 1:Ntotal) {\r\n            zy[i] ~ dt(zbeta0 + sum(delta[1:Nx] * zbeta[1:Nx] * zx[i,1:Nx] ) ,  1/zsigma^2 , nu)\r\n        }\r\n        # Priors vague on standardized scale:\r\n        zbeta0 ~ dnorm(0, 1/2^2)\r\n        for (j in 1:Nx) {\r\n            zbeta[j] ~ dt(0 , 1/sigmaBeta^2 , 1) \r\n            delta[j] ~ dbern( 0.5 )\r\n        }\r\n        zsigma ~ dunif(1.0E-5 , 1.0E+1)\r\n        \r\n        # sigmaBeta <- 2.0 ## uncomment one of the following specifications for sigmaBeta\r\n        # sigmaBeta ~ dunif( 1.0E-5 , 1.0E+2 )\r\n        sigmaBeta ~ dgamma(1.1051,0.1051) # mode 1.0, sd 10.0\r\n        # sigmaBeta <- 1/sqrt(tauBeta) ; tauBeta ~ dgamma(0.001,0.001) \r\n        nu ~ dexp(1/30.0)\r\n        \r\n        # Transform to original scale:\r\n        beta[1:Nx] <- (delta[1:Nx] * zbeta[1:Nx] / xsd[1:Nx])*ysd\r\n        beta0 <- zbeta0*ysd  + ym - sum(delta[1:Nx] * zbeta[1:Nx] * xm[1:Nx] / xsd[1:Nx])*ysd\r\n        sigma <- zsigma*ysd\r\n    }\r\n\"\r\n\r\n\r\nparameters <- c(\"beta0\",  \"beta\",  \"sigma\", \"delta\", \"sigmaBeta\", \"zbeta0\", \"zbeta\", \"zsigma\", \"nu\" )\r\nrunjagsMethod <- \"parallel\"  # change to \"rjags\" in case of working on 1-core CPU \r\n# run JAGS\r\n\r\n\r\nrunJagsOut <- run.jags(method = runjagsMethod,\r\n                       model = modelString, \r\n                       monitor = parameters, \r\n                       data = dataList,\r\n                       n.chains = 3, #nChains <- 3\r\n                       adapt = 500, #adaptSteps <- 500\r\n                       burnin = 1000, #burnInSteps <- 1000\r\n                       sample = ceiling(15000/3), #numSavedSteps <- 15000;  nChains <- 3\r\n                       thin = 25, #thinSteps <- 25\r\n                       summarise = FALSE,\r\n                       plots = FALSE)\r\n\r\n\r\n# generate summary info for all parameters\r\nsummary(runJagsOut)\r\nCalculating summary statistics...\r\nNote: The monitored variable 'delta[2]' appears to be\r\nnon-stochastic; it will not be included in the convergence\r\ndiagnostic\r\nCalculating the Gelman-Rubin statistic for 18 variables....\r\nError : The \"modeest\" package is required to calculate the mode of continuous variables\r\n               Lower95        Median      Upper95          Mean\r\nbeta0     944.66400000  1.011275e+03  1.11231e+03  1.018969e+03\r\nbeta[1]    -0.00128211  7.803640e+00  1.91766e+01  7.314723e+00\r\nbeta[2]    -3.26471000 -2.761835e+00 -2.22909e+00 -2.753613e+00\r\nbeta[3]    -5.57611000  0.000000e+00  4.47374e-03 -6.349370e-01\r\nbeta[4]    -0.26275600  0.000000e+00  3.55510e+00  3.305153e-01\r\nsigma      24.62630000  3.214530e+01  4.10201e+01  3.238811e+01\r\ndelta[1]    0.00000000  1.000000e+00  1.00000e+00  6.138000e-01\r\ndelta[2]    1.00000000  1.000000e+00  1.00000e+00  1.000000e+00\r\ndelta[3]    0.00000000  0.000000e+00  1.00000e+00  1.732667e-01\r\ndelta[4]    0.00000000  0.000000e+00  1.00000e+00  2.208667e-01\r\nsigmaBeta   0.02406400  1.127925e+00  7.94953e+00  2.198454e+00\r\nzbeta0     -0.12437500 -1.292225e-04  1.29719e-01  1.964852e-04\r\nzbeta[1]   -7.77130000  2.129285e-01  1.10354e+01  5.264458e-01\r\nzbeta[2]   -1.16775000 -9.878755e-01 -7.97319e-01 -9.849344e-01\r\nzbeta[3]  -24.01480000 -8.063585e-02  2.36377e+01 -2.338195e-01\r\nzbeta[4]  -17.73010000  1.036410e-01  1.87959e+01  3.378339e-01\r\nzsigma      0.32913900  4.296320e-01  5.48246e-01  4.328772e-01\r\nnu          1.91471000  2.685290e+01  9.61985e+01  3.558561e+01\r\n                   SD Mode        MCerr MC%ofSD SSeff       AC.250\r\nbeta0     43.89744488   NA 0.4821033688     1.1  8291  0.019090161\r\nbeta[1]    7.04248924   NA 0.1017558219     1.4  4790  0.064596992\r\nbeta[2]    0.27026073   NA 0.0033981576     1.3  6325  0.053310641\r\nbeta[3]    1.74666227   NA 0.0151369398     0.9 13315 -0.009957991\r\nbeta[4]    0.99368906   NA 0.0093047567     0.9 11405  0.010317963\r\nsigma      4.16357566   NA 0.0387315148     0.9 11556  0.016315434\r\ndelta[1]   0.48689359    1 0.0078742141     1.6  3823  0.085242124\r\ndelta[2]   0.00000000    1           NA      NA    NA           NA\r\ndelta[3]   0.37849026    0 0.0034208127     0.9 12242 -0.004375483\r\ndelta[4]   0.41484462    0 0.0038539951     0.9 11586  0.010031033\r\nsigmaBeta  3.24829579   NA 0.0687169662     2.1  2235  0.115133738\r\nzbeta0     0.06462281   NA 0.0005367156     0.8 14497 -0.007738634\r\nzbeta[1]   5.53722794   NA 0.2424652404     4.4   522  0.502781355\r\nzbeta[2]   0.09666902   NA 0.0012154802     1.3  6325  0.053310325\r\nzbeta[3]  18.13696053   NA 0.5572712553     3.1  1059  0.319423413\r\nzbeta[4]  14.61246772   NA 0.3817438159     2.6  1465  0.293091432\r\nzsigma     0.05564748   NA 0.0005176588     0.9 11556  0.016315483\r\nnu        30.03665106   NA 0.2442544957     0.8 15122 -0.003220931\r\n               psrf\r\nbeta0     1.0000092\r\nbeta[1]   1.0001584\r\nbeta[2]   1.0002489\r\nbeta[3]   1.0006953\r\nbeta[4]   1.0006186\r\nsigma     1.0000140\r\ndelta[1]  1.0006266\r\ndelta[2]         NA\r\ndelta[3]  1.0008028\r\ndelta[4]  1.0006483\r\nsigmaBeta 1.0016839\r\nzbeta0    1.0001875\r\nzbeta[1]  1.0614080\r\nzbeta[2]  1.0002489\r\nzbeta[3]  1.0456877\r\nzbeta[4]  1.0327248\r\nzsigma    1.0000147\r\nnu        0.9999383\r\n\r\n\r\n# plot all params\r\nplot(runJagsOut,\r\n     plot.type = c(\"trace\", \"ecdf\", \"histogram\", \"autocorr\"))\r\nGenerating summary statistics and plots (these will NOT be\r\nsaved for reuse)...\r\nCalculating summary statistics...\r\nNote: The monitored variable 'delta[2]' appears to be\r\nnon-stochastic; it will not be included in the convergence\r\ndiagnostic\r\nCalculating the Gelman-Rubin statistic for 18 variables....\r\nError : The \"modeest\" package is required to calculate the mode of continuous variables\r\n\r\n\r\nFind posterior probability of different combinations of predictors by\r\ncalculating frequencies with which they appeared in MCMC.\r\n\r\ntrajectoriesDelta <- as.matrix(runJagsOut$mcmc[,7:10])\r\nhead(trajectoriesDelta)\r\n     delta[1] delta[2] delta[3] delta[4]\r\n[1,]        0        1        0        0\r\n[2,]        1        1        0        0\r\n[3,]        1        1        0        0\r\n[4,]        1        1        0        0\r\n[5,]        1        1        1        0\r\n[6,]        0        1        0        0\r\nNchain <- nrow(trajectoriesDelta)\r\n\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake\\)\r\n\r\n(config1 <- sum(apply(trajectoriesDelta, 1, function(z) prod(z==c(1,1,0,0))))/Nchain)\r\n[1] 0.4738667\r\n\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend\\)\r\n\r\n(config2 <- sum(apply(trajectoriesDelta, 1,function(z) prod(z==c(1,0,0,0))))/Nchain)\r\n[1] 0\r\n\r\nmodel \\(SATT = \\beta_0 + \\beta_2 \\ PrcntTake\\)\r\n\r\n(config3 <- sum(apply(trajectoriesDelta, 1, function(z) prod(z==c(0,1,0,0))))/Nchain)\r\n[1] 0.2154667\r\n\r\nmodel\r\n\\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake + \\beta_3 \\ StuTeaRat + \\beta_4 \\ Salary\\)\r\n\r\n(config4 <- sum(apply(trajectoriesDelta, 1, function(z) prod(z==c(1,1,1,1))))/Nchain)\r\n[1] 0.02186667\r\n\r\nInclusion Probabilities\r\n\r\n(inclSpend<-sum(trajectoriesDelta[,1]==1)/Nchain) #Spend\r\n[1] 0.6138\r\n(inclPrcntTake<-sum(trajectoriesDelta[,2]==1)/Nchain) #PrcntTake\r\n[1] 1\r\n(inclStueTeaRat<-sum(trajectoriesDelta[,3]==1)/Nchain) #StueTeaRat\r\n[1] 0.1732667\r\n(inclSalary<-sum(trajectoriesDelta[,4]==1)/Nchain) #Salary\r\n[1] 0.2208667\r\n\r\nConclusions\r\nOut of 4 analyzed configurations of the model the most observed is\r\nconfiguration 1 (0.4738667).\r\nFor configuration 4 with all predictors observation rate is just\r\n0.0218667.\r\nOut of 4 variables two are selected with inclusion probabilities\r\nabove 0.5, those are Spend (0.6138) and\r\nPrcntTake (1).\r\nThe second variable PrcntTake is included with\r\nnon-stochastic probability of 1, standard deviation of \\(\\delta_2\\) is zero.\r\nWarning. Posterior probabilities of inclusion may be\r\nvery sensitive to prior information.\r\nReferences\r\nBayesian Methods, UC’s lecture\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-02-01T12:31:51-06:00",
    "input_file": "Series-5-of-10---Introduction-to-Variable-Selection-in-Bayesian-Approach.knit.md"
  },
  {
    "path": "posts/2022-01-31-Bayesian methods - Series 4 of 10/",
    "title": "Series 4 of 10 -- Fitting Linear Models - Multiple Regression",
    "description": "Review a simple linear regression using Bayesian Methods   \nHow to fit a linear multiple regression (Stan) using Bayesian Methods   \nCases of significant/insignificant, correlated and collinear predictors  \nShrinkage  \nBayesian model with or without shrinkage, ridge regression and lasso regression: An example of SAT scores",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-01-31",
    "categories": [
      "Biostatistics",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nReview a simple\r\nlinear regression in Bayes\r\nMultiple\r\nRegression\r\nExample 1: Two significant\r\npredictors\r\nReminder of\r\nStan\r\n\r\nExample 2: Insignificant\r\npredictor\r\nExample 3: Correlated\r\npredictors\r\nStrong\r\ncorrelation\r\nCollinearity\r\n\r\n\r\nShrinkage of regression\r\ncoefficients\r\nTwo significant predictors\r\nAnalysis and comparison\r\n\r\nInsignificant predictor\r\nAnalysis and comparison\r\n\r\nCorrelated predictors\r\nAnalysis and comparison\r\n\r\n\r\nIs school financing\r\nnecessary?\r\nNo-shrinkage\r\nShrinkage\r\nLinear model\r\nRidge and\r\nlasso\r\n\r\nFurther\r\nreading\r\n\r\nReview a simple\r\nlinear regression in Bayes\r\nWe can see a\r\npost how to fit a simple linear regression in both Frequentist\r\napproach and Bayesian methods. Now we move on to the linear multiple\r\nregression.\r\nMultiple Regression\r\nExample 1: Two significant\r\npredictors\r\nGenerate data for multiple linear regression with 2 independent\r\nsignificant predictors.\r\n\r\n# generate data\r\nset.seed(03182021)\r\nNtotal <- 500\r\nx <- cbind(rnorm(Ntotal, mean = 20, sd = 4), \r\n           rnorm(Ntotal, mean=10, sd = 6))\r\nNx <- ncol(x)\r\ny <- 4 + 1.1*x[,1] + 3*x[,2] + rnorm(Ntotal, mean = 0, sd = 1)\r\n\r\nCreate a data list.\r\n\r\ndataListRegression <- list(Ntotal = Ntotal, y = y, x = as.matrix(x), Nx = Nx)\r\n\r\nHere’s a model in the Frequentist method:\r\n\r\nsummary(lm(y~x))\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.83480 -0.63696  0.00131  0.69896  2.48803 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 4.565419   0.230456   19.81   <2e-16 ***\r\nx1          1.074206   0.010945   98.14   <2e-16 ***\r\nx2          2.995385   0.007954  376.57   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9759 on 497 degrees of freedom\r\nMultiple R-squared:  0.9969,    Adjusted R-squared:  0.9968 \r\nF-statistic: 7.895e+04 on 2 and 497 DF,  p-value: < 2.2e-16\r\n\r\nThe diagram of the model shows hierarchical structure with normal\r\npriors for intercept \\(\\beta_0\\) and\r\nslopes \\(\\beta_i\\), \\(i=1,2\\).\r\n\r\nDescription of the model:\r\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} +\r\n\\beta_2 x_{i,2} + \\epsilon_i \\\\\r\ny_i \\sim t(\\mu_i, \\sigma, \\nu) \\\\\r\n\\\\\r\n\\text{where mean, scale and degree of freedom, respectively, would be}\r\n\\\\\r\n\\mu_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} \\\\\r\n\\beta_0 \\sim N(M_0, S_0) \\ ; \\ \\beta_j \\sim N(M_1, S_1) \\\\\r\n\\sigma \\sim Unif(L, H) \\\\\r\n\\nu = \\gamma^{\\prime} + 1, \\ \\text{where} \\ \\gamma^{\\prime} \\sim\r\nexp(\\lambda)\\]\r\nNow, we come back to the problem. Here’s we write a model string:\r\nReminder of Stan: (see below, at the end of Example 1)\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int<lower=1> Nx;\r\n    vector[Ntotal] y;\r\n    matrix[Ntotal, Nx] x;\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    vector[Ntotal] zy; // normalized\r\n    vector[Nx] meanX;\r\n    vector[Nx] sdX;\r\n    matrix[Ntotal, Nx] zx; // normalized\r\n    \r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    zy = (y - meanY) / sdY;\r\n    for (j in 1:Nx) {\r\n        meanX[j] = mean(x[,j]);\r\n        sdX[j] = sd(x[,j]);\r\n        for ( i in 1:Ntotal ) {\r\n            zx[i,j] = (x[i,j] - meanX[j]) / sdX[j];\r\n        }\r\n    }\r\n}\r\nparameters {\r\n    real zbeta0;\r\n    vector[Nx] zbeta;\r\n    real<lower=0> nu;\r\n    real<lower=0> zsigma;\r\n}\r\ntransformed parameters{\r\n    vector[Ntotal] zy_hat;\r\n    zy_hat = zbeta0 + zx * zbeta;\r\n}\r\nmodel {\r\n    zbeta0 ~ normal(0, 2);\r\n    zbeta  ~ normal(0, 2);\r\n    nu ~ exponential(1/30.0);\r\n    zsigma ~ uniform(1.0E-5 , 1.0E+1);\r\n    zy ~ student_t(1+nu, zy_hat, zsigma);\r\n}\r\ngenerated quantities { \r\n    // Transform to original scale:\r\n    real beta0; \r\n    vector[Nx] beta;\r\n    real sigma;\r\n    // .* and ./ are element-wise product and divide\r\n    beta0 = zbeta0*sdY  + meanY - sdY * sum( zbeta .* meanX ./ sdX );\r\n    beta = sdY * ( zbeta ./ sdX );\r\n    sigma = zsigma * sdY;\r\n} \"\r\n\r\n\r\nRobustMultipleRegressionDso <- stan_model(model_code=modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(RobustMultipleRegressionDso, file=\"data/DSORobustMultRegr.Rds\")\r\nRobustMultipleRegressionDso <- readRDS(\"data/DSORobustMultRegr.Rds\")\r\n\r\nFit the model.\r\n\r\nfit1 <- sampling(RobustMultipleRegressionDso,\r\n                 data=dataListRegression,\r\n                 pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                 iter = 5000, chains = 2, cores = 2)\r\nstan_ac(fit1)\r\n\r\nstan_trace(fit1)\r\n\r\n\r\nLook at the results.\r\n\r\nsummary(fit1)$summary[,c(1,3,4,5,8,9)] #mean, sd, 2.5%, 97.5%, n_eff\r\n                mean           sd         2.5%         25%\r\nbeta0      4.5780310  0.231357219    4.1304172    4.422841\r\nbeta[1]    1.0738282  0.011006048    1.0521979    1.066428\r\nbeta[2]    2.9951527  0.008047017    2.9797451    2.989759\r\nnu        61.7987150 36.009712586   17.0095629   36.082778\r\nsigma      0.9611543  0.032896641    0.8978996    0.938909\r\nlp__    1014.2033841  1.585209650 1010.1168812 1013.426674\r\n              97.5%    n_eff\r\nbeta0      5.025614 7033.830\r\nbeta[1]    1.095054 6561.254\r\nbeta[2]    3.011046 6871.816\r\nnu       151.390448 4557.868\r\nsigma      1.027262 4328.823\r\nlp__    1016.293483 2600.401\r\npairs(fit1,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit1,pars=\"nu\")\r\n\r\nplot(fit1,pars=\"sigma\")\r\n\r\nplot(fit1,pars=\"beta0\")\r\n\r\nplot(fit1,pars=\"beta[1]\")\r\n\r\nplot(fit1,pars=\"beta[2]\")\r\n\r\n\r\nAnalyze fitted model using shinystan\r\n\r\nlaunch_shinystan(fit1)\r\n\r\nConclusions:\r\n\\(\\nu\\) (degree of freedom in\r\nt-distribution) is large enough to consider normal distribution:\r\n2.5% HDI level is 12.6257143, mean value is 49.9778865. Not surprising:\r\nwe simulated normal model \\(\\Rightarrow\\) normality parameter \\(\\nu\\)\r\nParameters \\(\\beta_0\\) and \\(\\beta_1\\) are significantly negatively\r\ncorrelated, as expected.\r\nParameters \\(\\beta_0\\) and \\(\\beta_2\\) are also negatively correlated,\r\nbut correlation is not so strong.\r\nAll parameter estimates are close to what we simulated.\r\nReminder of Stan\r\nA Stan program has three required “blocks”:\r\ndata block: where you declare the data types, their\r\ndimensions, any restrictions (i.e. upper = or lower = , which act as\r\nchecks for Stan), and their names. Any names you give to\r\nyour Stan program will also be the names used in other\r\nblocks.\r\nparameters block: This is where you indicate the\r\nparameters you want to model, their dimensions, restrictions, and name.\r\nFor a linear regression, we will want to model the intercept, any\r\nslopes, and the standard deviation of the errors around the regression\r\nline.\r\nmodel block: This is where you include any sampling\r\nstatements, including the “likelihood” (model) you are using. The model\r\nblock is where you indicate any prior distributions you want to include\r\nfor your parameters. If no prior is defined, Stan uses\r\ndefault priors with the specifications\r\nuniform(-infinity, +infinity). You can restrict priors\r\nusing upper or lower when declaring the parameters\r\n(i.e. <lower = 0> to make sure a parameter is\r\npositive). You can find more information about prior specification here.\r\nSampling is indicated by the ~ symbol, and Stan already\r\nincludes many common distributions as vectorized functions. You can\r\ncheck out the\r\nmanual for a comprehensive list and more information on the optional\r\nblocks you could include in your Stan model.\r\nThere are also four optional blocks:\r\nfunctions\r\ntransformed data: allows for preprocessing of the\r\ndata\r\ntransformed parameters: allows for parameter processing\r\nbefore the posterior is computed\r\nObjects declared in the “transformed parameters” block of a Stan\r\nprogram are:\r\nUnknown but are known given the values of the objects in the\r\nparameters block\r\nSaved in the output and hence should be of interest to the\r\nresearcher\r\nAre usually the arguments to the log-likelihood function that is\r\nevaluated in the model block, although in hierarchical\r\nmodels the line between the prior and the likelihood can be drawn in\r\nmultiple ways\r\n(if the third point is not the case, the object should usually be\r\ndeclared in the generated quantities block of a Stan\r\nprogram)\r\nThe purpose of declaring such things in the\r\ntransformed parameters block rather than the parameters\r\nblock is often to obtain more efficient sampling from the posterior\r\ndistribution. If there is a posterior PDF \\(f(\\theta \\mid \\text{data})\\), then for any\r\nobjective transformation from \\(\\alpha\\) to \\(\\theta\\), the posterior PDF of \\(\\alpha\\) is simply \\(f(\\)\\((\\)\\() \\mid\r\ndata)\\text{abs}|J|\\), where \\(|J|\\) is the determinant of the Jacobian\r\nmatrix of the transformation from \\(\\alpha\\) to \\(\\theta\\). Thus, you can make the same\r\ninferences about (functions of) \\(\\theta\\) either by drawing from the\r\nposterior whose PDF is \\(f(\\theta \\mid\r\n\\text{data})\\) where \\(\\theta\\)\r\nare the parameters or the posterior whose PDF is f(θ(α)|data)abs|J|\r\nwhere α are parameters and θ are transformed parameters. Since the\r\nposterior inferences about (functions of) θ are the same, you are free\r\nto choose a transformation that enhances the efficiency of the sampling\r\nby making α less correlated, unit scaled, more Gaussian, etc. than is\r\nθ.\r\n“generated quantities”\r\n\\(\\Rightarrow\\) Transformation to improve fit Comments are indicated by\r\n// in Stan. The\r\nwrite(\"model code\", \"file_name\") bit allows us to write the\r\nStan model in our R script and output the file to the working directory\r\n(or you can set a different file path)\r\nExample 2: Insignificant\r\npredictor\r\n\r\nRegression.Data <- as.matrix(read.csv(\"data/DtSim4RegANOVA.csv\", header=TRUE, sep=\",\"))\r\ntail(Regression.Data)\r\n          Output     Input1      Input2\r\n[495,] 2.4054442  0.9276934  0.07278244\r\n[496,] 1.8663026 -0.3678520  1.51715986\r\n[497,] 1.3590146  0.5369795  0.96209003\r\n[498,] 3.1836007  1.0171332 -0.56660564\r\n[499,] 2.3615061  1.1637966  0.07815352\r\n[500,] 0.8483407  1.1775607  1.59720356\r\n\r\nPrepare the data for Stan.\r\n\r\nNtotal <- nrow(Regression.Data)\r\nx <- Regression.Data[ ,2:3]\r\ntail(x)\r\n           Input1      Input2\r\n[495,]  0.9276934  0.07278244\r\n[496,] -0.3678520  1.51715986\r\n[497,]  0.5369795  0.96209003\r\n[498,]  1.0171332 -0.56660564\r\n[499,]  1.1637966  0.07815352\r\n[500,]  1.1775607  1.59720356\r\nNx <- ncol(x)\r\ny <- Regression.Data[ ,1]\r\ndataListInsig <- list(Ntotal=Ntotal, \r\n                      y=y, \r\n                      x=as.matrix(x), \r\n                      Nx=Nx)\r\n\r\nRun MCMC using the same DSO.\r\n\r\nfit2 <- sampling(RobustMultipleRegressionDso, data=dataListInsig,\r\n                 pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                 iter=5000, chains = 2, cores = 2)\r\n\r\n\r\nlaunch_shinystan(fit2)\r\n\r\nAnalyze the results.\r\n\r\nsummary(fit2)$summary[,c(1,3,4,8,9)] #mean, sd, 2.5%, 97.5%, n_eff\r\n                 mean          sd          2.5%         97.5%\r\nbeta0    1.217597e+00  0.05182680    1.11612951    1.31636194\r\nbeta[1]  7.999726e-01  0.02812159    0.74453626    0.85437808\r\nbeta[2]  9.416795e-03  0.02736786   -0.04397004    0.06323134\r\nnu       5.276307e+01 34.06776500   13.74259262  142.96095320\r\nsigma    5.979875e-01  0.02144590    0.55558445    0.64151495\r\nlp__    -1.817471e+02  1.53474481 -185.40442023 -179.67956203\r\n           n_eff\r\nbeta0   5838.281\r\nbeta[1] 6158.199\r\nbeta[2] 5787.610\r\nnu      4567.018\r\nsigma   4814.577\r\nlp__    2650.410\r\npairs(fit2,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit2,pars=\"nu\")\r\n\r\nplot(fit2,pars=\"sigma\")\r\n\r\nplot(fit2,pars=\"beta0\")\r\n\r\nplot(fit2,pars=\"beta[1]\")\r\n\r\nplot(fit2,pars=\"beta[2]\")\r\n\r\n\r\nWe see that parameter \\(\\beta_2\\) is\r\nnot significant.\r\nHowever, there is no strong correlation or redundancy between the\r\npredictors.\r\nCompare with the output of linear model\r\n\r\npairs(Regression.Data)\r\n\r\nsummary(lm(Output~., data=as.data.frame(Regression.Data)))\r\n\r\nCall:\r\nlm(formula = Output ~ ., data = as.data.frame(Regression.Data))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.76631 -0.39358 -0.01411  0.40432  1.91861 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  1.21480    0.05179  23.458   <2e-16 ***\r\nInput1       0.80116    0.02819  28.423   <2e-16 ***\r\nInput2       0.00970    0.02787   0.348    0.728    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6107 on 497 degrees of freedom\r\nMultiple R-squared:  0.6204,    Adjusted R-squared:  0.6188 \r\nF-statistic: 406.1 on 2 and 497 DF,  p-value: < 2.2e-16\r\n\r\nExample 3: Correlated\r\npredictors\r\nStrong correlation\r\n\r\nset.seed(03192021)\r\nNtotal <- 500\r\nx1 <- rnorm(Ntotal, mean = 20, sd = 4)\r\nx2 <- 1 - 1.5*x1 + rnorm(Ntotal, mean=0, sd = .1)\r\nx <- cbind(x1,x2)      \r\n\r\nplot(x)\r\n\r\nNx <- ncol(x)\r\ny <- 4 + .2*x[,1] + 3*x[,2]+rnorm(Ntotal, mean = 0, sd = 1)\r\nplot(x[,1],y)\r\n\r\nplot(x[,2],y)\r\n\r\nfitlm<-lm(y~x[,1]+x[,2])\r\nsummary(fitlm)\r\n\r\nCall:\r\nlm(formula = y ~ x[, 1] + x[, 2])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7765 -0.7585  0.0172  0.6577  3.1721 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.5619     0.4878   9.353  < 2e-16 ***\r\nx[, 1]       -0.5468     0.6707  -0.815    0.415    \r\nx[, 2]        2.5034     0.4479   5.589 3.76e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9988 on 497 degrees of freedom\r\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9962 \r\nF-statistic: 6.53e+04 on 2 and 497 DF,  p-value: < 2.2e-16\r\ndrop1(fitlm)\r\nSingle term deletions\r\n\r\nModel:\r\ny ~ x[, 1] + x[, 2]\r\n       Df Sum of Sq    RSS     AIC\r\n<none>              495.85  1.8338\r\nx[, 1]  1    0.6632 496.51  0.5021\r\nx[, 2]  1   31.1702 527.02 30.3165\r\n\r\n\r\ndataListShrink2 <- list(Ntotal=Ntotal, y=y, x=as.matrix(x), Nx=Nx)\r\n\r\nNote that actual coefficient for x[,1] is \\(+0.2\\), but slope on the plot plot(x[,1],y)\r\nis negative.\r\nAlso note that estimated model coefficients are different from\r\nactual because of correlation:\r\n\r\ncbind(actual=c(4,.2,3),estimated=fitlm$coefficients)\r\n            actual  estimated\r\n(Intercept)    4.0  4.5619222\r\nx[, 1]         0.2 -0.5468278\r\nx[, 2]         3.0  2.5034438\r\n\r\nRun the chains and analyze the results.\r\n\r\ntStart<-proc.time()\r\nfit3<-sampling(RobustMultipleRegressionDso,\r\n               data = dataListShrink2,\r\n               pars = c('beta0', 'beta', 'nu', 'sigma'),\r\n               iter = 5000, chains = 2, cores = 2)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.51    0.39   92.09 \r\n\r\nHow long did it take to run this MCMC? Why so\r\nlong?\r\nMonte Carlo methods assume that the samples are independent. This is\r\nusually not the case for sequential draws in Markov Chain Monte Carlo\r\n(MCMC) sampling, motivating the use of thinning. In thinning, we keep\r\nevery T sample (\\(P(x \\le t)\\)) and\r\nthrow away the other samples. For some types of MCMC (notably, Gibbs\r\nsampling), highly correlated variables result in highly correlated\r\nsamples (i.e., definitely not independent), requiring a large T to\r\ncompensate. Large values of T mean a large computational cost for each\r\neffective sample (i.e., for each sample that is kept).\r\nNote: The parameter thin allows the user to specify if and how\r\nmuch the MCMC chains should be thinned out before storing them. By\r\ndefault thin = 1 is used, which corresponds to keeping all values. A\r\nvalue thin = 10 would result in keeping every 10th value and discarding\r\nall other values.\r\nCheck convergence in shiny.\r\n\r\nlaunch_shinystan(fit3)\r\n\r\n\r\nstan_dens(fit3)\r\n\r\nstan_ac(fit3, separate_chains = T)\r\n\r\nsummary(fit3)$summary[,c(1,3,4,8,9)]\r\n               mean          sd        2.5%       97.5%    n_eff\r\nbeta0     4.5857595  0.48813790   3.6314232   5.5544965 2114.939\r\nbeta[1]  -0.5967437  0.67710628  -1.9314954   0.7388229 1713.891\r\nbeta[2]   2.4699613  0.45229741   1.5765319   3.3619226 1712.967\r\nnu       56.0426640 33.61895666  15.8382163 143.5659182 2718.137\r\nsigma     0.9816801  0.03355264   0.9166907   1.0483195 2731.866\r\nlp__    967.3463697  1.56455325 963.5132940 969.3727437 1911.020\r\npairs(fit3,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit3,pars=\"nu\")\r\n\r\nplot(fit3,pars=\"sigma\")\r\n\r\nplot(fit3,pars=\"beta0\")\r\n\r\nplot(fit3,pars=\"beta[1]\")\r\n\r\nplot(fit3,pars=\"beta[2]\")\r\n\r\n\r\n\r\nGeneral signs of collinear predictors:\r\n- High correlation between slopes (compensating sign)\r\n- Wide posterior distributions for slopes\r\n- Increased autocorrelation for slopes\r\n\r\n\r\npairs(cbind(y,x1,x2))\r\n\r\ncbind(actual=c(4,.2,3),estimatedLm=fitlm$coefficients,estimatedBayes=summary(fit3)$summary[1:3,1])\r\n            actual estimatedLm estimatedBayes\r\n(Intercept)    4.0   4.5619222      4.5857595\r\nx[, 1]         0.2  -0.5468278     -0.5967437\r\nx[, 2]         3.0   2.5034438      2.4699613\r\n\r\nLinear model shows the same information as Bayesian.\r\nCollinearity\r\nIn case when predictors have strong collinearity, linear model may\r\nstop working.\r\nSimulate the same model as in the previous section, but make predictors\r\ncollinear.\r\n\r\nset.seed(03192021)\r\nNtotal <- 500\r\nx1 <- rnorm(Ntotal, mean = 20, sd = 4)\r\nx2<-1-1.5*x1+rnorm(Ntotal, mean=0, sd = .000001) # sd closes to 0\r\nx<-cbind(x1,x2)           \r\nplot(x)\r\n\r\nNx <- ncol(x)\r\ny <- 4 + .2*x[,1] + 3*x[,2]+rnorm(Ntotal, mean = 0, sd = 1)\r\nplot(x[,1],y)\r\n\r\nplot(x[,2],y)\r\n\r\n\r\n\r\ndataListShrink2c <- list(Ntotal=Ntotal, y=y, x=as.matrix(x), Nx=Nx)\r\n(lmFit <- lm(y~x1+x2))\r\n\r\nCall:\r\nlm(formula = y ~ x1 + x2)\r\n\r\nCoefficients:\r\n(Intercept)           x1           x2  \r\n      7.094       -4.303           NA  \r\nsummary(lmFit)\r\n\r\nCall:\r\nlm(formula = y ~ x1 + x2)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7626 -0.7636  0.0244  0.6410  3.1747 \r\n\r\nCoefficients: (1 not defined because of singularities)\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  7.09394    0.24447   29.02   <2e-16 ***\r\nx1          -4.30334    0.01189 -361.96   <2e-16 ***\r\nx2                NA         NA      NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9991 on 498 degrees of freedom\r\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9962 \r\nF-statistic: 1.31e+05 on 1 and 498 DF,  p-value: < 2.2e-16\r\ndrop1(lmFit)\r\nSingle term deletions\r\n\r\nModel:\r\ny ~ x1 + x2\r\n       Df Sum of Sq    RSS    AIC\r\n<none>              497.08 1.0687\r\nx1      0 0.0001417 497.08 1.0688\r\nx2      0 0.0000000 497.08 1.0687\r\n\r\nLinear model stops working.\r\nSimulate Markov chains.\r\n\r\ncbind(actual=c(4,.2,3),estimated=fitlm$coefficients)\r\n            actual  estimated\r\n(Intercept)    4.0  4.5619222\r\nx[, 1]         0.2 -0.5468278\r\nx[, 2]         3.0  2.5034438\r\n\r\nRun the chains and analyze the results.\r\n\r\ntStart <- proc.time()\r\nfit3c <- sampling(RobustMultipleRegressionDso,\r\n                data=dataListShrink2c,\r\n                pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                iter=5000, chains = 1, cores = 2)\r\n\r\nSAMPLING FOR MODEL '8e2e8be6f09a2541ae94da0604a417ec' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 5000 [  0%]  (Warmup)\r\nChain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)\r\nChain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)\r\nChain 1: Iteration: 1500 / 5000 [ 30%]  (Warmup)\r\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 2501 / 5000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 155.086 seconds (Warm-up)\r\nChain 1:                114.928 seconds (Sampling)\r\nChain 1:                270.014 seconds (Total)\r\nChain 1: \r\ntEnd <- proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n 261.57    0.17  270.09 \r\n\r\nWith collinear predictors model definitely takes much longer time to\r\nsimulate.\r\n\r\nstan_dens(fit3c)\r\n\r\nstan_ac(fit3c, separate_chains = T)\r\n\r\nsummary(fit3c)$summary[,c(1,3,4,8,9)]\r\n               mean          sd        2.5%      97.5%    n_eff\r\nbeta0     5.2869784  4.12513863  -2.5410297  13.834655 258.6426\r\nbeta[1]  -1.5985501  6.17402618 -14.3329219  10.098421 255.8424\r\nbeta[2]   1.8031011  4.11591117  -6.6807200   9.601809 255.7672\r\nnu       57.5235574 34.84527395  15.7981673 148.866690 605.1602\r\nsigma     0.9834476  0.03440985   0.9179121   1.052713 331.5993\r\nlp__    967.5976423  1.48319659 964.0943396 969.579397 845.7838\r\npairs(fit3c,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit3c,pars=\"nu\")\r\n\r\nplot(fit3c,pars=\"sigma\")\r\n\r\nplot(fit3c,pars=\"beta0\")\r\n\r\nplot(fit3c,pars=\"beta[1]\")\r\n\r\nplot(fit3c,pars=\"beta[2]\")\r\n\r\n\r\nMarkov chains may go over limit on tree depths (yellow dots on pairs\r\ngraph).\r\nBut Bayesian method still works. It shows that one or both of the slopes\r\nare not significantly different from zero.\r\nShrinkage of regression\r\ncoefficients\r\nWhen there are many candidate predictors in the model it may be\r\nuseful to “motivate” them to become closer to zero if they are not very\r\nstrong.\r\nOne way to do it is to:\r\nSet a prior distribution for slopes as t-Student instead of\r\nnormal;\r\nMake mean of that distribution equal to zero;\r\nMake normality parameter \\(\\nu\\)\r\nsmall and dispersion parameter \\(\\sigma\\) also small\r\n\r\nSmall \\(\\sigma\\) forces slopes to\r\nshrink towards zero mean. At the same time, small \\(\\nu\\) makes the tails fat enough to allow\r\nsome strong slopes to be outliers.\r\nParameter \\(\\sigma\\) of the prior\r\nfor regression coefficients \\(\\beta_j\\)\r\ncan be either fixed, or given its own prior and estimated.\r\nIn the former case, all coefficients will be forced to have the same\r\nregularizator, if it is random and estimated from the same data then\r\nthere is mutual influence between \\(\\sigma\\) and regression coefficients: if\r\nmany of them are close to zero then \\(\\sigma\\) is going to be smaller, which in\r\nturn pushes coefficients even closer to zero.\r\nThese approaches reminds us the Ridge/LASSO/Elastic net regression in\r\nMachine Learning models.\r\nTwo significant predictors\r\nUse the same data dataListRegression as in the above\r\nsection.\r\nDescribe the model.\r\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} +\r\n\\beta_2 x_{i,2} + \\epsilon_i \\\\\r\ny_i \\sim t(\\mu_i, \\sigma, \\nu) \\\\\r\n\\\\\r\n\\text{where mean, scale and degree of freedom, respectively, would be}\r\n\\\\\r\n\\mu_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} \\\\\r\n\\beta_0 \\sim N(M_0, S_0) \\ ; \\ \\beta_j \\sim t(\\mu_j, \\nu_j,\r\n\\sigma_{\\beta}) \\\\\r\n\\sigma_{\\beta} \\sim gamma(shape,rate) \\\\\r\n\\sigma \\sim Unif(L, H) \\\\\r\n\\nu = \\gamma^{\\prime} + 1, \\ \\text{where} \\ \\gamma^{\\prime} \\sim\r\nexp(\\lambda)\\]\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int<lower=1> Nx;\r\n    vector[Ntotal] y;\r\n    matrix[Ntotal, Nx] x;\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    vector[Ntotal] zy; // normalized\r\n    vector[Nx] meanX;\r\n    vector[Nx] sdX;\r\n    matrix[Ntotal, Nx] zx; // normalized\r\n    \r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    zy = (y - meanY) / sdY;\r\n    for (j in 1:Nx) {\r\n        meanX[j] = mean(x[,j]);\r\n        sdX[j] = sd(x[,j]);\r\n        for (i in 1:Ntotal) {\r\n            zx[i,j] = (x[i,j] - meanX[j]) / sdX[j];\r\n        }\r\n    }\r\n}\r\nparameters {\r\n    real zbeta0;\r\n    real<lower=0> sigmaBeta;\r\n    vector[Nx] zbeta;\r\n    real<lower=0> nu;\r\n    real<lower=0> zsigma;\r\n}\r\ntransformed parameters{\r\n    vector[Ntotal] zy_hat;\r\n    zy_hat = zbeta0 + zx * zbeta;\r\n}\r\nmodel {\r\n    zbeta0 ~ normal(0, 2);\r\n    sigmaBeta ~ gamma(2.3,1.3); // mode=(alpha-1)/beta, var=alpha/beta^2\r\n    zbeta  ~ student_t(1.0/30.0, 0, sigmaBeta);\r\n    nu ~ exponential(1/30.0);\r\n    zsigma ~ uniform(1.0E-5 , 1.0E+1);\r\n    zy ~ student_t(1+nu, zy_hat, zsigma);\r\n}\r\ngenerated quantities { \r\n    // Transform to original scale:\r\n    real beta0; \r\n    vector[Nx] beta;\r\n    real sigma;\r\n    // .* and ./ are element-wise product and divide\r\n    beta0 = zbeta0*sdY  + meanY - sdY * sum( zbeta .* meanX ./ sdX );\r\n    beta = sdY * ( zbeta ./ sdX );\r\n    sigma = zsigma * sdY;\r\n} \"\r\n\r\nGamma distribution prior for sigmaBeta is selected to have relatively\r\nlow mode 1.\r\n\r\nxGamma <- seq(from = .00001, to = 10, by = .001)\r\nplot(xGamma,dgamma(xGamma,shape=2.3,rate=1.3),type=\"l\")\r\n\r\nxGamma[which.max(dgamma(xGamma,shape=2.3,rate=1.3))]\r\n[1] 1.00001\r\n\r\nCreate DSO.\r\n\r\nRegressionShrinkDso <- stan_model(model_code = modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# save(RegressionShrinkDso, file = \"data/DSOShrunkMultRegr.Rds\")\r\nload(\"data/DSOShrunkMultRegr.Rds\")\r\n\r\nGenerate Markov chains in case of 2 significant predictors.\r\n\r\ntStart<-proc.time()\r\n# fit model\r\nfit4 <- sampling(RegressionShrinkDso, \r\n             data=dataListRegression, \r\n             pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n             iter=5000, chains = 2, cores = 2\r\n)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.40    0.25   18.60 \r\n\r\nAnalyze fitted model using shinystan\r\n\r\nlaunch_shinystan(fit4)\r\n\r\n\r\nstan_dens(fit4)\r\n\r\nstan_ac(fit4, separate_chains = T)\r\n\r\nsummary(fit4)$summary[,c(1,3,4,8,9)]\r\n                  mean           sd         2.5%       97.5%    n_eff\r\nbeta0        4.5843612  0.228604011    4.1393299    5.027146 8748.136\r\nbeta[1]      1.0735281  0.010973133    1.0524275    1.095341 8200.715\r\nbeta[2]      2.9950789  0.007839406    2.9796239    3.009790 7482.511\r\nnu          61.3810270 35.734724611   17.0630928  149.281989 5570.822\r\nsigma        0.9613439  0.033176999    0.8953832    1.028195 5375.540\r\nsigmaBeta    1.3993861  0.899552223    0.2216032    3.632021 6406.796\r\nlp__      1010.3909262  1.749705508 1006.2179766 1012.779996 2263.114\r\npairs(fit4,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit4,pars=\"nu\")\r\n\r\nplot(fit4,pars=\"sigma\")\r\n\r\nplot(fit4,pars=\"beta0\")\r\n\r\nplot(fit4,pars=\"beta[1]\")\r\n\r\nplot(fit4,pars=\"beta[2]\")\r\n\r\n\r\nAnalysis and comparison\r\nCompare posterior mean values and 95% HDI with fit1\r\n(same model, but with no shrinkage).\r\n\r\ncbind(summary(fit1)$summary[1:3,c(1,4,8)],\r\n      summary(fit4)$summary[1:3,c(1,4,8)])\r\n            mean     2.5%    97.5%     mean     2.5%    97.5%\r\nbeta0   4.578031 4.130417 5.025614 4.584361 4.139330 5.027146\r\nbeta[1] 1.073828 1.052198 1.095054 1.073528 1.052427 1.095341\r\nbeta[2] 2.995153 2.979745 3.011046 2.995079 2.979624 3.009790\r\n\r\nMean values of both fits seem very similar.\r\nCheck widths of the HDI for coefficients.\r\n\r\ncbind(summary(fit1)$summary[1:3,c(8)]-summary(fit1)$summary[1:3,c(4)],\r\n      summary(fit4)$summary[1:3,c(8)]-summary(fit4)$summary[1:3,c(4)])\r\n              [,1]       [,2]\r\nbeta0   0.89519719 0.88781592\r\nbeta[1] 0.04285560 0.04291323\r\nbeta[2] 0.03130079 0.03016606\r\n\r\nShrinkage can be noticed after third digit of all coefficients.\r\nIn this example both slopes are significant and they practically did not\r\nshrink.\r\nFor comparison fit linear model, ridge and lasso regressions to the\r\nsame data.\r\n1- Linear model.\r\n\r\nlmFit<-lm(dataListRegression$y~dataListRegression$x[,1]+dataListRegression$x[,2])\r\n\r\n2- Ridge.\r\n\r\nlibrary(glmnet)\r\nset.seed(15)\r\ncv.outRidge <- cv.glmnet(x = dataListRegression$x, \r\n                         y = dataListRegression$y, \r\n                         alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 1.680471\r\nridgeFit <- glmnet(x = dataListRegression$x, y=dataListRegression$y,\r\n                   alpha = 0, lambda = bestlam, standardize = F)\r\n(ridge.coef <- predict(ridgeFit,type=\"coefficients\", s = bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 4.768742\r\nV1          1.068697\r\nV2          2.986144\r\n\r\n3- Lasso.\r\n\r\nset.seed(15)\r\ncv.outLasso <- cv.glmnet(x = dataListRegression$x, \r\n                         y = dataListRegression$y, \r\n                         alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 0.08363741\r\nlassoFit<-glmnet(x=dataListRegression$x,y=dataListRegression$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\n(lasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 4.689546\r\nV1          1.069233\r\nV2          2.992895\r\n\r\nCompare coefficients from all 5 models\r\n\r\ncomparison<-cbind(summary(fit1)$summary[1:3,c(1,4,8)],\r\n      summary(fit4)$summary[1:3,c(1,4,8)],\r\n      Ridge=ridge.coef,\r\n      Lasso=lasso.coef,\r\n      Linear=lmFit$coefficients)\r\ncolnames(comparison)<-c(paste(\"NoShrinkage\",c(\"mean\",\"2.5%\",\"97.5%\"),sep=\"_\"),\r\n                        paste(\"Shrinkage\",c(\"mean\",\"2.5%\",\"97.5%\"),sep=\"_\"),\r\n                        \"Ridge\",\"Lasso\",\"Linear\")\r\nt(comparison)\r\n9 x 3 sparse Matrix of class \"dgCMatrix\"\r\n                     beta0  beta[1]  beta[2]\r\nNoShrinkage_mean  4.578031 1.073828 2.995153\r\nNoShrinkage_2.5%  4.130417 1.052198 2.979745\r\nNoShrinkage_97.5% 5.025614 1.095054 3.011046\r\nShrinkage_mean    4.584361 1.073528 2.995079\r\nShrinkage_2.5%    4.139330 1.052427 2.979624\r\nShrinkage_97.5%   5.027146 1.095341 3.009790\r\nRidge             4.768742 1.068697 2.986144\r\nLasso             4.689546 1.069233 2.992895\r\nLinear            4.565419 1.074206 2.995385\r\n\r\nAll models show practically no shrinkage relative to linear\r\nmodel.\r\nBoth Ridge and Lasso regression have too high estimates of\r\nintercept.\r\nInsignificant predictor\r\nShrink estimates from data dataListInsig.\r\n\r\ntStart<-proc.time()\r\n# fit model\r\nfit5 <- sampling (RegressionShrinkDso, \r\n             data=dataListInsig, \r\n             pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n             iter=5000, chains = 2, cores = 2\r\n)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.44    0.17   18.02 \r\n\r\nWe can analyze fitted model with shinystan but\r\n\r\nstan_dens(fit5)\r\n\r\nstan_ac(fit5, separate_chains = T)\r\n\r\nsummary(fit5)$summary[,c(1,3,4,8,9)]\r\n                   mean          sd          2.5%         97.5%\r\nbeta0      1.221117e+00  0.05300762    1.11769338    1.32774329\r\nbeta[1]    7.988002e-01  0.02885518    0.74227587    0.85594136\r\nbeta[2]    7.983679e-03  0.02713646   -0.04570233    0.06256174\r\nnu         5.257628e+01 33.19871103   13.71139258  137.97027648\r\nsigma      5.977651e-01  0.02104643    0.55653680    0.63914284\r\nsigmaBeta  1.043781e+00  0.83037712    0.10282820    3.17391417\r\nlp__      -1.850178e+02  1.72628557 -189.19759519 -182.70019632\r\n             n_eff\r\nbeta0     6549.983\r\nbeta[1]   6984.701\r\nbeta[2]   6777.038\r\nnu        4536.272\r\nsigma     5500.091\r\nsigmaBeta 5994.464\r\nlp__      2157.592\r\npairs(fit5,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit5,pars=\"nu\")\r\n\r\nplot(fit5,pars=\"sigma\")\r\n\r\nplot(fit5,pars=\"beta0\")\r\n\r\nplot(fit5,pars=\"beta[1]\")\r\n\r\nplot(fit5,pars=\"beta[2]\")\r\n\r\n\r\nThis time posterior density of \\(\\beta_2\\) (beta[2]) is concentrated at\r\nzero.\r\nAnalysis and comparison\r\nCompare mean levels and HDI widths for fits with and without\r\nshrinkage.\r\n\r\ncbind(summary(fit2)$summary[1:3,c(1,4,8)],\r\n      summary(fit5)$summary[1:3,c(1,4,8)])\r\n               mean        2.5%      97.5%        mean        2.5%\r\nbeta0   1.217597421  1.11612951 1.31636194 1.221117106  1.11769338\r\nbeta[1] 0.799972585  0.74453626 0.85437808 0.798800184  0.74227587\r\nbeta[2] 0.009416795 -0.04397004 0.06323134 0.007983679 -0.04570233\r\n             97.5%\r\nbeta0   1.32774329\r\nbeta[1] 0.85594136\r\nbeta[2] 0.06256174\r\n\r\n\r\ncbind(summary(fit2)$summary[1:3,c(8)]-summary(fit2)$summary[1:3,c(4)],\r\n      summary(fit5)$summary[1:3,c(8)]-summary(fit5)$summary[1:3,c(4)])\r\n             [,1]      [,2]\r\nbeta0   0.2002324 0.2100499\r\nbeta[1] 0.1098418 0.1136655\r\nbeta[2] 0.1072014 0.1082641\r\n\r\nParameters shrunk a little more this time, second coefficient shrunk\r\nto zero.\r\nAgain, fit linear model, ridge and lasso regressions to the same\r\ndata.\r\n1- Linear model.\r\n\r\nlmFit<-lm(dataListInsig$y~dataListInsig$x[,1]+dataListInsig$x[,2])\r\n\r\n2- Ridge.\r\n\r\nset.seed(15)\r\ncv.outRidge=cv.glmnet(x=dataListInsig$x,y=dataListInsig$y,alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 0.07783229\r\nridgeFit<-glmnet(x=dataListInsig$x,y=dataListInsig$y,\r\n                 alpha=0,lambda=bestlam,standardize = F)\r\n(ridge.coef<-predict(ridgeFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                     s1\r\n(Intercept) 1.287098608\r\nInput1      0.739131424\r\nInput2      0.004155875\r\n\r\n3- Lasso.\r\n\r\nset.seed(15)\r\ncv.outLasso=cv.glmnet(x=dataListInsig$x,y=dataListInsig$y,alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 0.02067294\r\nlassoFit<-glmnet(x=dataListInsig$x,y=dataListInsig$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\n(lasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 1.249387\r\nInput1      0.778466\r\nInput2      .       \r\n\r\nCompare coefficients from all 3 models.\r\n\r\ncomparison<-cbind(summary(fit2)$summary[1:3,c(1)],\r\n      summary(fit5)$summary[1:3,c(1)],\r\n      Ridge=ridge.coef,\r\n      Lasso=lasso.coef,\r\n      Linear=lmFit$coefficients)\r\ncolnames(comparison)<-c(\"NoShrinkage\",\"Shrinkage\",\"Ridge\",\"Lasso\",\"Linear\")\r\nt(comparison)\r\n5 x 3 sparse Matrix of class \"dgCMatrix\"\r\n            (Intercept)    Input1      Input2\r\nNoShrinkage    1.217597 0.7999726 0.009416795\r\nShrinkage      1.221117 0.7988002 0.007983679\r\nRidge          1.287099 0.7391314 0.004155875\r\nLasso          1.249387 0.7784660 .          \r\nLinear         1.214802 0.8011573 0.009700042\r\n\r\nAll models correctly exclude second coefficient.\r\nRidge shrunk both slopes more than other models.\r\nThere is again tendency for Ridge and Lasso to overestimate\r\nintercept.\r\nCorrelated predictors\r\nShrink coefficients estimated from dataListShrink2.\r\n\r\ntStart<-proc.time()\r\n# fit model\r\nfit6 <- sampling (RegressionShrinkDso, \r\n             data=dataListShrink2, \r\n             pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n             iter=5000, chains = 2, cores = 2\r\n)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.54    0.10   91.97 \r\n\r\nWe could analyze model with shinystan but let’s check\r\ndensities, pairs and individual plots of parameters.\r\n\r\nstan_dens(fit6)\r\n\r\nstan_ac(fit6, separate_chains = T)\r\n\r\nsummary(fit6)$summary[,c(1,3,4,8,9)]\r\n                 mean          sd        2.5%       97.5%    n_eff\r\nbeta0       4.5437360  0.47198344   3.6607788   5.5095532 1845.144\r\nbeta[1]    -0.5261239  0.63908279  -1.8992740   0.6344545 1575.212\r\nbeta[2]     2.5172117  0.42658592   1.6035586   3.2916493 1575.367\r\nnu         56.5744375 34.07819768  14.7590894 143.3833364 3428.014\r\nsigma       0.9804926  0.03427887   0.9135965   1.0470397 3347.567\r\nsigmaBeta   1.2388283  0.85960126   0.1607706   3.3968145 3997.294\r\nlp__      963.8218087  1.75910037 959.4784596 966.1987987 2037.995\r\npairs(fit6,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit6,pars=\"nu\")\r\n\r\nplot(fit6,pars=\"sigma\")\r\n\r\nplot(fit6,pars=\"beta0\")\r\n\r\nplot(fit6,pars=\"beta[1]\")\r\n\r\nplot(fit6,pars=\"beta[2]\")\r\n\r\n\r\nAnalysis and comparison\r\nShow mean values and HDI.\r\n\r\ncbind(summary(fit3)$summary[1:3,c(1,4,8)],\r\n      summary(fit6)$summary[1:3,c(1,4,8)])\r\n              mean      2.5%     97.5%       mean      2.5%     97.5%\r\nbeta0    4.5857595  3.631423 5.5544965  4.5437360  3.660779 5.5095532\r\nbeta[1] -0.5967437 -1.931495 0.7388229 -0.5261239 -1.899274 0.6344545\r\nbeta[2]  2.4699613  1.576532 3.3619226  2.5172117  1.603559 3.2916493\r\ncbind(summary(fit3)$summary[1:3,c(8)]-summary(fit3)$summary[1:3,c(4)],\r\n      summary(fit6)$summary[1:3,c(8)]-summary(fit6)$summary[1:3,c(4)])\r\n            [,1]     [,2]\r\nbeta0   1.923073 1.848774\r\nbeta[1] 2.670318 2.533728\r\nbeta[2] 1.785391 1.688091\r\n\r\nIn this example \\(\\beta_1\\) shrunk\r\nmore significantly and is not different from zero.\r\nAt the same time \\(\\beta_2\\) has become\r\nmore different from zero.\r\nRegularization reinforced one of the two correlated predictors while\r\ndumping the other.\r\nAgain, fit linear model, ridge and lasso regressions to the same\r\ndata.\r\n1- Linear model.\r\n\r\nlmFit<-lm(dataListShrink2$y~dataListShrink2$x[,1]+dataListShrink2$x[,2])\r\nsummary(lmFit)\r\n\r\nCall:\r\nlm(formula = dataListShrink2$y ~ dataListShrink2$x[, 1] + dataListShrink2$x[, \r\n    2])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7765 -0.7585  0.0172  0.6577  3.1721 \r\n\r\nCoefficients:\r\n                       Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)              4.5619     0.4878   9.353  < 2e-16 ***\r\ndataListShrink2$x[, 1]  -0.5468     0.6707  -0.815    0.415    \r\ndataListShrink2$x[, 2]   2.5034     0.4479   5.589 3.76e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9988 on 497 degrees of freedom\r\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9962 \r\nF-statistic: 6.53e+04 on 2 and 497 DF,  p-value: < 2.2e-16\r\n\r\n2- Ridge.\r\n\r\nset.seed(15)\r\ncv.outRidge=cv.glmnet(x=dataListShrink2$x,y=dataListShrink2$y,alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 1.61435\r\nridgeFit<-glmnet(x=dataListShrink2$x,y=dataListShrink2$y,\r\n                 alpha=0,lambda=bestlam,standardize = F)\r\n(ridge.coef<-predict(ridgeFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                   s1\r\n(Intercept)  4.943942\r\nx1          -1.425696\r\nx2           1.910632\r\n\r\n3- Lasso.\r\n\r\nset.seed(15)\r\ncv.outLasso=cv.glmnet(x=dataListShrink2$x,y=dataListShrink2$y,alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 0.1062134\r\nlassoFit<-glmnet(x=dataListShrink2$x,y=dataListShrink2$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\n(lasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 4.116010\r\nx1          .       \r\nx2          2.865189\r\n\r\nCompare coefficients from all 3 models.\r\n\r\ncomparison<-cbind(summary(fit3)$summary[1:3,c(1)],\r\n      summary(fit6)$summary[1:3,c(1)],\r\n      Ridge=ridge.coef,\r\n      Lasso=lasso.coef,\r\n      Linear=lmFit$coefficients)\r\ncolnames(comparison)<-c(\"NoShrinkage\",\"Shrinkage\",\"Ridge\",\"Lasso\",\"Linear\")\r\nt(comparison)\r\n5 x 3 sparse Matrix of class \"dgCMatrix\"\r\n            (Intercept)         x1       x2\r\nNoShrinkage    4.585760 -0.5967437 2.469961\r\nShrinkage      4.543736 -0.5261239 2.517212\r\nRidge          4.943942 -1.4256958 1.910632\r\nLasso          4.116010  .         2.865189\r\nLinear         4.561922 -0.5468278 2.503444\r\n\r\nAll models correctly exclude first slope.\r\nLasso does it decisively, making slope \\(\\beta_1\\) and exactly equal to \\(0\\).\r\nLasso also estimated intercept and \\(\\beta_2\\) more accurately than other\r\nmodels: recall that for this data set we simulate \\(\\beta_0=4, \\beta_2=3\\).\r\nIs school financing\r\nnecessary?\r\nAnalysis of SAT scores, example from Kruschke, 2015, section\r\n18.3.\r\nThese data are analyzed in the article\r\nby Deborah Lynn Guber.\r\nThe variables observed are:\r\nstates (State)\r\nthe mean SAT score by state (SATV, SATM and SATT),\r\namount of money spent by student (Spend),\r\npercent of students who take SAT (PrcntTake) and\r\nother variables (student to teacher ratio, teacher salary).\r\nRead the data from file Guber1999data.csv available at\r\nKruschke,\r\n2015.\r\n\r\nmyData = read.csv(\"data/Guber1999data.csv\")  # section 18.3 @ Kruschke\r\nhead(myData)\r\n       State Spend StuTeaRat Salary PrcntTake SATV SATM SATT\r\n1    Alabama 4.405      17.2 31.144         8  491  538 1029\r\n2     Alaska 8.963      17.6 47.951        47  445  489  934\r\n3    Arizona 4.778      19.3 32.175        27  448  496  944\r\n4   Arkansas 4.459      17.1 28.934         6  482  523 1005\r\n5 California 4.992      24.0 41.078        45  417  485  902\r\n6   Colorado 5.443      18.4 34.571        29  462  518  980\r\npairs(myData[,-c(1,6:7)])\r\n\r\nplot(myData$Spend,myData$SATT)\r\n\r\nsummary(lm(myData$SATT~myData$Spend))$coeff\r\n               Estimate Std. Error   t value     Pr(>|t|)\r\n(Intercept)  1089.29372  44.389950 24.539197 8.168276e-29\r\nmyData$Spend  -20.89217   7.328209 -2.850925 6.407965e-03\r\n\r\nThe plots show that mean SAT score is negatively correlated with\r\namount of money states spend per student. These results were used in hot\r\ndebates about spending money on education to support argument in favor\r\nof reducing public support for schools.\r\nPrepare the data.\r\nUse the 2 predictors from the file, plus add 12 randomly generated\r\nnuisance predictors.\r\n\r\nNtotal <- nrow(myData)\r\ny <- myData$SATT\r\nx <- cbind(myData$Spend, myData$PrcntTake)\r\ncolnames(x) <- c(\"Spend\",\"PrcntTake\");\r\ndataList2Predict <- list(Ntotal=Ntotal,y=y,x=x,Nx=ncol(x))\r\n# generate 12 spurious predictors:\r\nset.seed(47405)\r\nNxRand <- 12\r\nfor (xIdx in 1:NxRand) {\r\n    xRand = rnorm(Ntotal)\r\n    x = cbind(x, xRand )\r\n    colnames(x)[ncol(x)] = paste0(\"xRand\", xIdx)\r\n}\r\ndataListExtraPredict <- list(Ntotal=Ntotal,y=y,x=x,Nx=ncol(x))\r\n\r\nNo-shrinkage\r\nUse the same model as in the example of the first section:\r\nRobustMultipleRegressionDso.\r\nFirst, run the model with 2 predictors.\r\n\r\nfit_noshrink2Pred <- sampling (RobustMultipleRegressionDso, \r\n                          data=dataList2Predict, \r\n                          pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                          iter=5000, chains = 2, cores = 2)\r\nsummary(fit_noshrink2Pred)$summary[,c(1,4,8)]\r\n               mean       2.5%      97.5%\r\nbeta0   991.4007496 947.253200 1036.31532\r\nbeta[1]  12.8196529   4.235574   21.51116\r\nbeta[2]  -2.8747599  -3.308695   -2.44884\r\nnu       33.4879111   3.455822  114.56115\r\nsigma    31.6173398  24.425564   39.83589\r\nlp__     -0.1425401  -4.083100    1.96456\r\n\r\nIt is clear that the slope of Spend is significantly\r\npositive and slope of PrcntTake is significantly\r\nnegative.\r\nThis shows that the negative correlation between SAT scores and the\r\nmoney spent as seen from the scatterplot is illusory: fewer students\r\nfrom underfunded schools take SAT, but these are only students who apply\r\nfor colleges; students who potentially would receive low SAT scores do\r\nnot apply to college and do not take the test.\r\nRun MCMC for the model with additional nuisance predictors.\r\n\r\nfit_noshrinkExtra <- sampling (RobustMultipleRegressionDso, \r\n                          data=dataListExtraPredict, \r\n                          pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                          iter=5000, chains = 2, cores = 2)\r\n\r\nAnalyze the output with shinystan .\r\n\r\nlaunch_shinystan(fit_noshrinkExtra)\r\n\r\nHere are the results of MCMC.\r\n\r\nstan_ac(fit_noshrinkExtra, separate_chains = T)\r\n\r\npairs(fit_noshrinkExtra,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit_noshrinkExtra,pars=c('beta'))\r\n\r\nstan_dens(fit_noshrinkExtra,pars=c(\"beta0\",\"beta\"))\r\n\r\n\r\nAll densities look symmetrical: mean values of posterior\r\ndistributions can be used as point estimates of betas.\r\n\r\nsummary(fit_noshrinkExtra)$summary[,c(1,4,8)]\r\n               mean       2.5%        97.5%\r\nbeta0    998.618086 952.442828 1044.8766561\r\nbeta[1]   10.234291   1.233829   19.2611967\r\nbeta[2]   -2.721493  -3.179022   -2.2554795\r\nbeta[3]    2.304234 -11.450917   15.7354294\r\nbeta[4]   -4.728814 -14.889133    5.4752469\r\nbeta[5]    6.250301  -7.252438   19.1997960\r\nbeta[6]   -5.696532 -15.225194    3.7955166\r\nbeta[7]    6.989249  -2.925123   16.8434431\r\nbeta[8]    1.908281  -8.072728   12.2999566\r\nbeta[9]    4.283368  -6.011922   14.5423902\r\nbeta[10]   2.651897 -11.337831   17.1438898\r\nbeta[11]  -4.221621 -13.803873    5.1213425\r\nbeta[12] -10.363432 -20.155584   -0.7626764\r\nbeta[13]  -1.894524 -12.602323    8.8656771\r\nbeta[14]   1.710674  -8.973928   12.1135095\r\nnu        31.187292   2.027864  105.5120668\r\nsigma     30.255439  21.367424   39.5905702\r\nlp__       1.240241  -6.398344    6.6903196\r\n\r\nThe variables corresponding to betas are:\r\n\r\ncolnames(x)\r\n [1] \"Spend\"     \"PrcntTake\" \"xRand1\"    \"xRand2\"    \"xRand3\"   \r\n [6] \"xRand4\"    \"xRand5\"    \"xRand6\"    \"xRand7\"    \"xRand8\"   \r\n[11] \"xRand9\"    \"xRand10\"   \"xRand11\"   \"xRand12\"  \r\n\r\nNote that the coefficient for variable Spend is still\r\npositive, but the left side of HDI interval is much closer to zero. The\r\ncoefficient for PrcntTake is still significantly\r\nnegative.\r\nOne of the nuisance predictors happened to be significantly negative:\r\nbeta[12].\r\nAs a result of adding nuisance predictors the accuracy of inference\r\nbecomes lower.\r\nShrinkage\r\nAnalyze the same data with the model encouraging shrinkage of\r\nparameters.\r\nFirst, fit the model without nuisance parameters.\r\n\r\nfit_shrink <- sampling (RegressionShrinkDso, \r\n                        data=dataList2Predict, \r\n                        pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n                        iter=5000, chains = 2, cores = 2)\r\n\r\nCheck convergence in shiny.\r\n\r\nlaunch_shinystan(fit_shrink)\r\n\r\n\r\npairs(fit_shrink,pars=c(\"beta0\",\"beta\",\"nu\",\"sigma\",\"sigmaBeta\"))\r\n\r\nplot(fit_shrink,pars=c('beta'))\r\n\r\nstan_dens(fit_shrink,pars=c('beta'))\r\n\r\nstan_ac(fit_shrink, separate_chains = T)\r\n\r\n\r\nCompare with the fit without nuisance parameters and without\r\nshrinkage.\r\n\r\ncbind(summary(fit_noshrink2Pred)$summary[1:4,c(1,4,8)],\r\n      summary(fit_shrink)$summary[1:4,c(1,4,8)])\r\n             mean       2.5%      97.5%       mean       2.5%\r\nbeta0   991.40075 947.253200 1036.31532 996.071254 951.012629\r\nbeta[1]  12.81965   4.235574   21.51116  11.770134   2.632324\r\nbeta[2]  -2.87476  -3.308695   -2.44884  -2.831416  -3.275607\r\nnu       33.48791   3.455822  114.56115  33.323074   3.756878\r\n              97.5%\r\nbeta0   1042.051358\r\nbeta[1]   20.514489\r\nbeta[2]   -2.361973\r\nnu       109.178594\r\n\r\nFirst variable shrunk closer to zero: mean value is smaller and left\r\nend of the 95%-HDI is closer to zero.\r\nNow fit the model with additional parameters.\r\n\r\nfit_shrinkExtra <- sampling (RegressionShrinkDso, \r\n                        data=dataListExtraPredict, \r\n                        pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n                        iter=5000, chains = 2, cores = 2)\r\nstan_ac(fit_shrinkExtra, separate_chains = T)\r\n\r\npairs(fit_shrinkExtra,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\",\"beta[3]\",\"beta[4]\",\"beta[11]\",\"beta[12]\"))\r\n\r\npairs(fit_shrinkExtra,pars=c(\"nu\",\"sigma\",\"sigmaBeta\"))\r\n\r\nplot(fit_shrinkExtra,pars=c('beta'))\r\n\r\nstan_dens(fit_shrinkExtra,pars=c('beta'))\r\n\r\n\r\nNote characteristic pinched tips of posterior densities for shrunk\r\nvariables.\r\n\r\nsummary(fit_shrinkExtra)$summary[,c(1:4,8)]\r\n                   mean     se_mean         sd          2.5%\r\nbeta0     1009.82602144 4.473582390 26.5068325  9.628679e+02\r\nbeta[1]      8.21954511 1.078486389  5.2814904 -2.802567e-01\r\nbeta[2]     -2.68755852 0.033018532  0.2279845 -3.164852e+00\r\nbeta[3]      1.16429414 0.538365949  3.1397865 -4.187581e+00\r\nbeta[4]     -1.16002937 0.229745431  2.6666174 -8.523255e+00\r\nbeta[5]      1.78068242 0.492660992  3.0628568 -2.483201e+00\r\nbeta[6]     -2.47265483 1.152596733  3.7247172 -1.093501e+01\r\nbeta[7]      2.76863569 1.135585515  3.9366726 -1.659493e+00\r\nbeta[8]      0.86387877 0.267798818  2.3636293 -2.570624e+00\r\nbeta[9]      1.36957487 0.314802537  2.8913159 -2.288281e+00\r\nbeta[10]    -1.39643294 1.838822871  3.7474360 -1.012010e+01\r\nbeta[11]    -1.00165428 0.236339260  2.3005006 -7.569141e+00\r\nbeta[12]    -6.61450384 0.710490722  5.4495075 -1.794561e+01\r\nbeta[13]    -0.38037233 0.100166767  1.9872390 -5.869640e+00\r\nbeta[14]     0.76980004 0.466670211  2.3077738 -3.803671e+00\r\nnu          40.07666808 5.561396626 34.6493271  3.403896e+00\r\nsigma       30.81178901 0.343535668  3.8888139  2.332439e+01\r\nsigmaBeta    0.01966483 0.002562242  0.0270782  5.482108e-04\r\nlp__        21.13792207 0.824755562  6.4649353  9.719103e+00\r\n                  97.5%\r\nbeta0     1058.28908450\r\nbeta[1]     18.23938239\r\nbeta[2]     -2.24414288\r\nbeta[3]      8.71009792\r\nbeta[4]      3.08755229\r\nbeta[5]      9.02094911\r\nbeta[6]      1.45812939\r\nbeta[7]     11.60701517\r\nbeta[8]      7.02230696\r\nbeta[9]      9.15217808\r\nbeta[10]     5.87438345\r\nbeta[11]     2.47309092\r\nbeta[12]     0.61143385\r\nbeta[13]     3.18343802\r\nbeta[14]     6.30005198\r\nnu         129.97213782\r\nsigma       38.72621370\r\nsigmaBeta    0.09534239\r\nlp__        35.39122557\r\n\r\nParameter beta[12] has shrunk to zero based on 95%-HDI\r\nas a result of regularized model.\r\nThis helped removing all nuisance parameters. But shrinkage also removed\r\nparameter beta[1] of variable Spend!!!\r\nLinear model\r\nCompare with linear model.\r\nWithout nuisance predictors:\r\n\r\nlmSAT<-lm(y~x[,1]+x[,2])\r\nsummary(lmSAT)\r\n\r\nCall:\r\nlm(formula = y ~ x[, 1] + x[, 2])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-88.400 -22.884   1.968  19.142  68.755 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 993.8317    21.8332  45.519  < 2e-16 ***\r\nx[, 1]       12.2865     4.2243   2.909  0.00553 ** \r\nx[, 2]       -2.8509     0.2151 -13.253  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 32.46 on 47 degrees of freedom\r\nMultiple R-squared:  0.8195,    Adjusted R-squared:  0.8118 \r\nF-statistic: 106.7 on 2 and 47 DF,  p-value: < 2.2e-16\r\nconfint(lmSAT)\r\n                 2.5 %      97.5 %\r\n(Intercept) 949.908859 1037.754459\r\nx[, 1]        3.788291   20.784746\r\nx[, 2]       -3.283679   -2.418179\r\n\r\nWith nuisance predictors:\r\n\r\nlmSATAll<-lm(y~.,data=as.data.frame(cbind(y,x)))\r\nsummary(lmSATAll)\r\n\r\nCall:\r\nlm(formula = y ~ ., data = as.data.frame(cbind(y, x)))\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-61.485 -17.643   1.093  15.349  64.549 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 1002.601     22.231  45.100  < 2e-16 ***\r\nSpend          9.503      4.313   2.203   0.0342 *  \r\nPrcntTake     -2.703      0.228 -11.853 8.29e-14 ***\r\nxRand1         2.164      6.842   0.316   0.7536    \r\nxRand2        -5.190      5.015  -1.035   0.3078    \r\nxRand3         6.424      6.599   0.974   0.3370    \r\nxRand4        -5.678      4.899  -1.159   0.2542    \r\nxRand5         7.363      4.957   1.485   0.1464    \r\nxRand6         1.606      5.074   0.316   0.7536    \r\nxRand7         3.909      5.034   0.777   0.4426    \r\nxRand8         3.060      7.072   0.433   0.6679    \r\nxRand9        -4.654      4.397  -1.058   0.2971    \r\nxRand10      -10.265      4.816  -2.131   0.0401 *  \r\nxRand11       -2.912      5.252  -0.555   0.5827    \r\nxRand12        1.334      5.258   0.254   0.8013    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.51 on 35 degrees of freedom\r\nMultiple R-squared:  0.8733,    Adjusted R-squared:  0.8226 \r\nF-statistic: 17.23 on 14 and 35 DF,  p-value: 1.083e-11\r\nconfint(lmSATAll)[2:3,2]-confint(lmSATAll)[2:3,1]\r\n     Spend  PrcntTake \r\n17.5101430  0.9258323 \r\nconfint(lmSAT)[2:3,2]-confint(lmSAT)[2:3,1]\r\n    x[, 1]     x[, 2] \r\n16.9964551  0.8655003 \r\n\r\nThese also show that addition of nuisance parameters widened\r\nconfidence intervals.\r\nRidge and lasso\r\n\r\nset.seed(15)\r\ncv.outRidge=cv.glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 6.570761\r\nridgeFit<-glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,\r\n                 alpha=0,lambda=bestlam,standardize = F)\r\nridge.coef<-predict(ridgeFit,type=\"coefficients\",s=bestlam)\r\nset.seed(15)\r\ncv.outLasso=cv.glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 7.732551\r\nlassoFit<-glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\nlasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam)\r\ncomparison<-round(cbind(summary(lmSATAll)$coefficients[,c(1,4)],\r\n                        summary(fit_noshrinkExtra)$summary[1:15,c(1,4,8)],\r\n                        summary(fit_shrinkExtra)$summary[1:15,c(1,4,8)],\r\n                        ridge.coef, lasso.coef),3)\r\ncomparison<-as.matrix(comparison)\r\ncolnames(comparison)<-c(\"LM\",\"LM-Pv\",\"NoShrink\",\"NoShrink-L\",\"NoShrink-H\",\r\n                        \"Shrink\",\"Shrink-L\",\"Shrink-H\",\"Ridge\",\"Lasso\")\r\ncomparison\r\n                  LM LM-Pv NoShrink NoShrink-L NoShrink-H   Shrink\r\n(Intercept) 1002.601 0.000  998.618    952.443   1044.877 1009.826\r\nSpend          9.503 0.034   10.234      1.234     19.261    8.220\r\nPrcntTake     -2.703 0.000   -2.721     -3.179     -2.255   -2.688\r\nxRand1         2.164 0.754    2.304    -11.451     15.735    1.164\r\nxRand2        -5.190 0.308   -4.729    -14.889      5.475   -1.160\r\nxRand3         6.424 0.337    6.250     -7.252     19.200    1.781\r\nxRand4        -5.678 0.254   -5.697    -15.225      3.796   -2.473\r\nxRand5         7.363 0.146    6.989     -2.925     16.843    2.769\r\nxRand6         1.606 0.754    1.908     -8.073     12.300    0.864\r\nxRand7         3.909 0.443    4.283     -6.012     14.542    1.370\r\nxRand8         3.060 0.668    2.652    -11.338     17.144   -1.396\r\nxRand9        -4.654 0.297   -4.222    -13.804      5.121   -1.002\r\nxRand10      -10.265 0.040  -10.363    -20.156     -0.763   -6.615\r\nxRand11       -2.912 0.583   -1.895    -12.602      8.866   -0.380\r\nxRand12        1.334 0.801    1.711     -8.974     12.114    0.770\r\n            Shrink-L Shrink-H    Ridge    Lasso\r\n(Intercept)  962.868 1058.289 1005.862 1027.326\r\nSpend         -0.280   18.239    8.932    5.032\r\nPrcntTake     -3.165   -2.244   -2.697   -2.607\r\nxRand1        -4.188    8.710    2.061    0.000\r\nxRand2        -8.523    3.088   -5.024    0.000\r\nxRand3        -2.483    9.021    5.004    0.000\r\nxRand4       -10.935    1.458   -5.064    0.000\r\nxRand5        -1.659   11.607    6.686    0.000\r\nxRand6        -2.571    7.022    1.916    0.000\r\nxRand7        -2.288    9.152    3.828    0.000\r\nxRand8       -10.120    5.874    2.090    0.000\r\nxRand9        -7.569    2.473   -4.497    0.000\r\nxRand10      -17.946    0.611   -9.411   -4.142\r\nxRand11       -5.870    3.183   -2.396    0.000\r\nxRand12       -3.804    6.300    0.807    0.000\r\n\r\nNote that there is no way to extract from ridge and lasso regressions\r\nany measure for comparison with zero, like confidence intervals.\r\nLinear model keeps both Spend and PrcntTake\r\nand removes with 5% level all nuisance coefficients except\r\nxRand10\r\nBayesian model without shrinkage does the same.\r\nBayesian model with shrinkage shrinks to zero all artificial predictors,\r\nbut it also removes Spend.\r\nRidge in general is consistent with linear model, but it is not clear if\r\nit shrinks any parameters to zero or not. Lasso fails to shrink to zero\r\nseveral artificial parameters.\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-02-01T13:44:21-06:00",
    "input_file": "Series-4-of-10---Fitting-Linear-Models_Multiple-Regression.knit.md"
  },
  {
    "path": "posts/2022-01-15-Bayesian methods - Series 3 of 10/",
    "title": "Series 3 of 10 -- How to Compare Two Groups with Robust Bayesian Estimation",
    "description": "4 years ago, the argument about the stop relying 100% on null hypothesis significance testing (NHST) which was the P-VALUE. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups' data based on the adaption of UC's lecture and Kruschke's textbook (Chapter 16).",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Biostatistics",
      "Bayesian methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nSetting to\r\ncompare two groups (w/ no predictors)\r\nIQ data\r\nData.\r\nPrepare data\r\n\r\nCall\r\nStan\r\nNormal\r\nassumption\r\nStan in\r\nregular way\r\nCoda\r\nShinystan\r\n\r\nRobust\r\nassumption\r\n\r\nComparison of the groups\r\nFrequentist probability\r\napproach\r\nBayesian\r\napproach\r\nPlot 95% HDI intervals of\r\ndifference\r\n\r\nFrequentist\r\nprobability approach to Markov chains\r\n\r\nReferences\r\n\r\nSetting to\r\ncompare two groups (w/ no predictors)\r\nIQ data\r\nOne example of two-groups problem is testing effect of a drug when\r\none group receives a placebo and another receives the drug. The response\r\nvariable is result of IQ test.\r\nIn this example we estimate mean and standard deviation for two\r\ngroups using normal distribution, then\r\nrobust \\(t\\)-distribution with common\r\nnormality parameter \\(\\lambda\\).\r\nThe diagram of the model structure shows\r\nhow the model needs to be coded.Data.\r\n\r\ndta <- read.csv(\"data/IQdrug.csv\")\r\nDT::datatable(dta)\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\"],[102,107,92,101,110,68,119,106,99,103,90,93,79,89,137,119,126,110,71,114,100,95,91,99,97,106,106,129,115,124,137,73,69,95,102,116,111,134,102,110,139,112,122,84,129,112,127,106,113,109,208,114,107,50,169,133,50,97,139,72,100,144,112,109,98,106,101,100,111,117,104,106,89,84,88,94,78,108,102,95,99,90,116,97,107,102,91,94,95,86,108,115,108,88,102,102,120,112,100,105,105,88,82,111,96,92,109,91,92,123,61,59,105,184,82,138,99,93,93,72],[\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Score<\\/th>\\n      <th>Group<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":1},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nPrepare data\r\n\r\ny <- as.numeric(dta[,\"Score\"])\r\nx <- as.numeric(as.factor(dta[,\"Group\"]))\r\n\r\n(xLevels <- levels(as.factor(dta[,\"Group\"])))\r\n[1] \"Placebo\"    \"Smart Drug\"\r\nNtotal = length(y)\r\n\r\n# Specify the data in a list for JAGS/Stan:\r\ndataList = list(\r\n    y = y,\r\n    x = x,\r\n    Ntotal = Ntotal,\r\n    meanY = mean(y),\r\n    sdY = sd(y)\r\n)\r\n\r\nCall Stan\r\n\r\n#source(\"../DBDA2Eprograms/DBDA2E-utilities.R\")\r\nlibrary(rstan)\r\noptions(mc.cores = parallel::detectCores())\r\nrstan_options(auto_write = TRUE)\r\n\r\nNormal assumption\r\nAssuming that both groups samples have normal distributions, estimate\r\nthe parameters and compare them.\r\nWrite description of the model for stan.\r\n\\[\r\ny_{i\\mid j} \\sim N(\\mu_j, \\sigma^2_j) \\\\\r\n\\mu_j \\sim N(\\bar{Y}, \\frac{1}{100*SD_Y^2}) \\\\\r\n\\sigma_j \\sim uniform(\\frac{SD_Y}{1000}, SD_Y*1000)\r\n\\]\r\n\r\n# Use the description for Stan from file \"ch16.2.stan\"\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    normalSigma = sdY*100;\r\n}\r\nparameters {\r\n    real mu[2];\r\n    real<lower=0> sigma[2];\r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi);\r\n    mu ~ normal(meanY, normalSigma);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ normal(mu[x[i]] , sigma[x[i]]);\r\n    }\r\n}\r\n\"\r\n\r\nIf running the description in modelString for the first\r\ntime create stanDSONormal, otherwise reuse it.\r\n\r\nstanDsoNormal <- stan_model(model_code=modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n#saveRDS(stanDsoNormal, file=\"data/DSONormal1.Rds\")\r\nstanDsoNormal<-readRDS(\"data/DSONormal1.Rds\")\r\n\r\nRun MCMC.\r\n\r\nparameters = c(\"mu\",\"sigma\")     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4\r\nthinSteps = 1\r\nnumSavedSteps = 5000\r\nstanFitNormal <- sampling(stanDsoNormal,\r\n                   data = dataList,\r\n                   pars = parameters, # optional\r\n                   chains = nChains,\r\n                   iter = (ceiling(numSavedSteps/nChains)*thinSteps+burnInSteps),\r\n                   warmup = burnInSteps,\r\n                   init = \"random\", # optional\r\n                   thin = thinSteps)\r\n\r\nSave the fit object for further analysis.\r\n\r\n# save(stanFitNormal,file=\"data/StanNormalFit2Groups.Rdata\")\r\nload(\"data/StanNormalFit2Groups.Rdata\")\r\n\r\nThen, we can explore the results.\r\nStan in regular way\r\n\r\n# text statistics:\r\nprint (stanFitNormal)\r\nInference for Stan model: a4cb1c74477f9a8b1e476a89a6556245.\r\n4 chains, each with iter=2250; warmup=1000; thin=1; \r\npost-warmup draws per chain=1250, total post-warmup draws=5000.\r\n\r\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nmu[1]     100.04    0.03 2.43   95.33   98.39  100.01  101.71  104.76\r\nmu[2]     107.79    0.05 3.36  101.37  105.50  107.78  110.00  114.44\r\nsigma[1]   18.34    0.03 1.81   15.28   17.07   18.22   19.44   22.38\r\nsigma[2]   26.02    0.03 2.44   21.77   24.31   25.80   27.58   31.23\r\nlp__     -423.27    0.03 1.47 -426.95 -424.00 -422.93 -422.20 -421.44\r\n         n_eff Rhat\r\nmu[1]     5453    1\r\nmu[2]     5035    1\r\nsigma[1]  5004    1\r\nsigma[2]  5772    1\r\nlp__      2598    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Mon Jan 30 00:57:34 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\n# estimates & hdi:\r\nplot(stanFitNormal)\r\n\r\n# samples\r\nrstan::traceplot(stanFitNormal, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFitNormal, pars=c('mu','sigma'))\r\n\r\nstan_scat(stanFitNormal, c('mu[1]','mu[2]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[1]','sigma[1]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[1]','sigma[2]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[2]','sigma[1]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[2]','sigma[2]'))\r\n\r\nstan_scat(stanFitNormal, c('sigma[1]','sigma[2]'))\r\n\r\nstan_hist(stanFitNormal)\r\n\r\nstan_dens(stanFitNormal)\r\n\r\n# autocorrelation:\r\nstan_ac(stanFitNormal, separate_chains = T)\r\n\r\nstan_diag(stanFitNormal,information = \"sample\",chain=0)\r\n\r\nstan_diag(stanFitNormal,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(stanFitNormal,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(stanFitNormal,information = \"divergence\",chain = 0)\r\n\r\n\r\nCoda\r\nIf we prefer output using coda class, reformat the chains into\r\ncoda:\r\n\r\nlibrary(coda)\r\nstan2coda <- function(stanFitNormal) {\r\n    # apply to all chains\r\n    mcmc.list(lapply(1:ncol(stanFitNormal), function(x) mcmc(as.array(stanFitNormal)[,x,])))\r\n}\r\ncodaSamples <- stan2coda(stanFitNormal)\r\nsummary(codaSamples)\r\n\r\nIterations = 1:1250\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 1250 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n            Mean    SD Naive SE Time-series SE\r\nmu[1]     100.04 2.431  0.03438        0.03166\r\nmu[2]     107.79 3.357  0.04748        0.04742\r\nsigma[1]   18.34 1.806  0.02554        0.02516\r\nsigma[2]   26.02 2.441  0.03452        0.03252\r\nlp__     -423.27 1.465  0.02072        0.02860\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%     25%     50%     75%   97.5%\r\nmu[1]      95.33   98.39  100.01  101.71  104.76\r\nmu[2]     101.37  105.50  107.78  110.00  114.44\r\nsigma[1]   15.28   17.07   18.22   19.44   22.38\r\nsigma[2]   21.77   24.31   25.80   27.58   31.23\r\nlp__     -426.95 -424.00 -422.93 -422.20 -421.44\r\nplot(codaSamples)\r\n\r\nautocorr.plot(codaSamples)\r\n\r\neffectiveSize(codaSamples)\r\n   mu[1]    mu[2] sigma[1] sigma[2]     lp__ \r\n6113.262 5058.326 5239.336 5666.196 2646.550 \r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n         Point est. Upper C.I.\r\nmu[1]             1          1\r\nmu[2]             1          1\r\nsigma[1]          1          1\r\nsigma[2]          1          1\r\nlp__              1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\nplot(density(codaSamples[[1]][,1]),xlim=c(10,120),ylim=c(0,.25),main=\"Posterior Densities\")  # mu[1], 1st chain\r\nlines(density(codaSamples[[1]][,2]))                         # mu[2], 1st chain\r\nlines(density(codaSamples[[1]][,3]))                         # sigma[1], 1st chain\r\nlines(density(codaSamples[[1]][,4]))                         # sigma[2], 1st chain\r\nlines(density(codaSamples[[2]][,1]),col=\"red\")               # mu[1], 2nd chain\r\nlines(density(codaSamples[[2]][,2]),col=\"red\")               # mu[2], 2nd chain\r\nlines(density(codaSamples[[2]][,3]),col=\"red\")               # sigma[1], 2nd chain\r\nlines(density(codaSamples[[2]][,4]),col=\"red\")               # sigma[2], 2nd chain\r\n\r\n\r\nShinystan\r\nOr you can use shinystan to analyze fitted model\r\n\r\nlibrary(shinystan)\r\n\r\nLaunch shiny application with the loaded object.\r\n\r\nlaunch_shinystan(stanFitNormal)\r\n\r\nRobust assumption\r\nUse the robust assumption of Student-t distribution instead of normal\r\ndistribution.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    real expLambda ;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    normalSigma = sdY*100;\r\n    expLambda = 1/29.0;\r\n}\r\nparameters {\r\n    real<lower=0> nuMinusOne;\r\n    real mu[2] ; // 2 groups\r\n    real<lower=0> sigma[2] ; // 2 groups\r\n}\r\ntransformed parameters {\r\n    real<lower=0> nu ;\r\n    nu = nuMinusOne + 1 ;\r\n}\r\nmodel {\r\n    sigma  ~ uniform( unifLo , unifHi ) ; // vectorized 2 groups\r\n    mu ~ normal( meanY , normalSigma ) ; // vectorized 2 groups\r\n    nuMinusOne ~ exponential( expLambda ) ;\r\n    for ( i in 1:Ntotal ) {\r\n      y[i] ~ student_t(nu, mu[x[i]] ,sigma[x[i]]) ; // nested index of group\r\n    }\r\n}\r\n\"\r\n\r\nIf running the description in modelString for the first time create\r\nstanDSO, otherwise reuse it.\r\n\r\nstanDsoRobust <- stan_model(model_code=modelString) \r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(stanDsoRobust,file=\"data/DSORobust1.Rds\")\r\nstanDsoRobust<-readRDS(file=\"data/DSORobust1.Rds\")\r\n\r\nIf necessary, initialize chains. Parameter init can be:\r\none of digit 0, string “0” or “random”, a function that returns a list,\r\nor a list of initial parameter values with which to indicate how the\r\ninitial values of parameters are specified.\r\n“0”: initialize all to be zero on the unconstrained support\r\n“random”: randomly generated\r\nlist: a list of lists equal in length to the number of chains\r\n(parameter chains), where each list in the list of lists * specifies the\r\ninitial values of parameters by name for the corresponding chain.\r\nfunction: a function that returns a list for specifying the initial\r\nvalues of parameters for a chain. The function can take an optional\r\nparameter chain_id.\r\nSince Stan has pretty complicated parameter tuning process during\r\nwhich among other parameters it selects initial values, it may be a good\r\nidea to let Stan select default initial parameters until we get enough\r\nexperience.\r\nRun MCMC.\r\n\r\nparameters = c( \"mu\" , \"sigma\" , \"nu\" )     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps<-5000\r\n# Get MC sample of posterior:\r\nstanFitRobust <- sampling( object=stanDsoRobust , \r\n                   data = dataList , \r\n                   pars = parameters , # optional\r\n                   chains = nChains ,\r\n                   cores=nChains,\r\n                   iter = (ceiling(numSavedSteps/nChains)*thinSteps\r\n                            + burnInSteps ) , \r\n                   warmup = burnInSteps , \r\n                   init = \"random\" , # optional\r\n                   thin = thinSteps )\r\n\r\nSave the fit object.\r\n\r\n# save(stanFitRobust,file=\"data/StanRobustFit2Groups.Rdata\")\r\nload(\"data/StanRobustFit2Groups.Rdata\")\r\n\r\nExplore the results.\r\n\r\nprint(stanFitRobust)\r\nInference for Stan model: 3b61ff01c98dc80a0961c8bae33b0b6e.\r\n4 chains, each with iter=2250; warmup=1000; thin=1; \r\npost-warmup draws per chain=1250, total post-warmup draws=5000.\r\n\r\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nmu[1]      99.28    0.03 1.76   95.89   98.11   99.32  100.47  102.63\r\nmu[2]     107.14    0.04 2.69  101.78  105.36  107.10  108.95  112.28\r\nsigma[1]   11.32    0.03 1.75    8.28   10.11   11.21   12.40   15.06\r\nsigma[2]   17.87    0.04 2.71   12.99   16.01   17.72   19.57   23.55\r\nnu          3.87    0.03 1.71    1.91    2.82    3.51    4.42    8.15\r\nlp__     -451.38    0.03 1.67 -455.61 -452.22 -451.03 -450.15 -449.20\r\n         n_eff Rhat\r\nmu[1]     4781    1\r\nmu[2]     5127    1\r\nsigma[1]  4350    1\r\nsigma[2]  4679    1\r\nnu        3694    1\r\nlp__      2314    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Mon Jan 30 01:00:14 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(stanFitRobust)\r\n\r\nrstan::traceplot(stanFitRobust, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFitRobust, pars=c('nu','mu','sigma'))\r\n\r\nstan_scat(stanFitRobust, c('nu','mu[1]'))\r\n\r\nstan_scat(stanFitRobust, c('nu','mu[2]'))\r\n\r\nstan_scat(stanFitRobust, c('nu','sigma[1]'))\r\n\r\nstan_scat(stanFitRobust, c('nu','sigma[2]'))\r\n\r\nstan_scat(stanFitRobust, c('mu[1]','sigma[1]'))\r\n\r\nstan_scat(stanFitRobust, c('sigma[1]','sigma[2]'))\r\n\r\n\r\nNote correlation between the sigma parameters.Is there’s maybe no sign of correlation in non-robust\r\napproach!How can you explain positive correlation?\r\nThe difference of scales (i.e., \\(\\sigma_2 −\r\n\\sigma_1\\)) shows a credible nonzero difference (in the\r\nbelow), suggesting that the smart drug causes greater variance than\r\nthe placebo.\r\n\r\nstan_hist(stanFitRobust)\r\n\r\nstan_dens(stanFitRobust)\r\n\r\nstan_ac(stanFitRobust, separate_chains = T)\r\n\r\nstan_diag(stanFitRobust,information = \"sample\",chain=0)\r\n\r\nstan_diag(stanFitRobust,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"divergence\",chain = 0)\r\n\r\n\r\n\r\nlaunch_shinystan(stanFitRobust)\r\n\r\nComparison of the groups\r\nFrequentist probability\r\napproach\r\nFor comparison decide whether the two groups are different or not,\r\nusing the Frequentist approach.\r\nUse t-test with unequal variances:\r\n\r\nqqnorm(y[x==1])\r\nqqline(y[x==1])\r\n\r\nqqnorm(y[x==2])\r\nqqline(y[x==2])\r\n\r\n\r\n\r\nt.test(Score ~ Group , data=dta, var.equal=FALSE, paired=FALSE)\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  Score by Group\r\nt = -1.958, df = 111.44, p-value = 0.05273\r\nalternative hypothesis: true difference in means between group Placebo and group Smart Drug is not equal to 0\r\n95 percent confidence interval:\r\n -15.70602585   0.09366161\r\nsample estimates:\r\n   mean in group Placebo mean in group Smart Drug \r\n                100.0351                 107.8413 \r\n\r\nResult: with 5% error I level the null (equality)\r\nhypothesis cannot be rejected.\r\nHowever, the case is not so clear: the p-value is at the marginal\r\n(very close to 5%). The sample size is pretty small (120\r\nobservations in both groups), the result is not very\r\nconclusive.\r\nThe test relies on the assumption that distributions of both samples are\r\nGaussian. As we can see in qq-plots this assumption does not hold.\r\nBayesian approach\r\nNow use Bayesian approach based on robust estimation.\r\nCreate matrices of combined chains for the two means and two standard\r\ndeviations of the robust fit.\r\n\r\nsummary(stanFitRobust)\r\n$summary\r\n                mean    se_mean       sd        2.5%         25%\r\nmu[1]      99.278479 0.02541479 1.757258   95.892836   98.106053\r\nmu[2]     107.135439 0.03759197 2.691692  101.782362  105.356701\r\nsigma[1]   11.324726 0.02646412 1.745481    8.283891   10.114247\r\nsigma[2]   17.871701 0.03961980 2.710205   12.992997   16.008138\r\nnu          3.865007 0.02819681 1.713714    1.906517    2.816141\r\nlp__     -451.375158 0.03470814 1.669462 -455.612524 -452.215782\r\n                 50%         75%       97.5%    n_eff      Rhat\r\nmu[1]      99.318108  100.465290  102.634056 4780.774 1.0001983\r\nmu[2]     107.104442  108.951046  112.281358 5126.967 1.0002329\r\nsigma[1]   11.214309   12.404448   15.055811 4350.261 0.9992927\r\nsigma[2]   17.718447   19.571494   23.554055 4679.288 0.9999055\r\nnu          3.506651    4.418275    8.154027 3693.830 1.0003792\r\nlp__     -451.034452 -450.145225 -449.198416 2313.613 1.0004085\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n          stats\r\nparameter        mean       sd        2.5%         25%         50%\r\n  mu[1]      99.31321 1.794266   95.953253   98.116266   99.321257\r\n  mu[2]     106.96172 2.739928  101.452693  105.249842  106.924883\r\n  sigma[1]   11.33591 1.739136    8.295615   10.138611   11.217442\r\n  sigma[2]   17.87345 2.746624   13.081372   15.908501   17.549351\r\n  nu          3.89781 1.758589    1.897855    2.791728    3.522191\r\n  lp__     -451.39555 1.654445 -455.559983 -452.246843 -451.069865\r\n          stats\r\nparameter          75%       97.5%\r\n  mu[1]     100.487503  102.712333\r\n  mu[2]     108.803910  112.141803\r\n  sigma[1]   12.430281   15.144620\r\n  sigma[2]   19.648503   23.691793\r\n  nu          4.469162    8.370698\r\n  lp__     -450.199223 -449.224826\r\n\r\n, , chains = chain:2\r\n\r\n          stats\r\nparameter         mean       sd        2.5%         25%         50%\r\n  mu[1]      99.359740 1.727473   96.002888   98.183860   99.424128\r\n  mu[2]     107.205724 2.710996  101.960121  105.333992  107.264773\r\n  sigma[1]   11.328668 1.715013    8.365028   10.140853   11.193846\r\n  sigma[2]   17.821753 2.637216   13.271496   15.984582   17.685629\r\n  nu          3.789725 1.446564    1.982413    2.796458    3.463222\r\n  lp__     -451.324191 1.638725 -455.461092 -452.162620 -450.963830\r\n          stats\r\nparameter          75%       97.5%\r\n  mu[1]     100.586306  102.610716\r\n  mu[2]     109.024107  112.217721\r\n  sigma[1]   12.358057   15.070945\r\n  sigma[2]   19.416344   23.651606\r\n  nu          4.371135    7.443861\r\n  lp__     -450.105134 -449.218868\r\n\r\n, , chains = chain:3\r\n\r\n          stats\r\nparameter         mean       sd        2.5%         25%         50%\r\n  mu[1]      99.241089 1.715029   95.621161   98.141599   99.310472\r\n  mu[2]     107.198150 2.761142  101.738884  105.413202  107.184480\r\n  sigma[1]   11.324729 1.786161    8.129510   10.087825   11.237923\r\n  sigma[2]   17.834568 2.789681   12.584832   16.032342   17.729390\r\n  nu          3.847617 1.677111    1.854929    2.795824    3.477059\r\n  lp__     -451.449925 1.738399 -455.821321 -452.303253 -451.091065\r\n          stats\r\nparameter          75%      97.5%\r\n  mu[1]     100.432571  102.53221\r\n  mu[2]     109.058140  112.43690\r\n  sigma[1]   12.455750   15.07465\r\n  sigma[2]   19.478207   23.53294\r\n  nu          4.352777    8.40313\r\n  lp__     -450.148846 -449.15265\r\n\r\n, , chains = chain:4\r\n\r\n          stats\r\nparameter         mean       sd        2.5%         25%         50%\r\n  mu[1]      99.199874 1.788565   95.891846   97.960787   99.213313\r\n  mu[2]     107.176156 2.544973  102.262461  105.526175  107.097533\r\n  sigma[1]   11.309597 1.742850    8.295669   10.087199   11.227140\r\n  sigma[2]   17.957035 2.665734   13.114103   16.071792   17.869635\r\n  nu          3.924874 1.935407    1.912542    2.882155    3.564496\r\n  lp__     -451.330962 1.643141 -455.604148 -452.095591 -450.986033\r\n          stats\r\nparameter         75%       97.5%\r\n  mu[1]     100.36449  102.632627\r\n  mu[2]     108.81906  112.363129\r\n  sigma[1]   12.38710   14.913067\r\n  sigma[2]   19.68608   23.495363\r\n  nu          4.44298    8.213977\r\n  lp__     -450.10343 -449.228235\r\n\r\n\r\ny.dis1<-cbind(Mu=rstan::extract(stanFitRobust,pars=\"mu[1]\")$'mu[1]',\r\n            Sigma=rstan::extract(stanFitRobust,pars=\"sigma[1]\")$'sigma[1]')\r\ny.dis2<-cbind(Mu=rstan::extract(stanFitRobust,pars=\"mu[2]\")$'mu[2]',\r\n            Sigma=rstan::extract(stanFitRobust,pars=\"sigma[2]\")$'sigma[2]')\r\nden.Dis1<-density(y.dis1[, \"Mu\"])\r\nden.Dis2<-density(y.dis2[, \"Mu\"])\r\nplot(den.Dis1,col=\"blue\", xlim=c(90,120), main=\"Compare 2 distributions\")\r\nlines(den.Dis2,col=\"red\")\r\n\r\n\r\nTraditional Bayesian approach: look at HDIs.\r\n\r\nlibrary(HDInterval)\r\n\r\n\r\nhdi(cbind(y.dis1[,1], y.dis2[,1]), credMass=.9)\r\n           [,1]     [,2]\r\nlower  96.40157 102.6255\r\nupper 102.14760 111.4564\r\nattr(,\"credMass\")\r\n[1] 0.9\r\n\r\n\r\nhdi(cbind(y.dis1[,2], y.dis2[,2]), credMass=.85)\r\n          [,1]     [,2]\r\nlower  8.79751 13.88659\r\nupper 13.71127 21.54786\r\nattr(,\"credMass\")\r\n[1] 0.85\r\n\r\nThe 95% HDI intervals overlap for both parameters, but with reduced\r\ncredible mass level they can be distinguished.\r\nPlot 95% HDI intervals of\r\ndifference\r\nMean difference\r\n\r\nparamSampleVec <- y.dis2[,1] - y.dis1[,1]\r\nplotPost(paramSampleVec=paramSampleVec)\r\n\r\n\r\nScale difference\r\n\r\nparamSampleVec <- y.dis2[,2] - y.dis1[,2]\r\nplotPost(paramSampleVec=paramSampleVec)\r\n\r\n\r\nFrequentist\r\nprobability approach to Markov chains\r\nApply Frequentist approach to the chain samples.\r\nFirst, check if the samples for two standard deviations are\r\nsignificantly different or not:\r\n\r\nc(mean(y.dis1[,2]),mean(y.dis2[,2])) # mean values\r\n[1] 11.32473 17.87170\r\nc(sd(y.dis1[,2]),sd(y.dis2[,2]))     # standard deviations of samples of MCMC standard deviations\r\n[1] 1.745481 2.710205\r\nks.test(y.dis1[,2],y.dis2[,2])       # Kolmogorov-Smirnov test for posterior distributions of standard deviations\r\n\r\n    Two-sample Kolmogorov-Smirnov test\r\n\r\ndata:  y.dis1[, 2] and y.dis2[, 2]\r\nD = 0.862, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\nden<-density(y.dis2[,2])\r\nplot(density(y.dis1[,2]),xlim=c(5,30))\r\nlines(den$x,den$y,col=\"red\")\r\n\r\nt.test(y.dis1[,2],y.dis2[,2], var.equal=F, paired=FALSE) #t-test for means of posterior distributions for standard deviations\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  y.dis1[, 2] and y.dis2[, 2]\r\nt = -143.61, df = 8537.3, p-value < 2.2e-16\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -6.636341 -6.457609\r\nsample estimates:\r\nmean of x mean of y \r\n 11.32473  17.87170 \r\n\r\nCheck shapes of the distributions of the mean and standard deviation\r\nparameters. How different are they between control and treated\r\ngroups?\r\nFor standard deviations:\r\n\r\nqqnorm(y.dis1[,2])  # control\r\nqqline(y.dis1[,2])\r\n\r\nqqnorm(y.dis2[,2])  # treatment\r\nqqline(y.dis2[,2])\r\n\r\n\r\nFor mean values:\r\n\r\nqqnorm(y.dis1[,1])  #control\r\nqqline(y.dis1[,1])\r\n\r\nqqnorm(y.dis2[,1])  # treatment\r\nqqline(y.dis2[,1])\r\n\r\n\r\nComparison of mean and standard deviations of the posterior sample\r\nfor standard deviations, Kolmogorov-Smirnov test, density plots and\r\nt-test for the two samples all indicate that the variances of the two\r\ngroups are different.\r\nThis means that we cannot apply ANOVA to compare the two group mean\r\nvalues directly.\r\nTry t-test for the two means of the posterior distributions with\r\ndifferent variances:\r\n\r\nt.test(y.dis1[,1],y.dis2[,1], var.equal=F, paired=FALSE)\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  y.dis1[, 1] and y.dis2[, 1]\r\nt = -172.83, df = 8605.2, p-value < 2.2e-16\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -7.946073 -7.767847\r\nsample estimates:\r\nmean of x mean of y \r\n 99.27848 107.13544 \r\n\r\nThe null hypothesis of equality of the means of the posterior\r\ndistributions for the mean values of the two groups decisively rejected:\r\nwe are testing with a lot longer samples.\r\nPlot the images of the two groups in the mean-standard deviation\r\nparameter space:\r\n\r\nplot(y.dis1,xlim=c(92,118),ylim=c(5,33),col=\"red\",xlab=\"Mean\",ylab=\"St. Dev.\")\r\npoints(y.dis2,col=\"blue\")\r\n\r\n\r\nWe can see that by using at least 2 different methods proving that\r\nthere is a significant difference between the 2 groups shown on the\r\nplot.\r\nReferences\r\nJohn K. Kruschke, Journal of Experimental Psychology: General, 2013,\r\nv.142(2), pp.573-603. (doi: 10.1037/a0029146), website\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial\r\nwith r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic Press is\r\nan imprint of Elsevier.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-01T13:29:58-06:00",
    "input_file": "Series-3-of-10---Comparing-2-Groups.knit.md"
  },
  {
    "path": "posts/2022-01-01-Bayesian methods - Series 2 of 10/",
    "title": "Series 2 of 10 -- MCMC for Estimation of Gaussian parameters",
    "description": "Run MCMC on binomial model Gaussian distribution, one sample Hierarchical model, two groups of Gaussian observations",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-01-01",
    "categories": [
      "Biostatistics",
      "Bayesian methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nReview of MCMC: binomial\r\nmodel\r\nData\r\nRunning MCMC\r\nChecking\r\nresults\r\nAnalytical solution\r\nHistograms of\r\naccepted and rejected values\r\n\r\n\r\nGaussian\r\ndistribution, one sample: known variance, unknown mean\r\nData\r\nFNP approach\r\nBayesian approach by\r\nformula, weak prior\r\nBayesian approach by\r\nformula, strong prior\r\nUsing\r\nJAGS to estimate mean of normal distribution with known variance\r\nData\r\nPreparation of the model for\r\nJAGS\r\nInitializing Markov chains\r\nSending information to\r\nJAGS\r\nRunning MCMC in JAGS:\r\nburn in and main run\r\nAnalyzing convergence\r\nAnalyzing and\r\ninterpreting the results\r\n\r\n\r\nHierarchical\r\nmodel: two groups of Gaussian observations\r\nData\r\nDifferent mean values,\r\ncommon weak prior\r\nModel\r\ndescription\r\nInitialization and\r\nsending the model to JAGS\r\nRunning the\r\nmodel\r\nAnalysis\r\n\r\nExperimenting with\r\nprior hyperparameters\r\nRun: hypeMean=130;\r\nhypeSD=0.2\r\nRun: hypeMean=160;\r\nhypeSD=0.2\r\n\r\nFNP\r\napproach: ANOVA\r\n\r\nFurther\r\nreading\r\n\r\nReview of MCMC: binomial\r\nmodel\r\nThe diagram of the model structure:\r\nsimplest binomial model with conjugate beta prior (adapted from\r\nTextbook).Model description contains 2 parts:\r\n\\[y_i \\sim \\text{Binom} (p = \\theta, \\\r\nsize = 1) \\\\\r\n\\theta \\sim \\text{Beta}(A_{prior},B_{prior})\\]\r\nwhere \\(\\theta\\) is the unknown\r\nprobability of success with prior beta distribution with fixed\r\nparameters \\(A,\\ B\\).\r\nTo check the results of simulation use analytical formulas for\r\nposterior distribution: if prior beta distribution has parameters \\(A_{prior},B_{prior}\\) and the data contain\r\nnumber of successes \\(s\\),\r\n\\[s = \\sum_{i=1}^k y_i\\]\r\nout of \\(k\\) observations then\r\nposterior beta distribution has parameters\r\n\\[A_{post} = A_{prior} + s; \\ B_{post} =\r\nB_{prior} + k − s\\]\r\nData\r\nCreate vector of \\(0\\) and \\(1\\) of length \\(k=20\\) and probability of success \\(\\theta = 0.6\\)\r\n\r\n# Y <- rbinom(41, size=1, p=.32)\r\nflips = 41\r\nheads = 13\r\na <- 10; b <- 10\r\n\r\nRecall that Metropolis-Hastings MCMC algorithm\r\nconsists of the following steps:\r\nGenerate new proposal using some convenient\r\ndistribution.\r\nDecide whether the proposal theta_prop is accepted\r\nor rejected\r\nRunning MCMC\r\n\r\nTo do that follow the logic:\r\nIf posterior probability density at theta_prop is\r\ngreater than posterior probability density at theta_curr\r\nthen accept theta_prop into the chain sample.\r\nIf posterior probability density at theta_prop is\r\nsmaller than posterior probability density at theta_curr\r\nthen accept theta_prop with probability\r\n\\[p_{dec} = \\frac{p(\\theta_{prop} \\mid\r\ny)}{p(\\theta_{curr} \\mid y)} = \\frac{p(y \\mid \\theta_{prop}) \\\r\np(\\theta_{prop} \\mid A, B)}{p(y \\mid \\theta_{curr}) \\ p(\\theta_{curr}\r\n\\mid A, B)} \\]\r\nand reject theta_prop with probability\r\n$1-p_{dec}$.\r\nAcceptance decision is made by simulating uniform random variable on\r\n\\(U_{[0,1]}\\) and comparing it with\r\n\\(p_{dec}\\). If \\(U < p_{dec}\\) then\r\ntheta_prop is accepted.\r\n\r\nmetropolis_algorithm <- function(samples, theta_seed, sd){\r\n   theta_curr <- theta_seed\r\n   # Create vector of NAs to store sampled parameters\r\n   posterior_thetas <- rep(NA, times = samples)\r\n   for (i in 1:samples){\r\n      # Typically new proposals are generated from Gaussian distribution centered at the current value \r\n      # of Markov chain with sufficiently small variance.\r\n      theta_prop <- rnorm(n = 1, mean = theta_curr, sd = sd) # Proposal distribution\r\n      # If the proposed parameter is outside its range then set it equal to its current\r\n      # value. Otherwise keep the proposed value\r\n      theta_prop <- ifelse((theta_prop < 0 | theta_prop > 1), theta_curr, theta_prop)\r\n      \r\n      # Bayes' numerators\r\n      posterior_prop <- dbeta(theta_prop, shape1 = a, shape2 = b) * dbinom(heads, size = flips, prob = theta_prop)\r\n      posterior_curr <- dbeta(theta_curr, shape1 = a, shape2 = b) * dbinom(heads, size = flips, prob = theta_curr)\r\n      # Calculate probability of accepting\r\n      p_accept_theta_prop <- min(posterior_prop/posterior_curr, 1.0)\r\n      rand_unif <- runif(n = 1)\r\n      # Probabilistically accept proposed theta\r\n      theta_select <- ifelse(p_accept_theta_prop > rand_unif, theta_prop, theta_curr)\r\n      posterior_thetas[i] <- theta_select\r\n      # Reset theta_curr for the next iteration of the loop\r\n      theta_curr <- theta_select\r\n   }\r\n   return(posterior_thetas)\r\n}\r\n\r\nChecking results\r\n\r\nset.seed(12423)\r\nposterior_thetas <- metropolis_algorithm(samples = 10000, theta_seed = 0.9, sd = 0.05)\r\n\r\nAfter simulating Markov chain check the results.\r\nAnalytical solution\r\nCompare prior parameters and posterior parameters given by\r\nformulas\r\nIt was due to since the beta distribution is conjugate to the\r\nbinomial distribution (above), we can see how close our sampled\r\nposterior distribution approximates the exact posterior for \\(\\theta\\). Recall that with a \\(Beta(10, 10)\\) prior and with \\(13\\) heads in \\(41\\) flips, the posterior distribution is\r\nalso beta distributed with \\(a =\r\n10+13\\) and \\(b = 10+(41−13)\\).\r\nSo, let’s overlay the exact posterior distribution on our previous graph\r\nalso.\r\n\r\nopar <- par()\r\npar(mar=c(2.5,3.5,3,2.1), mgp = c(1.7, 1, 0))\r\nd <- density(posterior_thetas)\r\nplot(d,\r\n     main = expression(paste('Kernel Density Plot for ', theta)),\r\n     xlab = expression(theta),\r\n     ylab = 'density',\r\n     yaxt = 'n',\r\n     cex.lab = 1.3\r\n     )\r\npolygon(d, col='dodgerblue1', border='dark blue')\r\n\r\nexactb <- rbeta(n = 10000, shape1 = 23, shape2 = 38)\r\nlines(density(exactb),                             # Plot of randomly drawn beta density\r\n     type = 's', col = 'red')\r\n\r\n\r\n\r\nhdi(posterior_thetas, credMass = 0.95)\r\n    lower     upper \r\n0.2563125 0.4986970 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nHistograms of\r\naccepted and rejected values\r\nGaussian\r\ndistribution, one sample: known variance, unknown mean\r\nData\r\nSimulate single sample from Gaussian distribution with\r\nunknown mean \\(\\theta\\)\r\nand known standard deviation \\(\\sigma\\).\r\n\r\nmu <- 120  \r\nsi <- 25\r\nnSample <- 200\r\nset.seed(05152022)\r\nY <- rnorm(nSample, mean = mu, sd = si)\r\nTheoretical.data <- c(Mean = mu, Sd = si)\r\nplot(Y)\r\n\r\n\r\nFNP approach\r\nEstimate mean of the distribution, check histogram, test hypothesis\r\n\\(H_0\\): \\(\\mu = 120\\) against two-sided alternative\r\n\\(H_a\\): \\(\\mu \\neq 120\\).\r\n\r\nmeanMLE <- mean(Y)\r\nhist(Y, freq = F)\r\n\r\nplot(density(Y))\r\n\r\n\r\n\r\n(zStat <- (meanMLE - Theoretical.data[\"Mean\"])/(Theoretical.data[\"Sd\"]/sqrt(nSample)))\r\n     Mean \r\n0.3157925 \r\n(sSidedP <- pnorm(zStat,lower.tail = F) + pnorm(-zStat,lower.tail = T))\r\n   Mean \r\n0.75216 \r\n\r\nFinally, distribution of the data estimated by FNP:\r\n\r\n(MLE.data <- c(Mean = meanMLE, Sd = Theoretical.data[\"Sd\"]))\r\n    Mean    Sd.Sd \r\n120.5582  25.0000 \r\n\r\nBayesian approach by\r\nformula, weak prior\r\nSet the model as \\[y_i \\sim N(\\theta,\r\n\\sigma)\\]\r\nwhere \\(\\theta\\) is unknown and\r\n\\(\\sigma = 25\\).\r\nSet prior distribution for the mean value as low informative Gaussian\r\nwith parameters \\(M = 100\\), \\(\\sigma_{\\mu} = 200\\), i.e. \r\n\\[ \\theta \\sim N(M,\r\n\\sigma_{\\mu})\\]\r\n\r\nM <- 100\r\npriorParamWeak <- c(Mean = M, Sd = 200)\r\n\r\nObtain posterior distribution using formulas for conjugate Gaussian\r\ndistribution:\r\n\\[ \\mu_{post} = \\frac{\\mu_{prior}\r\n\\gamma_{prior} + \\bar{y} \\gamma_{data}}{\\gamma_{prior} + \\gamma_{data}}\r\n= \\frac{M \\gamma_{prior} + \\bar{y} \\gamma_{data}}{\\gamma_{prior} +\r\n\\gamma_{data}} = M \\frac{\\gamma_{prior}}{\\gamma_{prior} + \\gamma_{data}}\r\n+ \\bar{y} \\frac{\\gamma_{data}}{\\gamma_{prior} + \\gamma_{data}}\r\n\\]\r\n\\[ \\sigma_{post}^2 =\r\n\\frac{1}{\\gamma_{prior} + \\gamma_{data}} \\]\r\nwhere \\(n\\) is the sample size and\r\n\\(\\gamma_{data}\\) and \\(\\gamma_{prior}\\) are data precision and\r\nprior precision, correspondingly:\r\n\\[ \\gamma_{data} ; \\gamma_{prior} =\r\n\\frac{1}{\\sigma_{\\mu}^2} \\]\r\n\r\nprecision_prior <- 1/200^2\r\nprecision_data <- nSample/25^2\r\npSigmasq <- 1/(precision_prior + precision_data)\r\npSd <- sqrt(pSigmasq)\r\npMean <- 100*precision_prior*pSigmasq + meanMLE*precision_data*pSigmasq\r\n(posterParamWeak <- c(Mean = pMean, Sd = pSd))\r\n      Mean         Sd \r\n120.556641   1.767698 \r\n\r\nBayesian estimate is obtained as \\(\\mu_{post}\\) = 120.5566415, showing no\r\ncompromise between likelihood 120.5582475 and prior 100.\r\nPlot theoretical (simulated), estimated by MLE and estimated by\r\nBayesian analysis densities.\r\n\r\nBayes.dataWeak <- c(posterParamWeak[\"Mean\"], Theoretical.data[\"Sd\"])\r\nX <- seq(from = 115, to = 125, by = 1)\r\nplot(X, dnorm(X, Theoretical.data[1], Theoretical.data[2]),\r\n     type=\"l\", xlab=\"X\", ylab=\"Density\")\r\nlines(X, dnorm(X,meanMLE,si), col=\"orange\", lwd=3)\r\nlines(X, dnorm(X,posterParamWeak[1],si), col=\"blue\")\r\nlegend(\"topright\",\r\n       legend = c(\"Theoretical\", \"MLE\", \"Bayesian\"),\r\n       col = c(\"black\", \"orange\", \"blue\"),\r\n       lty = 1)\r\n\r\n\r\nFor the sample length \\(200\\) and\r\nlow informative prior Bayesian estimate is very close to MLE in\r\ncomparison with the prior. In addition standard deviation of the\r\nposterior distribution collapsed from \\(200\\) to 1.7676979.\r\n\r\nrbind(Theoretical.data, MLE.data, Bayes.dataWeak)\r\n                     Mean Sd\r\nTheoretical.data 120.0000 25\r\nMLE.data         120.5582 25\r\nBayes.dataWeak   120.5566 25\r\nrbind(priorParamWeak, posterParamWeak)\r\n                    Mean         Sd\r\npriorParamWeak  100.0000 200.000000\r\nposterParamWeak 120.5566   1.767698\r\n\r\nBayesian approach by\r\nformula, strong prior\r\nRepeat Bayesian analysis with stronger prior \\(M = 100, \\  \\sigma_{\\mu} = 2\\).\r\n\r\npriorParamStrong <- c(Mean = M, Sd = 2)\r\nprecision_prior <- 1/2^2\r\nprecision_data <- nSample/25^2\r\npSigmasq <- 1/(precision_prior + precision_data)\r\npSd <- sqrt(pSigmasq)\r\npMean <- 100*precision_prior*pSigmasq + meanMLE*precision_data*pSigmasq\r\nposterParamStrong <- c(Mean = pMean, Sd = pSd)\r\nrbind(priorParamStrong, posterParamStrong)\r\n                      Mean       Sd\r\npriorParamStrong  100.0000 2.000000\r\nposterParamStrong 111.5415 1.324532\r\n\r\nBayesian estimate of the mean shifted towards mode of the prior\r\ndistribution: now the mean of posterior distribution is obtained by\r\n111.5414723\r\nShift of the lower level parameter towards mode of the higher level\r\nparameter is shrinkage.\r\nUsing\r\nJAGS to estimate mean of normal distribution with known variance\r\nData\r\nPrepare the data list dataList in the format required by\r\nJAGS\r\n\r\ndataList <- list(\"Y\" = Y, #data vector\r\n                 \"Ntotal\" = nSample) # length of the sample\r\n\r\nPreparation of the model for\r\nJAGS\r\nThe model diagram: \r\nInterpretation of the diagram starts from the bottom. Each arrow of\r\nthe diagram corresponds to a line of description code for the model.\r\nLine 1 of the model describes generation of the data according to the\r\nlikelihood function: \\(y_i \\sim N(\\mu,\r\n\\sigma)\\).\r\nLine 2 of the model describes generation of the parameter θ from the\r\nprior distribution: \\(\\mu \\sim N(M,\r\n\\sigma_{\\mu})\\).\r\nIn this case parameters of the prior distribution should be defined. For\r\nexample, like above set \\(M = 100,\r\n\\sigma_{\\mu} = 200\\).\r\nThe format of the model description for JAGS is a string of the\r\nform:\r\n\r\nmodel1NormalSample= \"\r\n  model { \r\n    # Description of data\r\n    for (i in 1:Ntotal){\r\n      Y[i] ~ dnorm(mu, 1/sigma^2) #tau = 1/sigma^2\r\n    }\r\n    # Description of prior\r\n    mu ~ dnorm(100 , Ntotal/200^2)\r\n    #tau <- pow(sigma, -2) #JAGS uses precision\r\n    sigma <- 25\r\n  }\r\n\"\r\n\r\nData are described as for loop.\r\nPrior is described as typical formula:\r\n“lower order parameter $\\sim$ density(higher order\r\nparameters)”.\r\nNote. In JAGS normal distribution is specified with\r\nmean value and precision, i.e. instead of\r\n$dnorm(\\mu, \\sigma)$ use\r\n$dnorm(\\mu, \\frac{1}{\\sigma^2})$.\r\nNote that variables names for the data and data length\r\nY, nSample in the description have to match\r\nthe names of the data list.\r\nThe next step of preparation of the model description for JAGS is\r\nsaving it to a temporary text file Tempmodel.txt.\r\n\r\nwriteLines(model1NormalSample, con=\"Tempmodel.txt\")\r\n\r\nInitializing Markov chains\r\nTo initialize trajectories define a list of lists with several values\r\nor create a function that returns an init value every time it is\r\ncalled.\r\nJAGS will call this function when it starts a new chain.\r\nGeneral syntax of initiation function is:\r\n\r\ninitsList<-function() {\r\n  # Definition of mu and sigma Init\r\n  upDown<-sample(c(1,-1),1)\r\n  m <- mean(Y)*(1+upDown*.05)\r\n   \r\n  list(mu = m)\r\n}\r\n\r\nFor example, select starting point from normal distribution centered\r\nat MLE with some reasonable standard deviation.\r\nCheck how initiation works by running the function several times.\r\n\r\ninitsList()\r\n$mu\r\n[1] 126.5862\r\n\r\nSending information to JAGS\r\nNext step is getting all information to JAGS using\r\njags.model() from library rjags.\r\nThis function transfers the data, the model specification and the\r\ninitial values to JAGS and requests JAGS to select appropriate sampling\r\nmethod.\r\n\r\njagsModel <- jags.model(file = \"TempModel.txt\", data = dataList, n.chains = 3, n.adapt = 500)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 200\r\n   Unobserved stochastic nodes: 1\r\n   Total graph size: 211\r\n\r\nInitializing model\r\n\r\nIn jags.model parameter n.chains specifies\r\nthe number of chains to run (defaults to 1) and parameter\r\nn.adapt sets the number of steps JAGS can take to tune the\r\nsampling method (defaults to 1000).\r\nThe object returned by jags.model contains all\r\ninformation that we need to communicate to JAGS about the problem in the\r\nformat suitable for JAGS.\r\nRunning MCMC in JAGS:\r\nburn in and main run\r\nNow run JAGS chains for 600 steps to complete burn in,\r\ni.e. transition to a stable distribution of Markov chain.\r\n\r\nupdate(jagsModel,n.iter=600)\r\n\r\nAfter completing burn in generate MCMC trajectories representing the\r\nposterior distribution for the model.\r\n\r\ncodaSamples <- coda.samples(jagsModel, variable.names=c(\"mu\"), n.iter = 3334)\r\nlist.samplers(jagsModel)\r\n$`bugs::ConjugateNormal`\r\n[1] \"mu\"\r\nhead(codaSamples)\r\n[[1]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n           mu\r\n[1,] 120.8079\r\n[2,] 121.9097\r\n[3,] 121.4606\r\n[4,] 119.9105\r\n[5,] 121.2375\r\n[6,] 120.4787\r\n[7,] 124.6473\r\n\r\n[[2]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n           mu\r\n[1,] 121.5389\r\n[2,] 122.0809\r\n[3,] 120.0403\r\n[4,] 121.2246\r\n[5,] 123.3045\r\n[6,] 122.4725\r\n[7,] 118.3484\r\n\r\n[[3]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n           mu\r\n[1,] 123.3235\r\n[2,] 119.6016\r\n[3,] 119.2175\r\n[4,] 118.0408\r\n[5,] 122.9108\r\n[6,] 117.5459\r\n[7,] 115.9657\r\n\r\nattr(,\"class\")\r\n[1] \"mcmc.list\"\r\n\r\nBesides the model specification object coda.samples()\r\ntakes a vector of character strings corresponding to the names of\r\nparameters to record variable.names and the number of\r\niterations to run n.iter.\r\nIn this example there are 3 chains, to the total number of iterations\r\nwill be about 10,000.\r\nUse function list.samplers to show the samplers applied to\r\nthe model.\r\nAnalyzing convergence\r\nAnalyze convergence of chains using following tools:\r\nSummary\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 601:3934\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3334 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean             SD       Naive SE Time-series SE \r\n     120.21007        1.76835        0.01768        0.01768 \r\n\r\n2. Quantiles for each variable:\r\n\r\n 2.5%   25%   50%   75% 97.5% \r\n116.7 119.0 120.2 121.4 123.6 \r\n\r\nTraceplot\r\n\r\ncoda::traceplot(codaSamples)\r\n\r\ndensplot(codaSamples)\r\n\r\nplot(codaSamples)\r\n\r\n\r\nAutocorrelation and effective size\r\n\r\nautocorr.plot(codaSamples,ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n   mu \r\n10002 \r\n\r\nThe ESS number is consistent with the autocorrelation\r\nfunction.\r\nShrink factor\r\n\r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n   Point est. Upper C.I.\r\nmu          1          1\r\ngelman.plot(codaSamples)\r\n\r\n\r\nAnalyzing and\r\ninterpreting the results\r\nFind the mean values and standard deviations of posterior\r\ndistributions corresponding to different chains and compare them with\r\nMLE:\r\n\r\nchainMeans <- lapply(codaSamples,function(z) mean(z))\r\nchainSd <- lapply(codaSamples,function(z) sd(z))\r\nrbind(Means = chainMeans, SD = chainSd)\r\n      [,1]     [,2]     [,3]    \r\nMeans 120.2441 120.1821 120.204 \r\nSD    1.736047 1.788146 1.780393\r\n\r\nCompare the posterior densities generated by 3 chains with analytical\r\nposterior distribution:\r\n\r\n(l<-min(unlist(codaSamples))-.05)\r\n[1] 113.1457\r\n(h<-max(unlist(codaSamples))+.05)\r\n[1] 127.3863\r\nhistBreaks<-seq(l,h,by=.05)\r\npostHist<-lapply(codaSamples,hist,breaks=histBreaks)\r\n\r\n\r\n\r\nplot(postHist[[1]]$mids,postHist[[1]]$density,type=\"l\",\r\n     col=\"black\",lwd=2,ylab=\"Distribution Density\",xlab=\"Theta\")\r\nlines(postHist[[2]]$mids,postHist[[2]]$density,type=\"l\",col=\"red\",lwd=2)\r\nlines(postHist[[3]]$mids,postHist[[3]]$density,type=\"l\",col=\"blue\",lwd=2)\r\nlines(postHist[[3]]$mids,\r\n      dnorm(postHist[[3]]$mids,posterParamWeak[\"Mean\"],posterParamWeak[\"Sd\"]),\r\n      type=\"l\",col=\"green\",lwd=3)\r\nlegend(\"topright\",legend=c(\"Chain1\",\"Chain2\",\"Chain3\",\"Theoretical\"),col=c(\"black\",\"red\",\"blue\",\"green\"),lwd=2)\r\n\r\n\r\nHierarchical\r\nmodel: two groups of Gaussian observations\r\nConsider now two groups of Gaussian observations with unknown means\r\n\\(\\mu_1, \\mu_2\\) and same known\r\nstandard deviation \\(\\sigma = 25\\).\r\nData\r\nCreate data, prepare them for JAGS.\r\nKeep the first group sample as in the previous section: theoretical mean\r\n120 and theoretical known standard deviation 25.\r\nSimulate second sample with the same standard deviation and theoretical\r\nmean 150.\r\nCombine both samples together and add second column containing group\r\nnumber.\r\n\r\nset.seed(05162022)\r\nY2 <- rnorm(nSample, mean = 150, sd = 25)\r\nY2 <- rbind(cbind(Y, rep(1, nSample)), cbind(Y2, rep(2, nSample)))\r\ncolnames(Y2) <- c(\"y\",\"s\")\r\nden1 <- density(subset(Y2,Y2[,2]==1)[,1])\r\nden2 <- density(subset(Y2,Y2[,2]==2)[,1])\r\nplot(den1$x, den1$y, type=\"l\", ylim=c(0,.02))\r\nlines(den2$x, den2$y, col=\"blue\")\r\n\r\n\r\nThe sample from both groups now is Y2.\r\nCreate data for JAGS.\r\n\r\ny <- Y2[,\"y\"]\r\ns <- Y2[,\"s\"]\r\nnSample <- length(y)\r\nnGr <- length(unique(s))\r\ndataList <- list(y = y, s = s, nSample = nSample, nGr = nGr)\r\nnames(dataList)\r\n[1] \"y\"       \"s\"       \"nSample\" \"nGr\"    \r\n\r\nDifferent mean values,\r\ncommon weak prior\r\nIn this section consider model structure with common weak prior for\r\nmean values of both groups.\r\nThink about situations in which common prior is a reasonable\r\nassumption.\r\nThe model diagram convenient for coding it in JAGS or Stan looks\r\nlike.\r\n\r\nAn equivalent diagram may help understanding difference between\r\nhierarchical model of random effects and non-hierarchical model of fixed\r\neffects.\r\n\r\nSelect hyper-parameters of the common prior normal distribution as in\r\nthe previous section: \\(M=100, \\\r\n\\sigma_{\\mu}=200\\).\r\nModel description\r\nPrepare the model string.\r\nThe difference from one-group data description is that now prior\r\ndistribution for \\(\\mu\\) is described\r\nin a loop over all groups.\r\n\r\nmodel2NormalSample= \"\r\n  model { \r\n    # Description of data\r\n    for (i in 1:nSample){\r\n      y[i] ~ dnorm(mu[s[i]], 1/sigma^2) #tau = 1/sigma^2\r\n    }\r\n    # Description of prior\r\n    #mu[1:nGr] <- c(100,100)\r\n    for (j in 1:nGr) {      \r\n      mu[j] ~ dnorm(M, nSample/sigma_mu^2) \r\n    }\r\n    sigma <- 25\r\n    M <- 100\r\n    sigma_mu <- 200\r\n  }\r\n\"\r\nwriteLines(model2NormalSample, con=\"Tempmodel2.txt\")\r\n\r\nInitialization and\r\nsending the model to JAGS\r\nInitialize chains randomly around MLE.\r\n\r\ninitsList<-function() {\r\n  # Definition of mu and sigma Init\r\n   theta <- vector()\r\n   for (i in 1:nGr){\r\n      upDown<-sample(c(1,-1),1)\r\n      theta[i] <- mean(Y2[which(s==i)],)*(1+upDown*.05)\r\n   }\r\n  list(theta = theta)\r\n}\r\n# Check initiation\r\ninitsList()\r\n$theta\r\n[1] 126.5862 159.6764\r\n\r\nSend the model to JAGS.\r\nSet main parameters:\r\nDefine vector of parameters for monitoring and recording\r\nDefine number of steps for adaptation of samplers\r\nDefine number of burn-in steps\r\nDefine number of chains \\(n_{chains}\\)\r\nDefine number of saved steps of Markov chains \\(n_{steps}\\)\r\nDefine number of iterations per chain as \\(n_{iter} =\r\n\\frac{n_{steps}}{n_{chains}}\\)\r\nUse jags.model() to transfer information to JAGS. Arguments\r\nfor this function are: model string, data list, initiation function\r\nname, number of chains and number of steps for adaptation.\r\n\r\njagsModel2 <- jags.model(file = \"data/Tempmodel2.txt\", data = dataList, n.chains = 3, n.adapt = 500)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\n\r\nRunning the model\r\nRun burn in by using update().\r\n\r\nupdate(jagsModel2, n.iter = 600)\r\n\r\nMake the main run by using coda.samples().\r\n\r\nparameters <- c(\"mu\")\r\nnIter <- 10000\r\ncodaSamples2Groups1Prior = coda.samples(jagsModel2,\r\n                                        variable.names = parameters,\r\n                                        n.iter = nIter)\r\nhead(codaSamples2Groups1Prior)\r\n[[1]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n        mu[1]    mu[2]\r\n[1,] 119.0491 150.3414\r\n[2,] 122.1502 151.0082\r\n[3,] 122.7450 149.9710\r\n[4,] 120.0234 152.2895\r\n[5,] 117.4740 151.8449\r\n[6,] 122.5458 148.8290\r\n[7,] 120.9293 151.7965\r\n\r\n[[2]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n        mu[1]    mu[2]\r\n[1,] 117.4513 152.2848\r\n[2,] 119.4885 148.5039\r\n[3,] 120.3405 148.3678\r\n[4,] 121.7400 148.3476\r\n[5,] 122.9963 151.5334\r\n[6,] 119.3981 149.8174\r\n[7,] 117.4447 150.6277\r\n\r\n[[3]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n        mu[1]    mu[2]\r\n[1,] 122.0128 151.5173\r\n[2,] 119.6147 149.5839\r\n[3,] 122.2726 153.7575\r\n[4,] 123.3186 149.9519\r\n[5,] 119.1519 152.2584\r\n[6,] 119.5729 151.7697\r\n[7,] 118.4261 150.4220\r\n\r\nattr(,\"class\")\r\n[1] \"mcmc.list\"\r\nlist.samplers(jagsModel2)\r\n$`bugs::ConjugateNormal`\r\n[1] \"mu[2]\"\r\n\r\n$`bugs::ConjugateNormal`\r\n[1] \"mu[1]\"\r\n\r\nAnalysis\r\nAnalyze convergence.\r\n\r\nsummary(codaSamples2Groups1Prior)\r\n\r\nIterations = 601:10600\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 10000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n       Mean    SD Naive SE Time-series SE\r\nmu[1] 119.9 1.740  0.01004        0.01015\r\nmu[2] 150.5 1.749  0.01010        0.01022\r\n\r\n2. Quantiles for each variable:\r\n\r\n       2.5%   25%   50%   75% 97.5%\r\nmu[1] 116.5 118.8 119.9 121.1 123.3\r\nmu[2] 147.1 149.3 150.5 151.7 154.0\r\nplot(codaSamples2Groups1Prior)\r\n\r\nautocorr.plot(codaSamples2Groups1Prior, ask = F)\r\n\r\neffectiveSize(codaSamples2Groups1Prior)\r\n   mu[1]    mu[2] \r\n29393.12 29330.27 \r\ngelman.diag(codaSamples2Groups1Prior)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu[1]          1          1\r\nmu[2]          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples2Groups1Prior)\r\n\r\n\r\nCheck estimated means.\r\n\r\nmatrix(unlist(lapply(codaSamples2Groups1Prior,function(z) apply(z,2,mean))),ncol=3)\r\n         [,1]     [,2]     [,3]\r\n[1,] 119.9505 119.9017 119.9444\r\n[2,] 150.5145 150.5201 150.5081\r\n\r\nPlot posterior densities.\r\n\r\nplot(density(codaSamples2Groups1Prior[[1]][,1]),xlim=c(110,160),ylim=c(0,.25))\r\nlines(density(codaSamples2Groups1Prior[[1]][,2]))\r\nlines(density(codaSamples2Groups1Prior[[2]][,1]),col=\"orange\")\r\nlines(density(codaSamples2Groups1Prior[[2]][,2]),col=\"orange\")\r\nlines(density(codaSamples2Groups1Prior[[3]][,1]),col=\"blue\")\r\nlines(density(codaSamples2Groups1Prior[[3]][,2]),col=\"blue\")\r\n\r\n\r\nCalculate HDIs for each chain.\r\n\r\nlapply(codaSamples2Groups1Prior,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n         mu[1]    mu[2]\r\nlower 116.5677 147.1342\r\nupper 123.3833 153.9649\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n         mu[1]    mu[2]\r\nlower 116.5921 147.2495\r\nupper 123.4209 154.0974\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n         mu[1]    mu[2]\r\nlower 116.5453 147.0507\r\nupper 123.2449 153.8743\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nFind differences between \\(\\mu_1\\)\r\nand \\(\\mu_2\\).\r\n\r\nchainDiffs <- lapply(codaSamples2Groups1Prior, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs, function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n          var1\r\nlower 25.81190\r\nupper 35.40347\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n          var1\r\nlower 25.76339\r\nupper 35.53369\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n          var1\r\nlower 25.86207\r\nupper 35.24615\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nFind left 95% HDI boundaries for the chain differences and plot\r\nthem.\r\n\r\n(leftBounds<-unlist(lapply(chainDiffs,function(z) hdi(as.matrix(z))[1])))\r\n[1] 25.81190 25.76339 25.86207\r\nplot(density(chainDiffs[[1]]),xlim=c(15,35),ylim=c(0,.2),col=\"black\")\r\nlines(density(chainDiffs[[2]]),col=\"red\")\r\nlines(density(chainDiffs[[3]]),col=\"blue\")\r\nabline(v=leftBounds,col=c(\"black\",\"orange\",\"blue\"))\r\n\r\n\r\nFor the given sample and prior distribution mean values of the 2\r\ngroups are clearly different based on 95% HDI for all 3 chains.\r\nExperimenting with\r\nprior hyperparameters\r\nRepeat calculations with different prior hyperparameters.\r\nTo do that organize all steps of running MCMC with JAGS in a function\r\nrunMCMC2Groups <- function(prMean, prSD, dat), where\r\nprMean and prSD are the the prior\r\nhyperparameters and dat is the data list.\r\nNote that in order to pass arguments of the function to JAGS model\r\ndescription you need to append them to the data list:\r\nFirst put them in a list\r\nhyper <- list(hypeMean = prMean, hypeSD = prSD)\r\nThen combine list hyper with the data when run\r\njags.model():\r\njags.model(\"TEMPmodel.txt\", data=append(dat,hyper),...)\r\nAfter that parameters can be used in the description of the model as\r\ndnorm(hypeMean, 1/hypeSD^2).\r\nThe function should return the simulated chains.\r\n\r\nrunMCMC2Groups <- function(prMean, prSD, dat){\r\n   model2NormalSamplesb = \"\r\n      model {\r\n         for (i in 1:nSample){\r\n         y[i] ~ dnorm(theta[s[i]], 1/25^2)\r\n         }\r\n         for (sIdx in 1:nGr) {# Different thetas from same prior\r\n            theta[sIdx] ~ dnorm(hypeMean,1/hypeSD^2) \r\n         }\r\n      }\r\n   \" # close quote for modelString\r\nwriteLines(model2NormalSamplesb, con=\"data/TEMPmodel3.txt\")\r\n   \r\nparameters = c(\"theta\") # The parameters to be monitored\r\nadaptSteps = 500        # Number of steps to adapt the samplers\r\nburnInSteps = 500       # Number of steps to burn-in the chains\r\nnChains = 3             # nChains should be 2 or more for diagnostics \r\nnumSavedSteps <- 50000\r\nnIter = ceiling(numSavedSteps/nChains )\r\nhyper <- list(hypeMean = prMean, hypeSD=prSD)\r\n\r\n# Create, initialize, and adapt the model:\r\njagsModel = jags.model(\"data/TEMPmodel3.txt\", \r\n                       data = append(dat,hyper), \r\n                       inits = initsList, \r\n                       n.chains = nChains, \r\n                       n.adapt = adaptSteps)\r\n\r\nupdate(jagsModel , n.iter=burnInSteps)\r\ncodaSamplesResult <- coda.samples(jagsModel, \r\n                                  variable.names = parameters, \r\n                                  n.iter = nIter)\r\ncodaSamplesResult\r\n}\r\n\r\nRun: hypeMean=130; hypeSD=200\r\n\r\nRun.130.200 <- runMCMC2Groups(130, 200, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.130.200,function(z) apply(z, 2, mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n120.5409 152.0595 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n120.5368 152.0831 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n120.5755 152.0856 \r\nchainDiffs<-lapply(Run.130.200,function(z) z[,2]-z[,1])\r\nlapply(chainDiffs,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n          var1\r\nlower 26.65749\r\nupper 36.48272\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n          var1\r\nlower 26.83886\r\nupper 36.51442\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n          var1\r\nlower 26.48141\r\nupper 36.27759\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRun: hypeMean=130; hypeSD=0.2\r\n\r\nRun.130.2<-runMCMC2Groups(130, .2, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.130.2,function(z) apply(z,2,mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n129.8813 130.2805 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n129.8776 130.2781 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n129.8831 130.2820 \r\nchainDiffs <- lapply(Run.130.2, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            var1\r\nlower -0.1427289\r\nupper  0.9552652\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            var1\r\nlower -0.1558484\r\nupper  0.9366406\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            var1\r\nlower -0.1446940\r\nupper  0.9607175\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRun: hypeMean=100; hypeSD=0.2\r\n\r\nRun.100.2 <- runMCMC2Groups(100, .2, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.100.2, function(z) apply(z,2,mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n100.2592 100.6595 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n100.2611 100.6562 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n100.2607 100.6579 \r\nchainDiffs <- lapply(Run.100.2, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs, function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            var1\r\nlower -0.1599613\r\nupper  0.9393321\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            var1\r\nlower -0.1536713\r\nupper  0.9502405\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            var1\r\nlower -0.1511402\r\nupper  0.9589705\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRun: hypeMean=160; hypeSD=0.2\r\n\r\nRun.170.2 <- runMCMC2Groups(170, .2, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.170.2,function(z) apply(z,2,mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n169.3765 169.7750 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n169.3731 169.7741 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n169.3775 169.7736 \r\nchainDiffs <- lapply(Run.170.2, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs, function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n           var1\r\nlower -0.162517\r\nupper  0.933026\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            var1\r\nlower -0.1493182\r\nupper  0.9609448\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            var1\r\nlower -0.1544489\r\nupper  0.9363789\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nExplain the results of running hierarchical model with different sets\r\nof hyperparameters:\r\n- more informative hypeSD would lead to narrow the difference between 2\r\nmeans\r\n- with informative hypeSD, the poster would be mainly affected by the\r\nprior\r\nFNP approach: ANOVA\r\nCompare results with fixed effects ANOVA model.\r\n\r\nhead(Y2)\r\n             y s\r\n[1,] 128.41478 1\r\n[2,]  99.92492 1\r\n[3,] 149.86236 1\r\n[4,] 178.42914 1\r\n[5,]  96.67612 1\r\n[6,] 115.41590 1\r\nmANOVA <- lm(y~as.factor(s), as.data.frame(Y2))\r\nsummary(mANOVA)\r\n\r\nCall:\r\nlm(formula = y ~ as.factor(s), data = as.data.frame(Y2))\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-71.077 -18.636   0.522  16.592  78.619 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)    120.558      1.782   67.65   <2e-16 ***\r\nas.factor(s)2   31.515      2.520   12.51   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.2 on 398 degrees of freedom\r\nMultiple R-squared:  0.2821,    Adjusted R-squared:  0.2803 \r\nF-statistic: 156.4 on 1 and 398 DF,  p-value: < 2.2e-16\r\nanova(mANOVA)\r\nAnalysis of Variance Table\r\n\r\nResponse: y\r\n              Df Sum Sq Mean Sq F value    Pr(>F)    \r\nas.factor(s)   1  99316   99316  156.37 < 2.2e-16 ***\r\nResiduals    398 252793     635                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-02-01T11:20:14-06:00",
    "input_file": "Series-2-of-10---MCMC-for-Estimation-of-Gaussian-parameters.knit.md"
  },
  {
    "path": "posts/2021-11-20-Bayesian methods - Series 1 of 10/",
    "title": "From Unconditional to Multivariates Bayesian Model Workshop -- Series 1 of 10 -- Gaussian vs Robust Estimation",
    "description": "Fitting an unconditional model (without predictors) Robust estimates -- Gaussian vs. t-distribution",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-11-20",
    "categories": [
      "Biostatistics",
      "Bayesian methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nTypical response\r\ndistribution\r\nFitting\r\nnormal model for 1 group with no predictors\r\nRunning in\r\nJAGS\r\nRunning in\r\nStan\r\n\r\nRobust estimation using\r\nt-distribution\r\nGeneralized Student\r\ndistribution\r\nJAGS\r\nStan\r\n\r\nFurther\r\nreading\r\n\r\nHow to fitting an unconditional model (without predictors)? From\r\nhere, we can see the philosophy of the Bayesian’s perspective.\r\nTypical response\r\ndistribution\r\nScale y\r\nDist. \\(y \\sim\r\nf(\\mu, ...)\\)\r\nInverse link \\(\\mu = g^{-1}(\\eta)\\)\r\nMetric\r\n\\(y \\sim\r\ndnorm(\\mu,\\sigma)\\)\r\n\\(\\mu =\r\n\\eta\\)\r\nBinary\r\n\\(y \\sim\r\ndbern(\\mu)\\)\r\n\\(\\mu =\r\nlogistic(\\eta)\\)\r\nNominal\r\n\\(y \\sim\r\ndmultinom(\\mu_1, ...,\\mu_p)\\)\r\n\\(\\mu_k =\r\n\\frac{exp(\\eta_k)}{\\sum_j exp(\\eta_j)}\\)\r\nOrdinal\r\n\\(y \\sim\r\ndmultinom(\\mu_1, ...,\\mu_p)\\)\r\n\\(\\mu =\r\n\\Phi(\\frac{\\theta_k - \\eta}{\\sigma}) - \\Phi(\\frac{\\theta_{k-1} -\r\n\\eta}{\\sigma})\\)\r\nCount\r\n\\(y \\sim\r\ndpois(\\mu)\\)\r\n\\(\\mu =\r\nexp(\\eta)\\)\r\nFitting\r\nnormal model for 1 group with no predictors\r\nWe just need to estimate both parameters of normal distribution for\r\nrandom variable \\(y\\).\r\nThe diagram of the model structure\r\n(adapted from Textbook)\r\n# preparing data\r\nset.seed(03172021)\r\ny <- rnorm(100, mean = 6, sd = 3)\r\nNtotal = length(y)\r\n\r\nCreate a data list, include sample mean and standard deviation in\r\nit.\r\n\r\ndataList = list(\r\n  y = y ,\r\n  Ntotal = Ntotal ,\r\n  mean_mu = mean(y) ,\r\n  sd_mu = sd(y)\r\n)\r\n\r\nRunning in JAGS\r\nSpecify the model.\r\nRecall that in JAGS [@RN733] normal distribution is specified by\r\nprecision \\(\\frac{1}{\\sigma^2}\\)\r\ninstead of standard deviation or variance. Select an uninformative prior\r\nfor \\(\\sigma\\) and normal (conjugate)\r\nprior for \\(\\mu\\).\r\nWhat do we suggest as parameters of the normal distribution based on\r\nthe sample? \\[\r\ny_i \\sim N(\\mu,\\tau), \\ \\text{where} \\ \\tau = \\frac{1}{\\sigma^2} \\\\\r\n\\mu \\sim N(M,S) \\\\\r\n\\sigma \\sim Uniform[Lw,Mx] \\ \\text{OR} \\ Gamma(\\alpha, \\beta)\r\n\\]\r\n\r\nmodelString = \"\r\n  model {\r\n    for (i in 1:Ntotal) {\r\n      y[i] ~ dnorm(mu , 1/sigma^2) #JAGS uses precision\r\n    }\r\n    # mu ~ dnorm(mean_mu , 1/(100*sd_mu)^2)\r\n    mu ~ dnorm(mean_mu , Ntotal/sd_mu^2) #JAGS uses precision\r\n    sigma ~ dunif(sd_mu/1000, sd_mu*1000)\r\n  }\r\n  \" # close quote for modelString\r\n\r\n\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"data/TEMPmodel.txt\")\r\n\r\nInitialize chains.\r\n\r\ninitsList <- function(){\r\n   upDown <- sample(c(1,-1),1)\r\n   m <- mean(y)*(1+upDown*.05)\r\n   s <- sd(y)*(1-upDown*.1) \r\n   list(mu = m, sigma = s)\r\n}\r\n\r\nRun the chains.\r\nSet parameters:\r\n\r\n# Create, initialize, and adapt the model:\r\nparameters = c(\"mu\", \"sigma\")     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnumSavedSteps = 50000\r\nnChains = 4 \r\nthinSteps = 1\r\nnIter = ceiling((numSavedSteps*thinSteps)/nChains )\r\n\r\nSend model to JAGS:\r\n\r\njagsModel = jags.model(\"data/TEMPmodel.txt\", data=dataList, inits=initsList,\r\n                       n.chains=nChains, n.adapt=adaptSteps)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 100\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 114\r\n\r\nInitializing model\r\n\r\nBurn in and run:\r\n\r\n# Burn-in:\r\nupdate(jagsModel, n.iter=burnInSteps)\r\n\r\n# Run it\r\n# The saved MCMC chain:\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters,\r\n                           n.iter=nIter, thin=thinSteps)\r\n\r\nCheck how chains converged.\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:14000\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 12500 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n       Mean     SD Naive SE Time-series SE\r\nmu    5.922 0.2247 0.001005       0.001008\r\nsigma 3.205 0.2305 0.001031       0.001364\r\n\r\n2. Quantiles for each variable:\r\n\r\n       2.5%   25%   50%   75% 97.5%\r\nmu    5.482 5.771 5.923 6.073 6.365\r\nsigma 2.787 3.045 3.192 3.352 3.691\r\n\r\nThe parameters are estimated close to what we simulated and very\r\nsimilar to what point estimation would give.\r\n\r\nmean(y)\r\n[1] 5.922201\r\nsd(y)\r\n[1] 3.173288\r\n\r\nThe plot of the samples and the densities of the parameters.\r\n\r\nplot(codaSamples)\r\n\r\n\r\nPlot autocorrelations.\r\n\r\nautocorr.plot(codaSamples, ask = F)\r\n\r\n\r\nAutocorrelation function plot shows that standard deviation effective\r\nsize must be pretty small.\r\nIn fact:\r\n\r\neffectiveSize(codaSamples)\r\n      mu    sigma \r\n49664.03 28676.99 \r\n\r\nShrink factor shows that even with long memory for standard deviation\r\ndistributions converged:\r\n\r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu             1          1\r\nsigma          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\n\r\nObserved HDIs of the chains:\r\n\r\nlapply(codaSamples,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            mu    sigma\r\nlower 5.497905 2.770000\r\nupper 6.387002 3.669489\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            mu    sigma\r\nlower 5.454910 2.775516\r\nupper 6.335939 3.663572\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            mu    sigma\r\nlower 5.483835 2.773250\r\nupper 6.362358 3.670152\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[4]]\r\n            mu    sigma\r\nlower 5.477642 2.768277\r\nupper 6.354808 3.668637\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRunning in Stan\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    real mean_mu;\r\n    real sd_mu;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    unifLo = sd_mu/100;\r\n    unifHi = sd_mu*100;\r\n    normalSigma = sd_mu*100;  // 100*10 times larger than MLE\r\n}\r\nparameters {\r\n    real mu;\r\n    real<lower=0> sigma;\r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi);\r\n    mu ~ normal(mean_mu, normalSigma); \r\n    y ~ normal(mu, sigma);\r\n}\r\n\" # close quote for modelString\r\n\r\nCreate a DSO and save it to disk to reuse later or just keep it in\r\nmemory.\r\n\r\nstanDso <- stan_model(model_code = modelString)\r\nsave(stanDso, file = \"data/DSONormal1.Rds\")\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\nload(file= \"data/DSONormal1.Rds\")\r\n\r\nRun chains by either using the existing DSO:\r\n\r\nstanFit <- sampling(object=stanDso,\r\n                    data = dataList,\r\n                    pars=c('mu', 'sigma'),\r\n                    chains = 2,\r\n                    cores= 2,\r\n                    iter = 5000,\r\n                    warmup = 200, \r\n                    thin = 1)\r\n\r\nOr alternatively, use the description of the model saved in\r\nch16_1.stan directly:\r\n\r\n# fit model\r\nfit <- stan(file = \"modelString.stan\", \r\n            data = list(Ntotal = length(y),\r\n                        y = y,\r\n                        meanY=mean(y),\r\n                        sdY=sd(y)), \r\n            pars=c('mu', 'sigma'),\r\n            # control=list(adapt_delta=0.99),\r\n            iter=5000, chains = 2, cores = 2\r\n            )\r\n\r\nObjects fit and stanFit should return very\r\nsimilar results. The difference between stan() and\r\nsampling() is in the argument object which is\r\nDSO. If you expect to repeat same calculations with different data\r\ncompiling a DSO and reusing it with sampling() is\r\nfaster.\r\n\r\n# text statistics:\r\nprint (stanFit)\r\nInference for Stan model: 7287f24c81dcdd127c0648b2cf81052e.\r\n2 chains, each with iter=5000; warmup=200; thin=1; \r\npost-warmup draws per chain=4800, total post-warmup draws=9600.\r\n\r\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nmu       5.92    0.00 0.32    5.30    5.71    5.93    6.14    6.56\r\nsigma    3.21    0.00 0.23    2.80    3.05    3.20    3.36    3.71\r\nlp__  -164.81    0.02 1.00 -167.47 -165.17 -164.51 -164.11 -163.85\r\n      n_eff Rhat\r\nmu     7607    1\r\nsigma  7580    1\r\nlp__   4216    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Mon Jan 30 01:09:12 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\n# estimates & hdi:\r\nplot(stanFit)\r\n\r\n# samples\r\ntraceplot(stanFit, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFit, pars=c('mu','sigma'))\r\n\r\nstan_scat(stanFit, c('mu', 'sigma'))\r\n\r\nstan_hist(stanFit)\r\n\r\nstan_dens(stanFit)\r\n\r\n# autocorrelation:\r\nstan_ac(stanFit, separate_chains = T)\r\n\r\n\r\n\r\n# or work with familiar coda class:\r\nstan2coda <- function(fit) {\r\n    # apply to all chains\r\n    mcmc.list(lapply(1:ncol(fit), function(x) mcmc(as.array(fit)[,x,])))\r\n}\r\ncodaSamples <- stan2coda(stanFit)\r\nsummary(codaSamples)\r\n\r\nIterations = 1:4800\r\nThinning interval = 1 \r\nNumber of chains = 2 \r\nSample size per chain = 4800 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean     SD Naive SE Time-series SE\r\nmu       5.924 0.3174 0.003240       0.003540\r\nsigma    3.209 0.2318 0.002366       0.002656\r\nlp__  -164.813 1.0006 0.010212       0.015505\r\n\r\n2. Quantiles for each variable:\r\n\r\n          2.5%      25%      50%      75%    97.5%\r\nmu       5.304    5.710    5.926    6.137    6.558\r\nsigma    2.801    3.047    3.196    3.356    3.711\r\nlp__  -167.475 -165.171 -164.509 -164.114 -163.848\r\nplot(codaSamples)\r\n\r\nautocorr.plot(codaSamples) \r\n\r\neffectiveSize(codaSamples)\r\n      mu    sigma     lp__ \r\n8585.257 7709.123 4173.029 \r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu          1.00       1.00\r\nsigma       1.00       1.01\r\nlp__        1.01       1.02\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\n\r\n\r\nplot(density(codaSamples[[1]][,1]),xlim=c(0,8),ylim=c(0,3))  # mu, 1st chain\r\nlines(density(codaSamples[[1]][,2]))                         # sigma, 1st chain\r\nlines(density(codaSamples[[2]][,1]),col=\"red\")               # mu, 2nd chain\r\nlines(density(codaSamples[[2]][,2]),col=\"red\")               # sigma, 2nd chain\r\n\r\n\r\nOr you can use shinystan to do similar analysis of fitted model:\r\n\r\nlaunch_shinystan(fit)\r\n\r\nRobust estimation using\r\nt-distribution\r\nCreate a sample with heavy tails in order to check robust estimation\r\nof parameters of normal distribution.\r\nRefer this simulation of Leptokurtic\r\nDistributions.\r\n\r\nnSample<-1000\r\nsd.Values<-c(2,3.4,.8,2.6)\r\nsd.process<-rep(c(rep(sd.Values[1],50),\r\n                  rep(sd.Values[2],75),\r\n                  rep(sd.Values[3],75),\r\n                  rep(sd.Values[4],50)), 4)\r\n            \r\nplot(sd.process,type=\"l\")\r\n\r\n\r\nVariable sd.process is a deterministically changing\r\nstandard deviation. Simulating perfect normal and independent\r\nrealizations with such different standard deviations make a leptokurtic\r\ndistribution.\r\n\r\nset.seed(05062022)\r\ny <- rnorm(nSample)*sd.process\r\ny <- y[1:300]\r\nplot(y, type=\"l\")\r\n\r\nden <- density(y)\r\nplot(den)\r\nlines(den$x, dnorm(den$x, mean(y), sd(y)), col=\"red\")\r\n\r\n\r\n\r\nNtotal = length(y)\r\n\r\nDensity plot clearly shows fat tails that will be most likely\r\nidentified as outliers under the assumption of normal distribution.\r\nCreate data list.\r\n\r\ndataList = list(\r\n   y = y ,\r\n   Ntotal = Ntotal ,\r\n   mean_mu = mean(y) ,\r\n   sd_mu = sd(y)\r\n   )\r\n\r\nGeneralized Student\r\ndistribution\r\nStudent distribution with \\(\\nu\\)\r\ndegrees of freedom appears in a sampling scheme from Gaussian\r\ndistribution with unknown mean and variance.\r\nIn such case the variable \\[ X \\sim N(\\mu,\r\n\\sigma)\\] has the \\(z\\)-score\r\n\\[z = \\frac{X - \\hat{\\mu}}{s/\\sqrt{n}} =\r\n\\frac{X - \\hat{\\mu}}{\\hat{\\sigma}} \\] where \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i -\r\n\\hat{\\mu})^2\\), has\\(t\\)-distribution with \\(n-1\\) degrees of freedom.\r\nThen random variable \\[ X = \\hat{\\mu} +\r\n\\hat{\\sigma}z\\]\r\nhas generalized Student distribution with location parameter \\(\\hat{\\mu}\\), scale parameter \\(\\hat{\\sigma}\\) and the number of degrees of\r\nfreedom \\(\\nu\\).\r\nThe parameters \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) are not exactly\r\ncorresponding to the corresponding parameters of Gaussian distribution,\r\nbut \\(\\hat{\\mu} \\rightarrow \\mu\\),\r\n\\(\\hat{\\sigma} \\rightarrow \\sigma\\) as\r\n\\(\\nu\\) becomes small and \\(t\\)-distribution becomes Gaussian.\r\nInterpretation of the parameters:\r\n\\(\\hat{\\mu}\\) - location\r\n\\(\\hat{\\sigma}\\) - scale\r\n\\(\\nu\\) - amount of mass in the\r\ntails.\r\nJAGS\r\n\r\nmodelString = \"\r\nmodel {\r\n   for ( i in 1:Ntotal ) {\r\n      y[i] ~ dt(mu,1/sigma^2,nu)\r\n   }\r\n   mu ~ dnorm( mean_mu , 100/sd_mu^2 )\r\n   sigma ~ dunif( sd_mu/1000 , sd_mu*1000 )\r\n   nu ~ dexp(1/30.0)\r\n}\r\n\" # close quote for modelString\r\n# Write out modelString to a text file\r\nwriteLines(modelString , con=\"data/TEMPmodel-jags.txt\")\r\n\r\nInitialize the model with MLE.\r\n\r\ninitsList <-function() {\r\n   upDown<-sample(c(1,-1),1)\r\n   m <- mean(y)*(1+upDown*.05)\r\n   s <- sd(y)*(1-upDown*.1) \r\n   list(mu = m, sigma = s, nu=2)\r\n   }\r\n\r\n\r\nparameters = c(\"mu\", \"sigma\", \"nu\")     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 3 \r\nthinSteps = 1\r\nnumSavedSteps=50000\r\n(nIter = ceiling((numSavedSteps*thinSteps)/nChains))\r\n[1] 16667\r\n\r\n\r\n# Create, initialize, and adapt the model:\r\njagsModel = jags.model(\"data/TEMPmodel-jags.txt\", data=dataList, inits=initsList, \r\n                       n.chains=nChains, n.adapt=adaptSteps )\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 300\r\n   Unobserved stochastic nodes: 3\r\n   Total graph size: 318\r\n\r\nInitializing model\r\n\r\nRun the chains.\r\n\r\n# Burn-in:\r\nupdate(jagsModel, n.iter=burnInSteps)\r\n# The saved MCMC chain:\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters , \r\n                           n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the results.\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:18167\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 16667 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean     SD  Naive SE Time-series SE\r\nmu    -0.2798 0.1122 0.0005020      0.0006708\r\nnu     4.1246 1.7491 0.0078222      0.0422100\r\nsigma  1.8089 0.1662 0.0007431      0.0020599\r\n\r\n2. Quantiles for each variable:\r\n\r\n        2.5%     25%     50%     75%    97.5%\r\nmu    -0.503 -0.3548 -0.2787 -0.2035 -0.06232\r\nnu     2.317  3.1416  3.7809  4.6724  7.83060\r\nsigma  1.501  1.6949  1.8026  1.9191  2.15050\r\nmean(y)\r\n[1] -0.4222642\r\nsd(y)\r\n[1] 2.495025\r\n\r\nNote that the robust estimate of \\(\\mu\\) is similar, but \\(\\sigma\\) is significantly smaller.\r\n\r\nplot(codaSamples)\r\n\r\nsummary(sd.process)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   0.80    0.80    2.30    2.18    3.40    3.40 \r\nautocorr.plot(codaSamples,ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n       mu        nu     sigma \r\n28003.509  3901.553  6529.706 \r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu             1          1\r\nnu             1          1\r\nsigma          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\nhead(codaSamples[1])\r\n[[1]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 1501 \r\nEnd = 1507 \r\nThinning interval = 1 \r\n             mu       nu    sigma\r\n[1,] -0.3872503 3.878832 1.914200\r\n[2,] -0.1979780 2.986797 1.653501\r\n[3,] -0.2024554 2.823693 1.576980\r\n[4,] -0.1487796 2.515444 1.604900\r\n[5,] -0.3249306 2.495957 1.670748\r\n[6,] -0.2074876 3.003838 1.640470\r\n[7,] -0.2795339 3.691965 1.678529\r\n\r\nattr(,\"class\")\r\n[1] \"mcmc.list\"\r\n(HDIofChains<-lapply(codaSamples,function(z) hdi(as.matrix(z))))\r\n[[1]]\r\n               mu       nu    sigma\r\nlower -0.49364460 2.050992 1.486012\r\nupper -0.05725804 6.966700 2.139252\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n               mu       nu    sigma\r\nlower -0.50086511 2.069708 1.494407\r\nupper -0.05972026 6.934030 2.133481\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n               mu       nu    sigma\r\nlower -0.49888331 2.015185 1.483335\r\nupper -0.05911619 6.916207 2.131834\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nNon-robust estimate of \\(\\sigma\\) is\r\noutside the HDI for all the chains.\r\nStan\r\nAdapt the model description to t-distribution with additional\r\nparameter \\(\\nu\\).\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    real mean_mu;\r\n    real sd_mu;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    real expLambda;         //New: parameter of prior for nu \r\n    unifLo = sd_mu/1000;\r\n    unifHi = sd_mu*1000;\r\n    normalSigma = sd_mu*100;\r\n    expLambda=1/29.0;      //New: setting value for expLambda\r\n}\r\nparameters {\r\n    real<lower=0> nuMinusOne; //New: definition of additional parameter nu\r\n    real mu;\r\n    real<lower=0> sigma;\r\n}\r\ntransformed parameters {\r\n    real<lower=0> nu;           //New: new parameter nu\r\n    nu=nuMinusOne+1;           //New: shifting nu to avoid zero\r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi);\r\n    mu ~ normal(mean_mu, normalSigma);\r\n    nuMinusOne~exponential(expLambda);      //New: exponential prior for nu\r\n    y ~ student_t(nu, mu, sigma);           //New: student_t distribution for nu\r\n}\r\n\" # close quote for modelString\r\n\r\nCreate DSO.\r\n\r\nstanDso <- stan_model(model_code = modelString)\r\nsave(stanDso,file = \"data/DSONormal2.Rds\")\r\n\r\n\r\nload(file = \"data/DSONormal2.Rds\")\r\n\r\nRun MCMC.\r\n\r\nstanFitRobust <- sampling( object=stanDso , \r\n                     data = dataList ,\r\n                     pars=c('nu','mu', 'sigma'),\r\n                     chains = 3,\r\n                     cores= 3,\r\n                     iter = 50000,\r\n                     warmup = 300, \r\n                     thin = 1 )\r\n\r\nExplore the results.\r\n\r\n# text statistics:\r\nprint(stanFitRobust)\r\nInference for Stan model: e67fa4c368c01a78b59917da9be5f705.\r\n3 chains, each with iter=50000; warmup=300; thin=1; \r\npost-warmup draws per chain=49700, total post-warmup draws=149100.\r\n\r\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nnu       4.05    0.01 1.53    2.30    3.10    3.73    4.59    7.69\r\nmu      -0.24    0.00 0.13   -0.49   -0.33   -0.24   -0.16    0.00\r\nsigma    1.80    0.00 0.17    1.49    1.69    1.80    1.91    2.14\r\nlp__  -514.39    0.01 1.27 -517.68 -514.96 -514.07 -513.47 -512.95\r\n      n_eff Rhat\r\nnu    57236    1\r\nmu    78604    1\r\nsigma 70574    1\r\nlp__  55132    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Mon Jan 30 01:12:08 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\n# estimates & hdi:\r\nplot(stanFitRobust)\r\n\r\n# samples\r\nclass(stanFitRobust)\r\n[1] \"stanfit\"\r\nattr(,\"package\")\r\n[1] \"rstan\"\r\nrstan::traceplot(stanFitRobust, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFitRobust, pars=c('nu','mu','sigma'))\r\n\r\nstan_scat(stanFitRobust, c('nu','mu'))\r\n\r\nstan_scat(stanFitRobust, c('nu','sigma'))\r\n\r\nstan_scat(stanFitRobust, c('mu','sigma'))\r\n\r\nstan_hist(stanFitRobust)\r\n\r\nstan_dens(stanFitRobust)\r\n\r\n# autocorrelation:\r\nstan_ac(stanFitRobust, separate_chains = T)\r\n\r\nstan_diag(stanFitRobust,information = \"sample\")\r\n\r\nstan_diag(stanFitRobust,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"divergence\",chain = 0)\r\n\r\n\r\nThere seems to be a pattern in relationship between \\(\\sigma\\) and \\(\\nu\\).\r\nCheck if there is dependency.\r\n\r\nstanRobustChains <- extract(stanFitRobust)\r\nnames(stanRobustChains)\r\n[1] \"nu\"    \"mu\"    \"sigma\" \"lp__\" \r\nplot(stanRobustChains$nu,stanRobustChains$sigma)\r\n\r\nplot(rank(stanRobustChains$nu),rank(stanRobustChains$sigma))\r\n\r\n\r\nInterpret the dependency: Degrees of freedom \\(\\nu\\) are related to sample size \\((n-1)\\). If the df increases, it also\r\nstands that the sample size is increasing; the graph of the\r\nt-distribution will have skinnier tails, pushing the critical value\r\ntowards the mean.\r\n\\(\\Rightarrow\\) t-copulas (guess\r\nfrom empirical copulas)\r\nExplore shiny object.\r\n\r\nlaunch_shinystan(stanFitRobust)\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th\r\ned. ed.). Boston: McGraw-Hill Irwin.\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-01-30T01:13:41-06:00",
    "input_file": "Series-1-of-10---Fitting-an-unconditional-models.knit.md"
  },
  {
    "path": "posts/2021-10-24-LPA Chicago Neighborhoods/",
    "title": "Latent Profile Analysis of Chicago Neighborhoods",
    "description": "American Census Survey data;    \nFactor Analysis;    \nLatent Profile Analysis: Gaussian Mixture modeling;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-10-24",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Factor Analysis",
      "Mclust",
      "Tutorial",
      "Mapping"
    ],
    "contents": "\r\n\r\nContents\r\nFactor Analysis\r\nACS data\r\nDetermine Number of Factors to Extract\r\n\r\nLatent Profile Analysis\r\nData to run LPA\r\nCheck how many clusters should create?\r\nFitted a 4-components GMM with unconstrained covariance matrices\r\n\r\nApplying the Clustering\r\nMapping on Chicago Neighborhood map\r\nClassification by race-ethnicity\r\n\r\nRefs\r\n\r\n\r\nlibrary(mclust)\r\nlibrary(psych)\r\nlibrary(tidyverse)\r\nlibrary(nFactors)\r\nlibrary(kableExtra)\r\n\r\nFactor Analysis\r\nACS data\r\nThe summary of the data “Chicago Census Data by Tract_American Census Survey (ACS)”. Data includes Census tract characteristics of 2010-2014 5-year ACS estimates that easy to download from https://data.census.gov/cedsci/.\r\nWe will use the following variables in the exploratory factor analysis:\r\nAgeDependencyRatio: age dependency ratio\r\nLimitedEngProf5andUP: limited English proficiency\r\nLessThanHS: less than high school in education level\r\nUnemployment\r\nPctForeignBorn: percentage of foreign born\r\nFemaleHHPct: percentage of female per household\r\nMedianIncomeHH: median income per household\r\nAllOcc2009Pct: percentage of all occupants in 2009\r\nPctVacHousing: percentage of vacant housing\r\nPctBelowPoverty: percentage below poverty\r\nPublicAssistancePct: percentage of public assistance\r\n\r\ncensus <- read.csv(\"data/Chicago Census Data by Tract_ACS 2015.csv\", header = TRUE) %>% \r\n  rename(tract = ï..tract) %>% \r\n  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)\r\n\r\nsummary(census)\r\n     tract        AgeDependencyRatio LimitedEngProf5andUP\r\n Min.   : 10100   Min.   :  4.80     Min.   : 0.000      \r\n 1st Qu.:160825   1st Qu.: 39.80     1st Qu.: 1.825      \r\n Median :351250   Median : 54.95     Median : 8.400      \r\n Mean   :403946   Mean   : 52.89     Mean   :13.974      \r\n 3rd Qu.:670275   3rd Qu.: 65.28     3rd Qu.:23.950      \r\n Max.   :843900   Max.   :147.40     Max.   :64.000      \r\n                                                         \r\n   LessThanHS     Unemployment   PctForeignBorn     FemaleHHPct    \r\n Min.   : 0.00   Min.   : 0.00   Min.   :0.00000   Min.   :0.0000  \r\n 1st Qu.: 8.20   1st Qu.: 7.00   1st Qu.:0.03864   1st Qu.:0.0871  \r\n Median :16.30   Median :11.85   Median :0.15108   Median :0.1651  \r\n Mean   :18.38   Mean   :14.51   Mean   :0.18641   Mean   :0.2013  \r\n 3rd Qu.:26.20   3rd Qu.:19.80   3rd Qu.:0.31061   3rd Qu.:0.2986  \r\n Max.   :62.50   Max.   :91.90   Max.   :0.71263   Max.   :0.7047  \r\n                                                                   \r\n MedianIncomeHH   AllOcc2009Pct    PctVacHousing     PctBelowPoverty\r\n Min.   :  5000   Min.   :0.0000   Min.   :0.00000   Min.   : 0.40  \r\n 1st Qu.: 29402   1st Qu.:0.5341   1st Qu.:0.07766   1st Qu.:12.53  \r\n Median : 42321   Median :0.6285   Median :0.11732   Median :21.85  \r\n Mean   : 49658   Mean   :0.6284   Mean   :0.13969   Mean   :24.08  \r\n 3rd Qu.: 62056   3rd Qu.:0.7279   3rd Qu.:0.18202   3rd Qu.:33.80  \r\n Max.   :160455   Max.   :0.9406   Max.   :0.70764   Max.   :74.20  \r\n NA's   :1                                                          \r\n PublicAssistancePct\r\n Min.   :0.0000     \r\n 1st Qu.:0.0929     \r\n Median :0.2207     \r\n Mean   :0.2448     \r\n 3rd Qu.:0.3667     \r\n Max.   :0.7985     \r\n                    \r\n\r\n\r\n# recode into % to scale to other variables\r\ndf <- census %>% mutate(\r\n  PctForeignBorn = PctForeignBorn*100,\r\n  FemaleHHPct = FemaleHHPct*100,\r\n  # MedianIncomeHH1Kinv = (max(MedianIncomeHH, na.rm=TRUE) - MedianIncomeHH)/1000,\r\n  # we would not use 1Kinv because of weird distribution (multi-mode)\r\n  MedianIncomeHH1K = MedianIncomeHH/1000,\r\n  AllOcc2009Pct = AllOcc2009Pct*100,\r\n  PctVacHousing = PctVacHousing*100,\r\n  PublicAssistancePct = PublicAssistancePct*100\r\n) %>% \r\n  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH1K, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)\r\n\r\ndf.fa <- df[complete.cases(df),] %>% \r\n  select(-tract)# 1 missing case\r\n\r\n\r\nsummary(df.fa)\r\n AgeDependencyRatio LimitedEngProf5andUP   LessThanHS   \r\n Min.   :  4.8      Min.   : 0.00        Min.   : 0.00  \r\n 1st Qu.: 39.8      1st Qu.: 1.90        1st Qu.: 8.20  \r\n Median : 55.0      Median : 8.40        Median :16.30  \r\n Mean   : 52.9      Mean   :13.99        Mean   :18.39  \r\n 3rd Qu.: 65.3      3rd Qu.:24.00        3rd Qu.:26.20  \r\n Max.   :147.4      Max.   :64.00        Max.   :62.50  \r\n  Unemployment   PctForeignBorn    FemaleHHPct     MedianIncomeHH1K\r\n Min.   : 0.00   Min.   : 0.000   Min.   : 0.000   Min.   :  5.00  \r\n 1st Qu.: 7.00   1st Qu.: 3.858   1st Qu.: 8.704   1st Qu.: 29.40  \r\n Median :11.90   Median :15.115   Median :16.502   Median : 42.32  \r\n Mean   :14.52   Mean   :18.659   Mean   :20.122   Mean   : 49.66  \r\n 3rd Qu.:19.80   3rd Qu.:31.076   3rd Qu.:29.859   3rd Qu.: 62.06  \r\n Max.   :91.90   Max.   :71.263   Max.   :70.470   Max.   :160.46  \r\n AllOcc2009Pct   PctVacHousing    PctBelowPoverty PublicAssistancePct\r\n Min.   : 0.00   Min.   : 0.000   Min.   : 0.40   Min.   : 0.000     \r\n 1st Qu.:53.45   1st Qu.: 7.752   1st Qu.:12.50   1st Qu.: 9.252     \r\n Median :62.85   Median :11.722   Median :21.80   Median :22.002     \r\n Mean   :62.85   Mean   :13.898   Mean   :24.07   Mean   :24.440     \r\n 3rd Qu.:72.80   3rd Qu.:18.066   3rd Qu.:33.80   3rd Qu.:36.667     \r\n Max.   :94.06   Max.   :50.673   Max.   :74.20   Max.   :79.851     \r\n\r\nDetermine Number of Factors to Extract\r\n\r\nev <- eigen(cor(df.fa)) # get eigenvalues\r\nap <- parallel(subject=nrow(df.fa),var=ncol(df.fa), rep=100,cent=.05)\r\nnS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)\r\nplotnScree(nS) \r\n\r\n\r\nWhen we run a factor analysis, we need to decide on three things:\r\nthe number of factors\r\nthe method of estimation\r\nthe rotation\r\n\\(\\Rightarrow\\) Maximum Likelihood Factor Analysis (bullet 2) entering raw data and extracting 3 factors (bullet 1), with varimax rotation (bullet 3)\r\n\r\nfit.f <- factanal(x = df.fa, factors = 3, n.obs = 797, rotation=\"varimax\")\r\nprint(fit.f, digits=2, cutoff=.3, sort=TRUE)\r\n\r\nCall:\r\nfactanal(x = df.fa, factors = 3, n.obs = 797, rotation = \"varimax\")\r\n\r\nUniquenesses:\r\n  AgeDependencyRatio LimitedEngProf5andUP           LessThanHS \r\n                0.37                 0.00                 0.19 \r\n        Unemployment       PctForeignBorn          FemaleHHPct \r\n                0.29                 0.10                 0.18 \r\n    MedianIncomeHH1K        AllOcc2009Pct        PctVacHousing \r\n                0.29                 0.21                 0.60 \r\n     PctBelowPoverty  PublicAssistancePct \r\n                0.12                 0.07 \r\n\r\nLoadings:\r\n                     Factor1 Factor2 Factor3\r\nAgeDependencyRatio    0.56            0.56  \r\nUnemployment          0.80                  \r\nFemaleHHPct           0.84                  \r\nMedianIncomeHH1K     -0.80                  \r\nPctVacHousing         0.61                  \r\nPctBelowPoverty       0.93                  \r\nPublicAssistancePct   0.96                  \r\nLimitedEngProf5andUP          0.99          \r\nLessThanHS            0.53    0.69          \r\nPctForeignBorn                0.91          \r\nAllOcc2009Pct                         0.89  \r\n\r\n               Factor1 Factor2 Factor3\r\nSS loadings       4.82    2.43    1.33\r\nProportion Var    0.44    0.22    0.12\r\nCumulative Var    0.44    0.66    0.78\r\n\r\nTest of the hypothesis that 3 factors are sufficient.\r\nThe chi square statistic is 222.31 on 25 degrees of freedom.\r\nThe p-value is 1.46e-33 \r\n\r\nIn general, we’d like to see low uniquenesses or high communalities (subtract the uniquenesses from 1—The communality is the proportion of variance of the \\(i^{th}\\) variable contributed by the \\(m\\) (here’s 3) common factors)\r\nAt the conclusion of a factor analysis of census data, we might determine that the census measures 3 factors: (1) social poverty disparity strength, (2) education strength, (3) age disparity strength\r\nFactor analysis seeks to model the correlation matrix with fewer variables called factors. If we succeed with, said here, 3 factors, we are able to model the correlation matrix using only 3 variables instead of 11. Just remember these 3 variables, or factors, are unobserved. We give them names like “latent variables”. They are not subsets of our original variables.\r\nGiven the data matrix \\(X\\), consider it’s covariance matrix \\(cov(X)=\\sum\\) and obtain the matrix \\(V\\) of eigenvectors from \\(\\sum\\). This matrix \\(V\\) is what you call the loadings.\r\nThis set of eigenvectors define an orthogonal change of basis matrix that maximize the variance from X. This means that, if I project X into the subspace generated by V I will obtain a matrix U by simply solving XV=U. This matrix U is what you call the scores (for each factor, we would have its own score).\r\nIn our case, we would manipulate a little bit due to we use only variables in the first factor. Thus, if the absolute value of loadings less than or equal 0.5, it would not contribute to the score\r\n\r\ndf.fa$concdisadv <- as.matrix(df.fa) %*% (fit.f$loadings[,1]*(abs(fit.f$loadings[,1])>0.5))\r\n\r\n# print out the scores\r\nkable(df.fa[1:10,], caption = \"FA scores -- The concentrated disadvantage index scores\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:concentrated disadvantage–fa score)FA scores – The concentrated disadvantage index scores\r\n\r\n\r\nAgeDependencyRatio\r\n\r\n\r\nLimitedEngProf5andUP\r\n\r\n\r\nLessThanHS\r\n\r\n\r\nUnemployment\r\n\r\n\r\nPctForeignBorn\r\n\r\n\r\nFemaleHHPct\r\n\r\n\r\nMedianIncomeHH1K\r\n\r\n\r\nAllOcc2009Pct\r\n\r\n\r\nPctVacHousing\r\n\r\n\r\nPctBelowPoverty\r\n\r\n\r\nPublicAssistancePct\r\n\r\n\r\nconcdisadv\r\n\r\n\r\n30.6\r\n\r\n\r\n3.5\r\n\r\n\r\n6.6\r\n\r\n\r\n13.0\r\n\r\n\r\n15.09985\r\n\r\n\r\n13.586956\r\n\r\n\r\n32.188\r\n\r\n\r\n52.35507\r\n\r\n\r\n15.885714\r\n\r\n\r\n28.7\r\n\r\n\r\n24.36594\r\n\r\n\r\n76.40009\r\n\r\n\r\n39.4\r\n\r\n\r\n22.7\r\n\r\n\r\n21.0\r\n\r\n\r\n9.9\r\n\r\n\r\n35.50975\r\n\r\n\r\n18.823979\r\n\r\n\r\n39.122\r\n\r\n\r\n47.99542\r\n\r\n\r\n12.903226\r\n\r\n\r\n33.3\r\n\r\n\r\n27.56777\r\n\r\n\r\n90.84744\r\n\r\n\r\n47.7\r\n\r\n\r\n15.9\r\n\r\n\r\n16.3\r\n\r\n\r\n10.9\r\n\r\n\r\n31.07639\r\n\r\n\r\n11.643836\r\n\r\n\r\n27.318\r\n\r\n\r\n56.84932\r\n\r\n\r\n16.707417\r\n\r\n\r\n29.6\r\n\r\n\r\n24.65753\r\n\r\n\r\n93.39844\r\n\r\n\r\n44.4\r\n\r\n\r\n19.4\r\n\r\n\r\n22.2\r\n\r\n\r\n7.1\r\n\r\n\r\n32.23630\r\n\r\n\r\n5.439942\r\n\r\n\r\n37.111\r\n\r\n\r\n51.47864\r\n\r\n\r\n16.084559\r\n\r\n\r\n19.9\r\n\r\n\r\n18.76597\r\n\r\n\r\n63.53275\r\n\r\n\r\n23.4\r\n\r\n\r\n2.4\r\n\r\n\r\n0.9\r\n\r\n\r\n9.2\r\n\r\n\r\n13.57015\r\n\r\n\r\n6.634564\r\n\r\n\r\n38.384\r\n\r\n\r\n52.27394\r\n\r\n\r\n15.006821\r\n\r\n\r\n18.1\r\n\r\n\r\n13.32263\r\n\r\n\r\n34.51310\r\n\r\n\r\n23.7\r\n\r\n\r\n6.9\r\n\r\n\r\n7.4\r\n\r\n\r\n14.5\r\n\r\n\r\n23.37764\r\n\r\n\r\n7.255245\r\n\r\n\r\n25.309\r\n\r\n\r\n43.31294\r\n\r\n\r\n10.729614\r\n\r\n\r\n21.3\r\n\r\n\r\n20.89161\r\n\r\n\r\n60.98266\r\n\r\n\r\n16.2\r\n\r\n\r\n10.7\r\n\r\n\r\n13.6\r\n\r\n\r\n5.1\r\n\r\n\r\n31.99531\r\n\r\n\r\n5.332456\r\n\r\n\r\n35.822\r\n\r\n\r\n47.20211\r\n\r\n\r\n19.330855\r\n\r\n\r\n39.2\r\n\r\n\r\n17.18236\r\n\r\n\r\n60.81030\r\n\r\n\r\n54.1\r\n\r\n\r\n25.1\r\n\r\n\r\n23.0\r\n\r\n\r\n10.0\r\n\r\n\r\n31.82692\r\n\r\n\r\n2.746781\r\n\r\n\r\n19.813\r\n\r\n\r\n46.43777\r\n\r\n\r\n13.703704\r\n\r\n\r\n38.3\r\n\r\n\r\n30.21459\r\n\r\n\r\n109.96227\r\n\r\n\r\n32.7\r\n\r\n\r\n14.8\r\n\r\n\r\n13.1\r\n\r\n\r\n9.1\r\n\r\n\r\n25.23635\r\n\r\n\r\n10.178282\r\n\r\n\r\n37.338\r\n\r\n\r\n52.73906\r\n\r\n\r\n5.657492\r\n\r\n\r\n17.7\r\n\r\n\r\n13.16045\r\n\r\n\r\n43.67896\r\n\r\n\r\n52.7\r\n\r\n\r\n22.2\r\n\r\n\r\n18.4\r\n\r\n\r\n9.4\r\n\r\n\r\n34.15854\r\n\r\n\r\n9.987113\r\n\r\n\r\n58.319\r\n\r\n\r\n68.49227\r\n\r\n\r\n8.920188\r\n\r\n\r\n14.9\r\n\r\n\r\n23.45361\r\n\r\n\r\n50.27430\r\n\r\n\r\nLatent Profile Analysis\r\nData to run LPA\r\nThe latent profile analysis (normal mixture) will use variables loaded on factor 1 identified from the factor analysis:\r\nAgeDependencyRatio\r\nUnemployment\r\nFemaleHHPct\r\nMedianIncomeHH1K\r\nPctVacHousing\r\nPctBelowPoverty\r\nPublicAssistancePct\r\nLessThanHS\r\n\r\n\r\n\r\n    If we run Mclust for all 11 indicators, the data-driven would produce up to \r\n    8, 9 clusters. Indeed, I have checked the problem. That's the reason \r\n    we would use factor analysis to select covariates in the first components.\r\nCheck how many clusters should create?\r\n\r\nX <- as.matrix(df2[,-1])\r\nmod <- Mclust(X)\r\nsummary(mod$BIC)\r\nBest BIC values:\r\n            VVE,7        VVE,6        VVV,4\r\nBIC      -45583.8 -45597.13834 -45603.11491\r\nBIC diff      0.0    -13.33485    -19.31142\r\nplot(mod, what=\"BIC\", ylim=range(mod$BIC[,-(1:2)], na.rm=TRUE), legendArgs = list(x = \"bottomleft\")) \r\n\r\nsummary(mod)\r\n---------------------------------------------------- \r\nGaussian finite mixture model fitted by EM algorithm \r\n---------------------------------------------------- \r\n\r\nMclust VVE (ellipsoidal, equal orientation) model with 7 components: \r\n\r\n log-likelihood   n  df      BIC       ICL\r\n       -22304.2 797 146 -45583.8 -45709.47\r\n\r\nClustering table:\r\n  1   2   3   4   5   6   7 \r\n 74 143 129  19 253 101  78 \r\nhead(mod$classification)\r\n1 2 3 4 5 6 \r\n1 2 1 2 1 1 \r\nhead(mod$uncertainty)\r\n           1            2            3            4            5 \r\n0.0007328577 0.0256582102 0.0863792453 0.0139170110 0.0018607666 \r\n           6 \r\n0.0001025574 \r\nmod$uncertainty[mod$classification==4]\r\n          14           76          106          130          136 \r\n3.977713e-04 1.459000e-02 1.891587e-04 5.432918e-01 4.584159e-03 \r\n         164          337          349          395          399 \r\n2.167873e-10 4.375842e-06 1.160644e-03 3.932567e-02 4.171542e-01 \r\n         404          474          501          514          556 \r\n1.909584e-14 4.237307e-06 4.835498e-02 2.901384e-01 5.423051e-01 \r\n         606          615          730          735 \r\n3.456849e-01 4.081981e-01 0.000000e+00 1.035645e-06 \r\nplot(mod, what=\"classification\")\r\n\r\n\r\nIn the above Mclust() function call, the number of mixing components and the covariance parameterization are selected using the Bayesian Information Criterion (BIC). A summary showing the top-three models and a plot of the BIC traces (the first fig.) for all the models considered is then obtained.\r\nWe would use the indication of a 4-component mixture with covariances having different shapes, volume and orientation (VVV). The use can be confirmed by the second plot, there are no clear regions of 7 or 6-component on the plot.\r\nFitted a 4-components GMM with unconstrained covariance matrices\r\n\r\nmod.4 <- Mclust(df2[,-1], G=4) \r\nsummary(mod.4$BIC) \r\nBest BIC values:\r\n             VVV,4       VEV,4      VVE,4\r\nBIC      -45603.11 -45750.7443 -45805.360\r\nBIC diff      0.00   -147.6294   -202.245\r\nplot(mod.4, what=\"classification\")\r\n\r\nsummary(mod.4, parameters = TRUE)\r\n---------------------------------------------------- \r\nGaussian finite mixture model fitted by EM algorithm \r\n---------------------------------------------------- \r\n\r\nMclust VVV (ellipsoidal, varying volume, shape, and orientation)\r\nmodel with 4 components: \r\n\r\n log-likelihood   n  df       BIC       ICL\r\n      -22203.62 797 179 -45603.11 -45706.84\r\n\r\nClustering table:\r\n  1   2   3   4 \r\n 93 294 139 271 \r\n\r\nMixing probabilities:\r\n        1         2         3         4 \r\n0.1149777 0.3725480 0.1729934 0.3394809 \r\n\r\nMeans:\r\n                         [,1]     [,2]      [,3]     [,4]\r\nAgeDependencyRatio  23.610425 50.09446 48.738241 68.03065\r\nLessThanHS           2.443075 25.44106  9.556613 20.53996\r\nUnemployment         4.670161 11.64512  6.805765 24.94439\r\nFemaleHHPct          3.605628 15.91133  8.987856 36.01190\r\nMedianIncomeHH1K    94.399365 42.76009 74.934483 29.19450\r\nPctVacHousing       11.171810 11.36435  7.438423 20.89255\r\nPctBelowPoverty     10.542959 23.73358  9.221573 36.57643\r\nPublicAssistancePct  3.680982 22.34020  7.395136 42.46116\r\n\r\nVariances:\r\n[,,1]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          110.827954   4.505709   -2.5527991\r\nLessThanHS                    4.505709   4.825718    1.3439579\r\nUnemployment                 -2.552799   1.343958    7.4948966\r\nFemaleHHPct                   9.592752   2.963274    0.8724782\r\nMedianIncomeHH1K             21.948939  -6.215072   -5.8068859\r\nPctVacHousing                 3.617609   5.612807   13.4091673\r\nPctBelowPoverty             -11.759843   2.453223    4.0747698\r\nPublicAssistancePct           9.902373   4.939258    1.0298916\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    9.5927518       21.9489390      3.617609\r\nLessThanHS            2.9632739       -6.2150719      5.612807\r\nUnemployment          0.8724782       -5.8068859     13.409167\r\nFemaleHHPct           6.4553757        0.3348242      4.420610\r\nMedianIncomeHH1K      0.3348242      624.2423262      3.149840\r\nPctVacHousing         4.4206103        3.1498396     83.845803\r\nPctBelowPoverty       2.5312080      -45.4496868      8.420220\r\nPublicAssistancePct   6.0893513       -2.1182191      7.549918\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio       -11.759843            9.902373\r\nLessThanHS                 2.453223            4.939258\r\nUnemployment               4.074770            1.029892\r\nFemaleHHPct                2.531208            6.089351\r\nMedianIncomeHH1K         -45.449687           -2.118219\r\nPctVacHousing              8.420220            7.549918\r\nPctBelowPoverty           26.561832            3.884233\r\nPublicAssistancePct        3.884233           13.178735\r\n[,,2]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          214.300185 126.992111    16.865825\r\nLessThanHS                  126.992111 206.987885    12.301917\r\nUnemployment                 16.865825  12.301917    18.503181\r\nFemaleHHPct                  45.098013  48.787591     9.238511\r\nMedianIncomeHH1K            -23.094658 -56.231510   -11.054392\r\nPctVacHousing                -3.977919   9.033062     1.628468\r\nPctBelowPoverty               2.423802  33.812115     8.997776\r\nPublicAssistancePct          60.250147  80.617836    16.223181\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    45.098013       -23.094658     -3.977919\r\nLessThanHS            48.787591       -56.231510      9.033062\r\nUnemployment           9.238511       -11.054392      1.628468\r\nFemaleHHPct           45.928065        -7.903785      2.925860\r\nMedianIncomeHH1K      -7.903785       125.877832    -17.588453\r\nPctVacHousing          2.925860       -17.588453     21.535258\r\nPctBelowPoverty        9.689907       -73.612703     15.542755\r\nPublicAssistancePct   30.032800       -64.034777     12.149998\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio         2.423802            60.25015\r\nLessThanHS                33.812115            80.61784\r\nUnemployment               8.997776            16.22318\r\nFemaleHHPct                9.689907            30.03280\r\nMedianIncomeHH1K         -73.612703           -64.03478\r\nPctVacHousing             15.542755            12.15000\r\nPctBelowPoverty           85.926470            49.99227\r\nPublicAssistancePct       49.992266            90.50270\r\n[,,3]\r\n                    AgeDependencyRatio   LessThanHS Unemployment\r\nAgeDependencyRatio          294.197105 -12.24417798   16.7767540\r\nLessThanHS                  -12.244178  24.62164001    4.8263400\r\nUnemployment                 16.776754   4.82634001    8.0873820\r\nFemaleHHPct                   9.701178   9.04745430    6.1616615\r\nMedianIncomeHH1K             13.966827 -56.09164058  -21.9078480\r\nPctVacHousing                 5.163643  -0.03837313   -0.7952366\r\nPctBelowPoverty             -28.576144  13.88339665    4.2317608\r\nPublicAssistancePct          -7.729161  14.21609068    3.7741089\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    9.7011782        13.966827   5.163642823\r\nLessThanHS            9.0474543       -56.091641  -0.038373126\r\nUnemployment          6.1616615       -21.907848  -0.795236625\r\nFemaleHHPct          19.6797355       -28.074212  -0.444020130\r\nMedianIncomeHH1K    -28.0742123       332.351159  -7.866381763\r\nPctVacHousing        -0.4440201        -7.866382  12.781134605\r\nPctBelowPoverty       5.6968101       -48.427943   1.159862443\r\nPublicAssistancePct   8.3047060       -43.608455  -0.005379067\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio       -28.576144        -7.729161059\r\nLessThanHS                13.883397        14.216090678\r\nUnemployment               4.231761         3.774108900\r\nFemaleHHPct                5.696810         8.304706000\r\nMedianIncomeHH1K         -48.427943       -43.608455461\r\nPctVacHousing              1.159862        -0.005379067\r\nPctBelowPoverty           24.687016        15.650477682\r\nPublicAssistancePct       15.650478        22.034142873\r\n[,,4]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          218.842790   25.84154     6.528224\r\nLessThanHS                   25.841542   67.06932    13.320989\r\nUnemployment                  6.528224   13.32099    95.039772\r\nFemaleHHPct                  21.881252   17.59145    37.153627\r\nMedianIncomeHH1K            -38.416907  -39.71887   -39.349411\r\nPctVacHousing                 3.741222   22.53556    23.335309\r\nPctBelowPoverty              58.648501   57.09121    65.051666\r\nPublicAssistancePct          43.325884   47.59794    63.689544\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio     21.88125        -38.41691      3.741222\r\nLessThanHS             17.59145        -39.71887     22.535563\r\nUnemployment           37.15363        -39.34941     23.335309\r\nFemaleHHPct           115.25845        -28.87802     25.653844\r\nMedianIncomeHH1K      -28.87802        100.69000    -30.670522\r\nPctVacHousing          25.65384        -30.67052     90.092820\r\nPctBelowPoverty        73.51904       -110.34219     48.794240\r\nPublicAssistancePct    80.89187        -99.63891     42.436649\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio         58.64850            43.32588\r\nLessThanHS                 57.09121            47.59794\r\nUnemployment               65.05167            63.68954\r\nFemaleHHPct                73.51904            80.89187\r\nMedianIncomeHH1K         -110.34219           -99.63891\r\nPctVacHousing              48.79424            42.43665\r\nPctBelowPoverty           184.51805           149.38841\r\nPublicAssistancePct       149.38841           178.56187\r\n# compare classification between mod and mod.4\r\n#table(mod$classification, mod.4$classification)\r\n\r\ndf2$ConcDisadv_cluster <- mod.4$classification\r\nwith(df2, table(ConcDisadv_cluster))\r\nConcDisadv_cluster\r\n  1   2   3   4 \r\n 93 294 139 271 \r\ndf2[,11:14] <- mod.4$z\r\nnames(df2)[11:14] <- c(\"ConcDisadv_prob1\", \"ConcDisadv_prob2\", \"ConcDisadv_prob3\", \"ConcDisadv_prob4\")\r\nkable(df2[20:50,], caption = \"Concentrated Disadvantage Clustering Selection based on the highest probability\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:4-comp VVV)Concentrated Disadvantage Clustering Selection based on the highest probability\r\n\r\n\r\n\r\n\r\ntract\r\n\r\n\r\nAgeDependencyRatio\r\n\r\n\r\nLessThanHS\r\n\r\n\r\nUnemployment\r\n\r\n\r\nFemaleHHPct\r\n\r\n\r\nMedianIncomeHH1K\r\n\r\n\r\nPctVacHousing\r\n\r\n\r\nPctBelowPoverty\r\n\r\n\r\nPublicAssistancePct\r\n\r\n\r\nConcDisadv_cluster\r\n\r\n\r\nConcDisadv_prob1\r\n\r\n\r\nConcDisadv_prob2\r\n\r\n\r\nConcDisadv_prob3\r\n\r\n\r\nConcDisadv_prob4\r\n\r\n\r\n20\r\n\r\n\r\n20701\r\n\r\n\r\n65.8\r\n\r\n\r\n12.9\r\n\r\n\r\n9.1\r\n\r\n\r\n13.217391\r\n\r\n\r\n68.750\r\n\r\n\r\n17.857143\r\n\r\n\r\n13.7\r\n\r\n\r\n8.869565\r\n\r\n\r\n3\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.0081983\r\n\r\n\r\n0.9918002\r\n\r\n\r\n0.0000015\r\n\r\n\r\n21\r\n\r\n\r\n20702\r\n\r\n\r\n57.6\r\n\r\n\r\n28.1\r\n\r\n\r\n4.7\r\n\r\n\r\n12.505623\r\n\r\n\r\n54.364\r\n\r\n\r\n13.837209\r\n\r\n\r\n19.8\r\n\r\n\r\n25.551057\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9998515\r\n\r\n\r\n0.0000269\r\n\r\n\r\n0.0001216\r\n\r\n\r\n22\r\n\r\n\r\n20801\r\n\r\n\r\n50.7\r\n\r\n\r\n24.9\r\n\r\n\r\n10.5\r\n\r\n\r\n13.394683\r\n\r\n\r\n45.263\r\n\r\n\r\n12.834225\r\n\r\n\r\n21.7\r\n\r\n\r\n20.858896\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9948329\r\n\r\n\r\n0.0043355\r\n\r\n\r\n0.0008316\r\n\r\n\r\n23\r\n\r\n\r\n20802\r\n\r\n\r\n45.5\r\n\r\n\r\n18.9\r\n\r\n\r\n7.4\r\n\r\n\r\n9.287777\r\n\r\n\r\n42.346\r\n\r\n\r\n14.870954\r\n\r\n\r\n29.1\r\n\r\n\r\n29.018287\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9992171\r\n\r\n\r\n0.0000094\r\n\r\n\r\n0.0007735\r\n\r\n\r\n24\r\n\r\n\r\n20901\r\n\r\n\r\n48.0\r\n\r\n\r\n16.0\r\n\r\n\r\n5.3\r\n\r\n\r\n15.280236\r\n\r\n\r\n38.799\r\n\r\n\r\n21.199442\r\n\r\n\r\n28.2\r\n\r\n\r\n19.115044\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9633811\r\n\r\n\r\n0.0000115\r\n\r\n\r\n0.0366074\r\n\r\n\r\n25\r\n\r\n\r\n20902\r\n\r\n\r\n55.0\r\n\r\n\r\n13.5\r\n\r\n\r\n5.3\r\n\r\n\r\n10.437052\r\n\r\n\r\n33.114\r\n\r\n\r\n9.982384\r\n\r\n\r\n26.7\r\n\r\n\r\n21.461187\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9775131\r\n\r\n\r\n0.0002441\r\n\r\n\r\n0.0222428\r\n\r\n\r\n26\r\n\r\n\r\n30101\r\n\r\n\r\n36.2\r\n\r\n\r\n5.0\r\n\r\n\r\n8.7\r\n\r\n\r\n7.772727\r\n\r\n\r\n41.188\r\n\r\n\r\n11.646586\r\n\r\n\r\n21.0\r\n\r\n\r\n12.954546\r\n\r\n\r\n2\r\n\r\n\r\n0.0123186\r\n\r\n\r\n0.9845259\r\n\r\n\r\n0.0029942\r\n\r\n\r\n0.0001613\r\n\r\n\r\n27\r\n\r\n\r\n30102\r\n\r\n\r\n23.5\r\n\r\n\r\n7.4\r\n\r\n\r\n14.9\r\n\r\n\r\n8.969132\r\n\r\n\r\n43.632\r\n\r\n\r\n18.779565\r\n\r\n\r\n14.5\r\n\r\n\r\n10.308678\r\n\r\n\r\n2\r\n\r\n\r\n0.0128167\r\n\r\n\r\n0.9713396\r\n\r\n\r\n0.0000043\r\n\r\n\r\n0.0158394\r\n\r\n\r\n28\r\n\r\n\r\n30103\r\n\r\n\r\n12.7\r\n\r\n\r\n14.6\r\n\r\n\r\n8.5\r\n\r\n\r\n0.000000\r\n\r\n\r\n37.383\r\n\r\n\r\n10.844371\r\n\r\n\r\n19.5\r\n\r\n\r\n11.142061\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.7999190\r\n\r\n\r\n0.2000236\r\n\r\n\r\n0.0000574\r\n\r\n\r\n29\r\n\r\n\r\n30104\r\n\r\n\r\n9.5\r\n\r\n\r\n9.9\r\n\r\n\r\n14.4\r\n\r\n\r\n1.628106\r\n\r\n\r\n36.529\r\n\r\n\r\n8.542320\r\n\r\n\r\n39.8\r\n\r\n\r\n21.679520\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9999999\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000001\r\n\r\n\r\n30\r\n\r\n\r\n30200\r\n\r\n\r\n33.0\r\n\r\n\r\n5.5\r\n\r\n\r\n7.5\r\n\r\n\r\n3.708333\r\n\r\n\r\n67.700\r\n\r\n\r\n11.699779\r\n\r\n\r\n10.5\r\n\r\n\r\n2.500000\r\n\r\n\r\n1\r\n\r\n\r\n0.5761337\r\n\r\n\r\n0.0351601\r\n\r\n\r\n0.3887062\r\n\r\n\r\n0.0000000\r\n\r\n\r\n31\r\n\r\n\r\n30300\r\n\r\n\r\n42.8\r\n\r\n\r\n18.2\r\n\r\n\r\n8.2\r\n\r\n\r\n7.496464\r\n\r\n\r\n36.020\r\n\r\n\r\n8.062419\r\n\r\n\r\n24.8\r\n\r\n\r\n19.024045\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9637376\r\n\r\n\r\n0.0352999\r\n\r\n\r\n0.0009626\r\n\r\n\r\n32\r\n\r\n\r\n30400\r\n\r\n\r\n44.3\r\n\r\n\r\n12.3\r\n\r\n\r\n18.1\r\n\r\n\r\n14.767255\r\n\r\n\r\n41.250\r\n\r\n\r\n8.247423\r\n\r\n\r\n27.8\r\n\r\n\r\n30.577849\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9820020\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0179980\r\n\r\n\r\n33\r\n\r\n\r\n30500\r\n\r\n\r\n29.1\r\n\r\n\r\n13.3\r\n\r\n\r\n2.7\r\n\r\n\r\n6.931383\r\n\r\n\r\n62.453\r\n\r\n\r\n11.087024\r\n\r\n\r\n15.7\r\n\r\n\r\n5.781957\r\n\r\n\r\n3\r\n\r\n\r\n0.0000003\r\n\r\n\r\n0.2383814\r\n\r\n\r\n0.7616182\r\n\r\n\r\n0.0000001\r\n\r\n\r\n34\r\n\r\n\r\n30601\r\n\r\n\r\n35.0\r\n\r\n\r\n6.8\r\n\r\n\r\n9.7\r\n\r\n\r\n2.740409\r\n\r\n\r\n38.822\r\n\r\n\r\n13.079255\r\n\r\n\r\n21.4\r\n\r\n\r\n15.695067\r\n\r\n\r\n2\r\n\r\n\r\n0.0000029\r\n\r\n\r\n0.9992705\r\n\r\n\r\n0.0004956\r\n\r\n\r\n0.0002309\r\n\r\n\r\n35\r\n\r\n\r\n30603\r\n\r\n\r\n29.2\r\n\r\n\r\n7.2\r\n\r\n\r\n10.5\r\n\r\n\r\n11.488251\r\n\r\n\r\n28.631\r\n\r\n\r\n17.159337\r\n\r\n\r\n35.5\r\n\r\n\r\n20.626632\r\n\r\n\r\n2\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.9999831\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000169\r\n\r\n\r\n36\r\n\r\n\r\n30604\r\n\r\n\r\n34.4\r\n\r\n\r\n4.1\r\n\r\n\r\n6.0\r\n\r\n\r\n3.826531\r\n\r\n\r\n32.069\r\n\r\n\r\n12.421805\r\n\r\n\r\n22.2\r\n\r\n\r\n12.551020\r\n\r\n\r\n2\r\n\r\n\r\n0.0148111\r\n\r\n\r\n0.9846784\r\n\r\n\r\n0.0004018\r\n\r\n\r\n0.0001087\r\n\r\n\r\n37\r\n\r\n\r\n30701\r\n\r\n\r\n31.7\r\n\r\n\r\n15.5\r\n\r\n\r\n15.1\r\n\r\n\r\n6.310160\r\n\r\n\r\n33.504\r\n\r\n\r\n3.409091\r\n\r\n\r\n31.0\r\n\r\n\r\n14.866310\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9996054\r\n\r\n\r\n0.0000713\r\n\r\n\r\n0.0003233\r\n\r\n\r\n38\r\n\r\n\r\n30702\r\n\r\n\r\n39.6\r\n\r\n\r\n1.5\r\n\r\n\r\n7.8\r\n\r\n\r\n3.538928\r\n\r\n\r\n48.870\r\n\r\n\r\n15.107296\r\n\r\n\r\n7.8\r\n\r\n\r\n1.718908\r\n\r\n\r\n1\r\n\r\n\r\n0.9065519\r\n\r\n\r\n0.0830127\r\n\r\n\r\n0.0098448\r\n\r\n\r\n0.0005907\r\n\r\n\r\n39\r\n\r\n\r\n30703\r\n\r\n\r\n35.6\r\n\r\n\r\n5.0\r\n\r\n\r\n6.3\r\n\r\n\r\n5.128205\r\n\r\n\r\n46.174\r\n\r\n\r\n12.883117\r\n\r\n\r\n13.7\r\n\r\n\r\n17.829457\r\n\r\n\r\n2\r\n\r\n\r\n0.0001535\r\n\r\n\r\n0.9958622\r\n\r\n\r\n0.0020986\r\n\r\n\r\n0.0018857\r\n\r\n\r\n40\r\n\r\n\r\n30706\r\n\r\n\r\n54.6\r\n\r\n\r\n8.8\r\n\r\n\r\n2.2\r\n\r\n\r\n5.118755\r\n\r\n\r\n36.362\r\n\r\n\r\n8.914584\r\n\r\n\r\n21.8\r\n\r\n\r\n11.916462\r\n\r\n\r\n2\r\n\r\n\r\n0.0004362\r\n\r\n\r\n0.9797384\r\n\r\n\r\n0.0005719\r\n\r\n\r\n0.0192536\r\n\r\n\r\n41\r\n\r\n\r\n30800\r\n\r\n\r\n39.2\r\n\r\n\r\n6.7\r\n\r\n\r\n9.5\r\n\r\n\r\n3.619909\r\n\r\n\r\n70.361\r\n\r\n\r\n12.262902\r\n\r\n\r\n7.5\r\n\r\n\r\n2.664656\r\n\r\n\r\n3\r\n\r\n\r\n0.0819534\r\n\r\n\r\n0.0953799\r\n\r\n\r\n0.8226666\r\n\r\n\r\n0.0000001\r\n\r\n\r\n42\r\n\r\n\r\n30900\r\n\r\n\r\n26.7\r\n\r\n\r\n7.2\r\n\r\n\r\n2.6\r\n\r\n\r\n9.686411\r\n\r\n\r\n80.657\r\n\r\n\r\n5.716163\r\n\r\n\r\n3.2\r\n\r\n\r\n10.871080\r\n\r\n\r\n3\r\n\r\n\r\n0.1331410\r\n\r\n\r\n0.0036901\r\n\r\n\r\n0.8631689\r\n\r\n\r\n0.0000000\r\n\r\n\r\n43\r\n\r\n\r\n31000\r\n\r\n\r\n30.1\r\n\r\n\r\n5.6\r\n\r\n\r\n5.9\r\n\r\n\r\n8.908202\r\n\r\n\r\n66.875\r\n\r\n\r\n11.191510\r\n\r\n\r\n5.8\r\n\r\n\r\n5.377512\r\n\r\n\r\n3\r\n\r\n\r\n0.0880318\r\n\r\n\r\n0.1053896\r\n\r\n\r\n0.8065776\r\n\r\n\r\n0.0000009\r\n\r\n\r\n44\r\n\r\n\r\n31100\r\n\r\n\r\n21.8\r\n\r\n\r\n12.9\r\n\r\n\r\n7.9\r\n\r\n\r\n5.386221\r\n\r\n\r\n57.994\r\n\r\n\r\n4.543643\r\n\r\n\r\n8.6\r\n\r\n\r\n7.891440\r\n\r\n\r\n3\r\n\r\n\r\n0.0000005\r\n\r\n\r\n0.0988186\r\n\r\n\r\n0.9011727\r\n\r\n\r\n0.0000082\r\n\r\n\r\n45\r\n\r\n\r\n31200\r\n\r\n\r\n23.6\r\n\r\n\r\n24.0\r\n\r\n\r\n10.9\r\n\r\n\r\n6.720122\r\n\r\n\r\n20.887\r\n\r\n\r\n16.111467\r\n\r\n\r\n34.9\r\n\r\n\r\n42.802596\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9878027\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0121973\r\n\r\n\r\n46\r\n\r\n\r\n31300\r\n\r\n\r\n57.4\r\n\r\n\r\n10.7\r\n\r\n\r\n10.0\r\n\r\n\r\n11.997074\r\n\r\n\r\n35.349\r\n\r\n\r\n3.392226\r\n\r\n\r\n23.6\r\n\r\n\r\n25.042672\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.8653997\r\n\r\n\r\n0.0003787\r\n\r\n\r\n0.1342215\r\n\r\n\r\n47\r\n\r\n\r\n31400\r\n\r\n\r\n27.4\r\n\r\n\r\n4.9\r\n\r\n\r\n4.0\r\n\r\n\r\n6.587404\r\n\r\n\r\n74.737\r\n\r\n\r\n9.508578\r\n\r\n\r\n8.1\r\n\r\n\r\n3.566838\r\n\r\n\r\n3\r\n\r\n\r\n0.3567877\r\n\r\n\r\n0.0026885\r\n\r\n\r\n0.6405238\r\n\r\n\r\n0.0000000\r\n\r\n\r\n48\r\n\r\n\r\n31501\r\n\r\n\r\n47.9\r\n\r\n\r\n15.1\r\n\r\n\r\n11.9\r\n\r\n\r\n10.254854\r\n\r\n\r\n24.643\r\n\r\n\r\n14.300572\r\n\r\n\r\n50.7\r\n\r\n\r\n36.165048\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9993333\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0006667\r\n\r\n\r\n49\r\n\r\n\r\n31502\r\n\r\n\r\n34.5\r\n\r\n\r\n7.6\r\n\r\n\r\n16.1\r\n\r\n\r\n6.619280\r\n\r\n\r\n21.265\r\n\r\n\r\n9.670947\r\n\r\n\r\n32.5\r\n\r\n\r\n32.119058\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9972245\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0027755\r\n\r\n\r\n50\r\n\r\n\r\n31700\r\n\r\n\r\n20.0\r\n\r\n\r\n8.2\r\n\r\n\r\n12.7\r\n\r\n\r\n9.429569\r\n\r\n\r\n40.129\r\n\r\n\r\n10.520833\r\n\r\n\r\n33.8\r\n\r\n\r\n22.002328\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9999976\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.0000023\r\n\r\n\r\nHere is the interesting point. Even though, we would see the mismatch among the clustering with the indicators, for example, cluster 4 and 3 may have the contradict between income, less than HS and poverty level. But overall, we will apply on the real cases, the clustering reflexes the correct situations. We can see below 2 applications on mapping and on classifying on race-ethnicity.\r\nApplying the Clustering\r\nMapping on Chicago Neighborhood map\r\nWith the level:\r\n1: not/mild disadvantage\r\n2, 3: medium disadvantage\r\n4: highly disadvantage\r\nWe can map on Chicago Neighborhood map (using ArcGIS) as such\r\n\r\nClassification by race-ethnicity\r\nSave for later on!!!\r\nRefs\r\nPugach, Oksana. 2018. “Latent Profile Analysis of Chicago Neighborhoods” Unpublished Work. Chalk Talk - Institute for Health Resereach; Policy - Methodology Research Core.\r\nUniversity of Virginia Library, Research Data Services + Sciences. “Getting Started with Factor Analysis” - https://data.library.virginia.edu/getting-started-with-factor-analysis/\r\nStats Stackexchange. “How to calculate the loading matrix from the score matrix and a data matrix X (PCA)?” - https://stats.stackexchange.com/questions/447952/how-to-calculate-the-loading-matrix-from-the-score-matrix-and-a-data-matrix-x-p\r\nLuca Scrucca. 2020. “A quick tour of mclust” - https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html\r\nLuca Scrucca, Michael Fop, T. Brendan Murphy,Adrian E. Raftery. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” R J. 2016 August ; 8(1): 289-317\r\n\r\n\r\n",
    "preview": "posts/2021-10-24-LPA Chicago Neighborhoods/distill-preview.png",
    "last_modified": "2021-10-25T11:17:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-25-selection-bias/",
    "title": "Selection bias",
    "description": "Understanding: definition, examples;    \nApplying in DAG;    \nGeneral solution;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-08-25",
    "categories": [
      "Causal Inference",
      "DiagrammeR"
    ],
    "contents": "\r\n\r\nContents\r\nSelection bias: set up\r\nSelection bias happens when? ~ Examples\r\nWhat’s the target population?\r\nWhy not?\r\nConditioning on a collider\r\nCommon structure\r\nIs the selected group different?\r\nConditioning on a collider\r\n\r\nA note about confounding\r\nMore examples\r\nWhat to do?\r\n\r\nSelection bias occurs when some part of the target population is not in the sampled population, or, more generally, when some population units are sampled at a different rate than intended by the investigator. A good sample will be as free from selection bias as possible.\r\n— Sharon L. Lohr, Sampling: Design and Analysis 2nd\r\nSelection bias happens in different fields. Everything can be “selection bias”\r\nModel selection\r\nProblems with statistical inference\r\n\r\nConfounding\r\nIn certain fields… “selection into treatment”\r\n\r\nNon-generalizability/transportability\r\nEffect in sample not the same as in target population\r\n\r\nCollider stratification (worst form of selection bias)\r\nBias for causal effects even within sample and under the null\r\n\r\nSome other names refer to “selection bias”\r\nBerkson’s bias\r\nHealthy worker effect\r\nCensoring/truncation\r\nNon-response bias\r\nPrevalent-user bias\r\nVolunteer bias\r\nIncidence-prevalence bias\r\nIndex-event bias\r\nSurvivor bias\r\nSelection bias: set up\r\n\\(X\\): exposure of interest\\(Y\\): outcome of interest\\(S\\): selection into study (S = 1 if selected)\r\nWe can estimate \\[ RR^s_{XY} = \\frac{Pr(Y = 1 \\mid X = 1; S = 1)}{Pr(Y = 1 \\mid X = 0; S = 1)} \\]\r\nwhich may not equal \\(RR^t_{XY}\\)\r\n\\(s\\): subject to bias\\(t\\): true\r\nWhat is \\(RR^t_{XY}\\)? \\(RR^t_{XY}\\) is the true causal effect in the target population.\r\n\r\nWe will assume that if we estimated \\(\\frac{Pr(Y =1\\mid X=1)}{Pr(Y =1\\mid X=0)}\\) , this is what we’d get\r\nSelection bias happens when? ~ Examples\r\n— (Hernán, Hernández-Díaz, and Robins 2004)\r\nConsider a randomized trial of anti-retroviral therapy (\\(X\\)) among people living with HIV, with a goal of preventing the development of AIDS (\\(Y\\))\r\n\\(\\frac{Pr(Y = 1 \\mid X = 1)}{Pr(Y = 1 \\mid X = 0)}\\) is the risk ratio among people randomized to the intervention arm vs. standard of care\r\nIf some people drop out of the study, we estimate \\(\\frac{Pr(Y = 1 \\mid X = 1; S = 1)}{Pr(Y = 1 \\mid X = 0; S = 1)}\\)\r\n\r\n\r\nhide\r\nlibrary(DiagrammeR) #grViz\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode[shape=none]\r\nX \r\n\r\nnode [shape = box,\r\n      fontname = Helvetica]\r\nS\r\n\r\nnode[shape=none]\r\nY\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode[shape=none]\\nX \\n\\nnode [shape = box,\\n      fontname = Helvetica]\\nS\\n\\nnode[shape=none]\\nY\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\nWhat’s the target population?\r\nTarget population The complete collection of observations we want to study. Defining the target population is an important and often difficult part of the study. For example, in a political poll, should the target population be all adults eligible to vote? All registered voters? All persons who voted in the last election? The choice of target population will profoundly affect the statistics that result.\r\n— Sharon L. Lohr, Sampling: Design and Analysis, 2nd\r\nThe study participants are not a random sample of all people living with HIV … is that a problem?\r\nPerhaps, if we’re trying to estimate how effective treatment would be in another context.\r\nBut not when it comes to estimating valid causal effects. With complete follow-up, we can estimate the effect of the drug in the target population from which the participants came.\r\nWithout loss to follow-up, we can’t even estimate that — not to mention generalize to another context.\r\nWhy not?\r\nThe participants who were lost to follow-up are not a random sample of all participants\r\nPerhaps the most severely immunocompromised people have trouble coming to study visits\r\nThey are also at higher risk of developing AIDS\r\nPerhaps people experiencing side effects of treatment no longer want to participate\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U;\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  S -> U\r\n  Y -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> S\r\n  S -> Y [color = white]\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U;\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  S -> U\\n  Y -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> S\\n  S -> Y [color = white]\\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConditioning on a collider\r\nSelection bias can occur when a non-causal X-Y path is opened by conditioning on \\(S\\)\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U;\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back, color = red]\r\n  S -> U \r\n  Y -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> S [color = red]\r\n  S -> Y [color = white]\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U;\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back, color = red]\\n  S -> U \\n  Y -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> S [color = red]\\n  S -> Y [color = white]\\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nCommon structure\r\nDoes Zika virus infection (\\(X\\)) increase the risk of microcephaly (\\(Y\\))?\r\nWe only assess microcephaly among live births (\\(S\\) = 1).\r\nElective terminations are not included (\\(S\\) = 0).\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode[shape=none]\r\nX \r\n\r\nnode [shape = box,\r\n      fontname = Helvetica]\r\nS\r\n\r\nnode[shape=none]\r\nY\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode[shape=none]\\nX \\n\\nnode [shape = box,\\n      fontname = Helvetica]\\nS\\n\\nnode[shape=none]\\nY\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nIs the selected group different?\r\nWe might assume that\r\nPeople who have more exposure to the virus are more likely to choose to end their pregnancies (worried about risks)\r\nPeople with less access to health care are less likely to have access to abortion services\r\nThere are factors that affect risk of microcephaly that are correlated with access to health care\r\nConditioning on a collider\r\nIntuitively, pregnancies are either in our study if:\r\nThey have lower-than-average probability of exposure to Zika virus (but average risk of microcephaly)\r\nThe have higher-than-average risk of microcephaly (but average risk of Zika exposure)\r\nThe already low-risk pregnancies also have lower exposure to the virus…. It looks like exposure to Zika virus is associated with microcephaly.\r\nA note about confounding\r\nThere are also confounders of the \\(X - Y\\) relationship, of course, since this study is observational\r\ni.e., other reasons why people might be simultaneously at high risk of Zika exposure and microcephaly\r\n\r\nSome of those might be the same factors causing selection bias\r\nIf we properly adjust for them to control confounding, we also control selection bias\r\n\r\nIf there are additional factors leading to selection bias that aren’t confounders, we may not plan to measure or adjust for them\r\nWe’ll tie confounding and selection bias (and misclassification!) together at the end\r\nMore examples\r\nConsider a pharmacoepidemiology study comparing outcomes (\\(Y\\)) in current users vs. never users of a drug (\\(X\\)):\r\nIf people more at risk (\\(U_2\\)) of outcome \\(Y\\) also have more side effects \\(U_1\\), they are more likely to discontinue the drug and not be included in the study (\\(S\\) = 0).\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U1; U2\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  U1 -> U2 \r\n  Y -> U2\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> U1 -> S\r\n  S -> Y [color = white]\r\n  \r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U1; U2\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  U1 -> U2 \\n  Y -> U2\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> U1 -> S\\n  S -> Y [color = white]\\n  \\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConsider a case-control study of coffee consumption (\\(X\\)) and pancreatic cancer (\\(Y\\)), in which controls are chosen from the same hospital:\r\nSelection is based on case status (\\(Y\\)). If controls with gastrointestinal disease are used (\\(U\\)), the fact that they are more likely to avoid coffee can make coffee look like it causes cancer.\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  X -> U \r\n  S -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  edge[color=gray]\r\n  X -> Y \r\n  edge[color=black]\r\n  Y -> S\r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  X -> U \\n  S -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  edge[color=gray]\\n  X -> Y \\n  edge[color=black]\\n  Y -> S\\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConsider a study of the effect of antidepressants (\\(X\\)) on lung cancer (\\(Y\\)) among people with coronary artery disease (\\(S\\)):\r\nIf depression (\\(U1\\)) causes \\(X\\) and \\(S\\), and smoking (\\(U_2\\)) causes \\(S\\) and \\(Y\\) , selection bias (“\\(M\\)-bias”) can result.\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U1; U2\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  X -> U1\r\n  Y -> U2\r\n  S -> U1\r\n  S -> U2\r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U1; U2\\n\\nsubgraph C{\\n  rank=same;\\n  edge[color=gray]\\n  X -> Y \\n}\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  X -> U1\\n  Y -> U2\\n  S -> U1\\n  S -> U2\\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n— (Hernán, Hernández-Díaz, and Robins 2004) and (Smith 2020)\r\nWhat to do?\r\nGo back to the drawing board\r\nDesign a better study …\r\nTrack down those lost to follow-up\r\n\r\nAttempt to correct for it\r\nMeasure the variables causing bias\r\nModeling assumptions\r\n\r\nRedefine question\r\nConsider interpretability\r\n\r\nOR\r\nSensitivity analysis!\r\n\r\n\r\nHernán, Miguel A., Sonia Hernández-Díaz, and James M. Robins. 2004. “A Structural Approach to Selection Bias.” Journal Article. Epidemiology (Cambridge, Mass.) 15 (5): 615–25. https://doi.org/10.1097/01.ede.0000135174.63482.43.\r\n\r\n\r\nSmith, Louisa H. 2020. “Selection Mechanisms and Their Consequences: Understanding and Addressing Selection Bias.” Journal Article. Current Epidemiology Reports 7 (4): 179–89. https://doi.org/10.1007/s40471-020-00241-6.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-26T09:32:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-03-matching in R-causal-inference/",
    "title": "Matching in R",
    "description": "Observational study;    \nCausal Inference;    \nMatching;    \nR;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-07-03",
    "categories": [
      "Biostatistics",
      "Causal Inference",
      "Tutorial",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nResearch\r\nquestions\r\nGoal\r\nContext\r\nData\r\nCovariates with Missing\r\nValues\r\n\r\nPropensity\r\nScore\r\nDistance\r\nMatrix\r\nConstructing the Match (B. B. Hansen\r\n2007)\r\nChecking Covariate Balance\r\nOutcomes\r\nFurther\r\nreading\r\n\r\n\r\nlibrary(DOS) # Design of Observational Studies\r\nlibrary(tidyverse)\r\nlibrary(kableExtra)\r\nlibrary(optmatch)\r\n\r\nMultivariate matching is involved in 3 steps:\r\n(i) creating a distance matrix,\r\n(ii) adding a propensity score caliper to the distance matrix, and\r\n(iii) finding an optimal match.\r\nMatching in R created largely due to the efforts of Ben Hansen (B. B. Hansen 2007), who created an R\r\nfunction, fullmatch, to do optimal matching from Fortan code created by\r\nDemetri Bertsekas D. P.\r\nBertsekas (1981).\r\nResearch questions\r\nDoes financial aid increase college attendance?\r\nGoal\r\nFour matched samples will be constructed: The\r\nconstruction of one of the four matched samples is presented in\r\nstep-by-step detail.\r\nContext\r\nDuring 1979-1981\r\nSocial Security Student Benefit Program provided a substantial\r\ntuition benefit for a child of a deceased father, before the\r\ncancellation in 1982.\r\nAnalyzing the impact on college attendance and completed\r\nschooling of the elimination of the Social Security Student Benefit\r\nProgram in 1982\r\nFrom 1965 to 1982, the Social Security Administration paid for\r\nmillions of students to go to college. Under this program, the 18- to\r\n22-year-old children of deceased, disabled, or retired Social Security\r\nbeneficiaries received monthly payments while enrolled full time in\r\ncollege.\r\nIn 1981, Congress voted to eliminate the program. Then was\r\ncancelled in 1982\r\nThe program’s demise provides an opportunity to measure the\r\nincentive effects of financial aid. Using difference-in-differences\r\nmethodology, and proxying for benefit eligibility with the death of a\r\nparent during an individual’s childhood, found that ? the\r\nelimination of the Social Security student benefit program reduced\r\ncollege attendance probabilities\r\nData\r\nData Xb with 2820 rows and 8 columns:\r\n- faminc: family income in units of $10,000\r\n- incmiss: income missing (1 if family income is missing, 0\r\no.w.)\r\n- black: 1 if black, 0 o.w.\r\n- hispanic: 1 if hispanic, 0 o.w.\r\n- afqtpct: the Armed Forces Qualifications Test\r\n- edm: mother’s education (1 for less than high school, 2\r\nfor high school, 3 for some college, 4 for BA degree or more)\r\n- edmissm: mother’s education missing (1 if missing, 0\r\no.w.)\r\n- female: gender (1 for female, 0 for male)\r\n\r\nImputation: faminc is set to 2 when\r\nincmiss = 1 indicating that faminc is missing,\r\nand edm is set to 0 when edmissm=1 indicating\r\nthat edm is missing\r\n\r\n\r\nAFQT was missing for less than 2% of subjects, and these subjects are\r\nnot used in the matching. With this restriction, there are 131 high\r\nschool seniors with deceased fathers and 2689 other high school seniors\r\nin the 1979-1981 cohort (131+2689 = 2820)\r\n\r\nThe treatment zb, where is 1 if the father is deceased, and\r\n0 o.w.\r\ndata(\"dynarski\")\r\ndim(dynarski)\r\n[1] 2820   10\r\nkable(head(dynarski, n=20), caption=\"First 20 people in the 1979-1981 cohort, the treatment zb and the eight covariates Xb\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 1: First 20 people in the 1979-1981 cohort, the treatment zb and\r\nthe eight covariates Xb\r\n\r\n\r\nid\r\n\r\n\r\nzb\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n5.3497560\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n71.901660\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2.4627060\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.158050\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n9.4876030\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.785250\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n0\r\n\r\n\r\n0.4388592\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90.617160\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n0\r\n\r\n\r\n10.4363600\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n81.761160\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n7\r\n\r\n\r\n0\r\n\r\n\r\n6.2694180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n97.917710\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\n3.2204620\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.916710\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\n0\r\n\r\n\r\n9.4719470\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n20.446560\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\n0\r\n\r\n\r\n2.7167480\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n57.175110\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n14\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n8.278976\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n15\r\n\r\n\r\n0\r\n\r\n\r\n7.1157020\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n50.928250\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n0\r\n\r\n\r\n7.7669970\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n71.149020\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\n0\r\n\r\n\r\n12.5388400\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n74.912190\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n18\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n98.394380\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n21\r\n\r\n\r\n0\r\n\r\n\r\n9.4041260\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n98.720520\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n23\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.724040\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n24\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n73.281490\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n29\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1.781234\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n31\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n42.900150\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n1\r\n\r\n\r\n2.6090910\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.623680\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n#write.csv(dynarski, file=\"dynarski.csv\")\r\n\r\n\r\nzb<-dynarski$zb\r\nzbf<-factor(zb,levels=c(1,0),labels=c(\"Father deceased\",\"Father not deceased\"))\r\ntable(zbf)\r\nzbf\r\n    Father deceased Father not deceased \r\n                131                2689 \r\nXb<-dynarski[,3:10]\r\n\r\n\\(\\Rightarrow\\) The 131 seniors with\r\ndeceased fathers will each be matched to 10 controls whose fathers were\r\nnot deceased.\r\nCovariates with Missing\r\nValues\r\n\r\nwith(dynarski, table(incmiss))\r\nincmiss\r\n   0    1 \r\n2282  538 \r\nincmiss.per<-round(table(dynarski$incmiss)[2]/dim(dynarski)[1]*100,1)\r\npaste0(\"Percentage of income missing: \", incmiss.per)\r\n[1] \"Percentage of income missing: 19.1\"\r\nwith(dynarski, table(edmissm))\r\nedmissm\r\n   0    1 \r\n2704  116 \r\nedmissm.per<-round(table(dynarski$edmissm)[2]/dim(dynarski)[1]*100,1)\r\npaste0(\"Percentage of mother's education missing: \", edmissm.per)\r\n[1] \"Percentage of mother's education missing: 4.1\"\r\n\r\nDiscussion of the missing covariate values will be later on another\r\npost.\r\nPropensity Score\r\nThe propensity score is estimated using a logit model.\r\n\r\n# Estimate the propensity score\r\np <- glm(zb ~ Xb$faminc + Xb$incmiss + Xb$black + Xb$hisp + Xb$afqtpct + Xb$edmissm + Xb$edm + Xb$female, family = binomial)$fitted.values\r\n\r\n\\(\\Rightarrow\\) The vector\r\np contains the 2820 estimated propensity scores, \\(\\hat{e}(\\mathbf{x}_l)\\), \\(l\\) = 1, 2, …, 2820.\r\n\r\nIt is reasonable to be able to improve the model: perhaps including\r\ninteraction terms or transformations or polynomials or whatnot.\r\n\r\nEstimated propensity scores for 131 seniors with deceased fathers and\r\n2689 other seniors in the 1979-1981 cohort, before the Social Security\r\nStudent Benefit Program was eliminated.\r\n\r\nboxplot(p~zbf, ylab=\"Propensity score\", main=\"1979-1981 Cohort\")\r\n\r\n\r\n\r\ndynarski$p <- p\r\nggplot(dynarski, aes(p, fill = zbf)) + \r\n  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity') +\r\n  ggtitle(\"Figure of Overlay Histograms of PS in 2 Groups\") +\r\n  theme_bw()\r\n\r\n\r\n\\(\\Longrightarrow\\) the median \\(\\hat{e}(\\mathbf{x})\\) in the treated group\r\n(0.064) is about equal to the upper quartile among potential controls\r\n(0.063). Nonetheless, the distributions in Figure of Overlay Histogram\r\nof PS in 2 Groups overlap substantially, so matching appears to be\r\npossible.\r\nDistance Matrix\r\nConstructing in 2 steps:\r\n- Step 1: computing the rankbased Mahalanobis distance \\(\\rightarrow\\) 131x2689 distance\r\nmatrix\r\n#Robust Mahalanobis distance matrix, treated x control\r\ndmat<-smahal(zb,Xb)\r\ndim(dmat)\r\n[1]  131 2689\r\nkable(round(dmat[1:5,1:5],2), caption=\"First five rows and columns of the 131×2689 distance matrix using the rank-based Mahalanobis distance\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 2: First five rows and columns of the 131×2689 distance matrix\r\nusing the rank-based Mahalanobis distance\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n3.86\r\n\r\n\r\n2.61\r\n\r\n\r\n5.06\r\n\r\n\r\n6.86\r\n\r\n\r\n6.72\r\n\r\n\r\n20\r\n\r\n\r\n3.47\r\n\r\n\r\n3.03\r\n\r\n\r\n7.58\r\n\r\n\r\n6.23\r\n\r\n\r\n5.82\r\n\r\n\r\n108\r\n\r\n\r\n9.60\r\n\r\n\r\n19.47\r\n\r\n\r\n20.02\r\n\r\n\r\n24.62\r\n\r\n\r\n13.03\r\n\r\n\r\n126\r\n\r\n\r\n6.81\r\n\r\n\r\n8.05\r\n\r\n\r\n12.93\r\n\r\n\r\n10.74\r\n\r\n\r\n9.88\r\n\r\n\r\n145\r\n\r\n\r\n8.70\r\n\r\n\r\n15.09\r\n\r\n\r\n17.74\r\n\r\n\r\n18.86\r\n\r\n\r\n12.37\r\n\r\n\r\nStep 2: adding the caliper on the propensity score. The caliper was\r\nat \\(0.2 \\times\r\nsd(\\hat{e}(\\mathbf{x}))\\).\r\n\r\n#Add a caliper on the propensity score using a penalty function\r\ndmat<-addcaliper(dmat,zb,p,caliper=.2)\r\ndim(dmat)\r\n[1]  131 2689\r\nkable(round(dmat[1:5,1:5],2), caption=\"First five rows and columns of the 131×2689 distance matrix after adding the propensity score calipers\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 3: First five rows and columns of the 131×2689 distance\r\nmatrix after adding the propensity score calipers\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n18.60\r\n\r\n\r\n20.64\r\n\r\n\r\n42.04\r\n\r\n\r\n79.91\r\n\r\n\r\n46.10\r\n\r\n\r\n20\r\n\r\n\r\n46.32\r\n\r\n\r\n3.03\r\n\r\n\r\n72.66\r\n\r\n\r\n51.18\r\n\r\n\r\n73.30\r\n\r\n\r\n108\r\n\r\n\r\n82.94\r\n\r\n\r\n47.40\r\n\r\n\r\n115.60\r\n\r\n\r\n39.07\r\n\r\n\r\n111.01\r\n\r\n\r\n126\r\n\r\n\r\n57.81\r\n\r\n\r\n13.64\r\n\r\n\r\n86.16\r\n\r\n\r\n47.54\r\n\r\n\r\n85.51\r\n\r\n\r\n145\r\n\r\n\r\n8.70\r\n\r\n\r\n54.51\r\n\r\n\r\n33.32\r\n\r\n\r\n113.31\r\n\r\n\r\n30.34\r\n\r\n\r\n\\(\\Rightarrow\\) Among these 25\r\nentries, only 2 respected the caliper and did not incur a penalty (3.03\r\nand 8.70).\r\nConstructing the Match (B. B. Hansen 2007)\r\nMatching ten controls to each senior with a deceased father: The\r\nfullmatch function needs to know the distance matrix, here\r\ndmat, matching 10-to-1 means 10x131 will be used, omitting\r\n2689−10x131 = 1379, which is the omit fraction be: 51%.\r\n\r\nm<-fullmatch(dmat, data=dynarski, min.controls=10, max.controls=10, omit.fraction=1379/2689)\r\n\r\nThere is an entry in m for each of the 2820 seniors\r\n\r\nlength(m)\r\n[1] 2820\r\n\r\nThe first ten entries in m are\r\n\r\nm[1:10]\r\n    1     2     3     4     5     6     7     8     9    10 \r\n1.129 1.100  <NA>  1.54  <NA> 1.121 1.114  <NA> 1.111  1.87 \r\n\r\ni.e. the first senior of the 2820 seniors is in matched set #34 and\r\nthe second senior is in matched set #10. The third senior was one of the\r\n1379 unmatched controls; this is the meaning of the zero in m.01. The\r\nfourth senior is in matched set #87, the fifth is unmatched, and so\r\non.\r\nThe function matched(·) indicates who is matched. The first ten\r\nentries of matched(m) are\r\n\r\nmatched(m)[1:10]\r\n    1     2     3     4     5     6     7     8     9    10 \r\n TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE \r\n\r\nthe first two seniors were matched but the third was not, and so\r\non.\r\nThere are 1441 matched seniors, where 1441 = 131×11, because the 131\r\nseniors with deceased fathers were each matched to ten controls, making\r\n131 matched sets each of size 11.\r\n\r\nsum(matched(m))\r\n[1] 1441\r\nsum(matched(m))/11\r\n[1] 131\r\n\r\nThe first 3 matched sets are in Table below. The first match consists\r\nof 11 female high school seniors, neither black nor hispanic, whose\r\nmothers had a high school education, with family incomes between $30,000\r\nand $40,000, mostly with test scores between 59% and 77%. In the second\r\nmatched set, incomes were lower but test scores were higher. And so\r\non.\r\n\r\n# Housekeeping\r\nim<-as.integer(m) # matched set from 1 to 131\r\ndynarski<-cbind(dynarski,im) \r\ndm<-dynarski[matched(m),] # only select matched cases, note that matched(m): T\\F\r\ndm<-dm[order(dm$im,1-dm$zb),] # sort data according to matched set then zb decreasing\r\n\r\n# Table of matched set example\r\n#which(dm$id==10) # [1] 188\r\n#which(dm$id==396) # [1] 23\r\n#which(dm$id==3051) # [1] 1068\r\n\r\nkable(rbind(dm[188:198,-c(11,12)], dm[23:33,-c(11,12)],dm[1068:1078,-c(11,12)]), caption=\"The 3 of 131 matched sets, each set containing one treated subject and 10 matched controls\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:matched set ex)The 3 of 131 matched sets, each set containing one\r\ntreated subject and 10 matched controls\r\n\r\n\r\n\r\n\r\nid\r\n\r\n\r\nzb\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\n7\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\n3.220462\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.91671\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n239\r\n\r\n\r\n350\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n59.58355\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n252\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n3.599340\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.23934\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n322\r\n\r\n\r\n465\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n56.37230\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n361\r\n\r\n\r\n518\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n67.00953\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n383\r\n\r\n\r\n550\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n63.49724\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n928\r\n\r\n\r\n1294\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n67.31059\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1481\r\n\r\n\r\n2072\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n64.97742\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1489\r\n\r\n\r\n2082\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n63.62268\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1560\r\n\r\n\r\n2183\r\n\r\n\r\n0\r\n\r\n\r\n3.970631\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n76.51781\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2810\r\n\r\n\r\n3965\r\n\r\n\r\n0\r\n\r\n\r\n3.761650\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n72.57903\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n268\r\n\r\n\r\n396\r\n\r\n\r\n1\r\n\r\n\r\n2.371901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n88.50979\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2.462706\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.15805\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n101\r\n\r\n\r\n147\r\n\r\n\r\n0\r\n\r\n\r\n2.273267\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n77.09483\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n375\r\n\r\n\r\n537\r\n\r\n\r\n0\r\n\r\n\r\n2.595314\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.96086\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n665\r\n\r\n\r\n933\r\n\r\n\r\n0\r\n\r\n\r\n2.846281\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n96.11139\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n698\r\n\r\n\r\n974\r\n\r\n\r\n0\r\n\r\n\r\n1.897521\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.59859\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n707\r\n\r\n\r\n987\r\n\r\n\r\n0\r\n\r\n\r\n2.134711\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n81.18414\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1394\r\n\r\n\r\n1947\r\n\r\n\r\n0\r\n\r\n\r\n2.050298\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n91.44506\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n1518\r\n\r\n\r\n2124\r\n\r\n\r\n0\r\n\r\n\r\n2.298786\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n72.40341\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1877\r\n\r\n\r\n2618\r\n\r\n\r\n0\r\n\r\n\r\n2.211560\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n68.91621\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2816\r\n\r\n\r\n3975\r\n\r\n\r\n0\r\n\r\n\r\n2.371901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90.74260\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2182\r\n\r\n\r\n3051\r\n\r\n\r\n1\r\n\r\n\r\n3.409901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n62.87004\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n427\r\n\r\n\r\n606\r\n\r\n\r\n0\r\n\r\n\r\n4.179612\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n81.73608\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n472\r\n\r\n\r\n664\r\n\r\n\r\n0\r\n\r\n\r\n4.388592\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n91.57050\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n628\r\n\r\n\r\n884\r\n\r\n\r\n0\r\n\r\n\r\n2.846281\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n48.77070\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n713\r\n\r\n\r\n995\r\n\r\n\r\n0\r\n\r\n\r\n3.134709\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n55.11791\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n724\r\n\r\n\r\n1008\r\n\r\n\r\n0\r\n\r\n\r\n3.320661\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n51.60562\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1006\r\n\r\n\r\n1399\r\n\r\n\r\n0\r\n\r\n\r\n3.439256\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n90.01505\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2087\r\n\r\n\r\n2908\r\n\r\n\r\n0\r\n\r\n\r\n3.795041\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n80.28098\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n2328\r\n\r\n\r\n3262\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n57.27546\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n2424\r\n\r\n\r\n3400\r\n\r\n\r\n0\r\n\r\n\r\n2.925728\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n80.88309\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2576\r\n\r\n\r\n3624\r\n\r\n\r\n0\r\n\r\n\r\n3.320661\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n74.08430\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nChecking Covariate Balance\r\nNote that: for covariate \\(k\\),\r\n- \\(\\bar{x}_{tk}\\) is the mean in the\r\nfirst group in the comparison,\r\n- \\(\\bar{x}_{ck}\\) is the mean in the\r\nsecond group in the comparison, and\r\n- \\(\\bar{x}_{mck}\\) is the mean in the\r\nmatched comparison group formed from the second group\r\n- \\(sd_{bk}\\) is the standardized\r\ndifference in means before matching and\r\n- \\(sd_{mk}\\) is the standardized\r\ndifference in means after matching; they have the same denominator but\r\ndifferent numerators\r\n\r\n#fd <- dynarski[dynarski$zb==1,-c(1,2,12)]\r\n#round(apply(fd, 2, FUN=mean),2)\r\n\r\nd.tk<-dynarski %>%\r\n  filter(zb==1) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im)\r\nx.tk<-d.tk %>% summarise_all(mean, na.rm=TRUE)\r\ns.tk<-d.tk %>% summarise_all(sd, na.rm=TRUE)\r\n\r\nd.ck<-dynarski %>%\r\n  filter(zb==0) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im)\r\nx.ck<-d.ck %>% summarise_all(mean, na.rm=TRUE)\r\ns.ck<-d.ck %>% summarise_all(sd, na.rm=TRUE)\r\n\r\nsd.bk<-abs(x.tk-x.ck)/sqrt((s.tk^2+s.ck^2)/2)\r\n\r\nd.cmk<-dm %>%\r\n  filter(zb==0) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im) \r\nx.cmk<-d.cmk %>% summarise_all(mean, na.rm=TRUE)\r\nsd.cmk<-abs(x.tk-x.cmk)/sqrt((s.tk^2+s.ck^2)/2)\r\n\r\nset<-round(rbind(x.tk,x.cmk,x.ck,sd.bk,sd.cmk),2)\r\nrow.names(set) <- c(\"x.tk\",\"x.cmk\",\"x.ck\",\"sd.bk\",\"sd.cmk\")\r\n\r\nkable(set, caption=\"Balance on covariates before and after matching\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 4: Balance on covariates before and after matching\r\n\r\n\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\np\r\n\r\n\r\nx.tk\r\n\r\n\r\n2.78\r\n\r\n\r\n0.15\r\n\r\n\r\n0.35\r\n\r\n\r\n0.15\r\n\r\n\r\n49.58\r\n\r\n\r\n0.08\r\n\r\n\r\n1.62\r\n\r\n\r\n0.49\r\n\r\n\r\n0.07\r\n\r\n\r\nx.cmk\r\n\r\n\r\n2.77\r\n\r\n\r\n0.15\r\n\r\n\r\n0.34\r\n\r\n\r\n0.15\r\n\r\n\r\n49.10\r\n\r\n\r\n0.04\r\n\r\n\r\n1.61\r\n\r\n\r\n0.50\r\n\r\n\r\n0.06\r\n\r\n\r\nx.ck\r\n\r\n\r\n4.58\r\n\r\n\r\n0.19\r\n\r\n\r\n0.29\r\n\r\n\r\n0.15\r\n\r\n\r\n52.39\r\n\r\n\r\n0.04\r\n\r\n\r\n1.91\r\n\r\n\r\n0.50\r\n\r\n\r\n0.05\r\n\r\n\r\nsd.bk\r\n\r\n\r\n0.71\r\n\r\n\r\n0.11\r\n\r\n\r\n0.13\r\n\r\n\r\n0.02\r\n\r\n\r\n0.10\r\n\r\n\r\n0.19\r\n\r\n\r\n0.33\r\n\r\n\r\n0.03\r\n\r\n\r\n0.67\r\n\r\n\r\nsd.cmk\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n0.02\r\n\r\n\r\n0.19\r\n\r\n\r\n0.02\r\n\r\n\r\n0.02\r\n\r\n\r\n0.09\r\n\r\n\r\n\\(\\Longrightarrow\\) They were quite\r\ndifferent before matching but were much closer after matching. The\r\nfamily income of seniors with deceased fathers was lower, they were more\r\noften black, and their mothers had less education. Between 1979-1981,\r\nAFQT test scores decline. The imbalances are much smaller after\r\nmatching.\r\nOutcomes\r\nThe outcomes were not provided in the Dynarski’s dataset. But I\r\nguessed we can report the proportions and MH-odds ratio between 2 groups\r\nafter matching.\r\nIn short, during 1979-1981, when the Social Security Student Benefit\r\nProgram provided tuition aid to students of a deceased Social Security\r\nbeneficiary, seniors with deceased fathers were more likely to\r\nattend college and complete one year of college than were\r\nmatched controls, with an odds ratio of about 1.65, but\r\nthere is no sign of this in 1982-1983 after the program ended (the\r\nlater can be checked because of lack the data for analysis).\r\nStandardized differences in covariate means before and after matching\r\nin matched comparisons. The boxplot displays standardized differences in\r\nmeans, for the nine covariates and the propensity score.\r\n\r\nm<-c(0,1)\r\nd.sd<-rbind(sd.bk,sd.cmk)\r\nd.sd<-cbind(d.sd,m)\r\nd.sd.long<-d.sd %>%\r\n  pivot_longer(-m,names_to=\"absstdzdiff\", values_to=\"sd\")\r\nd.sd.long$m<-factor(d.sd.long$m,levels=c(0,1),labels=c(\"Unmatched\",\"Matched\"))\r\nboxplot(sd~m, data=d.sd.long, xlab=\"\", ylab=\"Absolute Standardized Difference\", main=\"1979-1981 Cohort: FD vs. FND\")\r\n\r\n\r\nFurther reading\r\nDynarski (1999)’s fine\r\nstudy\r\nB. B. Hansen (2007)’s\r\nfullmatch function\r\nBen B. Hansen and Klopfer\r\n(2006), Paul R. Rosenbaum (1989), P. R. Rosenbaum (1991) showed how\r\nfullmatch is doing\r\nMing and Rosenbaum (2001) showed\r\nproc assign in SAS\r\n\r\n\r\nBertsekas. 1991. Linear Network Optimization. Book. Cambridge,\r\nMA: MIT Press. http://web.mit.edu/dimitrib/www/net.html.\r\n\r\n\r\nBertsekas, Dimitri P. 1981. “A New Algorithm for the Assignment\r\nProblem.” Journal Article. Mathematical Programming 21\r\n(1): 152–71. https://doi.org/10.1007/BF01584237.\r\n\r\n\r\nDynarski, Susan M. 1999. “Does Aid Matter? Measuring the Effect of\r\nStudent Aid on College Attendance and Completion.” Journal\r\nArticle, 1 online resource. http://HZ9PJ6FE4T.search.serialssolutions.com/?V=1.0&L=HZ9PJ6FE4T&S=JCs&C=TC_008322359&T=marc&tab=BOOKS\r\nAvailable only to UIC users.\r\n\r\n\r\nHansen, B. B. 2007. “Optmatch: Flexible, Optimal Matching for\r\nObservational Studies.” Journal Article. R News 7:\r\n18–24.\r\n\r\n\r\nHansen, Ben B., and Stephanie Olsen Klopfer. 2006. “Optimal Full\r\nMatching and Related Designs via Network Flows.” Journal Article.\r\nJournal of Computational and Graphical Statistics 15 (3):\r\n609–27. www.jstor.org/stable/27594200.\r\n\r\n\r\nMing, Kewei, and Paul R. Rosenbaum. 2001. “A Note on Optimal\r\nMatching with Variable Controls Using the Assignment Algorithm.”\r\nJournal Article. Journal of Computational and Graphical\r\nStatistics 10 (3): 455–63. www.jstor.org/stable/1391099.\r\n\r\n\r\nRosenbaum, P. R. 1991. “A Characterization of Optimal Designs for\r\nObservational Studies.” Journal Article. Journal of the Royal\r\nStatistical Society. Series B (Methodological) 53 (3): 597–610. www.jstor.org/stable/2345589.\r\n\r\n\r\nRosenbaum, Paul R. 1989. “Optimal Matching for Observational\r\nStudies.” Journal Article. Journal of the American\r\nStatistical Association 84 (408): 1024–32. https://doi.org/10.2307/2290079.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-01-30T01:04:36-06:00",
    "input_file": "Matching-in-R.knit.md"
  },
  {
    "path": "posts/2021-06-09-mets-using-nhanesdata/",
    "title": "Metabolic Syndrome Prevalence across time using NHANES Data",
    "description": "How to correctly approach NHANES data to make estimates that are representative of the population    \nDefine Metabolic Syndrome based on ATP III  \nLook at the MetS prevalence over time",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [
      "Biostatistics",
      "NHANES approach",
      "R",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nNHANES Cycle 2013/2014\r\nImport data from NHANES cycle 2013/2014\r\nData Processing\r\nDefine metabolic syndrome\r\nDefine sub-population of interest\r\nMetS proportion in the random sample (unweighted)\r\nWith all age groups\r\nWith all age groups & none missing of MetS indicator\r\nWith age 20+ & none missing of MetS indicator\r\nWith age 20+ and ind.non_missing is TRUE:\r\n\r\nMetS prevalence with weighted\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n\r\nCycle 2015/2016\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\nCycle 2017/2018\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\nSummary\r\nReferences\r\n\r\nI need to use these packages:\r\ntidyverse: data management (tidyr, dplyr) and plots (ggplot2)\r\nforeign: reading xport files\r\nsurvey: for using survey weights # survey::svydesign, svymean, svyglm\r\n\r\nlibrary(tidyverse)\r\nlibrary(foreign) \r\nlibrary(survey) \r\noptions(survey.lonely.psu='adjust')\r\nlibrary(kableExtra)\r\n\r\nDisplay Version Information\r\n\r\n#cat(\"R package versions:\\n\")\r\nfor (p in c(\"base\", \"survey\",\"tidyverse\")) { \r\n  cat(p, \": \", as.character(packageVersion(p)), \"\\n\")\r\n}\r\nbase :  4.0.4 \r\nsurvey :  4.0 \r\ntidyverse :  1.3.1 \r\n\r\nNHANES Cycle 2013/2014\r\nImport data from NHANES cycle 2013/2014\r\nCode for data import was adapted from a NHANES tutorials example.\r\nfunction to download the required survey cycles for a component file\r\n\r\ndownloadNHANES <- function(fileprefix){\r\n  print (fileprefix)\r\n  outdf <- data.frame(NULL)\r\n  for (j in 1:length(letters)){\r\n    urlstring <- paste('https://wwwn.cdc.gov/nchs/nhanes/',yrs[j],'/',fileprefix,letters[j],'.XPT', sep='')\r\n    download.file(urlstring, tf <- tempfile(), mode=\"wb\")\r\n    tmpframe <- foreign::read.xport(tf)\r\n    outdf <- bind_rows(outdf, tmpframe)\r\n  }\r\n  return(outdf)\r\n}\r\n\r\nImport NHANES cycle 2013/2014\r\n\r\n# Specify the survey cycles required, with corresponding file suffixes\r\nyrs <- c('2013-2014')\r\nletters <- c('_h')\r\n\r\n# Download data for each component\r\n# (1) Demographic (DEMO)\r\nDEMO <- downloadNHANES('DEMO') # 10175\r\n# (2) Blood Pressure & Cholesterol\r\nBPQ <- downloadNHANES('BPQ') # 6464\r\n# (3) Diabetes \r\nDIQ <- downloadNHANES('DIQ') # 9770\r\n# (4) Blood Pressure\r\nBPX <- downloadNHANES('BPX') # 9813\r\n# (5) Body Measures\r\nBMX <- downloadNHANES('BMX') # 9813\r\n# (6) Cholesterol - High - Density Lipoprotein\r\nHDL <- downloadNHANES('HDL') # 8291\r\n# (7) Cholesterol - Low-Density Lipoproteins (LDL) & Triglycerides (TRIGLY_J)\r\nTRIGLY <- downloadNHANES('TRIGLY') # 3329\r\n# (8) Plasma Fasting Glucose (GLU_J)\r\nGLU <- downloadNHANES('GLU') # 3329\r\n\r\nMerge all datasets, preserve all observations\r\nNHANES Data Structure\r\n# Merging using \"piping\" \r\nnhanesdata.merged <- left_join(DEMO, BPQ, by=\"SEQN\") %>% \r\n  left_join(DIQ, by=\"SEQN\") %>% \r\n  left_join(BPX, by='SEQN') %>% \r\n  left_join(BMX, by='SEQN') %>%\r\n  left_join(HDL, by='SEQN') %>%\r\n  left_join(TRIGLY, by='SEQN') %>%\r\n  left_join(GLU, by='SEQN') # 10175 obs\r\n\r\nData Processing\r\nSelect necessary variables\r\nCreate indicator for overall summary one\r\nCreate age categories for adults aged 18 and over: ages 18-39, 40-59, 60 and over\r\nRe-code the levels of variables: gender, ethnicity/race, marital status, medication prescription due to ‘refuse to answer’ or ‘don’t know’ (to missing)\r\n\r\nnhanes.vbls <- nhanesdata.merged %>%\r\n  select(SEQN, # Respondent sequence number\r\n         RIDAGEYR, # Age in years at screening\r\n         RIAGENDR, # Gender\r\n\r\n         RIDRETH3, # Race/Hispanic origin w/ NH Asian\r\n         DMDMARTL, # Marital status\r\n         DMDEDUC2, # Education level - Adults 20+\r\n         \r\n         # Survey design variables\r\n         SDMVPSU, # Masked variance pseudo-PSU\r\n         SDMVSTRA, # Masked variance pseudo-stratum\r\n         WTMEC2YR, # Full sample 2 year MEC exam weight\r\n         WTINT2YR, # Full sample 2 year interview weight\r\n         \r\n         BPXSY1, # Systolic: Blood pres (1st rdg) mm Hg\r\n         BPXDI1, # Diastolic: Blood pres (1st rdg) mm Hg\r\n         BPXSY2, # Systolic: Blood pres (2nd rdg) mm Hg\r\n         BPXDI2, # Diastolic: Blood pres (2nd rdg) mm Hg\r\n         BPXSY3, # Systolic: Blood pres (3rd rdg) mm Hg\r\n         BPXDI3, # Diastolic: Blood pres (3rd rdg) mm Hg\r\n         BPXSY4, # Systolic: Blood pres (4th rdg) mm Hg\r\n         BPXDI4, # Diastolic: Blood pres (4th rdg) mm Hg\r\n         LBXGLU, # Fasting Glucose (mg/dL)\r\n         LBDHDD, # Direct HDL-Cholesterol (mg/dL)\r\n         LBXTR, # Triglyceride (mg/dL)\r\n         BMXWAIST, # Waist Circumference (cm)\r\n         \r\n         BPQ090D, # Told to take prescription for cholesterol\r\n         BPQ040A, # Taking prescription for hypertension\r\n         DIQ070 # Take diabetic pills to lower blood sugar\r\n         ) %>%\r\n  mutate(# create indicator for overall summary\r\n         one = 1,\r\n         id = SEQN,\r\n         psu = SDMVPSU, strata = SDMVSTRA, \r\n         persWeight = WTINT2YR, persWeightMEC= WTMEC2YR,\r\n         age = RIDAGEYR,\r\n         # Create age categories for adults aged 18 and over: ages 18-39, 40-59, 60 and over\r\n        ageCat = cut(RIDAGEYR,\r\n                    breaks = c(19, 29, 39, 49, 59, 69, Inf),\r\n                    labels = c('20-29','30-39', '40-49', '50-59', '60-69', '70+')),\r\n         female = factor(if_else(RIAGENDR == 1, 0, 1)), # 1:male ==> 0\r\n         ethnicity = case_when(\r\n           RIDRETH3 %in% c(1,2) ~ 0, # Hispanic\r\n           RIDRETH3 == 3 ~ 1, # Non-Hispanic White\r\n           RIDRETH3 == 4 ~ 2, # Non-Hispanic Black\r\n           RIDRETH3 == 6 ~ 3, # Non-Hispanic Asian\r\n           RIDRETH3 == 7 ~ 4, # Multi-Racial\r\n         is.na(RIDRETH3) ~ NA_real_\r\n         ),\r\n         chol.prsn = case_when(\r\n           BPQ090D == 2 ~ 0, # No\r\n           BPQ090D == 1 ~ 1, # Yes\r\n           BPQ090D %in% c(7,9) ~ NA_real_\r\n         ),\r\n         bp.prsn = case_when(\r\n           BPQ040A == 2 ~ 0, # No\r\n           BPQ040A == 1 ~ 1, # Yes\r\n           BPQ040A %in% c(7,9) ~ NA_real_\r\n         ),\r\n         glu.prsn = case_when(\r\n           DIQ070 == 2 ~ 0, # No\r\n           DIQ070 == 1 ~ 1, # Yes\r\n           DIQ070 %in% c(7,9) ~ NA_real_\r\n         ),\r\n         dia = rowMeans(.[, c(\"BPXDI1\",\"BPXDI2\",\"BPXDI3\",\"BPXDI4\")], na.rm = TRUE),\r\n         sys = rowMeans(.[, c(\"BPXSY1\",\"BPXSY2\",\"BPXSY3\",\"BPXSY4\")], na.rm = TRUE),\r\n         glu = LBXGLU,\r\n         hdl = LBDHDD,\r\n         tri = LBXTR,\r\n         waist = BMXWAIST\r\n        ) %>%\r\n  select(one, id, psu, strata, persWeight, persWeightMEC, age, ageCat, female, ethnicity, sys, dia, glu, hdl, tri, waist, bp.prsn, chol.prsn, glu.prsn) # 10175\r\n\r\nDefine metabolic syndrome\r\nAccording to Alberti et al. (2009), the presence of any 3 of 5 risk factors constitutes a diagnosis of metabolic syndrome:\r\nelevated waist circumference (≥88 cm for women and ≥102 cm for men),\r\nelevated triglycerides (≥150 mg/dL) or drug treatment for elevated triglycerides (there was no variable indicating the medication for lower triglycerides),\r\nlow HDL cholesterol (<40 mg/dL for men and <50 mg/dL for women) or drug treatment for low HDL cholesterol,\r\nelevated blood pressure (systolic ≥130 mm Hg, or diastolic ≥85 mm Hg, or both) or antihypertensive drug treatment for a history of hypertension, and\r\nelevated fasting glucose (≥100 mg/dL) or drug treatment for elevated glucose.” (Moore et al., 2017; Preventing Chronic Disease).\r\nBesides, I intentionally created the indicators for missing data: - return NA if all indicators are NA for mets.sum: when I summed up all 5 MetS indicators, if all are NA, so sum should be NA (not 0)\r\n- ind.non_missing: return TRUE if none of indicators are none-NA, otherwise FALSE\r\nFinally, mets.ind (MetS indicator) would be binary of 1 and 0 if sum of 5 indicators above greater or equal to 3.\r\n\r\nnhanes.MetS <- nhanes.vbls %>%\r\n  mutate(waist.ind = case_when(waist >= 88  & female == 1 ~ 1, #condition 1\r\n                               waist >= 102 & female == 0 ~ 1, #condition 2\r\n                               is.na(waist) | is.na(female) ~ NA_real_, #condition 3\r\n                               TRUE ~ 0), #all other cases\r\n         tri.ind = case_when(tri >= 150 | chol.prsn == 1 ~ 1,\r\n                             is.na(tri) ~ NA_real_,\r\n                             TRUE ~0),\r\n         hdl.ind = case_when((hdl < 40 & female == 0) | chol.prsn == 1 ~ 1,\r\n                             (hdl < 50 & female == 1) | chol.prsn == 1 ~ 1,\r\n                             is.na(hdl) | is.na(female) ~ NA_real_,\r\n                             TRUE ~ 0),\r\n         bp.ind = case_when(sys >= 130 | dia >= 85 | bp.prsn == 1 ~ 1,\r\n                            is.na(sys) & is.na(dia) ~ NA_real_,\r\n                            TRUE ~ 0\r\n                            ),\r\n         glu.ind = case_when(glu >= 100 | glu.prsn == 1 ~ 1,\r\n                             is.na(glu) ~ NA_real_,\r\n                             TRUE ~ 0)\r\n         ) # 10175 obs\r\n\r\n\r\n\r\nnhanes.MetS$mets.sum = rowSums(nhanes.MetS[20:24], na.rm = T) # from waist.ind to glu.ind # 20:24\r\nnhanes.MetS$mets.sum[rowSums(!is.na(nhanes.MetS[20:24])) == 0] <- NA # return NA if all indicators are NA\r\nnhanes.MetS$ind.non_missing <- !rowSums(!is.na(nhanes.MetS[20:24])==0) # create an indicator in which none of criteria is none NA\r\nnhanes.MetS$mets.ind <- ifelse(nhanes.MetS$mets.sum >= 3, 1, 0) # 10175 obs\r\n\r\nHere we can get the sense of data for all indicators created: \r\nDefine sub-population of interest\r\nage of 20+ Reasons for the consistency in comparison:Ford, Giles, and Dietz (2002) used NHANES III, 1988 to 1994 based on 2001 Adult Treatment Panel III (ATP III) criteria (same as Moore et al.) with age groups.\r\n\r\nSmiley, King, and Bidulescu (2019) did not specify the age range but they used the same criteria for adults (e.g. “Metabolic syndrome was defined according to NCEP ATPIII (National Cholesterol Education Program Adult Treatment Panel III) criteria.”).\r\ninAnalysis1: age of 20+ and none of missing of MetS indicatorinAnalysis2: age of 20+ and none of indicators are none-NA\r\n\r\n\r\nnhanes.MetS <- nhanes.MetS %>% \r\n  mutate(\r\n    #  Define sub-population of interest \r\n    inAnalysis1 = (age >=20 & !is.na(mets.ind)),\r\n    inAnalysis2 = (age >=20 & ind.non_missing)\r\n    ) # 10175 obs\r\n\r\n\r\nsaveRDS(nhanes.MetS, file = \"data/nhanesMetS13_14snd.rds\")\r\n\r\nOur sample size would be if I subset data on:\r\ninAnlysis1: 5649\r\ninAnlysis2: 2587\r\nMetS proportion in the random sample (unweighted)\r\nWith all age groups\r\n\r\nnhanes.MetS %>%\r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n6940\r\n\r\n\r\n68.2\r\n\r\n\r\n1\r\n\r\n\r\n2184\r\n\r\n\r\n21.5\r\n\r\n\r\nNA\r\n\r\n\r\n1051\r\n\r\n\r\n10.3\r\n\r\n\r\nWith all age groups & none missing of MetS indicator\r\n\r\nnhanes.MetS %>%\r\n  filter(!is.na(mets.ind)) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n6940\r\n\r\n\r\n76.1\r\n\r\n\r\n1\r\n\r\n\r\n2184\r\n\r\n\r\n23.9\r\n\r\n\r\nWith age 20+ & none missing of MetS indicator\r\n\r\nnhanes.MetS %>% \r\n  filter(age>=20 & !is.na(mets.ind)) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n3511\r\n\r\n\r\n62.2\r\n\r\n\r\n1\r\n\r\n\r\n2138\r\n\r\n\r\n37.8\r\n\r\n\r\nWith age 20+ and ind.non_missing is TRUE:\r\n\r\nNote: ind.non_missing was created an indicator in which none of criteria is none NA\r\n\r\n\r\nnhanes.MetS %>% \r\n  filter(age>=20 & ind.non_missing) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n1273\r\n\r\n\r\n49.2\r\n\r\n\r\n1\r\n\r\n\r\n1314\r\n\r\n\r\n50.8\r\n\r\n\r\nMetS prevalence with weighted\r\nNote:  \r\npsu = SDMVPSU, \r\nstrata = SDMVSTRA, \r\npersWeight = WTINT2YR, \r\npersWeightMEC= WTMEC2YR\r\nApply weights then subset data\r\n\r\n# Define survey design for overall dataset \r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\n# Create a survey design object for the subset of interest \r\n# Subsetting the original survey design object ensures we keep the design information about the number of clusters and strata\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1       36 0.8\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      7.0 1.0\r\n30-39  30-39     21.6 1.0\r\n40-49  40-49     30.0 1.3\r\n50-59  50-59     45.7 2.1\r\n60-69  60-69     59.8 2.8\r\n70+      70+     67.2 1.4\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     30.1 1.1\r\n1         1     39.0 1.1\r\n2         2     34.1 1.6\r\n3         3     23.4 2.8\r\n4         4     28.3 3.9\r\n\r\nCycle 2015/2016\r\nI processed the same procedures as the cycle 2013/2014. Then saved as RDS file. Here I just showed the prevalence after reading the RDS data into R using weights.\r\n\r\nnhanes.MetS <- readRDS(file = \"data/nhanesMetS15_16.rds\")\r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1     37.7 1.8\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      9.2 1.5\r\n30-39  30-39     19.3 1.4\r\n40-49  40-49     33.2 3.2\r\n50-59  50-59     48.1 2.5\r\n60-69  60-69     61.7 2.4\r\n70+      70+     66.7 2.5\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     34.0 1.8\r\n1         1     40.1 2.4\r\n2         2     34.0 1.2\r\n3         3     23.9 2.8\r\n4         4     43.9 4.1\r\n\r\nCycle 2017/2018\r\n\r\nnhanes.MetS <- readRDS(file = \"data/nhanesMetS17_18.rds\")\r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1     38.2 1.3\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      9.0 1.5\r\n30-39  30-39     18.4 1.6\r\n40-49  40-49     31.1 1.8\r\n50-59  50-59     49.2 3.4\r\n60-69  60-69     60.5 2.8\r\n70+      70+     71.3 1.4\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     34.5 1.7\r\n1         1     40.3 1.7\r\n2         2     33.5 1.7\r\n3         3     32.1 2.3\r\n4         4     42.5 4.4\r\n\r\nSummary\r\n\r\nCycle <- c(\"2013-2014\",\"2015-2016\",\"2017-2018\")\r\nPrevalence <- c(36, 37.7, 38.2)\r\nyear.prl <- as.tibble(data.frame(Cycle, Prevalence))\r\n\r\nAge.Categories <- c('20-29','30-39','40-49','50-59','60-69','70+')\r\nPrevalence.Cycle.13_14 <- c(7,21.6,30,45.7,59.8,67.2)\r\nPrevalence.Cycle.15_16 <- c(9.2,19.3,33.2,48.1,61.7,66.7)\r\nPrevalence.Cycle.17_18 <- c(9,18.4,31.1,49.2,60.5,71.3)\r\nage.g.p <- as.tibble(data.frame(Age.Categories,Prevalence.Cycle.13_14,Prevalence.Cycle.15_16,Prevalence.Cycle.17_18))\r\nage.g.p.l <- gather(age.g.p, Prevalence, Percent, Prevalence.Cycle.13_14:Prevalence.Cycle.17_18)\r\n\r\nRace.Ethnic.Categories <- c('Hispanic','White','Black','Asian','Multi-Racial')\r\nPrevalence.Cycle.13_14 <- c(34.0,40.1,34.0,23.9,43.9)\r\nPrevalence.Cycle.15_16 <- c(34.0,40.1,34.0,23.9,43.9)\r\nPrevalence.Cycle.17_18 <- c(34.5,40.3,33.5,32.1,42.5)\r\nr.e.g.p <- as.tibble(data.frame(Race.Ethnic.Categories,Prevalence.Cycle.13_14,Prevalence.Cycle.15_16,Prevalence.Cycle.17_18))\r\nr.e.g.p.l <- gather(r.e.g.p, Prevalence, Percent, Prevalence.Cycle.13_14:Prevalence.Cycle.17_18)\r\n\r\n\r\nyear.prl %>% ggplot(aes(x = Cycle, y = Prevalence)) + \r\n  geom_point() +\r\n  geom_line(aes(group=1)) +\r\n  geom_text(label=paste0(Prevalence,\"%\"), hjust=1.5, vjust=0) +\r\n  ylim(30, 45) +\r\n  theme_bw()\r\n\r\n\r\n\r\nage.g.p.l %>% ggplot(aes(fill=Prevalence, y=Percent, x=Age.Categories)) + \r\n  geom_bar(position=\"dodge\", stat=\"identity\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nr.e.g.p.l %>% ggplot(aes(fill=Prevalence, y=Percent, x=Race.Ethnic.Categories)) + \r\n  geom_bar(position=\"dodge\", stat=\"identity\") +\r\n  theme_bw()\r\n\r\n\r\nMetabolic syndrome at the cycle 2013-2014 was close to Smiley, King, and Bidulescu (2019) (NHANES 2013-2014 with 31.5% in which the authors defined MetS using the criteria established by Lee, Gurka and DeBoer (Expert Rev. Cardiovasc. Ther, 2016), which takes into account racial and age-specific differences in populations, resulting in a complex formula to determine a score so they perhaps got the lower MetS cases), and Moore, Chaudhary, and Akinyemiju (2017) reported 34.2% in 2007–2012 (also applying in NHANES data).\r\nMetabolic syndrome is on the rise.\r\nThe rise emphasized on the older age groups.\r\nMore recently, Asian groups rose the most in metabolic syndrome.\r\nReferences\r\n\r\n\r\nAlberti, K. G. M. M., Robert H. Eckel, Scott M. Grundy, Paul Z. Zimmet, James I. Cleeman, Karen A. Donato, Jean-Charles Fruchart, W. Philip T. James, Catherine M. Loria, and Jr. Smith Sidney C. 2009. “Harmonizing the Metabolic Syndrome: A Joint Interim Statement of the International Diabetes Federation Task Force on Epidemiology and Prevention; National Heart, Lung, and Blood Institute; American Heart Association; World Heart Federation; International Atherosclerosis Society; and International Association for the Study of Obesity.” Journal Article. Circulation 120 (16): 1640–45. https://doi.org/10.1161/CIRCULATIONAHA.109.192644.\r\n\r\n\r\nFord, Earl S., Wayne H. Giles, and William H. Dietz. 2002. “Prevalence of the Metabolic Syndrome Among US Adults: Findings from the Third National Health and Nutrition Examination Survey.” Journal Article. JAMA : The Journal of the American Medical Association 287 (3): 356–59. https://doi.org/10.1001/jama.287.3.356.\r\n\r\n\r\nMoore, Justin Xavier, Ninad Chaudhary, and Tomi Akinyemiju. 2017. “Metabolic Syndrome Prevalence by Race/Ethnicity and Sex in the United States, National Health and Nutrition Examination Survey, 1988-2012.” Journal Article. Preventing Chronic Disease 14: E24–24. https://doi.org/10.5888/pcd14.160287.\r\n\r\n\r\nSmiley, Abbas, David King, and Aurelian Bidulescu. 2019. “The Association Between Sleep Duration and Metabolic Syndrome: The NHANES 2013/2014.” Journal Article. Nutrients 11 (11): 2582. https://doi.org/10.3390/nu11112582.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-09-mets-using-nhanesdata/distill-preview.png",
    "last_modified": "2021-06-16T13:11:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-26-Mplus-as-a-knitr-engine/",
    "title": "Set up to run Mplus inside Rmarkdown",
    "description": "Mplus as a knitr engine in Rmarkdown  \nMplusAutomation: a brief guide",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Mplus",
      "R",
      "MplusAutomation",
      "Tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nGoals\r\nSet up Mplus in Rmarkdown\r\nGentle introduction to MplusAutomation\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nLatent Profile Analysis using Mplus\r\nRun direct in Rmarkdown\r\nUsing runModels\r\n\r\nCompare the model fit\r\n\r\n\r\nGoals\r\nThis post I aim to set up the Mplus engine using knitr package in Rmarkdown.\r\nAfter that, I run a simple model example to demonstrate how Mplus works within Rmarkdown. (Therefore, I do not intent to explain the model results and fit here.)\r\nSet up Mplus in Rmarkdown\r\nGlobally we can change the engine interpreters for one/multiple engine(s), we applied for Mplus:\r\n\r\nknitr::opts_chunk$set(engine.path = list(\r\n  mplus = \"C:/Program Files/Mplus/Mplus\"\r\n))\r\n\r\nNext step, we register a Mplus custom language engine:\r\n\r\nknitr::knit_engines$set(mplus = function(options) {\r\n    code <- paste(options$code, collapse = \"\\n\")\r\n    fileConn<-file(\"formplus.inp\")\r\n    writeLines(code, fileConn)\r\n    close(fileConn)\r\n    out  <- system2(\"C:/Program Files/Mplus/Mplus\", \"formplus.inp\")\r\n    fileConnOutput <- file(\"formplus.out\")\r\n    mplusOutput <- readLines(fileConnOutput)\r\n    knitr::engine_output(options, code, mplusOutput)\r\n})\r\n\r\nFor more language engines, we can refer demos from Yihui Xie’s post.\r\nFurther reading, we may read a Rich Jones’ post on rpubs\r\nGentle introduction to MplusAutomation\r\nThe MplusAutomation package for R (Hallquist and Wiley 2018):\r\nCreating many similar syntax files:\r\nSimulations with different sample sizes\r\nExcluding different parts of a sample\r\n\r\nRunning batches of input files\r\nExtracting and tabulating model parameters and test statistics.\r\nFour core routines support these aims:\r\ncreateModels\r\nrunModels\r\nreadModels\r\ncompareModels\r\nThe MplusAutomation package can be installed within R using the following call:\r\n\r\nif (!require(MplusAutomation)) install.packages(\"MplusAutomation\")\r\nlibrary(MplusAutomation)\r\n\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nI use Edgar Anderson’s Iris Data with 150 cases (rows) and 5 variables named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.\r\niris data set gives the measurements in cm of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris: Iris setosa, versicolor, and virginica.\r\n\r\n?iris\r\n\r\n\r\nhead(iris)\r\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1          5.1         3.5          1.4         0.2  setosa\r\n2          4.9         3.0          1.4         0.2  setosa\r\n3          4.7         3.2          1.3         0.2  setosa\r\n4          4.6         3.1          1.5         0.2  setosa\r\n5          5.0         3.6          1.4         0.2  setosa\r\n6          5.4         3.9          1.7         0.4  setosa\r\n\r\nprepareMPlusData() function prepares a data file in the MPlus’ format, namely, a tab-separated .dat file with no column names.\r\n\r\nprepareMplusData(iris[, -5], \"iris.dat\", inpfile=TRUE)\r\n\r\nLatent Profile Analysis using Mplus\r\nWe can directly run in Rmarkdown or apply runModels to run the .inp files.\r\nRun direct in Rmarkdown\r\nOne for which we estimate different means between 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:33 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n*** WARNING in MODEL command\r\n  All variables are uncorrelated with all other variables within class.\r\n  Check that this is what is intended.\r\n   2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -488.915  253358           2\r\n            -488.915  68985            17\r\n            -488.915  76974            16\r\n            -488.915  573096           20\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       13\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -488.915\r\n          H0 Scaling Correction Factor      0.9851\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                    1003.830\r\n          Bayesian (BIC)                  1042.968\r\n          Sample-Size Adjusted BIC        1001.825\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         0.991\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.003    0.997\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   0.995    0.005\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1      5.216    0.000\r\n    2    -13.816    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n Means\r\n    X1                 5.006      0.049    102.032      0.000\r\n    X2                 3.423      0.055     61.909      0.000\r\n    X3                 1.471      0.026     55.788      0.000\r\n    X4                 0.250      0.016     15.938      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nLatent Class 2\r\n\r\n Means\r\n    X1                 6.265      0.068     92.358      0.000\r\n    X2                 2.873      0.034     85.125      0.000\r\n    X3                 4.911      0.085     57.798      0.000\r\n    X4                 1.678      0.043     38.643      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.685      0.175     -3.924      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.810E-03\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -741.018\r\n          2 Times the Loglikelihood Difference            504.205\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             18.069\r\n          Standard Deviation                               25.180\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           484.852\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:33:15\r\n        Ending Time:  22:33:16\r\n       Elapsed Time:  00:00:01\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means between the 2 profiles and the model is specified to estimate the correlation (or covariance) for the variables\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n\r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:33 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      x1 WITH x2-x4;\r\n      x2 WITH x3-x4;\r\n      x3 WITH x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n   1 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -296.448  533738           11\r\n            -296.448  68985            17\r\n            -296.448  unperturbed      0\r\n            -296.448  27071            15\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       19\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -296.448\r\n          H0 Scaling Correction Factor      1.0304\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                     630.895\r\n          Bayesian (BIC)                   688.097\r\n          Sample-Size Adjusted BIC         627.966\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         1.000\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1     11.903    0.000\r\n    2    -12.706    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 5.006      0.049    101.442      0.000\r\n    X2                 3.428      0.053     64.589      0.000\r\n    X3                 1.462      0.024     60.137      0.000\r\n    X4                 0.246      0.015     16.674      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nLatent Class 2\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 6.262      0.066     94.947      0.000\r\n    X2                 2.872      0.033     86.743      0.000\r\n    X3                 4.906      0.082     59.719      0.000\r\n    X4                 1.676      0.042     39.652      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.693      0.173     -4.002      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.149E-04\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -379.915\r\n          2 Times the Loglikelihood Difference            166.934\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             11.054\r\n          Standard Deviation                                9.531\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           160.527\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:33:16\r\n        Ending Time:  22:33:17\r\n       Elapsed Time:  00:00:01\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means and the model is specified to different covariances (and variable variances) between the 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n\r\n    %c#1%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\n\r\n\r\nThe disadvantage is that Rmarkdown shows a lengthy output, making “eye strain” to track the necessary information.\r\n\r\nUsing runModels\r\nSave each chunk of the following models (either in a .txt file or in MPlus style using a .inp file type), then run with runModels\r\n\r\n# Model 1\r\nrunModels(\"2-iris-LPA_means.inp\")\r\n\r\nRunning model: 2-iris-LPA_means.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means.inp\" \r\n# Model 2\r\nrunModels(\"2-iris-LPA_means_correlated.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated.inp\" \r\n# Model 3\r\nrunModels(\"2-iris-LPA_means_correlated_free_variances.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated_free_variances.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated_free_variances.inp\" \r\n\r\n\r\nm1 <- readModels(\"2-iris-LPA_means.out\")\r\nReading model:  2-iris-LPA_means.out \r\nm2 <- readModels(\"2-iris-LPA_means_correlated.out\")\r\nReading model:  2-iris-LPA_means_correlated.out \r\nm3 <- readModels(\"2-iris-LPA_means_correlated_free_variances.out\")\r\nReading model:  2-iris-LPA_means_correlated_free_variances.out \r\n\r\nCompare the model fit\r\nNow, we inspect the fit statistics and other summary information for the three models:\r\n\r\nm1$summaries$BIC\r\n[1] 1042.968\r\nm2$summaries$BIC\r\n[1] 688.097\r\nm3$summaries$BIC\r\n[1] 574.018\r\n\r\nAnd examine parameters:\r\n\r\nm1$parameters[[1]][-nrow(m1$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1        Means    X1 5.006 0.049 102.032    0           1\r\n2        Means    X2 3.423 0.055  61.909    0           1\r\n3        Means    X3 1.471 0.026  55.788    0           1\r\n4        Means    X4 0.250 0.016  15.938    0           1\r\n5    Variances    X1 0.328 0.042   7.853    0           1\r\n6    Variances    X2 0.121 0.017   7.347    0           1\r\n7    Variances    X3 0.459 0.063   7.340    0           1\r\n8    Variances    X4 0.123 0.013   9.126    0           1\r\n9        Means    X1 6.265 0.068  92.358    0           2\r\n10       Means    X2 2.873 0.034  85.125    0           2\r\n11       Means    X3 4.911 0.085  57.798    0           2\r\n12       Means    X4 1.678 0.043  38.643    0           2\r\n13   Variances    X1 0.328 0.042   7.853    0           2\r\n14   Variances    X2 0.121 0.017   7.347    0           2\r\n15   Variances    X3 0.459 0.063   7.340    0           2\r\n16   Variances    X4 0.123 0.013   9.126    0           2\r\nm2$parameters[[1]][-nrow(m2$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1      X1.WITH    X2 0.113 0.019   5.805    0           1\r\n2      X1.WITH    X3 0.305 0.050   6.104    0           1\r\n3      X1.WITH    X4 0.114 0.019   6.112    0           1\r\n4      X2.WITH    X3 0.098 0.022   4.359    0           1\r\n5      X2.WITH    X4 0.056 0.010   5.330    0           1\r\n6      X3.WITH    X4 0.193 0.024   8.175    0           1\r\n7        Means    X1 5.006 0.049 101.442    0           1\r\n8        Means    X2 3.428 0.053  64.589    0           1\r\n9        Means    X3 1.462 0.024  60.137    0           1\r\n10       Means    X4 0.246 0.015  16.674    0           1\r\n11   Variances    X1 0.331 0.042   7.870    0           1\r\n12   Variances    X2 0.120 0.016   7.574    0           1\r\n13   Variances    X3 0.460 0.063   7.333    0           1\r\n14   Variances    X4 0.123 0.013   9.137    0           1\r\n15     X1.WITH    X2 0.113 0.019   5.805    0           2\r\n16     X1.WITH    X3 0.305 0.050   6.104    0           2\r\n17     X1.WITH    X4 0.114 0.019   6.112    0           2\r\n18     X2.WITH    X3 0.098 0.022   4.359    0           2\r\n19     X2.WITH    X4 0.056 0.010   5.330    0           2\r\n20     X3.WITH    X4 0.193 0.024   8.175    0           2\r\n21       Means    X1 6.262 0.066  94.947    0           2\r\n22       Means    X2 2.872 0.033  86.743    0           2\r\n23       Means    X3 4.906 0.082  59.719    0           2\r\n24       Means    X4 1.676 0.042  39.652    0           2\r\n25   Variances    X1 0.331 0.042   7.870    0           2\r\n26   Variances    X2 0.120 0.016   7.574    0           2\r\n27   Variances    X3 0.460 0.063   7.333    0           2\r\n28   Variances    X4 0.123 0.013   9.137    0           2\r\nm3$parameters[[1]][-nrow(m3$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se  pval LatentClass\r\n1      X1.WITH    X2 0.097 0.022   4.469 0.000           1\r\n2      X1.WITH    X3 0.016 0.010   1.655 0.098           1\r\n3      X1.WITH    X4 0.010 0.004   2.486 0.013           1\r\n4      X2.WITH    X3 0.011 0.008   1.418 0.156           1\r\n5      X2.WITH    X4 0.009 0.005   1.763 0.078           1\r\n6      X3.WITH    X4 0.006 0.003   2.316 0.021           1\r\n7        Means    X1 5.006 0.049 101.439 0.000           1\r\n8        Means    X2 3.428 0.053  64.591 0.000           1\r\n9        Means    X3 1.462 0.024  60.132 0.000           1\r\n10       Means    X4 0.246 0.015  16.673 0.000           1\r\n11   Variances    X1 0.122 0.022   5.498 0.000           1\r\n12   Variances    X2 0.141 0.033   4.267 0.000           1\r\n13   Variances    X3 0.030 0.007   4.222 0.000           1\r\n14   Variances    X4 0.011 0.003   3.816 0.000           1\r\n15     X1.WITH    X2 0.121 0.027   4.467 0.000           2\r\n16     X1.WITH    X3 0.449 0.070   6.377 0.000           2\r\n17     X1.WITH    X4 0.166 0.026   6.282 0.000           2\r\n18     X2.WITH    X3 0.141 0.033   4.330 0.000           2\r\n19     X2.WITH    X4 0.079 0.015   5.295 0.000           2\r\n20     X3.WITH    X4 0.286 0.031   9.107 0.000           2\r\n21       Means    X1 6.262 0.066  94.948 0.000           2\r\n22       Means    X2 2.872 0.033  86.743 0.000           2\r\n23       Means    X3 4.906 0.082  59.719 0.000           2\r\n24       Means    X4 1.676 0.042  39.652 0.000           2\r\n25   Variances    X1 0.435 0.059   7.332 0.000           2\r\n26   Variances    X2 0.110 0.017   6.442 0.000           2\r\n27   Variances    X3 0.675 0.086   7.822 0.000           2\r\n28   Variances    X4 0.179 0.018  10.148 0.000           2\r\n\r\nOne last thing, I want to mention about the createModels() which can creates a set of models using a template. We can save file as mplus_iris_lpa_template.txt\r\n[[init]]\r\niterators = classes;\r\nclasses = 1:9;\r\nfilename = \"[[classes]]-iris-LPA.inp\";\r\noutputDirectory = the_dir;\r\n[[/init]]\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1 x2 x3 x4;\r\n\r\n    Classes = c([[classes]]) ;\r\n\r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    [x1-x4];\r\n\r\n            \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nHere is an example that would create models with different numbers of profiles, from 1 to 9.\r\n\r\n#Set the folder containing mplus.txt as working\r\ncreateModels(\"mplus_iris_lpa_template.txt\")\r\n\r\n#Set the folder containing input files as working directory\r\nrunModels()\r\n\r\n#Set the folder containing output files as working directory\r\nmodels_list <- readModels()\r\noutputs<-extractModelParameters()\r\n\r\nFurther reading, we can look at a Josh Rosenberg’s post and the Mplus website\r\nMore information about the MplusAutomation package in GitHub.\r\n\r\n\r\nHallquist, M. N., and J. F. Wiley. 2018. “MplusAutomation: An r Package for Facilitating Large-Scale Latent Variable Analyses in Mplus.” Journal Article. Struct Equ Modeling 25 (4): 621–38. https://doi.org/10.1080/10705511.2017.1402334.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-26T22:33:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/",
    "title": "A Note on 5-day Workshop on Mplus ~ Day 1",
    "description": "A Crash Course on Social Psychology Research from Michael Zyphur: Regression - Pathway analysis - Model fit  \nCombination of using Mplus and R  \nDrawing a pathway graph/causal graph using `DiagrammeR` package",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Mplus",
      "Pathway/Causal Graph"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nCourse outline\r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nA variance \\(\\sigma^2\\)\r\nGraphical\r\nMultivariate Parameters\r\nCovariance \\(\\sigma_{xy}\\)\r\nA Linear Regression Model\r\nIn Terms of Distributions?\r\nWhy Regression? Causality\r\nControlling for variables\r\nWith Path Diagrams\r\nSummary\r\n\r\nPart 2: Mplus and Estimation\r\nData\r\nVariable\r\nDefine\r\nAnalysis\r\nEstimation and Inference\r\nModel\r\nOutput, Savedata, Plot, Montecarlo\r\nSummary\r\n\r\nPart 3: Path Analysis\r\nHistory\r\nShow case\r\nStructural Model Specification\r\nModel diagram\r\nReport\r\n\r\nWhat Does This Mean? How to Specify It?\r\nSummary\r\n\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nML Estimator\r\nModel Build\r\nModel Diagnostics\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nAbsolute Fit: SRMR\r\nAbsolute Fit: RMSEA\r\nRelative Fit: CFI\r\nRelative Fit: TLI/NNFI\r\nInformation Criteria: AIC\r\nInformation Criteria: BIC\r\n\r\nModification Indices\r\nBayesian Estimates of Fit\r\nPosterior Predictive Checking\r\nDeviance Information Criterion\r\n\r\nModel Selection\r\n\r\nFurther Readings\r\nPart 1\r\nPart 2\r\nPart 3\r\nPart 4\r\n\r\n\r\nMotivation\r\nWhile working on project Harmony at MRC-IHRP, my primary task is building an R package to build the tables and plots from alignment analysis designed for the multi-factor categorical case by extracting the Mplus output’s information. Since I have a chance to expose myself to Mplus, why don’t I self-learn a new tool and share it on my blog?\r\nThe Mplus and I see that a change to dive into Social Psychology Research. What on earth? The same concepts with statistics but a whole new world of terminology!!!\r\nThese can separate into five parts (equivalent to 5 days, then I divided into five topics) of taking notes on Structural Equation and Multilevel Modeling in the Mplus workshop of Prof. Michael Zyphur.\r\nHere’s a first part (1) of the so-called ‘A Crash Course on Social Psychology Research.’\r\n\r\nCourse outline\r\nStructural Equation and Multilevel Modeling in Mplus\r\nDay 1 - Introducing Mplus\r\nRegression, Covariation, and Statistical Models\r\nMplus and Parameter Estimation\r\nPath Analysis\r\nModel Fit and Model Selection\r\n\r\nDay 2 - Path Analysis\r\nMediation\r\nInstrumental Variable Methods\r\nModeration\r\nModerated Mediation\r\n\r\nDay 3 - SEM\r\nLatent Variables\r\nConfirmatory Factor Analysis\r\nStructural Equation Modeling\r\nModel Identification\r\n\r\nDay 4 - MLM\r\nMultilevel Data and Regression\r\nMultilevel Path Analysis\r\nMultilevel Confirmatory Factor Analysis and Structural Equation Modeling\r\nRandom Slopes\r\n\r\nDay 5 - LGM\r\nLongitudinal Data and Processes\r\nLatent Growth Models as Multilevel Models\r\nLatent Growth Models as Structural Equation Models\r\nDynamic Latent Growth Modeling\r\n\r\nThe material can be downloaded here\r\nReminder:\r\n  - Path analysis: regression for observed variables  \r\n  - CFA: regression from latent --> observed variables  \r\n  - SEM: regression among latent variables  \r\n  - Multilevel models: regression at multiple 'levels'  \r\n  - Letant growth: model change with latent variables  \r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nParameters depend on the distributions we model\r\nAssuming a normal distribution for y, we have:\r\nMean \\(\\mu_y\\): a location parameter\r\nVariance \\(\\sigma^2_y\\) : a scale parameter\r\n\r\n\r\np <- seq(-5,5,by=0.1)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(.2)), aes(colour = \"mu=0,sigma2=.2\")) + \r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(1.0)), aes(colour = \"mu=0,sigma2=1.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(5.0)), aes(colour = \"mu=0,sigma2=5.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=-2, sd=sqrt(2.0)), aes(colour = \"mu=-2,sigma2=2\")) +\r\n  scale_y_continuous(limits=c(0,1.0)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Normal Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\nA variance \\(\\sigma^2\\)\r\nTells us the spread of scores along a variable\r\nTo estimate a variance, we assume variation is ‘random’ or due to ‘chance’\r\nThis allows using distributions & their rules\r\nEither way, a variable varies (by definition)\r\nTo model a variable y, we model its variation\r\nTo speak of a variable is to speak of variance!\r\n\r\nGraphical\r\nWe’ll show models with diagrams\r\nSquares and rectangles are observed variables\r\nVariance is double-headed arrow on one variable\r\nFor concision, we will be selective about what we diagram\r\n\r\n\r\nTo refer to y’s variance in Mplus, we will write “y”\r\nMultivariate Parameters\r\nFor multiple variables x and y:\r\nMultivartiate mean vector \\(\\mu\\) for x and y\r\nCovariance \\(\\Sigma\\) as matrix of 2 variances and a covariance\r\n\r\n\r\n#generate the data\r\ngibbs<-function (n, rho) {\r\n    mat <- matrix(ncol = 2, nrow = n)\r\n    x <- 0\r\n    y <- 0\r\n    mat[1, ] <- c(x, y)\r\n    for (i in 2:n) {\r\n        x <- rnorm(1, rho * y, (1 - rho^2))\r\n        y <- rnorm(1, rho * x, (1 - rho^2))\r\n        mat[i, ] <- c(x, y)\r\n    }\r\n    mat\r\n}\r\nbvn <- gibbs(10000, 0.98)\r\n\r\n#setup\r\nlibrary(rgl) # plot3d, quads3d, lines3d, grid3d, par3d, axes3d, box3d, mtext3d\r\nlibrary(car) # dataEllipse\r\n\r\n#process the data\r\nhx <- hist(bvn[,2], plot=FALSE)\r\nhxs <- hx$density / sum(hx$density)\r\nhy <- hist(bvn[,1], plot=FALSE)\r\nhys <- hy$density / sum(hy$density)\r\n\r\n## [xy]max: so that there's no overlap in the adjoining corner\r\nxmax <- tail(hx$breaks, n=1) + diff(tail(hx$breaks, n=2))\r\nymax <- tail(hy$breaks, n=1) + diff(tail(hy$breaks, n=2))\r\nzmax <- max(hxs, hys)\r\n\r\n#Basic scatterplot on the floor\r\n## the base scatterplot\r\nplot3d(bvn[,2], bvn[,1], 0, zlim=c(0, zmax), pch='.',\r\n       xlab='X', ylab='Y', zlab='', axes=FALSE)\r\npar3d(scale=c(1,1,3))\r\n\r\n#Histograms on the back walls\r\n## manually create each histogram\r\nfor (ii in seq_along(hx$counts)) {\r\n    quads3d(hx$breaks[ii]*c(.9,.9,.1,.1) + hx$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            rep(ymax, 4),\r\n            hxs[ii]*c(0,1,1,0), color='gray80')\r\n}\r\nfor (ii in seq_along(hy$counts)) {\r\n    quads3d(rep(xmax, 4),\r\n            hy$breaks[ii]*c(.9,.9,.1,.1) + hy$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            hys[ii]*c(0,1,1,0), color='gray80')\r\n}\r\n\r\n#Summary Lines\r\n## I use these to ensure the lines are plotted \"in front of\" the\r\n## respective dot/hist\r\nbb <- par3d('bbox')\r\ninset <- 0.02 # percent off of the floor/wall for lines\r\nx1 <- bb[1] + (1-inset)*diff(bb[1:2])\r\ny1 <- bb[3] + (1-inset)*diff(bb[3:4])\r\nz1 <- bb[5] + inset*diff(bb[5:6])\r\n\r\n## even with draw=FALSE, dataEllipse still pops up a dev, so I create\r\n## a dummy dev and destroy it ... better way to do this?\r\n###dev.new()\r\nde <- dataEllipse(bvn[,1], bvn[,2], draw=FALSE, levels=0.95)\r\n###dev.off()\r\n\r\n## the ellipse\r\nlines3d(de[,2], de[,1], z1, color='green', lwd=3)\r\n\r\n## the two density curves, probability-style\r\ndenx <- density(bvn[,2])\r\nlines3d(denx$x, rep(y1, length(denx$x)), denx$y / sum(hx$density), col='red', lwd=3)\r\ndeny <- density(bvn[,1])\r\nlines3d(rep(x1, length(deny$x)), deny$x, deny$y / sum(hy$density), col='blue', lwd=3)\r\n\r\n#Beautifications\r\ngrid3d(c('x+', 'y+', 'z-'), n=10)\r\nbox3d()\r\naxes3d(edges=c('x-', 'y-', 'z+'))\r\noutset <- 1.2 # place text outside of bbox *this* percentage\r\nmtext3d('P(X)', edge='x+', pos=c(0, ymax, outset * zmax))\r\nmtext3d('P(Y)', edge='y+', pos=c(xmax, 0, outset * zmax))\r\n\r\n\r\nCovariance \\(\\sigma_{xy}\\)\r\nTells us how two variables ‘hang together’\r\nCorrelation is just a standardized covariance\r\n\r\nIf we assume variation is random …\r\nCovariance implies a non-causal relationship\r\nNo predictor or outcome, x and y simply co-vary\r\nBlue eyes & blond hair?\r\n\r\n\r\n\r\nIn Mplus we will write “y with x” or “x with y” If the relationship is non-causal, does it matter?\r\n\r\nA Linear Regression Model\r\nWhat is the rule \\(\\beta\\) ? Our linear model\r\n\\[ y_i = \\nu + \\beta x_i + \\epsilon_i \\]\r\n\\(y\\) and \\(x\\) are variables\r\n\\(\\epsilon\\) is a variable: unobserved residual/error\r\n\\(\\nu\\) and \\(\\beta\\) are constants/fixed that we estimate\r\nIn Terms of Distributions?\r\ny is decomposed into parts\r\nHow is the mean of \\(y\\) modeled?\\[ \\nu = \\mu_y — \\beta \\mu_x \\]\r\nHow is \\(y\\),\\(x\\) covariance modeled? \\[ \\beta = \\frac{\\sigma_{xy}}{\\sigma^2_x} \\]\r\nHow is the variance of \\(y\\) modeled? \\[  \\sigma^2_y =  \\sigma^2_y R^2 + \\sigma^2_{\\epsilon} \\]\r\nWhy Regression? Causality\r\nCausal statements are helpful for action\r\nConditions for \\(x \\rightarrow y\\) causality\r\n\\(x\\) precedes \\(y\\) in time\r\n\\(x\\) and \\(y\\) are related\r\n\\(x \\rightarrow y\\) effect isn’t due to a third variable z\r\n\r\nRegression assesses the second, and helps with the third\r\nControlling for variables\r\nRemoving the effect of \\(z\\) in \\(x \\rightarrow y\\) effect is\r\nControlling for \\(z\\)\r\nHolding \\(z\\) constant (it no longer varies)\r\nThe ‘independent effect’ of \\(x\\) on \\(y\\)\r\nWe ‘partial out’ the effect of \\(z\\)\r\n\r\nThese all mean the same thing:\r\nWe somehow make \\(z\\) irrelevant for \\(x \\rightarrow y\\)\r\nAllows estimating independent effects\r\n\\[ y_i = \\nu + \\beta_1 x_i + \\beta_2 z_i+ \\epsilon_i \\]\r\nEffects are independent and additive\r\nRead it left to right: \\(y\\) depends ON \\(x\\) \\(z\\)\r\nIn Mplus simply write “y ON x z”\r\nDoes it matter if “y ON x z” or “y ON z x” ?\r\nEach \\(\\beta\\) is just a slope, and \\(\\nu\\) is intercept on y-axis\r\nWith Path Diagrams\r\nRegression effects & residual as single-headed arrows\r\n\r\nResidual indicated as single-headed arrow w/out predictor\r\nRegress “y ON x z” reads the equation left-to-right\r\nNote: Predictors correlated by default in regression\r\nThe whole point is that the predictors are correlated\r\n\r\nSummary\r\nWe estimate rules/parameters to test theory\r\nMeans, (co)variances, & causal effects\r\n\r\nMplus has a simply language for these\r\nName of variable “y” refers to variance\r\n“With” refers to covariance\r\n“On” refers to a regression slope\r\n\r\nWe use multiple predictors to control for each (i.e., we estimate independent effects)\r\nPart 2: Mplus and Estimation\r\nMplus Team\r\nBengt Muthen\r\nFormer president of psychometrics society\r\nPh.D. in stats (Utrecht)\r\nAdviser: Joreskög\r\n\r\nLinda Muthen\r\nPh.D. in education (UCLA)\r\n\r\nTihomir Asparouhov\r\nPh.D. in stats (Cal Tech)\r\n\r\nStatmodel.com has a message board!\r\nstatmodel.com/discussion/messages/board-topics.html\r\n\r\nMplus Overview\r\nSolves for unknowns in regression equations:\r\nAll observed/manifest variables\r\nPath Analysis and traditional regression\r\n\r\n\r\nContinuous latent variables\r\nSEM (latent variables are factors)\r\nMultilevel/random effects/hierarchical linear models\r\nSurvival and other analyses with latent frailties/liabilities\r\n\r\nDiscrete latent variables (i.e., categorical)\r\nLatent profile analysis: continuous observed variables\r\nLatent class (cluster) analysis: discrete observed variables\r\nVarious forms of finite mixture models (LTA/Markov)\r\n\r\nMplus Features\r\nObserved variables\r\nContinuous: normal, skew-normal, t-distributed, skew-t, censored\r\nCategorical & Count: ordered, nominal, zero-inflated\r\n\r\nLinking functions: Identity, logit, probit\r\nEstimators\r\nULS, WLS, GLS, ML, MLR, MLF, BAYES, etc…\r\n\r\nSimulation with Monte Carlo & Bootstrap\r\nRead the manual – many examples!\r\nMplus Caveat\r\nInput is done by hand\r\nThere is a diagrammer, but we want a stick shift!\r\n\r\nVery little input is needed\r\nThis means many default options are used\r\n\r\nEach part of Mplus input has defaults\r\nCheck the manual, read model descriptions, look at output & parameter matrices (\\(TECH_1\\) option)\r\n\r\nExample\r\nTitle: \r\n    Start input section with the heading and a colon, then type specific commands/options and end each statement with a semi-colon;\r\n    * To ignore a command line, start it with an asteriks\r\nData: \r\n    File is data.dat;\r\nVariable: \r\n    Names are employ salary education height weight;\r\n    Usevariables are employ salary height education;\r\nAnalysis:\r\n    Estimator = ML;\r\nModel:\r\n    Employ salary education on height;\r\nOutput:\r\n    Standardized;\r\nInput Command Headings\r\nTitle: Ch. 15\r\nData: Ch. 15\r\nVariable: Ch. 15\r\nDefine: Ch. 15\r\nAnalysis: Ch. 16\r\nModel: Ch. 17\r\nModel Indirect, Model Constraint, Model Priors\r\n\r\nOutput: Ch. 18\r\nSavedata: Ch. 18\r\nPlot: Ch. 18\r\nMonteCarlo: Ch. 19\r\nData\r\nIndicates location of datafile\r\nStore in same folder as Mplus input file\r\n\r\nIndicates format of data\r\nIndividual data or summary data\r\nCorrelations, SDs, means\r\n\r\nDefault is individual data\r\n\r\nMplus requires numeric data only\r\nDo not save data with variable names\r\nFor missing data I use -999\r\nCan save from SPSS, Stata, Excel, etc…\r\nMust be tab-delimited or CSV\r\nEach variable is a column, separated by tabs or commas\r\nMissing data are -999\r\nWhere are the variables names?\r\n\r\n\r\nContains\r\nMplus input files\r\n\r\nIndividual example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Individual Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is individual.dat;\r\nAnalysis: \r\nEstimator = ml;\r\n!Estimator = Bayes;\r\nVariable: \r\nnames are y x1 x2;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1 TECH8;\r\n\r\nSummary example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Summary Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is summary.dat;\r\nType = means stdeviations correlation;\r\nNobservations = 500;\r\nVariable: \r\nnames are y x1 x2;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1;\r\n\r\nIndividual dataset\r\nIndividual.dat\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/individual.dat', n = 20), sep = '\\n')\r\n   -0.354517     0.573051    -0.175230\r\n    0.561655    -0.368095     1.090042\r\n    0.315551    -0.577052     0.425472\r\n    3.347049     1.088520     1.149353\r\n   -0.122389    -0.694153    -0.766538\r\n   -0.251276    -0.017487    -1.367410\r\n   -0.517996    -0.817974    -1.559255\r\n    1.888854    -0.658335     1.007614\r\n    0.461254     0.463916    -0.898300\r\n    2.237483     1.533398     0.180512\r\n    0.480991    -0.096545    -0.352276\r\n    0.165901    -1.341994    -1.445909\r\n    1.864947     1.027419     0.677408\r\n   -0.466245    -0.138712    -0.759287\r\n    2.567804     0.483444     0.959731\r\n   -0.024201    -0.507631    -0.517296\r\n   -1.912698     0.761720    -1.901134\r\n   -1.350069    -0.736562     2.318569\r\n    0.433773     0.723880     0.111837\r\n   -0.977083     0.155868    -0.897112\r\n\r\nSummary dataset\r\nSummary.dat\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/summary.dat'), sep = '\\n')\r\n.485    .001    -.042\r\n1.552   1.046   .978\r\n1.0\r\n.665    1.0\r\n.427    .028    1.0\r\n\r\n\r\nNote: Great! We can get the parameter estimates from the data with means, standard deviations and correlation matrix besides the `individual data. Furthermore, we can simulate data from ‘summary’ information (using Monte Carlo methods).\r\nOther options allowed (see manual)\r\nVariable\r\nAll info about our variables\r\nNames are arbitrary, but the number of names must match the number of columns in data file\r\nNames are y x1 x2;\r\n\r\nType of variable distribution\r\nContinuous normal assumed, but you can say other\r\n\r\nWhich variables will we use?\r\nUsevariables are y x1 x2;\r\n\r\nWeighting variables\r\nMany other options\r\n\r\nDefine\r\nCreates new variables that can be used\r\nYou must list them with Usevariables are\r\n\r\nLogical & arithmetic operators & functions\r\nConditional statements (to recode)\r\nIF ___ then ___;\r\n\r\nTransformations of various kinds\r\nCentering (grand-mean or group-mean)\r\nCluster_Mean, Sum, Mean, Cut\r\n\r\nDefine: x1x2 = x1*x2;\r\nAnalysis\r\nOptions\r\nType = nature of model desired\r\nType = General is default\r\nType = Twolevel or Threelevel means multilevel\r\nType = Random implies random slopes\r\n\r\n\r\nEstimators, Algorithms\r\nEstimator = ML, or Estimator = Bayes\r\n\r\nBootstrapping\r\nLinking functions (logit or probit)\r\nMany options for ML and Bayes estimation\r\nEstimation and Inference\r\nFrequentist: Estimation uses data & model\r\nTypical estimators, such as OLS or ML\r\nIterative process trying to find best parameter estimates\r\nIteratively alters estimates to find the ‘most likely’ values\r\nThis means probability of data is being maximized (i.e., ML)\r\n\r\nConvergence achieved when estimates change very little\r\nMplus gives the estimates, SE, p-values, & CIs upon request\r\nBut, even ‘most likely’ estimates may not be very likely\r\nYour model and its estimates may be the best of a bad lot!\r\nThis is why we’ll look at fit statistics\r\n\r\nBayes: Estimation uses data, model, & prior prob.\r\nDefault is ‘uninformative prior’, so results # ML\r\nIterative process that computes ‘posterior probabilities’\r\nIt uses MCMC estimation in at least two separate ‘chains’\r\nThe estimates produce a posterior distribution for each parameter\r\n\r\nConvergence achieved when chains agree (PSR < 1.05)\r\nMedian/mode of each distribution is like an ML estimate\r\nThe SD is like the SE\r\nGives Bayesian p-value and ‘credibility interval’\r\n\r\nAgain, bad models/estimates may agree across chains\r\nModel\r\nModel: ON Command\r\nON refers to a regression slope \\(\\beta\\)\r\ny ON x; \\[ y_i = \\nu + \\beta_1 x_i + \\epsilon_i \\]\r\nRegress y on x\r\nReads the equation from left to right\r\n\r\n\r\nThink of “y ON x” as \\(x \\rightarrow y\\)\r\nFreely estimates a \\(\\beta\\)\r\nThe single-headed arrow from nowhere is a residual\r\n\r\n\r\nModel: WITH Command\r\nWITH refers to covariance \\(\\Theta\\) or \\(\\Psi\\)\r\ny1 WITH y2;\r\nEstimates covariance among y1 and y2\r\n\r\nWhat if y1 and y2 are dependent variables?\r\nThen we estimate a residual covariance\r\n\r\n\r\n\r\nModel: BY Command\r\nBY refers to factor loadings (slopes)\r\nf1 BY y1 y2 y3;\r\nlatent variable “f1” is indicated by y1, y2, y3\r\n\r\nHuh BY y1 y2 y3\r\nlatent variable is called “Huh”… name is irrelevant\r\n\r\nFreely estimate \\(\\lambda\\) for each observed variable\r\nExcept first loading is fixed to 1.0 by default\r\n\r\n\r\n\r\nModel notation\r\nTypical SEM notation for vectors/matrices\r\n\\(B\\): matrix of regression coefficients, elements \\(\\beta\\)\r\n\\(\\Lambda\\): matrix of factor loadings, elements \\(\\lambda\\)\r\n\\(\\eta\\): vector of latents (factors/random effects)\r\n\\(\\Psi\\): matrix of latent (co)variances, elements ψ\r\n\\(\\Theta\\): matrix of observed (co)variances, elements \\(\\theta\\)\r\n\\(\\nu\\): intercepts, \\(\\alpha\\) is latent intercepts or means\r\n\r\nAll commands will tell Mplus to either:\r\nFreely estimate an element in vector/matrix\r\nThis is what our Model commands do\r\n\r\nFix an element in vector/matrix to some value\r\nMplus defaults often do this for us\r\n\r\nMplus notation for freeing and fixing estimates\r\ny ON x;\r\n* freely-estimated \\(\\beta\\)y ON x@.5;\r\n* \\(\\beta\\) is NOT estimated, but fixed to .5[y@0];\r\n* an intercept for y1 constrained to 0.0f1 BY y*;\r\n* * = freely estimate, so estimates factor loading for y on factor “f1”f1 BY y1-y5@1;\r\n* factor loadings for “f1” = 1 for variables y1, y2, y3, y4, y5\r\nLabeling and constraining/fixing estimates\r\nLabels are put in parenthesesy ON x (b1);\r\nNow, the slope is called “b1” and can be used later\r\n\r\nIf parameters have the same label, it’s like they’re the same thingy ON x z (b2);\r\nNow x\\(\\rightarrow\\)y and z\\(\\rightarrow\\)y slopes are constrained to equality\r\nMplus will estimate them but keep them equal\r\n\r\nModel Constraint\r\nHere we play with labeled parameters\r\nAllows linear/non-linear model constraints\r\nb1+b2=0;\r\n\r\nPlay with parameters labeled in Model command\r\n\r\nToo much creativity to describe, but:\r\nCan create new “phantom” parameters\r\nSee manual for the “New” command\r\nCheck example 5.21\r\n\r\n\r\nTITLE:     this is an example of a two-group twin\r\n           model for  continuous outcomes using parameter constraints\r\n\r\nDATA:      FILE = ex5.21.dat;\r\n\r\nVARIABLE:  NAMES = y1 y2 g;\r\n           GROUPING = g(1 = mz 2 = dz);\r\n\r\nMODEL:     [y1-y2]    (1);\r\n           y1-y2      (var);\r\n           y1 WITH y2 (covmz);\r\n\r\nMODEL dz:  y1 WITH y2 (covdz);\r\n\r\nMODEL CONSTRAINT:\r\n           NEW(a c e h);\r\n           var = a**2 + c**2 + e**2;\r\n           covmz = a**2 + c**2;\r\n           covdz = 0.5*a**2 + c**2;\r\n           h = a**2/(a**2 + c**2 + e**2);\r\nModel Priors\r\nFor Bayes, we can specify prior probabilities\r\nThese are distributions…\r\nModel:y2 ON y1 (b1);\r\nModel Priors:b1~N(.25, 1);\r\nWe say “b1” is distributed as (\\(\\sim\\)) normal (\\(N\\)) with mean and variance (\\(\\mu, \\sigma^2\\)) of .25 and 1\r\nMplus has defaults that are ‘diffuse priors’\r\nEg, regression coefficients are \\(\\sim N(0,10^10)\\)\r\n\r\nModel Indirect\r\nComputes mediation effects to show\r\n“Decomposition” of total and indirect effects\r\nUse bootstrapping in Analysis command with ML to get bootstrapped indirect effects\r\nBayesian estimation gives distribution of effects, so no bootstrapping required\r\n\r\nOutput, Savedata, Plot, Montecarlo\r\nOutput\r\nTECH, Standardized\r\n\r\nSavedata\r\nEstimated data: factor scores, co(var) matrices\r\n\r\nPlot\r\nHelpful for various models\r\n\r\nMonteCarlo\r\nData generation facility… allows parametric bootstrap\r\nMakes running simulations a snap\r\nEmpirically-derived estimates of power\r\nCan publish from this if you’re inclined\r\n\r\n\r\nSummary\r\nOur job is to\r\nTell Mplus about data and estimation method\r\nSpecify a statistical model to reflect our theory\r\nUse ML or Bayes to estimate parameters\r\nUse results to make inferences\r\nPart 3: Path Analysis\r\nHistory\r\nDeveloped by Sewell Wright in 1918 (1921)\r\nGeneticist modeling relations of family members\r\nBrother Philip borrows to create ‘simultaneous equations’ with IVs in 1928 (S&D of flaxseed)\r\n\r\nTaken up in health, biological, and social sciences to model complex relationships\r\nSubsumes simultaneous equation analysis\r\n\r\nShow case\r\nStructural Model Specification\r\nSpecify causal model of interest\r\nGrandey & Cropanzano (1999). The conservation of resources model applied to work–family conflict and strain. Journal of Vocational Behavior, 54, 350-370.\r\n\r\nCausal effects (regression)\r\nWork role stress \\(\\rightarrow\\) Job distress\r\nWork role stress \\(\\rightarrow\\) Work-family conflict\r\nWork-family conflict \\(\\rightarrow\\) Job distress\r\nJob distress \\(\\rightarrow\\) Turnover intentions\r\nJob distress \\(\\rightarrow\\) Life distress\r\n\r\nStochastic, non-causal relationships\r\nTurnover intentions\\(\\leftrightarrow\\)Life distress\r\n\r\nModel diagram\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>>]\r\nTI->LD[dir=both, label=<&psi;>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>>]\\nTI->LD[dir=both, label=<&psi;>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nVariable:\r\nNames are WRS FRS WFC FWC JD FD LD TI PPH SE;\r\nUsevariables are WRS TI WFC JD LD;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOR\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nTI WFC on WRS; LD on WFC;\r\n\r\nNote: for 2 dependent variables which are not predicting anything else, just the outcomes. Mplus will automatically covariance for us. If not estimate the residual covariance would only hurt the model fit. In other words, if there is a residual covariance there and we do not estimat them, it would drive the model fit down.\r\nCode in Mplus:\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS TI WFC JD LD;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS; ! left-side: dependent vbl; right-side: predictors\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\nNote: here’s again we use the ‘summary’ data from the article instead of ‘individual’ data (don’t have it!!!)\r\nMplus output\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.out'), sep = '\\n')\r\nMODEL RESULTS ! Unstandardized Output\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000 ! 1-unit increase on JD, .536 increase on TI\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880 ! Residual covariances\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.141E-02\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nSTANDARDIZED MODEL RESULTS \r\n\r\n\r\nSTDYX Standardization ! Standardized Output (STDYX)\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.075      0.092      0.813      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.376      0.073      5.178      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.490      0.066      7.408      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTDY Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.094      0.115      0.814      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.472      0.089      5.279      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.615      0.078      7.844      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTD Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nR-SQUARE\r\n\r\n    Observed                                        Two-Tailed\r\n    Variable        Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n    TI                 0.234      0.065      3.631      0.000\r\n    WFC                0.240      0.065      3.704      0.000\r\n    JD                 0.421      0.065      6.436      0.000\r\n    LD                 0.595      0.054     10.947      0.000\r\n\r\n\r\nTECHNICAL 1 OUTPUT\r\n\r\n     PARAMETER SPECIFICATION\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  0             0             0             0             0\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             0             0             0\r\n WFC                0             0             0             0             0\r\n JD                 0             0             0             0             0\r\n LD                 0             0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0\r\n WFC                0             0\r\n JD                 0             0             0\r\n LD                 0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  1             2             3             4             0\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             5             0             6\r\n WFC                0             0             0             0             7\r\n JD                 0             8             0             0             9\r\n LD                 0            10            11             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                12\r\n WFC                0            13\r\n JD                 0             0            14\r\n LD                15             0             0            16\r\n WRS                0             0             0             0             0\r\n\r\n     STARTING VALUES\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                0.000         0.000         0.000         0.000         0.000\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             1.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         1.000         0.000         0.000         0.000\r\n JD             0.000         0.000         1.000         0.000         0.000\r\n LD             0.000         0.000         0.000         1.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         1.000\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000\r\n WFC            0.000         0.000\r\n JD             0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                2.120         3.430         2.520         2.730         2.500\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         0.000         0.000         0.000         0.000\r\n JD             0.000         0.000         0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.490\r\n WFC            0.000         0.530\r\n JD             0.000         0.000         0.328\r\n LD             0.000         0.000         0.000         0.387\r\n WRS            0.000         0.000         0.000         0.000         0.635\r\n\r\nWhat’s \\(R^2\\) for our variables?\r\n(1 – standardized residual variance)\r\nCan also just look at bottom of output\r\nReport\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>=.63/.49**>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>=.09/.08>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>=.38/0.38**>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>=.30/.38**>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>=.19/.22**>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>=.54/.44**>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>=.68/.63**>]\r\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>=.63/.49**>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>=.09/.08>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>=.38/0.38**>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>=.30/.38**>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>=.19/.22**>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>=.54/.44**>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>=.68/.63**>]\\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nFor example: \\(\\beta_7\\) = .68/.63**, i.e. unstandardized/standardized both Y and X estimates and significance of p-value\r\nWhat Does This Mean? How to Specify It?\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (no covariance).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\n!Estimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nTI with LD@0;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model1).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nWRS on WFC JD TI;\r\nWFC on JD LD;\r\nJD on TI LD;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model2).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nWRS with WFC;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\nSummary\r\nPath models specify regression/covariation among observed variables\r\nRegression represents causality ON\r\nSo reduces magnitude of DV’s variance\r\n\r\nCovariation simply indicates WITH\r\nSo does not account for variance (there is no IV/DV)\r\n\r\nWe can draw and specify any diagram\r\nMplus then gives us parameter estimates\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nLots of literature & many indices for ML estimator\r\nIssue of Personality & Individual Differences (2007), lead by Barrett\r\nMany cite Hu & Bentler (1999)\r\n\r\nAbsolute fit\r\nHow well does our model explain our observed data?\r\n\r\nRelative/Incremental/Comparative fit\r\nCompares fit of different models using same dataset\r\n\r\nInformation Criteria\r\nMeasure info entropy, or info lost when using an estimated model rather than the data\r\n\r\nBayesians have less work on the topic …\r\nIn statistics programs we get fit from:\r\nAnalysis/Estimated model (\\(H_0\\) in Mplus output)\r\nModel we specify and estimate\r\n\r\nBaseline/Independence/Null model\r\nWorst fitting model (or at least all covariances = 0)\r\n\r\nUnrestricted/Saturated (\\(H_1\\) in Mplus output)\r\nBest fitting model (all parameters freely estimated)\r\n\r\n\r\nFor comparing models that we estimate:\r\nNull model (\\(H_0\\)): more-constrained model\r\nFewer parameters estimated, so more parsimonious\r\n\r\nAlternate model (\\(H_1\\)): less-constrained model\r\n\r\nML Estimator\r\nInformation we have vs. what’s estimated\r\nHow many parameters implied by our data?\r\nk means, k variances, and k\\(\\times\\)(k-1)/2 covariances\r\n\r\nHow many parameters are estimated?\r\nIntercepts, (residual) variances, slopes/covariances\r\n\r\n\r\nThe difference is our degrees of freedom (df)\r\nWhat about data/model fit ?\r\nFor ML, we have model log Likelihood (LL)\r\nIt turns out, -2\\(\\times\\)LL is \\(\\chi^2\\) distributed\r\nThe difference between two \\(\\chi^2\\) values is \\(\\chi^2\\) distributed\r\n\r\n\r\nModel Build\r\nAnalysis/Estimated model (\\(H_0\\))\r\nUnrestricted/Saturated/Alternate model (\\(H_1\\))\r\n\r\n\r\nModel:TI on JD WRS;LD on JD WFC;JD on WRS WFC;WFC on WRS;\r\nModel:JD LD TI WRS WFC with JD LD TI WRS WFC;\r\nBaseline/Null Model\r\n\r\nModel:\r\nModel Diagnostics\r\nContrasting Models\r\nChi-square testing of nested models\r\nCan use change in relative fit indices\r\nNon-nested model comparisons possible with AIC/BIC\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nUnrestricted vs Analysis model gives us \\(\\chi^2\\)\r\n\\(df\\) is difference in estimated model & unrestricted\r\n\r\nIf our model is good, the difference should be small\r\nConducting NHST with the \\(\\chi^2\\)\r\nWhat’s the null vs. alternate hypothesis?\r\n\r\nIssues\r\nSensitive to sample size\r\nRarely taken seriously… unless it’s non-significant!\r\nWe want to “accept the null” ???\r\n\r\nIf you want to look good on a relative basis… ??\r\nUsing \\(\\chi^2\\) to Compare Estimated Models\r\nGet \\(\\chi^2\\) and \\(df\\) for different estimated models\r\nSubtract \\(\\chi^2\\) and subtract \\(df\\), use diff. for NHST: \\(\\Delta \\chi^2\\) or \\(-2\\times LL\\)\r\nRecall the null is model with fewer parameters\r\n\r\nModel must be nested: null must be subset of alt.\r\nAbsolute Fit: SRMR\r\nStandardized Root Mean Square Residual\r\nStandardized difference between observed and model-implied data\r\nResidual in this case is the difference\r\n\r\nLess than .05 is good, Hu & Bentler say that .08 isn’t too bad…\r\nAbsolute Fit: RMSEA\r\nRoot mean squared error of approximation\r\nLike an adjusted root mean square standardized residual\r\n\r\nTakes model parsimony into account\r\nPenalized for estimating too many parameters\r\nPuts a positive value on df\r\n\r\n\\[ \\frac{\\sqrt{\\chi^2_{Estimated} - df_{Estimated}}}{\\sqrt{df\\times (N-1)}}\\]\r\nLess than .06 or .07 good (CI usually given)\r\nSteiger (2007), Understanding the limitations of global fit…\r\n\r\nRelative Fit: CFI\r\nComparative Fit Index\r\nCompares \\(\\chi^2\\) of estimated model to \\(\\chi^2\\) of baseline\r\nAdjusts for \\(df\\) to reward parsimony\r\n\r\n\\[ \\frac{\\sqrt{(\\chi^2_{Baseline} - df_{Baseline}) - (\\chi^2_{Estimated} - df_{Estimated})}}{\\sqrt{\\chi^2_{Baseline} - df_{Baseline}}}\\]\r\n.95 or higher generally accepted as cut-off\r\nRelative Fit: TLI/NNFI\r\nTucker-Lewis Index/Non-Normed Fit Index\r\nSimilar to CFI but different penalization\r\n\\[ \\frac{\\chi^2_{Baseline}/df_{Baseline} - \\chi^2_{Estimated}/df_{Estimated}}{\\chi^2_{Baseline}/ df_{Baseline} - 1}\\]\r\nWill always be smaller than CFI\r\nTo some decimal place\r\n\r\n.95 often thought of as cut-off\r\nInformation Criteria: AIC\r\nAkaikes Information Criterion\r\nk is number of estimated parameter\r\nA weighting of accuracy versus model complexity\r\nOnly useful for comparing two estimated models\r\nLower values are better\r\n\\[ \\chi^2_{Estimated} + k\\times (k-1) - 2\\times df_{Estimated} \\]\r\nInformation Criteria: BIC\r\nBayesian Information Criterion & Adjusted BIC\r\nSimilar to AIC, except weights for sample size\r\nOnly useful for comparing two estimated models\r\nSmaller values are better—BIC, then aBIC here:\r\n\\[ \\chi^2_{Estimated} + ln(N)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\n\\[ \\chi^2_{Estimated} + ln((N+2)/24)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\nFor both AIC and BIC, no significance tests… see recommendations in interpreting differences\r\nThese are all Global Measures\r\nThey collapse across all model parts\r\nSome authors call for separate fit measures\r\nWhatever… fit is necessary but insufficient\r\nThe model must make theoretical sense\r\n\r\nModification Indices\r\nMODINDICES command in Mplus\r\nIndicate change in model fit by estimating additional parameters\r\nHeated debates here\r\nFor frequentists this capitalizes on chance\r\nCan lead to nonsensical models, bias/variance tradeoff\r\n\r\nUseful for understanding sources of misfit\r\nLet’s try it with our path model…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (ML) modindices.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8 MODINDICES(1);\r\n\r\nBayesian Estimates of Fit\r\nNot as well developed\r\nBayes was unpopular for much of \\(20^{th}\\) Century\r\n\r\nLess obvious how to proceed\r\nThe idea of sampling from a population not formalized in the same way (so no \\(\\chi^2\\))\r\n\r\nMplus offers a few that are useful…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (Bayes).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = Bayes;\r\nfbiterations=10000;\r\nProcessors=2;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\nPosterior Predictive Checking\r\nSteps required during model estimation\r\nSample parameters from posteriors\r\n\r\nGenerate data with parameters\r\n\r\nEstimate fit of generated vs. observed data\r\n\r\n\r\nIf model is good, generated data should fit better/worse about half the time\r\nGet Predictive Posterior p-value (PPP)\r\nHow often the generated data fit better\r\nPPP < .05 bad, PPP ≈ .50 good, PPP > .95 weird\r\n\r\nDeviance Information Criterion\r\nLike other information criteria\r\nPenalizes for model complexity, rewards fit\r\nSmaller values = less deviance (i.e., better fit)\r\nCan compare models with different priors\r\nModel Selection\r\nHow do we choose the models we publish?\r\nSelecting models\r\nCan build them based on scripted steps9,10\r\nGive your model of interest a shot and look at fit\r\nIf that doesn’t work, try different theory-driven models\r\nLook at sources of misfit and sort out what’s going on\r\n\r\nIs this an ethical issue?\r\nScientific models are serious things\r\nPeople/groups use results for their own purposes\r\nWhen considering which model to estimate, consider the social & political implications\r\n\r\nFurther Readings\r\nPart 1\r\nGrace, J. B., & Bollen, K. A. (2005). Interpreting the results from multiple regression and structural equation models. Bulletin of the Ecological Society of America, 86, 283-295.\r\nCohen, Cohen, West, & Aiken (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (3rd). Lawrence Erlbaum & Associates.\r\nMyers, Well, & Lorch (2010). Research Design and Statistical Analysis (3rd). Routledge Academic.\r\nTabachnick & Fidell (2006). Using Multivariate Statistics (5th). Allyn & Bacon.\r\nFisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London, Series A 222, 309–368.\r\nPart 2\r\nMplus User’s Guide and Diagrammer Documentation\r\nPart 3\r\nStreiner, D. L. 2005. Finding Our Way: An Introduction to Path Analysis. Canadian Journal of Psychiatry, 50, 115-122.\r\nWright, S. (1921). Correlation and causation. J. Agricultural Research 20: 557–585.\r\nWright, S. (1934). The method of path coefficients. Annals of Mathematical Statistics 5: 161–215.\r\nAngrist, J. D., & Krueger, A. B. (2001). Instrumental variables and the search for identification: From supply and demand to natural experiments. Journal of Economics Perspectives, 15: 69-85.\r\nMuthén, B. (2002). Beyond SEM: General latent variable modeling. Behaviormetrika, 29, 81-117.\r\nPart 4\r\nHu & Bentler (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6(1), 1-55.\r\nPersonality and Individual Differences, Volume 42, Issue 5. 2007.\r\nNeyman, J., & Pearson, E. S. 1933. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society, A, 231, 289-337.\r\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111-16\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773-795.\r\nMcDonald, R. P. (2010). Structural models and the art of approximation. Perspectives on Psychological Science, 5, 675-686.\r\nCheung, G. W., & Rensvold, R. B. (1999). Testing factorial invariance across groups: a reconceptualization and proposed new method. Journal of Management, 25, 1–27.\r\nCheung, G. W., & Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. Structural Equation Modeling, 9, 233–255.\r\nAnderson, J. C., & Gerbing, D. W. (1988). Structural equation modeling in practice: A review and recommended two-step approach. Psychological Bulletin, 103, 411-423.\r\nMulaik, S. A., & Millsap, R. E. (2000). Doing the four-step right. Structural Equation Modeling, 7, 36-73. (see other papers in the same issue).\r\nShapin, S. 2008. The scientific life: A moral history of a late modern vocation. Chicago University Press.\r\nPorter, T. M. 1995. Trust in Numbers: The Pursuit of Objectivity in Science and Public Life.\r\nPoovey, M. 1997. A History of the Modern Fact: Problems of Knowledge in the Sciences of Wealth and Society. Chicago.\r\nMcCloskey, D. N. 1992. If you’re so smart: The narrative of economic expertise. University of Chicago Press.\r\nMcCloskey, D. N. 1992. The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives.\r\nAsparouhov, T., Muthén, B. & Morin, A. J. S. 2015. Bayesian structural equation modeling with cross-loadings and residual covariances: Comments on Stromeyer et al. Journal of Management, 41, 1561-1577.\r\n\r\n\r\n",
    "preview": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/distill-preview.png",
    "last_modified": "2021-05-26T22:32:56-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/",
    "title": "Fitting a Simple Linear Regression in Bayesian Context",
    "description": "How to fit a linear regression using Bayesian Methods  \nConsider a Bayesian model fit as a remedial measures for influential case",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nFit a model\r\nModel fit diagnostics\r\nMessage take-away\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\nMCMC in JAGS\r\nDescribe the model.\r\nExplore the MCMC object\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nExplore the MCMC object\r\n\r\nUsing fitted regression model for prediction\r\n\r\nCompare the fit between FA and BA\r\nFurther reading\r\n\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nI use the data in Kruschke (2015) which has\r\n- male\r\n- height\r\n- weight\r\nWe will use height to predict weight of a person\r\n\r\nDT::datatable(dta, \r\n          rownames = FALSE,\r\n          filter = list(position = \"top\"))\r\n\r\n{\"x\":{\"filter\":\"top\",\"filterHTML\":\"<tr>\\n  <td data-type=\\\"integer\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"54.6\\\" data-max=\\\"76\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"89\\\" data-max=\\\"356.8\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[0,0,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1],[64,62.3,67.9,64.2,64.8,57.5,65.6,70.2,63.9,71.1,66.5,68.1,62.9,75.1,64.6,69.2,68.1,72.6,63.2,64.1,64.1,71.5,76,69.7,73.3,61.7,66.4,65.7,68.3,66.9,62.4,64.5,60.6,70.8,61,66,59.6,70.1,66.6,59.8,68.5,61.4,64.7,67.4,68.3,67.3,62.5,72.4,64.4,70.6,66.3,65.9,61.1,67.8,64.4,71.2,64.5,65.4,67,70.9,62.4,70.4,64.6,69.6,65.9,70.6,65.4,73.9,70.1,64.8,59.7,69.2,61.4,70.4,71.1,60.8,68.3,67.4,63.5,66.8,73.4,64.8,68.9,66.5,64.7,66.2,69.1,66,60.9,72.4,67.5,71.6,70.6,64.7,72.7,64.7,67.9,65.9,54.6,64.1,68.3,72.9,64,62.1,67.3,60.4,63.4,63.3,61.4,67.7,65.1,67.5,67,65.6,70.9,61.5,59.4,59.7,69,71.7,65.7,67.2,65.9,64.9,73.6,70.5,63.6,64.7,69.4,69.2,64.5,70.9,63.6,73.7,67.6,65.9,68.1,64.1,61.6,71.2,66.7,71.1,68.6,62.8,67.6,63.1,65.2,67.4,64.3,59.9,61.9,63.4,59.8,67.7,64.5,67.4,66.1,75.1,67.9,65.7,64.9,64.8,55.3,63,64.1,64.3,63.8,65.1,71.8,66.7,64.5,60.7,59.6,68.4,63.2,66,62.5,67.9,70.2,70.3,67.1,66.2,62.1,72.4,65.2,72.1,65.6,67.2,66.2,66,63.6,65,57.5,63.4,67.2,62.8,65.3,66.7,70.3,68.6,66.4,62.9,57.6,65.3,70.9,68,65.4,67.4,62.8,73.3,64.6,70,68.1,69.4,71.9,67.8,63.5,69.8,66.1,62,71.5,71.9,72.6,64.3,71.7,70.1,64.3,63.9,74.6,65.5,63.1,66.7,65.2,62,71,69.4,69.8,66.6,71,74.7,63.6,69.8,58.9,67.9,65.2,70.6,59.8,71.2,71.5,63.7,65.1,64.8,66.8,64.5,68.6,65.4,66.5,68.8,70.4,57,60.6,62.8,73.3,73.2,61.9,66.4,69.9,60.8,65.9,61.8,66.5,64,68.2,71.8,70.4,63,60.1,66.3,67.1,61.5,66.4,63.8,69.9,66.1,66.2,66.5,70.2,64,69,65.7,69.5,64.1,61.2,62.6,67.9,68.5,69.2,65.9,68.3,70.2],[136.4,215.1,173.6,117.3,123.3,96.5,178.3,191.1,158,193.9,127.1,147.9,119,204.4,143.4,124.4,140.9,164.7,139.8,110.2,134.1,193.6,180,155,188.2,187.4,139.2,147.9,178.6,111.1,119.2,184.4,100.1,207.3,159.8,120.7,102.8,195.7,130.1,156.5,113.7,119.1,142.8,179.9,166.3,135.4,118.9,173.9,117.8,192.6,122,129.5,116.9,177.1,160.1,199.5,111,177.4,187.9,177.1,185.3,223.5,128.4,184.4,122.1,216.8,173.8,197.8,181.1,136.6,105,150.1,125,172.2,143.9,132,110.7,155.9,174.9,176.6,167.1,133.5,211.4,150.6,144.7,120.7,222.5,168.4,134.3,182.8,187.2,193.4,195.2,131.1,204.5,108.6,128.1,152.7,120.3,183.5,210.6,163.1,143.9,114.4,170.5,93.2,147.8,161.2,114.6,158,144.5,184,225,116.9,183.2,131.5,217.1,123.6,145.9,170.6,133.5,165.9,134.1,111.1,201.1,156.3,161.5,145,159.8,149,222,149.7,130.6,242.5,150,191.3,164.1,135.6,139.4,114.8,191.6,194.7,170,146.9,186.1,125.8,96.2,156.8,117.6,149.6,125.4,140.7,150.2,156.5,137.6,140.3,174.7,186.1,191.3,135.2,130.5,137.1,166.5,280.5,126,128.3,166.5,124,120.8,193.8,157,190.8,110.9,185.9,163.2,167.9,119.8,122,178.8,181.1,168.8,120.9,96.5,210.4,139.3,187,146.5,141.8,162,173.8,160.7,125.3,125.9,193.7,234.9,156.9,221.2,241.6,170.4,152.6,172.8,176.6,123.5,202.1,182.2,190.9,146.6,158.5,140.5,168,182,275.2,164.1,153.5,159.5,141.7,194.1,149.6,157.5,149.9,210.8,154.5,206.5,123.4,166,160.8,167.6,141.9,186.2,155.7,148.7,132.6,356.8,164.9,187.2,169.6,178.8,202.2,158.9,186.1,169.7,147.6,110.6,146.8,146.2,164.8,115.7,191.5,198.6,143.7,126.7,123.1,231.5,134.7,159.4,141.5,144.5,150.5,173.7,215.5,146.4,133.6,240.2,216.6,89,213.6,211.3,140.4,135.7,151.7,198.3,158.7,218,172.7,161.9,130.3,123,157,131.2,108.7,202.7,131.9,164.5,179.2,154.4,120.1,189.7,160.2,145.7,186.2,144.8,147.4,120.8,134.9,164.8,205.9,172.5,130.8,146.5,173.8]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>male<\\/th>\\n      <th>height<\\/th>\\n      <th>weight<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[0,1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\nFit a model\r\n\r\nfit <- lm(dta$weight ~ dta$height)\r\nsummary(fit)\r\n\r\nCall:\r\nlm(formula = dta$weight ~ dta$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-63.95 -21.17  -5.26  16.24 201.94 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -104.7832    31.5056  -3.326 0.000992 ***\r\ndta$height     3.9822     0.4737   8.406 1.77e-15 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.59 on 298 degrees of freedom\r\nMultiple R-squared:  0.1917,    Adjusted R-squared:  0.189 \r\nF-statistic: 70.66 on 1 and 298 DF,  p-value: 1.769e-15\r\n\r\nThen, we can see the fitted regression line overlay with data\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Scatter Plot between Height and Weight by Gender\",\r\n     pch = as.numeric(dta$male), col = as.factor(dta$male))\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\n\r\nModel fit diagnostics\r\nThere are four assumptions associated with a linear regression model:\r\nLinearity: The relationship between X and the mean of Y is linear.\r\nHomoscedasticity: The variance of residual is the same for any value of X.\r\nIndependence: Observations are independent of each other.\r\nNormality: For any fixed value of X, Y is normally distributed.\r\nI check the model fit by plotting:\r\nplot of residuals against predictor variable (how to check residuals have non-linear patterns or not)\r\nnormal probability plot (how to check residuals are normally distributed: residuals follow a straight line well or do they deviate severely; it’s good if residuals are lined well on the straight dashed line.)\r\nplot of square root of standardized residual absolute value (how to check the assumption of equal variance (homoscedasticity): if residuals are spread equally along the ranges of predictors; it’s good if we see a horizontal line with equally (randomly) spread points.)\r\nAnd to look at the outlier and leverage via\r\nplot standardized residuals vs leverage\r\nResiduals are the difference between the observed score and the predicted score.\r\n\\[ e_i = Y^{obs}_i - \\hat{Y}_i\\]\r\nResiduals come in three varieties:\r\nRaw Residuals: The difference between the raw observed score and the predicted score.\r\nStandardized Residuals: These are the raw residuals divided by the standard error of estimate.\r\nStudentized Residuals: These are raw residuals divided by the standard error of the residual with that case deleted. These are sometimes called studentized deleted residuals or studentized jackknifed residuals.\r\n\r\npar(mfrow=c(2,2))\r\nplot(fit)\r\n\r\n\r\nLastly, we look at the influential points\r\n\r\nn <- dim(dta)[1]\r\ncooksD <- cooks.distance(fit)\r\n#identify influential points\r\n(influential_obs <- as.numeric(names(cooksD)[(cooksD > (4/n))]))\r\n [1]   2 117 134 140 163 164 169 172 212 233 260 263\r\n#plot cooksD\r\nplot(cooksD, pch=\"*\", cex=2, main=\"Influential Obs by Cooks distance\")  \r\nabline(h = 4/n, col=\"red\")  # add cutoff line\r\ntext(x=1:length(cooksD), y=cooksD, labels=ifelse((cooksD>(4/n)),names(cooksD),\"\"), col=\"red\", pos = 4)\r\n\r\n\r\nTill now, we can say that the linear regression assumption is violated in this case, e.g. error is not following the normal distribution. Therefore, how about we delete the influential points and re-fit the model:\r\n\r\ndta.outliers_removed <- dta[-influential_obs, ]\r\n\r\nfit.outliers_removed <- lm(dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\nsummary(fit.outliers_removed)\r\n\r\nCall:\r\nlm(formula = dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-55.16 -18.87  -3.18  16.55  83.27 \r\n\r\nCoefficients:\r\n                             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)                 -155.9116    26.8300  -5.811 1.65e-08 ***\r\ndta.outliers_removed$height    4.7112     0.4032  11.685  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.36 on 286 degrees of freedom\r\nMultiple R-squared:  0.3231,    Adjusted R-squared:  0.3208 \r\nF-statistic: 136.5 on 1 and 286 DF,  p-value: < 2.2e-16\r\n\r\nSomehow, we saw the model assumption is satisfied when the influential cases removed.\r\n\r\npar(mfrow=c(1,2))\r\nplot(fit.outliers_removed, which=c(1,2))\r\n\r\n\r\nThe regression line changes when we remove the influential observations. Dangerous!!!\r\n\r\npar(mfrow=c(1,2))\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\nplot(dta.outliers_removed$height, dta.outliers_removed$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Outliers removed\")\r\nabline(fit.outliers_removed, col = \"orange\", lwd = 3)\r\n\r\n\r\nBut, the action of deleting of influential cases is often not the solution due to produce the bias estimates.\r\nThrough this case, we have reviewed:\r\nInfluential points = Outliers & Leverage\r\nA point that makes a lot of difference in a regression case, is called ‘an influential point.’ Usually influential points have two characteristics:\r\nThey are outliers, i.e. graphically they are far from the pattern described by the other points, that means that the relationship between x and y is different for that point than for the other points.\r\nThey are in a position of high leverage, meaning that the value of the variable x is far from the mean. Observations with very low or very high values of x are in positions of high leverage.\r\nRemind knowledge\r\nDiscrepancy: Difference between the predicted and observed value\r\n- Measured by Studentized Residuals\r\nLeverage: high leverage if it has “extreme” predictor x values\r\n- Measured by Hat Value\r\nInfluence: Assesses how much regression equation would change if an observation/potential outlier was dropped from the analysis\r\n- Measured by Cook’s Distance, Difference in Fit (DFFITS), or Difference in coefficients (DFBETAS)\r\nMessage take-away\r\nRemoving influential cases is not the optimal solution.\r\nIn the real-life analysis, we can use other types of regression:\r\nRidge regression\r\nRobust regression\r\nIRLS Robust regression\r\nLowess method\r\nRegression trees\r\nand etc…\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon \\]\r\nWith Bayesian approach distribution of \\(\\epsilon\\) does not have to be Gaussian (normal), we are going to use robust assumption.\r\nParameterization of the model for MCMC (adapted from Kruschke (2015))Bayes theorem for this model:\r\n\\[\r\np(\\beta_0, \\beta_1, \\sigma, \\gamma \\mid D) = \\frac{p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma)}{\\int \\int \\int \\int p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma) \\ d\\beta_0 \\ d\\beta_1 \\ d\\sigma \\ d\\gamma}\r\n\\]\r\nCreate the data list.\r\n\r\ny <- dta$weight\r\nx <- dta$height\r\ndataList <- list(x = x, y = y)\r\n\r\nMCMC in JAGS\r\nDescribe the model.\r\nBased on the Normal distribution (demonstrating purpose, not run)\r\n\r\nmodstring_norm = \"\r\n# Specify the Normal model for none-standardized data:\r\nmodel {\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ dnorm(mu[i], prec)\r\n        mu[i] = b[1] + b[2]*log_income[i] \r\n    }\r\n    \r\n    for (i in 1:2) {\r\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\r\n    }\r\n    \r\n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\r\n    sig2 = 1.0 / prec\r\n    sig = sqrt(sig2)\r\n} \"\r\n\r\nBased on the Student-t distribution, robust assumption\r\n\r\n modelString = \"\r\n# Standardize the data:\r\ndata {\r\n    Ntotal <- length(y)\r\n    xm <- mean(x)\r\n    ym <- mean(y)\r\n    xsd <- sd(x)\r\n    ysd <- sd(y)\r\n    for ( i in 1:length(y) ) {\r\n      zx[i] <- (x[i] - xm) / xsd\r\n      zy[i] <- (y[i] - ym) / ysd\r\n    }\r\n}\r\n# Specify the model for standardized data:\r\nmodel {\r\n    for ( i in 1:Ntotal ) {\r\n      zy[i] ~ dt( zbeta0 + zbeta1 * zx[i] , 1/zsigma^2 , nu )\r\n    }\r\n    # Priors vague on standardized scale:\r\n    zbeta0 ~ dnorm(0, 1/(10)^2 )  \r\n    zbeta1 ~ dnorm(0, 1/(10)^2 )\r\n    zsigma ~ dunif(1.0E-3, 1.0E+3 )\r\n    nu ~ dexp(1/30.0)\r\n    # Transform to original scale:\r\n    beta1 <- zbeta1 * ysd / xsd  \r\n    beta0 <- zbeta0 * ysd  + ym - zbeta1 * xm * ysd / xsd \r\n    sigma <- zsigma * ysd\r\n}\r\n\"\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"TEMPmodel.txt\")\r\n\r\nIn the tutorial, we just want to execute the model specified by t-student distribution.\r\nEvery arrow has a corresponding line in the descriptive diagram.\r\nVariable names starting with “z” mean that these variables are standardized (z-scores).\r\nThe intention of using z-scores in JAGS is to overcome a problem of correlation of the parameters (as the simulation the correlation between \\(\\beta_0\\) and \\(\\beta_1\\)).\r\nStrong correlation creates thin and long shape on scatter-plot of the variables which makes Gibbs sampling very slow and inefficient.\r\nBut remember to scale back to the original measures.\r\nHMC implemented in Stan does not have this problem. This can be applied to STAN in all situation !!!\r\n\r\nparameters = c(\"beta0\" ,  \"beta1\" ,  \"sigma\", \r\n              \"zbeta0\" , \"zbeta1\" , \"zsigma\", \"nu\")\r\nadaptSteps = 500  # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps=20000\r\nnIter = ceiling((numSavedSteps*thinSteps ) / nChains )\r\njagsModel = jags.model(\"TEMPmodel.txt\", data=dataList ,\r\n                      n.chains=nChains, n.adapt=adaptSteps)\r\nupdate(jagsModel, n.iter=burnInSteps)\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters, \r\n                          n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the MCMC object\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:6500\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 5000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n             Mean       SD  Naive SE Time-series SE\r\nbeta0  -139.96225 27.61760 0.1952859      0.2180717\r\nbeta1     4.46120  0.41330 0.0029225      0.0032290\r\nnu        5.37901  1.62796 0.0115114      0.0236101\r\nsigma    23.97943  1.65132 0.0116766      0.0204363\r\nzbeta0   -0.09632  0.04806 0.0003398      0.0004283\r\nzbeta1    0.49046  0.04544 0.0003213      0.0003550\r\nzsigma    0.68372  0.04708 0.0003329      0.0005827\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%       25%        50%       75%      97.5%\r\nbeta0  -193.3580 -158.7464 -140.16434 -121.3445 -8.624e+01\r\nbeta1     3.6527    4.1830    4.46491    4.7407  5.257e+00\r\nnu        3.1509    4.2578    5.05987    6.1485  9.410e+00\r\nsigma    20.8323   22.8476   23.94254   25.0519  2.728e+01\r\nzbeta0   -0.1888   -0.1282   -0.09682   -0.0644 -4.115e-04\r\nzbeta1    0.4016    0.4599    0.49087    0.5212  5.779e-01\r\nzsigma    0.5940    0.6514    0.68267    0.7143  7.779e-01\r\nplot(codaSamples, trace=TRUE, density=FALSE) # note: many graphs\r\n\r\nautocorr.plot(codaSamples, ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n    beta0     beta1        nu     sigma    zbeta0    zbeta1    zsigma \r\n16054.484 16401.066  4768.900  6535.965 12892.980 16401.066  6535.965 \r\n#gelman.diag(codaSamples)\r\ngelman.plot(codaSamples)   # lines may return error ==> Most likely reason: collinearity of parameters \r\n\r\n(HDIofChains <- lapply(codaSamples, function(z) cbind(Mu = hdi(codaSamples[[1]][,1]), Sd = hdi(codaSamples[[1]][,2]))))\r\n[[1]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[2]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[3]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[4]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\nLook at strong correlation between beta0 and beta1 which slows Gibbs sampling down.\r\n\r\nhead(as.matrix(codaSamples[[1]]))\r\n         beta0    beta1       nu    sigma      zbeta0    zbeta1\r\n[1,] -168.7337 4.890447 5.623225 23.90661 -0.10407253 0.5376553\r\n[2,] -158.6082 4.744414 6.371647 24.56065 -0.09181883 0.5216004\r\n[3,] -166.9662 4.873701 6.346842 22.92533 -0.08537616 0.5358143\r\n[4,] -161.6680 4.795882 3.209357 23.00506 -0.08162923 0.5272588\r\n[5,] -160.5706 4.801212 3.153090 22.15810 -0.04024801 0.5278448\r\n[6,] -155.9331 4.684888 4.394775 21.44653 -0.12823063 0.5150561\r\n        zsigma\r\n[1,] 0.6816415\r\n[2,] 0.7002899\r\n[3,] 0.6536626\r\n[4,] 0.6559359\r\n[5,] 0.6317868\r\n[6,] 0.6114980\r\npairs(as.matrix(codaSamples[[1]])[,1:4])\r\n\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nIn order to give a vague priors to slope and intercept consider the following arguments:\r\nThe largest possible value of slope is\r\n\\[ \\frac{\\sigma_y}{\\sigma_x} \\]\r\nwhen variables \\(x\\) and \\(y\\) are perfectly correlated.\r\nThen standard deviation of the slope parameter \\(\\beta_1\\) should be large enough to make the maximum value easily achievable.\r\nSize of intercept is defined by value of\r\n\\[ E[X] \\frac{\\sigma_y}{\\sigma_x} \\]\r\nSo, the prior should have enough width to include this value.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n    real meanX;\r\n    real sdX;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real expLambda;\r\n    real beta0sigma;\r\n    real beta1sigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    expLambda = 1/30.0;\r\n    beta1sigma = 10*fabs(sdY/sdX);\r\n    beta0sigma = 10*(sdY^2+sdX^2)    / 10*fabs(meanX*sdY/sdX);\r\n}\r\nparameters {\r\n    real beta0;\r\n    real beta1;\r\n    real<lower=0> nu; \r\n    real<lower=0> sigma; \r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi); \r\n    nu ~ exponential(expLambda);\r\n    beta0 ~ normal(0, beta0sigma);\r\n    beta1 ~ normal(0, beta1sigma);\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ student_t(nu, beta0 + beta1 * x[i], sigma);\r\n    }\r\n}\r\n\"\r\n\r\n\r\nstanDsoRobustReg <- stan_model(model_code=modelString) \r\n\r\n\r\ndat<-list(Ntotal=length(dta$weight), \r\n          y=dta$weight, \r\n          meanY=mean(dta$weight),\r\n          sdY=sd(dta$weight),\r\n          x=dta$height,\r\n          meanX=mean(dta$height),\r\n          sdX=sd(dta$height))\r\n\r\n\r\nfitSimRegStan <- sampling(stanDsoRobustReg, \r\n             data=dat, \r\n             pars=c('beta0', 'beta1', 'nu', 'sigma'),\r\n             iter=5000, chains = 4, cores = 4)\r\n\r\nSave the fitted object.\r\nExplore the MCMC object\r\n\r\nprint(fitSimRegStan)\r\nInference for Stan model: 99bd7b261ff9240bf0c6d7b1b21831d6.\r\n4 chains, each with iter=5000; warmup=2500; thin=1; \r\npost-warmup draws per chain=2500, total post-warmup draws=10000.\r\n\r\n          mean se_mean    sd     2.5%      25%      50%      75%\r\nbeta0  -139.35    0.50 27.89  -194.81  -157.61  -139.51  -120.74\r\nbeta1     4.45    0.01  0.42     3.64     4.17     4.45     4.72\r\nnu        5.37    0.03  1.61     3.15     4.25     5.07     6.16\r\nsigma    23.98    0.03  1.66    20.81    22.84    23.96    25.07\r\nlp__  -1265.00    0.02  1.43 -1268.55 -1265.74 -1264.68 -1263.94\r\n         97.5% n_eff Rhat\r\nbeta0   -85.12  3072    1\r\nbeta1     5.28  3117    1\r\nnu        9.39  3420    1\r\nsigma    27.31  3579    1\r\nlp__  -1263.20  3332    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Apr 22 15:10:38 2021.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(fitSimRegStan)\r\n\r\nrstan::traceplot(fitSimRegStan, ncol=1, inc_warmup=F)\r\n\r\npairs(fitSimRegStan, pars=c('nu','beta0','beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','beta1'))\r\n\r\nstan_scat(fitSimRegStan, c('beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('nu','sigma'))\r\n\r\nstan_dens(fitSimRegStan)\r\n\r\nstan_ac(fitSimRegStan, separate_chains = T)\r\n\r\nstan_diag(fitSimRegStan,information = \"sample\",chain=0)\r\n\r\nstan_diag(fitSimRegStan,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"divergence\",chain = 0)\r\n\r\n\r\nWork with shinystan object.\r\n\r\nlaunch_shinystan(fitSimRegStan)\r\n\r\nUsing fitted regression model for prediction\r\nRecall that the data in this example contains predictor height and output weight for a group of people from Ht-Wt.csv (data above).\r\nPlot all heights observed in the sample and check the summary of the variable.\r\n\r\nplot(1:length(dat$x),dat$x)\r\n\r\nsummary(dat$x)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  54.60   64.00   66.20   66.39   69.20   76.00 \r\n\r\nCan we predict weight of a person who is 50 or 80 inches tall?\r\nTo do this we can go through all pairs of simulated parameters (\\(\\beta_0\\), \\(\\beta_1\\)) and use them to simulate \\(y(50)\\) and \\(y(80)\\).\r\nThis gives distribution of predicted values.\r\n\r\nsummary(fitSimRegStan)\r\n$summary\r\n              mean     se_mean         sd         2.5%          25%\r\nbeta0  -139.354196 0.503202352 27.8915373  -194.808900  -157.605829\r\nbeta1     4.451593 0.007474065  0.4173043     3.636611     4.172879\r\nnu        5.366643 0.027556563  1.6115181     3.151177     4.246276\r\nsigma    23.982791 0.027670259  1.6554405    20.814300    22.838675\r\nlp__  -1265.002519 0.024734258  1.4276951 -1268.552672 -1265.738311\r\n               50%          75%        97.5%    n_eff     Rhat\r\nbeta0  -139.512830  -120.737878   -85.121838 3072.271 1.000770\r\nbeta1     4.454489     4.723609     5.282330 3117.396 1.000699\r\nnu        5.069790     6.157253     9.391004 3419.954 1.000994\r\nsigma    23.957637    25.072876    27.313013 3579.322 1.000550\r\nlp__  -1264.682923 -1263.938424 -1263.195323 3331.756 1.000423\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.496987 27.9128445  -195.452362  -159.028578\r\n    beta1     4.468322  0.4176474     3.643375     4.180768\r\n    nu        5.333266  1.6129391     3.146164     4.239934\r\n    sigma    23.996371  1.6756857    20.745408    22.833622\r\n    lp__  -1264.973734  1.3931068 -1268.262683 -1265.705478\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.690732  -121.306242   -85.591653\r\n    beta1     4.473340     4.745248     5.301112\r\n    nu        5.036853     6.074138     9.402810\r\n    sigma    23.966000    25.101448    27.242245\r\n    lp__  -1264.658401 -1263.936454 -1263.201543\r\n\r\n, , chains = chain:2\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.119919 27.7217798  -195.188356  -158.408294\r\n    beta1     4.462118  0.4146886     3.668582     4.177839\r\n    nu        5.396927  1.5988486     3.155521     4.277927\r\n    sigma    23.990640  1.6168350    21.056831    22.846187\r\n    lp__  -1264.946317  1.4072435 -1268.606024 -1265.657309\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.000666  -121.163812   -86.601618\r\n    beta1     4.462406     4.731267     5.288368\r\n    nu        5.129432     6.181671     9.348566\r\n    sigma    23.918413    25.056950    27.345459\r\n    lp__  -1264.644540 -1263.891813 -1263.183147\r\n\r\n, , chains = chain:3\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -137.758043 28.0473599  -193.976570  -156.709899\r\n    beta1     4.428479  0.4196264     3.620966     4.142890\r\n    nu        5.273822  1.5474425     3.130205     4.160287\r\n    sigma    23.905531  1.6471567    20.672028    22.808588\r\n    lp__  -1265.023664  1.3965268 -1268.555138 -1265.759590\r\n         stats\r\nparameter          50%         75%        97.5%\r\n    beta0  -137.963462  -118.57774   -83.786093\r\n    beta1     4.430414     4.70807     5.278970\r\n    nu        4.999793     6.07770     9.087928\r\n    sigma    23.906472    24.98220    27.219559\r\n    lp__  -1264.712841 -1263.99760 -1263.196266\r\n\r\n, , chains = chain:4\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -139.041834 27.8184750  -193.491107  -156.858089\r\n    beta1     4.447452  0.4163603     3.625069     4.186441\r\n    nu        5.462557  1.6789159     3.209953     4.299050\r\n    sigma    24.038623  1.6794892    20.805817    22.868034\r\n    lp__  -1265.066361  1.5085815 -1268.785015 -1265.858500\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -139.288564  -121.639438   -83.861266\r\n    beta1     4.449144     4.707789     5.268918\r\n    nu        5.106526     6.256394     9.576797\r\n    sigma    24.014382    25.132032    27.449061\r\n    lp__  -1264.738186 -1263.933356 -1263.202000\r\nregParam<-cbind(Beta0=rstan::extract(fitSimRegStan,pars=\"beta0\")$'beta0',\r\n                Beta1=rstan::extract(fitSimRegStan,pars=\"beta1\")$'beta1')\r\nhead(regParam)\r\n         Beta0    Beta1\r\n[1,] -174.7224 4.962250\r\n[2,] -118.6827 4.113129\r\n[3,] -152.0820 4.701561\r\n[4,] -147.0165 4.564773\r\n[5,] -159.9549 4.751269\r\n[6,] -126.6841 4.251516\r\npredX50<-apply(regParam,1,function(z) z%*%c(1,50))\r\npredX80<-apply(regParam,1,function(z) z%*%c(1,80))\r\n\r\nPlot both distributions, look at their summaries and HDIs.\r\n\r\nsuppressWarnings(library(HDInterval))\r\nden<-density(predX50)\r\nplot(density(predX80),xlim=c(60,240))\r\nlines(den$x,den$y)\r\n\r\nsummary(cbind(predX50,predX80))\r\n    predX50          predX80     \r\n Min.   : 55.42   Min.   :195.2  \r\n 1st Qu.: 78.48   1st Qu.:212.9  \r\n Median : 83.19   Median :216.8  \r\n Mean   : 83.23   Mean   :216.8  \r\n 3rd Qu.: 87.97   3rd Qu.:220.6  \r\n Max.   :111.77   Max.   :240.3  \r\nrbind(predX50=hdi(predX50),predX80=hdi(predX80))\r\n            lower     upper\r\npredX50  69.09173  97.20656\r\npredX80 205.32725 228.15029\r\n\r\nBoth JAGS and Stan produced the identical results.\r\nCompare the fit between FA and BA\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 2)\r\nabline(a=-139.96225, b= 4.46120, col = \"blue\", lwd = 1)\r\nabline(fit.outliers_removed, col = \"red\", lty=\"dashed\", lwd =1)\r\n\r\n\r\n\r\nWith influential\r\nWithout influential\r\nBayesian approach w/ robust assumption\r\n\r\norange, solid\r\nred, dashed\r\nblue, solid\r\nIntercept\r\n-104.78\r\n-155.91\r\n-139.96\r\nSlope\r\n3.98\r\n4.71\r\n4.46\r\n\r\nGreat! From the comparison, we can see that using Bayesian Methods to fit simple linear regression can be robust when the traditional regression has the influential points.\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., & Rubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition (3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th ed. ed.). Boston: McGraw-Hill Irwin.\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial with r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic Press is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/distill-preview.png",
    "last_modified": "2021-04-23T11:48:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-17-machine learning-discriminant-analysis/",
    "title": "Discriminant Analysis -- A Classification by Maximizing Class Separation",
    "description": "An Gentle Introduction of Discriminant Analysis & Its Applicant",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nLinear Discriminant Analysis\r\nWhy Discriminant Analysis\r\nQuadratic Discriminant Analysis\r\nLDA vs QDA\r\nBuilding our first linear and quadratic discriminant models\r\nCreating the task and learner, and training the LDA model\r\nCreating the task and learner, and training the QDA model\r\n\r\nReferences\r\n\r\nLinear Discriminant Analysis\r\nApproach for multiclass classification.\r\nA discriminant is a function that takes an input vector x and assigns to one of the multiple classes.\r\nModel the distribution of the predictors \\(X\\) in each response class\r\nUse Bayes theorem to flip these around into estimates for\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nWhen the distributions are assumed to be normal, the LDA model is very similar in form to logistic regression.\r\nWhy Discriminant Analysis\r\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\r\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\r\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\r\nQuadratic Discriminant Analysis\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nLDA = \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\sum\\) in each class.\r\nQDA = With Gaussians but different \\(\\sum_k\\) in each class, we get quadratic discriminant analysis.\r\nNOTE = By proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\r\nLDA vs QDA\r\nThe strengths of the LDA and QDA algorithms are:\r\nThey can reduce a high-dimensional feature space into a much more manageable number\r\nCan be used for classification or as a preprocessing (dimension reduction) technique to other classification algorithms that may perform better on the dataset\r\nQDA can learn curved decision boundaries between classes (this isn’t the case for LDA)\r\nThe weaknesses of the LDA and QDA algorithms are:\r\nThey can only handle continuous predictors (although recoding a categorical variable as numeric may help in some cases)\r\nThey assume the data are normally distributed across the predictors. If the data are not, performance will suffer\r\nLDA can only learn linear decision boundaries between classes (this isn’t the case for QDA)\r\nLDA assumes equal covariances of the classes and performance will suffer if this isn’t the case (this isn’t the case for QDA)\r\nQDA is more flexbile than LDA, and so can be more prone to overfitting\r\nWhen we should apply LDA vs QDA\r\nLDA needs lot less parameters than QDA.\r\nLDA is a much less flexible classifier than QDA \\(\\Rightarrow\\) substantially low variance.\r\nIf LDA’s assumption of common covariance matrix is poor, then LDA has high bias.\r\nLDA better bet if training set is small so reducing variance is important.\r\nQDA better bet if training set is large so variance of classifier not a major concern.\r\nBuilding our first linear and quadratic discriminant models\r\nWe have a tibble containing 178 cases and 14 variables of measurements made on various wine bottles: data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\r\nThe analysis determined the quantities of 13 constituents (Alcohol, Malic acid, Ash, Alcalinit of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline)found in each of the three types of wines.\r\n\r\n\r\n#install.packages(\"mlr\")\r\nlibrary(mlr)\r\nlibrary(tidyverse)\r\n#install.packages(\"HDclassif\")\r\ndata(wine, package = \"HDclassif\")\r\nwineTib <- as_tibble(wine)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   class    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10\r\n   <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1     1  14.2  1.71  2.43  15.6   127  2.8   3.06 0.28   2.29  5.64\r\n 2     1  13.2  1.78  2.14  11.2   100  2.65  2.76 0.26   1.28  4.38\r\n 3     1  13.2  2.36  2.67  18.6   101  2.8   3.24 0.3    2.81  5.68\r\n 4     1  14.4  1.95  2.5   16.8   113  3.85  3.49 0.24   2.18  7.8 \r\n 5     1  13.2  2.59  2.87  21     118  2.8   2.69 0.39   1.82  4.32\r\n 6     1  14.2  1.76  2.45  15.2   112  3.27  3.39 0.34   1.97  6.75\r\n 7     1  14.4  1.87  2.45  14.6    96  2.5   2.52 0.3    1.98  5.25\r\n 8     1  14.1  2.15  2.61  17.6   121  2.6   2.51 0.31   1.25  5.05\r\n 9     1  14.8  1.64  2.17  14      97  2.8   2.98 0.290  1.98  5.2 \r\n10     1  13.9  1.35  2.27  16      98  2.98  3.15 0.22   1.85  7.22\r\n# ... with 168 more rows, and 3 more variables: V11 <dbl>, V12 <dbl>,\r\n#   V13 <int>\r\n\r\n\r\n\r\nnames(wineTib) <- c(\"Class\", \"Alco\", \"Malic\", \"Ash\", \"Alk\", \"Mag\",\r\n                    \"Phe\", \"Flav\", \"Non_flav\", \"Proan\", \"Col\", \"Hue\",\r\n                    \"OD\", \"Prol\")\r\nwineTib$Class <- as.factor(wineTib$Class)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   Class  Alco Malic   Ash   Alk   Mag   Phe  Flav Non_flav Proan\r\n   <fct> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl>    <dbl> <dbl>\r\n 1 1      14.2  1.71  2.43  15.6   127  2.8   3.06    0.28   2.29\r\n 2 1      13.2  1.78  2.14  11.2   100  2.65  2.76    0.26   1.28\r\n 3 1      13.2  2.36  2.67  18.6   101  2.8   3.24    0.3    2.81\r\n 4 1      14.4  1.95  2.5   16.8   113  3.85  3.49    0.24   2.18\r\n 5 1      13.2  2.59  2.87  21     118  2.8   2.69    0.39   1.82\r\n 6 1      14.2  1.76  2.45  15.2   112  3.27  3.39    0.34   1.97\r\n 7 1      14.4  1.87  2.45  14.6    96  2.5   2.52    0.3    1.98\r\n 8 1      14.1  2.15  2.61  17.6   121  2.6   2.51    0.31   1.25\r\n 9 1      14.8  1.64  2.17  14      97  2.8   2.98    0.290  1.98\r\n10 1      13.9  1.35  2.27  16      98  2.98  3.15    0.22   1.85\r\n# ... with 168 more rows, and 4 more variables: Col <dbl>, Hue <dbl>,\r\n#   OD <dbl>, Prol <int>\r\n\r\nWe got:\r\n- 13 continuous measurements made on 178 bottles of wine, where each measurement is the amount of a different compound/element in the wine.\r\n- Class: vineyard the bottle comes from.\r\n\r\n\r\nwineUntidy <- gather(wineTib, \"Variable\", \"Value\", -Class)\r\nggplot(wineUntidy, aes(Class, Value)) +\r\n  facet_wrap(~ Variable, scales = \"free_y\") +\r\n  geom_boxplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nBox and whisker plots of each continuous variable in the data against vineyard number. For the box and whiskers: the thick horizontal line represents the median, the box represents the interquartile range (IQR), the whiskers represent the Tukey range (1.5 times the IQR above and below the quartiles), and the dots represent data outside of the Tukey range.   \r\nCreating the task and learner, and training the LDA model\r\n\r\n\r\nwineTask <- makeClassifTask(data = wineTib, target = \"Class\")\r\nlda <- makeLearner(\"classif.lda\")\r\nldaModel <- train(lda, wineTask)\r\n\r\n\r\n\r\nExtracting discriminant function values for each case\r\n\r\n\r\nldaModelData <- getLearnerModel(ldaModel)\r\nldaPreds <- predict(ldaModelData)$x\r\nhead(ldaPreds)\r\n\r\n\r\n        LD1       LD2\r\n1 -4.700244 1.9791383\r\n2 -4.301958 1.1704129\r\n3 -3.420720 1.4291014\r\n4 -4.205754 4.0028715\r\n5 -1.509982 0.4512239\r\n6 -4.518689 3.2131376\r\n\r\nPlotting the discriminant function values against each other\r\n\r\n\r\nwineTib %>%\r\n  mutate(LD1 = ldaPreds[, 1],\r\n         LD2 = ldaPreds[, 2]) %>%\r\n  ggplot(aes(LD1, LD2, col = Class)) + \r\n    geom_point() +\r\n    stat_ellipse() +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nCreating the task and learner, and training the QDA model\r\n\r\n\r\nqda <- makeLearner(\"classif.qda\")\r\nqdaModel <- train(qda, wineTask)\r\n\r\n\r\n\r\nCross-validating the LDA and QDA models\r\n\r\n\r\nkFold <- makeResampleDesc(method = \"RepCV\", folds = 10, reps = 50,\r\nstratify = TRUE)\r\n\r\nldaCV <- resample(learner = lda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nqdaCV <- resample(learner = qda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nldaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n    0.01133544     0.98866456 \r\n\r\nqdaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n   0.008314886    0.991685114 \r\n\r\nOur LDA model correctly classified 98.8% of wine bottles, on average! There isn’t much room for improvement here, but\r\nour QDA model managed to correctly classify 99.2% of cases!\r\nCalculating confusion matrices\r\n\r\n\r\ncalculateConfusionMatrix(ldaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.999/1e+00 0.001/9e-04 0.000/0e+00 0.001      \r\n  2      0.010/1e-02 0.977/1e+00 0.014/2e-02 0.023      \r\n  3      0.000/0e+00 0.007/5e-03 0.993/1e+00 0.007      \r\n  -err.-       0.011       0.005       0.020 0.01       \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2947    3    0      3\r\n  2        34 3468   48     82\r\n  3         0   16 2384     16\r\n  -err.-   34   19   48    101\r\n\r\ncalculateConfusionMatrix(qdaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.994/0.984 0.006/0.005 0.000/0.000 0.006      \r\n  2      0.014/0.016 0.986/0.993 0.000/0.000 0.014      \r\n  3      0.000/0.000 0.004/0.003 0.996/1.000 0.004      \r\n  -err.-       0.016       0.007       0.000 0.008      \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2933   17    0     17\r\n  2        48 3502    0     48\r\n  3         0    9 2391      9\r\n  -err.-   48   26    0     74\r\n\r\nPredicting which vineyard the poisoned wine came from\r\n\r\n\r\npoisoned <- tibble(Alco = 13, Malic = 2, Ash = 2.2, Alk = 19, Mag = 100,\r\n                   Phe = 2.3, Flav = 2.5, Non_flav = 0.35, Proan = 1.7,\r\n                   Col = 4, Hue = 1.1, OD = 3, Prol = 750)\r\npredict(qdaModel, newdata = poisoned)\r\n\r\n\r\nPrediction: 1 observations\r\npredict.type: response\r\nthreshold: \r\ntime: 0.00\r\n  response\r\n1        1\r\n\r\nThe model predicts that the poisoned bottle came from vineyard 1.\r\nHere’s we ends the analytic example.\r\nReferences\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-18T10:46:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-14-machine-learning-&-predictive-analytics/",
    "title": "Machine Learning & Predictive Analytics",
    "description": "An Overview of Machine Learning & Predictive Analytics",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [
      "Biostatistics",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Machine Learning\r\nLearning Types\r\nSupervised\r\nUnsupervised Learning\r\nSemi-supervised Learning\r\n\r\nActive Learning\r\nReinforcement Learning\r\nTransfer Learning\r\n\r\nUniversal Workflow of ML\r\nML Terminologies\r\nData Assumptions\r\nOverfitting and Underfitting\r\nParametric & Nonparametric Models\r\nRegression Analysis\r\n\r\nUse R or Python for machine learning?\r\nMachine Learning with mlr Package in R\r\n\r\nReferences\r\n\r\nWhat is Machine Learning\r\nMeaningful data transformations from input to output data.\r\nTransformations: represent or encode the data (RGB or HSV for color pixel).\r\nLearning is automatic search for better data representations.\r\nSearch through a predefined space of possibilities using guidance from feedback signal.\r\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E”\r\n\r\n-Tom Mitchell, Machine Learning, McGraw Hill, 1997\r\n\r\n\r\nExperience E, Task T, Performance P\r\n1. Chess: T: playing chess, P: % of games won, E: playing practice games against itself.\r\n2. Driving: T: driving a vehicle, P: avgdistance before error, E: sequence of images and steering commands recoded during manual driving.\r\n3. Handwriting Recognition: T: recognizing and classifying handwritten words in images, P: % of correctly classified words, E: DB of handwritten words with given classifications.\r\n\r\nLearning Types\r\nSupervised\r\nUnsupervised\r\nSemi-supervised\r\nReinforcement\r\nTransfer\r\nActive\r\nSupervised\r\nThe majority of practical machine learning uses supervised learning.\r\nSupervised learning is where you have input variables (\\(x\\)) and an output variable (\\(y\\)) and you use an algorithm to learn the mapping function from the input to the output.\\[ y = f(x) \\]\r\nThe goal is to approximate the mapping function so well that when you have new input data \\(x\\) that you can predict the output variables \\(y\\) for that data.\r\nIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\r\nWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\r\nSupervised Learning Examples:\r\nLinear Regression\r\nLogistic Regression\r\nK-NN (k-Nearest Neighbors)\r\nSupport Vector Machines (SVMs)\r\nDecision Tress and Random Forests\r\nNeural Networks\r\nUnsupervised Learning\r\nUnsupervised learning is where you only have input data \\(x\\) and no corresponding output variables.\r\nThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\r\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.\r\nUnsupervised Learning problems can be further grouped into Clustering and Association Problems.\r\nClustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\r\nAssociation: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy \\(A\\) also tend to buy \\(B\\).\r\nUnsupervised Learning Examples:\r\nClustering\r\nK-Means\r\nHierarchical Cluster Analysis (HCA)\r\nExpectation Maximization\r\n\r\nVisualization and Dimensionality Reduction\r\nPrincipal Component Analysis (PCA)\r\nKernel PCA\r\nt-distributed Stochastic Neighbor Embedding (t-SNE)\r\n\r\nAssociation Rule\r\nApriori\r\n\r\nNeural Networks\r\nAutoencoders\r\nBoltzmann machines\r\n\r\nSemi-supervised Learning\r\nSemi-supervised learning is halfway between supervised and unsupervised learning.\r\nTraditional classification methods use labeled data to build classifiers.\r\nThe labeled training sets used as input in Supervised learning is very certain and properly defined.\r\nHowever, they are limited, expensive and takes a lot of time to generate them.\r\nOn the other hand, unlabeled data is cheap and is readily available in large volumes.\r\nHence, semi-supervised learning is learning from a combination of both labeled and unlabeled data\r\nWhere we make use of a combination of small amount of labeled data and large amount of unlabeled data to increase the accuracy of our classifiers.\r\nActive Learning\r\nActive learning (sometimes called “query learning” or “optimal experimental design” in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence.\r\nThe key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns—to be “curious,” if you will—it will perform better with less training.\r\nActive learning is a special case of semi-supervised learning.\r\nReinforcement Learning\r\nReinforcement Learning is learning what to do and how to map situations to actions.\r\nThe end result is to maximize the numerical reward signal.\r\nThe learner is not told which action to take, but instead must discover which action will yield the maximum reward\r\nTransfer Learning\r\nA machine learning technique where a model trained on one task is re-purposed on a second related task.\r\nAn optimization that allows rapid progress or improved performance when modeling the second task.\r\nUniversal Workflow of ML\r\nDefine the problem\r\nAssemble dataset\r\nChoose a metric to quantify project outcome\r\nDecide on how to calculate the metric\r\nPrepare dataset\r\nDefine standard baseline\r\nDevelop model that beats baseline\r\nIdeal model is at the border of overfit and underfit–cross the border to know where it is so overfit model\r\nRegularize model and tune hyperparameters\r\nML Terminologies\r\nDataset\r\nTraining –Learn the parameters\r\nValidation –select hyperparameters\r\nTest –test the model aka generalization error\r\n\r\nBatch –set of examples used in one iteration of model training.\r\nMini-batch –A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference.\r\nEpoch –A full training pass over the entire data set such that each example has been seen once.\r\nIteration –A single update of a model’s weights during training.\r\nData Assumptions\r\nTraining and test data are from the same probability distribution.\r\nTraining and test data are iid.\r\nOverfitting and Underfitting\r\nOverfitting –model fits very well to the training data, aka detects patterns in the noise also\r\nDetect:\r\nLow training error, high generalization error.\r\n\r\nRemedies:\r\nReduce model capacity by removing features and/or parameters.\r\nGet more training data.\r\nImprove training data quality by reducing noise.\r\n\r\n\r\nUnderfitting–model too simple to detect patterns in the data\r\nDetect\r\nHigh training error.\r\n\r\nRemedies:\r\nIncrease model capacity by adding more parameters and/or features.\r\nReduce model constraints.\r\n\r\n\r\nParametric & Nonparametric Models\r\n\\[ 𝑦= 𝑓(𝑥) \\]\r\nEstimate the unknown function 𝑓 as \\(\\hat{f}\\)\r\nParametric Models:\r\nAssume the functional form or shape of 𝑓\r\nApply methodology to train model\r\nAdvantage –simple estimation\r\nDisadvantage – \\(\\hat{f}\\)may be far from true 𝑓\r\n\r\nNonparametric Models:\r\nNo assumption on the functional form or shape of 𝑓\r\nEstimate to fit as close as possible to the data\r\nAdvantage –can accurately fit a wide range of possible shapes of 𝑓\r\nDisadvantage –need large datasets (since there is no fixed # of params to estimate)\r\n\r\nRegression Analysis\r\nOLS\r\nMSE\r\nComputational Complexity of matrix inversion\r\nComplete training set\r\n\r\nBatch Gradient Descent\r\nCost function (MSE)\r\nLearning rate hyperparameter\r\nPartial derivative\r\nComplete training set\r\n\r\nStochastic Gradient Descent\r\nMini-batch Gradient Descent\r\nLinear Regression with OLS\r\n\\[ 𝑦= \\theta^T𝑋\\]\r\nThe cost function minimization is a closed-form solution called the Normal Equation: \\[ \\hat{\\theta} = (X^T . X)^{-1} X^T.y  \\]\r\nAdvantage –equation is linear with size of training set so it can handle large training sets efficiently.\r\nDisadvantage –\r\ncomputational complexity of inverting a matrix that increases with size of training set.\r\ndifficult to do online learning with new data arriving regularly (need to recalculate estimates), i.e. no iterative parameter updates.\r\n\r\nUse R or Python for machine learning?\r\nThere is something of a rivalry between the two most commonly used data science languages: R and Python. Of course, there are no machine learning tasks which are only possible to apply in one language or the other.\r\nR:\r\nR is geared specifically for mathematical and statistical applications, i.e. R can focus purely on data, but may feel restricted if they ever need to build applications based on their models.\r\nCurrently, there are modern tools in R designed specifically to make data science tasks simple and human-readable, such as those from the tidyverse.\r\nPreviously, ML algorithms in R were scattered across multiple packages, written by different authors. But R has now followed suit, with the caret and mlr packages (which stands for machine learning in R). While quite similar in purpose and functionality to caret, mlr package provides an interface for a large number of machine learning algorithms, and allows you to perform extremely complicated machine learning tasks with very little coding.\r\nPython:\r\nFirst of all, some of the more cutting-edge deep learning approaches are easier to apply in Python (they tend to be written in Python first and implemented in R later).\r\nPython, while very good for data science, is a more general purpose programming language.\r\nProponents of python could use this as en example of why it was better suited for machine learning, as it has the well known scikit-learn package which has a plethora of machine learning algorithms built into it.\r\nGoogle Trends demonstrates the search interest relative to the highest point on the chart for the given region and over the past 5 years.\r\n\r\n trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05z1_\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0212jm\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0jgqg\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/09gbxjr\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0j3djl7\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&q=%2Fm%2F05z1_,%2Fm%2F0212jm,%2Fm%2F0jgqg,%2Fm%2F09gbxjr,%2Fm%2F0j3djl7\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Machine Learning with mlr Package in R\r\nR users got mlr package similar to Scikit-Learn from Python. The package synthesizes all the ML functions from other packages in which we can perform most of ML tasks. mlr package has several algorithms in its bouquet. These algorithms have been categorized into regression, classification, clustering, survival, multiclassification and cost sensitive classification:\r\n\r\n\r\nlibrary(mlr)\r\nlistLearners(\"classif\")[c(\"class\",\"package\")]\r\n\r\n\r\n                            class                   package\r\n1                     classif.ada                 ada,rpart\r\n2              classif.adaboostm1                     RWeka\r\n3             classif.bartMachine               bartMachine\r\n4                classif.binomial                     stats\r\n5                classif.boosting              adabag,rpart\r\n6                     classif.bst                 bst,rpart\r\n7                     classif.C50                       C50\r\n8                 classif.cforest                     party\r\n9              classif.clusterSVM        SwarmSVM,LiblineaR\r\n10                  classif.ctree                     party\r\n11               classif.cvglmnet                    glmnet\r\n12                 classif.dbnDNN                   deepnet\r\n13                  classif.dcSVM            SwarmSVM,e1071\r\n14                  classif.earth               earth,stats\r\n15                 classif.evtree                    evtree\r\n16             classif.extraTrees                extraTrees\r\n17             classif.fdausc.glm                   fda.usc\r\n18          classif.fdausc.kernel                   fda.usc\r\n19             classif.fdausc.knn                   fda.usc\r\n20              classif.fdausc.np                   fda.usc\r\n21                classif.FDboost            FDboost,mboost\r\n22            classif.featureless                       mlr\r\n23                   classif.fgam                    refund\r\n24                    classif.fnn                       FNN\r\n25               classif.gamboost                    mboost\r\n26               classif.gaterSVM                  SwarmSVM\r\n27                classif.gausspr                   kernlab\r\n28                    classif.gbm                       gbm\r\n29                  classif.geoDA               DiscriMiner\r\n30               classif.glmboost                    mboost\r\n31                 classif.glmnet                    glmnet\r\n32       classif.h2o.deeplearning                       h2o\r\n33                classif.h2o.gbm                       h2o\r\n34                classif.h2o.glm                       h2o\r\n35       classif.h2o.randomForest                       h2o\r\n36                    classif.IBk                     RWeka\r\n37                    classif.J48                     RWeka\r\n38                   classif.JRip                     RWeka\r\n39                   classif.kknn                      kknn\r\n40                    classif.knn                     class\r\n41                   classif.ksvm                   kernlab\r\n42                    classif.lda                      MASS\r\n43       classif.LiblineaRL1L2SVC                 LiblineaR\r\n44      classif.LiblineaRL1LogReg                 LiblineaR\r\n45       classif.LiblineaRL2L1SVC                 LiblineaR\r\n46      classif.LiblineaRL2LogReg                 LiblineaR\r\n47         classif.LiblineaRL2SVC                 LiblineaR\r\n48 classif.LiblineaRMultiClassSVC                 LiblineaR\r\n49                  classif.linDA               DiscriMiner\r\n50                 classif.logreg                     stats\r\n51                  classif.lssvm                   kernlab\r\n52                   classif.lvq1                     class\r\n53                    classif.mda                       mda\r\n54                    classif.mlp                     RSNNS\r\n55               classif.multinom                      nnet\r\n56             classif.naiveBayes                     e1071\r\n57              classif.neuralnet                 neuralnet\r\n58                   classif.nnet                      nnet\r\n59                classif.nnTrain                   deepnet\r\n60            classif.nodeHarvest               nodeHarvest\r\n61                   classif.OneR                     RWeka\r\n62                   classif.pamr                      pamr\r\n63                   classif.PART                     RWeka\r\n64              classif.penalized                 penalized\r\n65                    classif.plr                   stepPlr\r\n66             classif.plsdaCaret                 caret,pls\r\n67                 classif.probit                     stats\r\n68                    classif.qda                      MASS\r\n69                  classif.quaDA               DiscriMiner\r\n70           classif.randomForest              randomForest\r\n71        classif.randomForestSRC           randomForestSRC\r\n72                 classif.ranger                    ranger\r\n73                    classif.rda                      klaR\r\n74                 classif.rFerns                    rFerns\r\n75                   classif.rknn                      rknn\r\n76         classif.rotationForest            rotationForest\r\n77                  classif.rpart                     rpart\r\n78                    classif.RRF                       RRF\r\n79                  classif.rrlda                     rrlda\r\n80                 classif.saeDNN                   deepnet\r\n81                    classif.sda                       sda\r\n82              classif.sparseLDA sparseLDA,MASS,elasticnet\r\n83                    classif.svm                     e1071\r\n84                classif.xgboost                   xgboost\r\n\r\n– ANALYTICS VIDHYA\r\nThe entire structure of this package relies on this premise:\r\n\\[\\text{Create a Task.   Make a Learner.   Train Them.}\\]\r\nCreating a task means loading data in the package (e.g., makeClassifTask).\r\nMaking a learner means choosing an algorithm (makeLearner) which learns from task (or data).\r\nFinally, train them (train).\r\nReferences\r\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R (1st ed. 2013. ed.). New York, NY: Springer New York.\r\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts: The MIT Press.\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nGéron, A. l. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems (First edition. ed.). Sebastopol, California: O’Reilly Media, Inc.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-17T14:33:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/",
    "title": "Beta Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Beta Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nDefinition\r\nIntuitive interpretation\r\nBeta function\r\nGamma function\r\nMain facts\r\nIn actions\r\nPlots in shiny\r\nFurther reading\r\n\r\nMotivation\r\n\r\nEven though I had learned the beta distribution from UIC’s Bayesian methods course and tutored it, such as setting up it as the prior distribution in conjugate distribution context. But it was easy to forget because of its dried content and too abstract. Here I try to combine the rigid theory (UC coursework’s content) and intuitive thought. By that way, I was able to ‘permenently stamp’ the concept to my brain.\r\n\r\nThe Beta distribution is a probability distribution on/of probabilities\r\nThe beta distribution describes a family of continuous probability distributions that are nonzero only on the interval (0 1).\r\nFor example, we can use it to model the probabilities: the Click-Through Rate of the advertisement, the batting averages, the 5-year survival chance for women with breast cancer, and so on.\r\nDefinition\r\nA continuous random variable \\(X_B \\sim Beta(\\alpha, \\beta)\\) has Beta distribution if its probability density function (PDF) is\r\n\\[ \r\nf_{X_B} (x; \\alpha, \\beta) = \\frac{1}{B(α,β)} x^{\\alpha − 1} (1−x)^{\\beta − 1}, \\ \\ \\text{for} \\ 0 < x < 1.\r\n\\]\r\nwhere \\(B(\\cdot)\\) is the Beta function and shape parameters \\(\\alpha, \\beta > 0\\).\r\nIntuitive interpretation\r\n\r\nPDF\r\nProbability as a …\r\nBinomial\r\n\\(f(x) = {n \\choose x} p^x (1-p)^{n-x}\\)\r\nparameter\r\n\r\n\\(\\rightarrow\\) the function of \\(x\\)\r\n\r\nBeta\r\n\\(f(p) = \\frac{1}{B(α,β)} p^{\\alpha − 1} (1−p)^{\\beta − 1}\\)\r\nrandom variable\r\n\r\n\\(\\rightarrow\\) the function of \\(p\\)\r\n\r\nThe beta distribution intuitively comes into play when we look at it in terms of numerator—\\(x/p\\) to the power of something multiplied by \\(1-x/1-p\\) to the power of something—from the lens of the binomial distribution.\r\nThe difference between the binomial and the beta is that the above models the number of successes (\\(x\\)), while the below models the probability (\\(p\\)) of success. In other words, the probability is a parameter in binomial; In the Beta, the probability is a random variable.\r\nIn this context, the shape parameters \\(\\alpha\\) and \\(\\beta\\) or \\(\\alpha-1\\) as the number of successes and \\(\\beta-1\\) as the number of failures\r\nWe can explore the beauty of beta distribution via the the calculator for Beta distribution—Dr. Bognar at the University of Iowa built it.\r\nBeta distribution is very flexible: bell-curve (The PDF of a beta distribution is approximately normal if \\(\\alpha + \\beta\\) is large enough and \\(\\alpha\\) & \\(\\beta\\) are approximately equal), U-shaped (when \\(\\alpha\\) < 1, \\(\\beta\\) < 1) and even straight line. Here’s an graph excerpt from wikipedia.\r\nThe very flexible of Beta distributionBeta function\r\nThe beta function is\r\n\\[ \r\nB(x,y) = \\int_0^1 t^{x−1} (1−t)^{y−1} dt = \\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)},\r\n\\]\r\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function.\r\nGamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\):\r\n\\[ \r\n\\Gamma (n) = (n−1)! = 1 \\times 2 \\times 3 \\times ... \\times (n−1)\r\n\\]\r\nThe gamma function is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\r\n\\Gamma (t) = \\int_0^{\\infty} x^{t-1} e^{-x} dx\r\n\\]\r\n\r\nSimplify the Beta function with the Gamma Function \\(\\Rightarrow\\) we saw the PDF of Beta written in terms of the Gamma function. The Beta function is the ratio of the product of the Gamma function of each parameter divided by the Gamma function of the sum of the parameters (proof refered the further reading topic).\r\n\r\nMain facts\r\n\\[\r\nE[X_B] = \\mu = \\frac{\\alpha}{\\alpha + \\beta}; \\ \\ V[X_B] = \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\r\n\\]\r\nThe standard uniform distribution \\(\\text{Unif} \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nThe mode is \\(\\omega = \\frac{\\alpha − 1}{\\alpha + \\beta − 2}\\) for \\(\\alpha, \\beta > 1\\).\r\nThe concentration is \\(\\kappa = \\alpha + \\beta\\).\r\nDefinitions of \\(\\mu, \\omega\\) and \\(\\kappa\\) can be inverted:\r\n\\[ \\alpha = \\mu\\kappa,  \\beta = (1 − \\mu)\\kappa \\]\r\n\\[ \\alpha = \\omega(\\kappa−2)+1,  \\beta = (1 − \\omega)(\\kappa−2)+1, \\ \\kappa > 2. \\]\r\nParameter \\(\\kappa\\) is a measure of number of observations needed to change our previous belief about \\(\\mu\\).\r\nIf \\(\\kappa\\) is small we need only a few new observations.\r\nExample. Concentration \\(\\kappa = 8\\) around \\(\\mu = 0.5\\) corresponds to \\(\\alpha = \\mu \\kappa = 4\\) and \\(\\beta = (1 − \\mu) \\kappa = 4\\).\r\nParameterization in terms of mean value and standard deviation is:\r\n\\[ \\alpha = \\mu [\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1]; \\ \\ \\beta = (1 - \\mu)[\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1] \\]\r\nStandard deviation is typically smaller than standard deviation of uniform distribution on \\([0,1]\\), i.e. \\(0.28867\\).\r\nExamples.\r\nFor \\(\\mu = 0.5\\), \\(\\sigma = 0.28867\\) the shape parameters are \\(\\alpha = 1\\), \\(\\beta = 1\\).\r\nFind shape parameters of beta distribution with \\(\\mu = 0.5\\), \\(\\sigma = 0.1\\).\r\nThe standard uniform distribution \\(Unif \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nIn actions\r\nKeep parameter \\(\\beta\\) fixed. Move \\(\\alpha\\) up or down. Observe how the mass of the distribution moves\r\n\r\n\r\np <- seq(0,1,by=0.2)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=2), aes(colour = \"alpha=1,beta=2\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=2), aes(colour = \"alpha=4,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=2), aes(colour = \"alpha=6,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=8, shape2=2), aes(colour = \"alpha=8,beta=2\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDo the same as above, but keep \\(\\alpha\\) constant and move \\(\\beta\\) up or down\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=1), aes(colour = \"alpha=2,beta=1\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=5), aes(colour = \"alpha=2,beta=5\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=6), aes(colour = \"alpha=2,beta=6\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=8), aes(colour = \"alpha=2,beta=8\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nMake \\(\\alpha = \\beta = 1\\). What does the shape of the distribution tell you about your knowledge about random variable \\(\\theta\\)?\\(\\Rightarrow\\) The standard uniform distribution \\(Unif(0,1)\\) is a special case of the beta distribution \\(Beta (1,1)\\), when \\(\\alpha\\)=\\(\\beta\\)=1.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"green\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nKeep \\(\\alpha = \\beta\\) , but move both of them up or down. Interpret the shape of the distribution\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=0.5, shape2=0.5), aes(colour = \"alpha=0.5,beta=0.5\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=4), aes(colour = \"alpha=4,beta=4\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=6), aes(colour = \"alpha=6,beta=6\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nVariance changes based on 2 shape parameters.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=400, shape2=80), aes(colour = \"alpha=400,beta=80\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=40, shape2=8), aes(colour = \"alpha=40,beta=8\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=30, shape2=70), aes(colour = \"alpha=30,beta=70\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=7), aes(colour = \"alpha=3,beta=7\")) +\r\n  scale_y_continuous(limits=c(0,25)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\", \"green\", \"orange\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWhen beta distribution is used as a prior for parameter of binomial distribution, parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as previously observed numbers of successes (\\(\\alpha\\)) or failures (\\(\\beta\\)). For example, if in 2 Bernoulli experiments there was 1 success and 1 failure you can express opinion about probability of success as \\(Beta(1,1)\\). What would you assume as prior if in 6 previously observed outcomes there were 3 successes and 3 failures? What is the likely value of the parameter? Do we have more or less information than in case of 1 success and 1 failure? \\(\\Rightarrow\\) Think of more\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=3), aes(colour = \"alpha=3,beta=3\")) +\r\n  stat_function(fun=dbinom, args=list(size=1, prob=0.5), aes(colour = \"Bernoulli w/ prob=0.5\")) + # bernoulli\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"red\",\"green\",\"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDefine distribution with mode \\(\\omega\\)=.8 and concentration \\(\\kappa = 12\\). To do that find shape parameters \\(\\alpha = \\omega (\\kappa − 2) + 1 = 9\\) and \\(\\beta = (1 − \\omega)(\\kappa − 2) + 1 = 3\\).\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=9, shape2=3), aes(colour = \"alpha=9,beta=3\")) +\r\n  scale_y_continuous(limits=c(0,3.4)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the actions we notify that:\r\nThe special case \\(a=b=1\\) is the uniform distribution.\r\nThe distribution is roughly centered on \\(a/(a+b)\\). Actually, it turns out that the mean is exactly \\(a/(a+b)\\). Thus the mean of the distribution is determined by the relative values of \\(a\\) and \\(b\\).\r\nThe larger the values of \\(a\\) and \\(b\\), the smaller the variance of the distribution about the mean.\r\nFor moderately large values of \\(a\\) and \\(b\\) the distribution looks visually “kind of normal”, although unlike the normal distribution the Beta distribution is restricted to [0,1].\r\nPlots in shiny\r\nPlanning to build an shiny app to plot beta distribution on the specification of shape parameter (“still being in the process”).\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nDavid Robinson (Principal Data Scientist at Heap, works in R and Python), Understanding the beta distribution (using baseball statistics), http://varianceexplained.org/statistics/beta_distribution_and_baseball/\r\nAerin Kim, Beta Distribution — Intuition, Examples, and Derivation, https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/Beta-Distribution-in-an-Intuitive-Explanation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-15T00:29:44-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/",
    "title": "Gamma Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Gamma Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "ggplot2",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nFact\r\nGamma function\r\nDefinition\r\nParameters of Gamma: a shape with a scale or a rate\r\nPlots\r\nFurther reading\r\n\r\nFact\r\nWhy did we invent Gamma distribution? Answer: To predict the wait time until future events. Hmmm ok, but I thought that’s what the exponential distribution is for. Then, what’s the difference between exponential distribution and gamma distribution? The exponential distribution predicts the wait time until the very first event. The gamma distribution, on the other hand, predicts the wait time until the k-th event occurs.\r\n– Aerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nExamples:\r\nThe time I wait to receive an interview might follow an exponential. Now, I am waiting not for my first interview offer but my third interview offer. How long must I wait? This waiting time can be described by a gamma.\r\nI missed the first and second CTA train to go to the campus. Now, how long I am able to catch the third train?\r\nGamma function\r\nIn the lecture series of Statistics 110, Lecture 24: Gamma distribution and Poisson process | Statistics 110, Prof. Joe Blitzstein had connected the \\(n!\\) function to the Gamma function. Why?\r\nLet’s see the Gamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\): \\[\\Gamma(n) = (n-1)! = 1 \\times 2 \\times 3 \\times ... \\times (n-1)\\]\r\nOne beautiful formula, Stirling formula to approximate the \\(n!\\), actually an extremely good approximation: \\[ n! \\approx \\sqrt{2\\pi n} \\Big( \\frac{n}{e}\\Big)^n\\]\r\n\r\n\r\nn <- c(1:6)\r\ny <- vector(mode = \"numeric\", length = length(n))\r\ny[1] <- 1\r\nfor(i in 2:length(n)) {\r\n  y[i] = y[i-1] * i\r\n}\r\ndta <- as.data.frame(cbind(n,y))\r\nlibrary(ggplot2)\r\nggplot(dta, aes(n, y)) + \r\n  geom_point() +\r\n  scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\")) +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nThen how we connect the dots. There are many ways to do it, but there’s a philosophical way to do it by Gamma function, which is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\\Gamma(t) = \\int_0^{\\infty} x^t e^{−x} \\frac{dx}{x} \\]\r\nDefinition\r\nFrom the Gamma function, how we got the PDF of Gamma distribution. We would normalize the Gamma distribution, which means from:\r\n\\[ \\Gamma(k) = \\int_0^{\\infty} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nto,\r\n\\[ 1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nThen, \\(X = \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{1}{x}\\) \\(\\sim\\) \\(Gamma(k, 1)\\) which has shape of \\(k\\) and scale of \\(1\\).\r\nHow we turn the scale of \\(1\\) to a general scale of \\(\\theta\\)?\r\nImagine that \\(Y \\sim \\frac{X}{\\theta}\\) where \\(X \\sim \\ Gamma(k,1)\\)\r\n\\(f_Y(y) = f_X(x) \\frac{dx}{dy} = \\frac{1}{\\Gamma(k)} (\\theta y)^{k} e^{−\\theta y} \\frac{1}{\\theta y} \\theta\\) where \\(\\frac{dx}{dy} = \\theta\\)\r\nThus, \\(f(y) = \\frac{1}{\\Gamma(k) \\theta^{k}} (y)^{k} e^{−\\theta y} \\frac{1}{y}\\)\r\nParameters of Gamma: a shape with a scale or a rate\r\n\r\n\r\nknitr::include_graphics(\"Gamma_scalevsrate_inwiki.png\") \r\n\r\n\r\n\r\n\r\n(#fig:model diagram)From https://en.wikipedia.org/wiki/Gamma_distribution\r\n\r\n\r\n\r\nFor (\\(\\alpha\\), \\(\\beta\\)) parameterization: Using our notation \\(k\\) (the # of events) & \\(\\lambda\\) (the rate of events), simply substitute \\(\\alpha\\) with \\(k\\), \\(\\beta\\) with \\(\\lambda\\). The PDF stays the same format as what we’ve derived.\r\nFor (\\(k\\), \\(\\theta\\)) parameterization: \\(\\theta\\) is a reciprocal of the event rate \\(\\lambda\\), which is the mean wait time (the average time between event arrivals).\r\nPlots\r\nI plotted the gamma distribution with the shape of \\(k\\), and constantly rate = \\(1\\)\r\n\r\n\r\nT <- seq(0,20,by=2.5)\r\n\r\ndf <- data.frame(T)\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=1, rate=1), aes(colour = \"k= 1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=5, rate=1), aes(colour = \"k= 5\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"k=10\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"yellowgreen\", \"olivedrab\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nI plotted the gamma distribution with the constantly shape of k = 10, and variant rate from 1 to 3.\r\n\r\n\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"r=1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=10, rate=2), aes(colour = \"r=2\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=3), aes(colour = \"r=3\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"gold\", \"burlywood\", \"darkorange\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution (k=10)\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFurther reading\r\nLecture 24: Gamma distribution and Poisson process | Statistics 110\r\nAerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nWiki\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/Gamma-distribution-in-an-intuitive-explanation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-23T11:53:12-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
