[
  {
    "path": "posts/2021-10-24-LPA Chicago Neighborhoods/",
    "title": "Latent Profile Analysis of Chicago Neighborhoods",
    "description": "American Census Survey data;    \nFactor Analysis;    \nLatent Profile Analysis: Gaussian Mixture modeling;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-10-24",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Factor Analysis",
      "Mclust",
      "Tutorial",
      "Mapping"
    ],
    "contents": "\r\n\r\nContents\r\nFactor Analysis\r\nACS data\r\nDetermine Number of Factors to Extract\r\n\r\nLatent Profile Analysis\r\nData to run LPA\r\nCheck how many clusters should create?\r\nFitted a 4-components GMM with unconstrained covariance matrices\r\n\r\nApplying the Clustering\r\nMapping on Chicago Neighborhood map\r\nClassification by race-ethnicity\r\n\r\nRefs\r\n\r\n\r\nlibrary(mclust)\r\nlibrary(psych)\r\nlibrary(tidyverse)\r\nlibrary(nFactors)\r\nlibrary(kableExtra)\r\n\r\nFactor Analysis\r\nACS data\r\nThe summary of the data “Chicago Census Data by Tract_American Census Survey (ACS)”. Data includes Census tract characteristics of 2010-2014 5-year ACS estimates that easy to download from https://data.census.gov/cedsci/.\r\nWe will use the following variables in the exploratory factor analysis:\r\nAgeDependencyRatio: age dependency ratio\r\nLimitedEngProf5andUP: limited English proficiency\r\nLessThanHS: less than high school in education level\r\nUnemployment\r\nPctForeignBorn: percentage of foreign born\r\nFemaleHHPct: percentage of female per household\r\nMedianIncomeHH: median income per household\r\nAllOcc2009Pct: percentage of all occupants in 2009\r\nPctVacHousing: percentage of vacant housing\r\nPctBelowPoverty: percentage below poverty\r\nPublicAssistancePct: percentage of public assistance\r\n\r\ncensus <- read.csv(\"data/Chicago Census Data by Tract_ACS 2015.csv\", header = TRUE) %>% \r\n  rename(tract = ï..tract) %>% \r\n  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)\r\n\r\nsummary(census)\r\n     tract        AgeDependencyRatio LimitedEngProf5andUP\r\n Min.   : 10100   Min.   :  4.80     Min.   : 0.000      \r\n 1st Qu.:160825   1st Qu.: 39.80     1st Qu.: 1.825      \r\n Median :351250   Median : 54.95     Median : 8.400      \r\n Mean   :403946   Mean   : 52.89     Mean   :13.974      \r\n 3rd Qu.:670275   3rd Qu.: 65.28     3rd Qu.:23.950      \r\n Max.   :843900   Max.   :147.40     Max.   :64.000      \r\n                                                         \r\n   LessThanHS     Unemployment   PctForeignBorn     FemaleHHPct    \r\n Min.   : 0.00   Min.   : 0.00   Min.   :0.00000   Min.   :0.0000  \r\n 1st Qu.: 8.20   1st Qu.: 7.00   1st Qu.:0.03864   1st Qu.:0.0871  \r\n Median :16.30   Median :11.85   Median :0.15108   Median :0.1651  \r\n Mean   :18.38   Mean   :14.51   Mean   :0.18641   Mean   :0.2013  \r\n 3rd Qu.:26.20   3rd Qu.:19.80   3rd Qu.:0.31061   3rd Qu.:0.2986  \r\n Max.   :62.50   Max.   :91.90   Max.   :0.71263   Max.   :0.7047  \r\n                                                                   \r\n MedianIncomeHH   AllOcc2009Pct    PctVacHousing     PctBelowPoverty\r\n Min.   :  5000   Min.   :0.0000   Min.   :0.00000   Min.   : 0.40  \r\n 1st Qu.: 29402   1st Qu.:0.5341   1st Qu.:0.07766   1st Qu.:12.53  \r\n Median : 42321   Median :0.6285   Median :0.11732   Median :21.85  \r\n Mean   : 49658   Mean   :0.6284   Mean   :0.13969   Mean   :24.08  \r\n 3rd Qu.: 62056   3rd Qu.:0.7279   3rd Qu.:0.18202   3rd Qu.:33.80  \r\n Max.   :160455   Max.   :0.9406   Max.   :0.70764   Max.   :74.20  \r\n NA's   :1                                                          \r\n PublicAssistancePct\r\n Min.   :0.0000     \r\n 1st Qu.:0.0929     \r\n Median :0.2207     \r\n Mean   :0.2448     \r\n 3rd Qu.:0.3667     \r\n Max.   :0.7985     \r\n                    \r\n\r\n\r\n# recode into % to scale to other variables\r\ndf <- census %>% mutate(\r\n  PctForeignBorn = PctForeignBorn*100,\r\n  FemaleHHPct = FemaleHHPct*100,\r\n  # MedianIncomeHH1Kinv = (max(MedianIncomeHH, na.rm=TRUE) - MedianIncomeHH)/1000,\r\n  # we would not use 1Kinv because of weird distribution (multi-mode)\r\n  MedianIncomeHH1K = MedianIncomeHH/1000,\r\n  AllOcc2009Pct = AllOcc2009Pct*100,\r\n  PctVacHousing = PctVacHousing*100,\r\n  PublicAssistancePct = PublicAssistancePct*100\r\n) %>% \r\n  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH1K, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)\r\n\r\ndf.fa <- df[complete.cases(df),] %>% \r\n  select(-tract)# 1 missing case\r\n\r\n\r\nsummary(df.fa)\r\n AgeDependencyRatio LimitedEngProf5andUP   LessThanHS   \r\n Min.   :  4.8      Min.   : 0.00        Min.   : 0.00  \r\n 1st Qu.: 39.8      1st Qu.: 1.90        1st Qu.: 8.20  \r\n Median : 55.0      Median : 8.40        Median :16.30  \r\n Mean   : 52.9      Mean   :13.99        Mean   :18.39  \r\n 3rd Qu.: 65.3      3rd Qu.:24.00        3rd Qu.:26.20  \r\n Max.   :147.4      Max.   :64.00        Max.   :62.50  \r\n  Unemployment   PctForeignBorn    FemaleHHPct     MedianIncomeHH1K\r\n Min.   : 0.00   Min.   : 0.000   Min.   : 0.000   Min.   :  5.00  \r\n 1st Qu.: 7.00   1st Qu.: 3.858   1st Qu.: 8.704   1st Qu.: 29.40  \r\n Median :11.90   Median :15.115   Median :16.502   Median : 42.32  \r\n Mean   :14.52   Mean   :18.659   Mean   :20.122   Mean   : 49.66  \r\n 3rd Qu.:19.80   3rd Qu.:31.076   3rd Qu.:29.859   3rd Qu.: 62.06  \r\n Max.   :91.90   Max.   :71.263   Max.   :70.470   Max.   :160.46  \r\n AllOcc2009Pct   PctVacHousing    PctBelowPoverty PublicAssistancePct\r\n Min.   : 0.00   Min.   : 0.000   Min.   : 0.40   Min.   : 0.000     \r\n 1st Qu.:53.45   1st Qu.: 7.752   1st Qu.:12.50   1st Qu.: 9.252     \r\n Median :62.85   Median :11.722   Median :21.80   Median :22.002     \r\n Mean   :62.85   Mean   :13.898   Mean   :24.07   Mean   :24.440     \r\n 3rd Qu.:72.80   3rd Qu.:18.066   3rd Qu.:33.80   3rd Qu.:36.667     \r\n Max.   :94.06   Max.   :50.673   Max.   :74.20   Max.   :79.851     \r\n\r\nDetermine Number of Factors to Extract\r\n\r\nev <- eigen(cor(df.fa)) # get eigenvalues\r\nap <- parallel(subject=nrow(df.fa),var=ncol(df.fa), rep=100,cent=.05)\r\nnS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)\r\nplotnScree(nS) \r\n\r\n\r\nWhen we run a factor analysis, we need to decide on three things:\r\nthe number of factors\r\nthe method of estimation\r\nthe rotation\r\n\\(\\Rightarrow\\) Maximum Likelihood Factor Analysis (bullet 2) entering raw data and extracting 3 factors (bullet 1), with varimax rotation (bullet 3)\r\n\r\nfit.f <- factanal(x = df.fa, factors = 3, n.obs = 797, rotation=\"varimax\")\r\nprint(fit.f, digits=2, cutoff=.3, sort=TRUE)\r\n\r\nCall:\r\nfactanal(x = df.fa, factors = 3, n.obs = 797, rotation = \"varimax\")\r\n\r\nUniquenesses:\r\n  AgeDependencyRatio LimitedEngProf5andUP           LessThanHS \r\n                0.37                 0.00                 0.19 \r\n        Unemployment       PctForeignBorn          FemaleHHPct \r\n                0.29                 0.10                 0.18 \r\n    MedianIncomeHH1K        AllOcc2009Pct        PctVacHousing \r\n                0.29                 0.21                 0.60 \r\n     PctBelowPoverty  PublicAssistancePct \r\n                0.12                 0.07 \r\n\r\nLoadings:\r\n                     Factor1 Factor2 Factor3\r\nAgeDependencyRatio    0.56            0.56  \r\nUnemployment          0.80                  \r\nFemaleHHPct           0.84                  \r\nMedianIncomeHH1K     -0.80                  \r\nPctVacHousing         0.61                  \r\nPctBelowPoverty       0.93                  \r\nPublicAssistancePct   0.96                  \r\nLimitedEngProf5andUP          0.99          \r\nLessThanHS            0.53    0.69          \r\nPctForeignBorn                0.91          \r\nAllOcc2009Pct                         0.89  \r\n\r\n               Factor1 Factor2 Factor3\r\nSS loadings       4.82    2.43    1.33\r\nProportion Var    0.44    0.22    0.12\r\nCumulative Var    0.44    0.66    0.78\r\n\r\nTest of the hypothesis that 3 factors are sufficient.\r\nThe chi square statistic is 222.31 on 25 degrees of freedom.\r\nThe p-value is 1.46e-33 \r\n\r\nIn general, we’d like to see low uniquenesses or high communalities (subtract the uniquenesses from 1—The communality is the proportion of variance of the \\(i^{th}\\) variable contributed by the \\(m\\) (here’s 3) common factors)\r\nAt the conclusion of a factor analysis of census data, we might determine that the census measures 3 factors: (1) social poverty disparity strength, (2) education strength, (3) age disparity strength\r\nFactor analysis seeks to model the correlation matrix with fewer variables called factors. If we succeed with, said here, 3 factors, we are able to model the correlation matrix using only 3 variables instead of 11. Just remember these 3 variables, or factors, are unobserved. We give them names like “latent variables”. They are not subsets of our original variables.\r\nGiven the data matrix \\(X\\), consider it’s covariance matrix \\(cov(X)=\\sum\\) and obtain the matrix \\(V\\) of eigenvectors from \\(\\sum\\). This matrix \\(V\\) is what you call the loadings.\r\nThis set of eigenvectors define an orthogonal change of basis matrix that maximize the variance from X. This means that, if I project X into the subspace generated by V I will obtain a matrix U by simply solving XV=U. This matrix U is what you call the scores (for each factor, we would have its own score).\r\nIn our case, we would manipulate a little bit due to we use only variables in the first factor. Thus, if the absolute value of loadings less than or equal 0.5, it would not contribute to the score\r\n\r\ndf.fa$concdisadv <- as.matrix(df.fa) %*% (fit.f$loadings[,1]*(abs(fit.f$loadings[,1])>0.5))\r\n\r\n# print out the scores\r\nkable(df.fa[1:10,], caption = \"FA scores -- The concentrated disadvantage index scores\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:concentrated disadvantage–fa score)FA scores – The concentrated disadvantage index scores\r\n\r\n\r\nAgeDependencyRatio\r\n\r\n\r\nLimitedEngProf5andUP\r\n\r\n\r\nLessThanHS\r\n\r\n\r\nUnemployment\r\n\r\n\r\nPctForeignBorn\r\n\r\n\r\nFemaleHHPct\r\n\r\n\r\nMedianIncomeHH1K\r\n\r\n\r\nAllOcc2009Pct\r\n\r\n\r\nPctVacHousing\r\n\r\n\r\nPctBelowPoverty\r\n\r\n\r\nPublicAssistancePct\r\n\r\n\r\nconcdisadv\r\n\r\n\r\n30.6\r\n\r\n\r\n3.5\r\n\r\n\r\n6.6\r\n\r\n\r\n13.0\r\n\r\n\r\n15.09985\r\n\r\n\r\n13.586956\r\n\r\n\r\n32.188\r\n\r\n\r\n52.35507\r\n\r\n\r\n15.885714\r\n\r\n\r\n28.7\r\n\r\n\r\n24.36594\r\n\r\n\r\n76.40009\r\n\r\n\r\n39.4\r\n\r\n\r\n22.7\r\n\r\n\r\n21.0\r\n\r\n\r\n9.9\r\n\r\n\r\n35.50975\r\n\r\n\r\n18.823979\r\n\r\n\r\n39.122\r\n\r\n\r\n47.99542\r\n\r\n\r\n12.903226\r\n\r\n\r\n33.3\r\n\r\n\r\n27.56777\r\n\r\n\r\n90.84744\r\n\r\n\r\n47.7\r\n\r\n\r\n15.9\r\n\r\n\r\n16.3\r\n\r\n\r\n10.9\r\n\r\n\r\n31.07639\r\n\r\n\r\n11.643836\r\n\r\n\r\n27.318\r\n\r\n\r\n56.84932\r\n\r\n\r\n16.707417\r\n\r\n\r\n29.6\r\n\r\n\r\n24.65753\r\n\r\n\r\n93.39844\r\n\r\n\r\n44.4\r\n\r\n\r\n19.4\r\n\r\n\r\n22.2\r\n\r\n\r\n7.1\r\n\r\n\r\n32.23630\r\n\r\n\r\n5.439942\r\n\r\n\r\n37.111\r\n\r\n\r\n51.47864\r\n\r\n\r\n16.084559\r\n\r\n\r\n19.9\r\n\r\n\r\n18.76597\r\n\r\n\r\n63.53275\r\n\r\n\r\n23.4\r\n\r\n\r\n2.4\r\n\r\n\r\n0.9\r\n\r\n\r\n9.2\r\n\r\n\r\n13.57015\r\n\r\n\r\n6.634564\r\n\r\n\r\n38.384\r\n\r\n\r\n52.27394\r\n\r\n\r\n15.006821\r\n\r\n\r\n18.1\r\n\r\n\r\n13.32263\r\n\r\n\r\n34.51310\r\n\r\n\r\n23.7\r\n\r\n\r\n6.9\r\n\r\n\r\n7.4\r\n\r\n\r\n14.5\r\n\r\n\r\n23.37764\r\n\r\n\r\n7.255245\r\n\r\n\r\n25.309\r\n\r\n\r\n43.31294\r\n\r\n\r\n10.729614\r\n\r\n\r\n21.3\r\n\r\n\r\n20.89161\r\n\r\n\r\n60.98266\r\n\r\n\r\n16.2\r\n\r\n\r\n10.7\r\n\r\n\r\n13.6\r\n\r\n\r\n5.1\r\n\r\n\r\n31.99531\r\n\r\n\r\n5.332456\r\n\r\n\r\n35.822\r\n\r\n\r\n47.20211\r\n\r\n\r\n19.330855\r\n\r\n\r\n39.2\r\n\r\n\r\n17.18236\r\n\r\n\r\n60.81030\r\n\r\n\r\n54.1\r\n\r\n\r\n25.1\r\n\r\n\r\n23.0\r\n\r\n\r\n10.0\r\n\r\n\r\n31.82692\r\n\r\n\r\n2.746781\r\n\r\n\r\n19.813\r\n\r\n\r\n46.43777\r\n\r\n\r\n13.703704\r\n\r\n\r\n38.3\r\n\r\n\r\n30.21459\r\n\r\n\r\n109.96227\r\n\r\n\r\n32.7\r\n\r\n\r\n14.8\r\n\r\n\r\n13.1\r\n\r\n\r\n9.1\r\n\r\n\r\n25.23635\r\n\r\n\r\n10.178282\r\n\r\n\r\n37.338\r\n\r\n\r\n52.73906\r\n\r\n\r\n5.657492\r\n\r\n\r\n17.7\r\n\r\n\r\n13.16045\r\n\r\n\r\n43.67896\r\n\r\n\r\n52.7\r\n\r\n\r\n22.2\r\n\r\n\r\n18.4\r\n\r\n\r\n9.4\r\n\r\n\r\n34.15854\r\n\r\n\r\n9.987113\r\n\r\n\r\n58.319\r\n\r\n\r\n68.49227\r\n\r\n\r\n8.920188\r\n\r\n\r\n14.9\r\n\r\n\r\n23.45361\r\n\r\n\r\n50.27430\r\n\r\n\r\nLatent Profile Analysis\r\nData to run LPA\r\nThe latent profile analysis (normal mixture) will use variables loaded on factor 1 identified from the factor analysis:\r\nAgeDependencyRatio\r\nUnemployment\r\nFemaleHHPct\r\nMedianIncomeHH1K\r\nPctVacHousing\r\nPctBelowPoverty\r\nPublicAssistancePct\r\nLessThanHS\r\n\r\n\r\n\r\n    If we run Mclust for all 11 indicators, the data-driven would produce up to \r\n    8, 9 clusters. Indeed, I have checked the problem. That's the reason \r\n    we would use factor analysis to select covariates in the first components.\r\nCheck how many clusters should create?\r\n\r\nX <- as.matrix(df2[,-1])\r\nmod <- Mclust(X)\r\nsummary(mod$BIC)\r\nBest BIC values:\r\n            VVE,7        VVE,6        VVV,4\r\nBIC      -45583.8 -45597.13834 -45603.11491\r\nBIC diff      0.0    -13.33485    -19.31142\r\nplot(mod, what=\"BIC\", ylim=range(mod$BIC[,-(1:2)], na.rm=TRUE), legendArgs = list(x = \"bottomleft\")) \r\n\r\nsummary(mod)\r\n---------------------------------------------------- \r\nGaussian finite mixture model fitted by EM algorithm \r\n---------------------------------------------------- \r\n\r\nMclust VVE (ellipsoidal, equal orientation) model with 7 components: \r\n\r\n log-likelihood   n  df      BIC       ICL\r\n       -22304.2 797 146 -45583.8 -45709.47\r\n\r\nClustering table:\r\n  1   2   3   4   5   6   7 \r\n 74 143 129  19 253 101  78 \r\nhead(mod$classification)\r\n1 2 3 4 5 6 \r\n1 2 1 2 1 1 \r\nhead(mod$uncertainty)\r\n           1            2            3            4            5 \r\n0.0007328577 0.0256582102 0.0863792453 0.0139170110 0.0018607666 \r\n           6 \r\n0.0001025574 \r\nmod$uncertainty[mod$classification==4]\r\n          14           76          106          130          136 \r\n3.977713e-04 1.459000e-02 1.891587e-04 5.432918e-01 4.584159e-03 \r\n         164          337          349          395          399 \r\n2.167873e-10 4.375842e-06 1.160644e-03 3.932567e-02 4.171542e-01 \r\n         404          474          501          514          556 \r\n1.909584e-14 4.237307e-06 4.835498e-02 2.901384e-01 5.423051e-01 \r\n         606          615          730          735 \r\n3.456849e-01 4.081981e-01 0.000000e+00 1.035645e-06 \r\nplot(mod, what=\"classification\")\r\n\r\n\r\nIn the above Mclust() function call, the number of mixing components and the covariance parameterization are selected using the Bayesian Information Criterion (BIC). A summary showing the top-three models and a plot of the BIC traces (the first fig.) for all the models considered is then obtained.\r\nWe would use the indication of a 4-component mixture with covariances having different shapes, volume and orientation (VVV). The use can be confirmed by the second plot, there are no clear regions of 7 or 6-component on the plot.\r\nFitted a 4-components GMM with unconstrained covariance matrices\r\n\r\nmod.4 <- Mclust(df2[,-1], G=4) \r\nsummary(mod.4$BIC) \r\nBest BIC values:\r\n             VVV,4       VEV,4      VVE,4\r\nBIC      -45603.11 -45750.7443 -45805.360\r\nBIC diff      0.00   -147.6294   -202.245\r\nplot(mod.4, what=\"classification\")\r\n\r\nsummary(mod.4, parameters = TRUE)\r\n---------------------------------------------------- \r\nGaussian finite mixture model fitted by EM algorithm \r\n---------------------------------------------------- \r\n\r\nMclust VVV (ellipsoidal, varying volume, shape, and orientation)\r\nmodel with 4 components: \r\n\r\n log-likelihood   n  df       BIC       ICL\r\n      -22203.62 797 179 -45603.11 -45706.84\r\n\r\nClustering table:\r\n  1   2   3   4 \r\n 93 294 139 271 \r\n\r\nMixing probabilities:\r\n        1         2         3         4 \r\n0.1149777 0.3725480 0.1729934 0.3394809 \r\n\r\nMeans:\r\n                         [,1]     [,2]      [,3]     [,4]\r\nAgeDependencyRatio  23.610425 50.09446 48.738241 68.03065\r\nLessThanHS           2.443075 25.44106  9.556613 20.53996\r\nUnemployment         4.670161 11.64512  6.805765 24.94439\r\nFemaleHHPct          3.605628 15.91133  8.987856 36.01190\r\nMedianIncomeHH1K    94.399365 42.76009 74.934483 29.19450\r\nPctVacHousing       11.171810 11.36435  7.438423 20.89255\r\nPctBelowPoverty     10.542959 23.73358  9.221573 36.57643\r\nPublicAssistancePct  3.680982 22.34020  7.395136 42.46116\r\n\r\nVariances:\r\n[,,1]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          110.827954   4.505709   -2.5527991\r\nLessThanHS                    4.505709   4.825718    1.3439579\r\nUnemployment                 -2.552799   1.343958    7.4948966\r\nFemaleHHPct                   9.592752   2.963274    0.8724782\r\nMedianIncomeHH1K             21.948939  -6.215072   -5.8068859\r\nPctVacHousing                 3.617609   5.612807   13.4091673\r\nPctBelowPoverty             -11.759843   2.453223    4.0747698\r\nPublicAssistancePct           9.902373   4.939258    1.0298916\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    9.5927518       21.9489390      3.617609\r\nLessThanHS            2.9632739       -6.2150719      5.612807\r\nUnemployment          0.8724782       -5.8068859     13.409167\r\nFemaleHHPct           6.4553757        0.3348242      4.420610\r\nMedianIncomeHH1K      0.3348242      624.2423262      3.149840\r\nPctVacHousing         4.4206103        3.1498396     83.845803\r\nPctBelowPoverty       2.5312080      -45.4496868      8.420220\r\nPublicAssistancePct   6.0893513       -2.1182191      7.549918\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio       -11.759843            9.902373\r\nLessThanHS                 2.453223            4.939258\r\nUnemployment               4.074770            1.029892\r\nFemaleHHPct                2.531208            6.089351\r\nMedianIncomeHH1K         -45.449687           -2.118219\r\nPctVacHousing              8.420220            7.549918\r\nPctBelowPoverty           26.561832            3.884233\r\nPublicAssistancePct        3.884233           13.178735\r\n[,,2]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          214.300185 126.992111    16.865825\r\nLessThanHS                  126.992111 206.987885    12.301917\r\nUnemployment                 16.865825  12.301917    18.503181\r\nFemaleHHPct                  45.098013  48.787591     9.238511\r\nMedianIncomeHH1K            -23.094658 -56.231510   -11.054392\r\nPctVacHousing                -3.977919   9.033062     1.628468\r\nPctBelowPoverty               2.423802  33.812115     8.997776\r\nPublicAssistancePct          60.250147  80.617836    16.223181\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    45.098013       -23.094658     -3.977919\r\nLessThanHS            48.787591       -56.231510      9.033062\r\nUnemployment           9.238511       -11.054392      1.628468\r\nFemaleHHPct           45.928065        -7.903785      2.925860\r\nMedianIncomeHH1K      -7.903785       125.877832    -17.588453\r\nPctVacHousing          2.925860       -17.588453     21.535258\r\nPctBelowPoverty        9.689907       -73.612703     15.542755\r\nPublicAssistancePct   30.032800       -64.034777     12.149998\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio         2.423802            60.25015\r\nLessThanHS                33.812115            80.61784\r\nUnemployment               8.997776            16.22318\r\nFemaleHHPct                9.689907            30.03280\r\nMedianIncomeHH1K         -73.612703           -64.03478\r\nPctVacHousing             15.542755            12.15000\r\nPctBelowPoverty           85.926470            49.99227\r\nPublicAssistancePct       49.992266            90.50270\r\n[,,3]\r\n                    AgeDependencyRatio   LessThanHS Unemployment\r\nAgeDependencyRatio          294.197105 -12.24417798   16.7767540\r\nLessThanHS                  -12.244178  24.62164001    4.8263400\r\nUnemployment                 16.776754   4.82634001    8.0873820\r\nFemaleHHPct                   9.701178   9.04745430    6.1616615\r\nMedianIncomeHH1K             13.966827 -56.09164058  -21.9078480\r\nPctVacHousing                 5.163643  -0.03837313   -0.7952366\r\nPctBelowPoverty             -28.576144  13.88339665    4.2317608\r\nPublicAssistancePct          -7.729161  14.21609068    3.7741089\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    9.7011782        13.966827   5.163642823\r\nLessThanHS            9.0474543       -56.091641  -0.038373126\r\nUnemployment          6.1616615       -21.907848  -0.795236625\r\nFemaleHHPct          19.6797355       -28.074212  -0.444020130\r\nMedianIncomeHH1K    -28.0742123       332.351159  -7.866381763\r\nPctVacHousing        -0.4440201        -7.866382  12.781134605\r\nPctBelowPoverty       5.6968101       -48.427943   1.159862443\r\nPublicAssistancePct   8.3047060       -43.608455  -0.005379067\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio       -28.576144        -7.729161059\r\nLessThanHS                13.883397        14.216090678\r\nUnemployment               4.231761         3.774108900\r\nFemaleHHPct                5.696810         8.304706000\r\nMedianIncomeHH1K         -48.427943       -43.608455461\r\nPctVacHousing              1.159862        -0.005379067\r\nPctBelowPoverty           24.687016        15.650477682\r\nPublicAssistancePct       15.650478        22.034142873\r\n[,,4]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          218.842790   25.84154     6.528224\r\nLessThanHS                   25.841542   67.06932    13.320989\r\nUnemployment                  6.528224   13.32099    95.039772\r\nFemaleHHPct                  21.881252   17.59145    37.153627\r\nMedianIncomeHH1K            -38.416907  -39.71887   -39.349411\r\nPctVacHousing                 3.741222   22.53556    23.335309\r\nPctBelowPoverty              58.648501   57.09121    65.051666\r\nPublicAssistancePct          43.325884   47.59794    63.689544\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio     21.88125        -38.41691      3.741222\r\nLessThanHS             17.59145        -39.71887     22.535563\r\nUnemployment           37.15363        -39.34941     23.335309\r\nFemaleHHPct           115.25845        -28.87802     25.653844\r\nMedianIncomeHH1K      -28.87802        100.69000    -30.670522\r\nPctVacHousing          25.65384        -30.67052     90.092820\r\nPctBelowPoverty        73.51904       -110.34219     48.794240\r\nPublicAssistancePct    80.89187        -99.63891     42.436649\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio         58.64850            43.32588\r\nLessThanHS                 57.09121            47.59794\r\nUnemployment               65.05167            63.68954\r\nFemaleHHPct                73.51904            80.89187\r\nMedianIncomeHH1K         -110.34219           -99.63891\r\nPctVacHousing              48.79424            42.43665\r\nPctBelowPoverty           184.51805           149.38841\r\nPublicAssistancePct       149.38841           178.56187\r\n# compare classification between mod and mod.4\r\n#table(mod$classification, mod.4$classification)\r\n\r\ndf2$ConcDisadv_cluster <- mod.4$classification\r\nwith(df2, table(ConcDisadv_cluster))\r\nConcDisadv_cluster\r\n  1   2   3   4 \r\n 93 294 139 271 \r\ndf2[,11:14] <- mod.4$z\r\nnames(df2)[11:14] <- c(\"ConcDisadv_prob1\", \"ConcDisadv_prob2\", \"ConcDisadv_prob3\", \"ConcDisadv_prob4\")\r\nkable(df2[20:50,], caption = \"Concentrated Disadvantage Clustering Selection based on the highest probability\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:4-comp VVV)Concentrated Disadvantage Clustering Selection based on the highest probability\r\n\r\n\r\n\r\n\r\ntract\r\n\r\n\r\nAgeDependencyRatio\r\n\r\n\r\nLessThanHS\r\n\r\n\r\nUnemployment\r\n\r\n\r\nFemaleHHPct\r\n\r\n\r\nMedianIncomeHH1K\r\n\r\n\r\nPctVacHousing\r\n\r\n\r\nPctBelowPoverty\r\n\r\n\r\nPublicAssistancePct\r\n\r\n\r\nConcDisadv_cluster\r\n\r\n\r\nConcDisadv_prob1\r\n\r\n\r\nConcDisadv_prob2\r\n\r\n\r\nConcDisadv_prob3\r\n\r\n\r\nConcDisadv_prob4\r\n\r\n\r\n20\r\n\r\n\r\n20701\r\n\r\n\r\n65.8\r\n\r\n\r\n12.9\r\n\r\n\r\n9.1\r\n\r\n\r\n13.217391\r\n\r\n\r\n68.750\r\n\r\n\r\n17.857143\r\n\r\n\r\n13.7\r\n\r\n\r\n8.869565\r\n\r\n\r\n3\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.0081983\r\n\r\n\r\n0.9918002\r\n\r\n\r\n0.0000015\r\n\r\n\r\n21\r\n\r\n\r\n20702\r\n\r\n\r\n57.6\r\n\r\n\r\n28.1\r\n\r\n\r\n4.7\r\n\r\n\r\n12.505623\r\n\r\n\r\n54.364\r\n\r\n\r\n13.837209\r\n\r\n\r\n19.8\r\n\r\n\r\n25.551057\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9998515\r\n\r\n\r\n0.0000269\r\n\r\n\r\n0.0001216\r\n\r\n\r\n22\r\n\r\n\r\n20801\r\n\r\n\r\n50.7\r\n\r\n\r\n24.9\r\n\r\n\r\n10.5\r\n\r\n\r\n13.394683\r\n\r\n\r\n45.263\r\n\r\n\r\n12.834225\r\n\r\n\r\n21.7\r\n\r\n\r\n20.858896\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9948329\r\n\r\n\r\n0.0043355\r\n\r\n\r\n0.0008316\r\n\r\n\r\n23\r\n\r\n\r\n20802\r\n\r\n\r\n45.5\r\n\r\n\r\n18.9\r\n\r\n\r\n7.4\r\n\r\n\r\n9.287777\r\n\r\n\r\n42.346\r\n\r\n\r\n14.870954\r\n\r\n\r\n29.1\r\n\r\n\r\n29.018287\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9992171\r\n\r\n\r\n0.0000094\r\n\r\n\r\n0.0007735\r\n\r\n\r\n24\r\n\r\n\r\n20901\r\n\r\n\r\n48.0\r\n\r\n\r\n16.0\r\n\r\n\r\n5.3\r\n\r\n\r\n15.280236\r\n\r\n\r\n38.799\r\n\r\n\r\n21.199442\r\n\r\n\r\n28.2\r\n\r\n\r\n19.115044\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9633811\r\n\r\n\r\n0.0000115\r\n\r\n\r\n0.0366074\r\n\r\n\r\n25\r\n\r\n\r\n20902\r\n\r\n\r\n55.0\r\n\r\n\r\n13.5\r\n\r\n\r\n5.3\r\n\r\n\r\n10.437052\r\n\r\n\r\n33.114\r\n\r\n\r\n9.982384\r\n\r\n\r\n26.7\r\n\r\n\r\n21.461187\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9775131\r\n\r\n\r\n0.0002441\r\n\r\n\r\n0.0222428\r\n\r\n\r\n26\r\n\r\n\r\n30101\r\n\r\n\r\n36.2\r\n\r\n\r\n5.0\r\n\r\n\r\n8.7\r\n\r\n\r\n7.772727\r\n\r\n\r\n41.188\r\n\r\n\r\n11.646586\r\n\r\n\r\n21.0\r\n\r\n\r\n12.954546\r\n\r\n\r\n2\r\n\r\n\r\n0.0123186\r\n\r\n\r\n0.9845259\r\n\r\n\r\n0.0029942\r\n\r\n\r\n0.0001613\r\n\r\n\r\n27\r\n\r\n\r\n30102\r\n\r\n\r\n23.5\r\n\r\n\r\n7.4\r\n\r\n\r\n14.9\r\n\r\n\r\n8.969132\r\n\r\n\r\n43.632\r\n\r\n\r\n18.779565\r\n\r\n\r\n14.5\r\n\r\n\r\n10.308678\r\n\r\n\r\n2\r\n\r\n\r\n0.0128167\r\n\r\n\r\n0.9713396\r\n\r\n\r\n0.0000043\r\n\r\n\r\n0.0158394\r\n\r\n\r\n28\r\n\r\n\r\n30103\r\n\r\n\r\n12.7\r\n\r\n\r\n14.6\r\n\r\n\r\n8.5\r\n\r\n\r\n0.000000\r\n\r\n\r\n37.383\r\n\r\n\r\n10.844371\r\n\r\n\r\n19.5\r\n\r\n\r\n11.142061\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.7999190\r\n\r\n\r\n0.2000236\r\n\r\n\r\n0.0000574\r\n\r\n\r\n29\r\n\r\n\r\n30104\r\n\r\n\r\n9.5\r\n\r\n\r\n9.9\r\n\r\n\r\n14.4\r\n\r\n\r\n1.628106\r\n\r\n\r\n36.529\r\n\r\n\r\n8.542320\r\n\r\n\r\n39.8\r\n\r\n\r\n21.679520\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9999999\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000001\r\n\r\n\r\n30\r\n\r\n\r\n30200\r\n\r\n\r\n33.0\r\n\r\n\r\n5.5\r\n\r\n\r\n7.5\r\n\r\n\r\n3.708333\r\n\r\n\r\n67.700\r\n\r\n\r\n11.699779\r\n\r\n\r\n10.5\r\n\r\n\r\n2.500000\r\n\r\n\r\n1\r\n\r\n\r\n0.5761337\r\n\r\n\r\n0.0351601\r\n\r\n\r\n0.3887062\r\n\r\n\r\n0.0000000\r\n\r\n\r\n31\r\n\r\n\r\n30300\r\n\r\n\r\n42.8\r\n\r\n\r\n18.2\r\n\r\n\r\n8.2\r\n\r\n\r\n7.496464\r\n\r\n\r\n36.020\r\n\r\n\r\n8.062419\r\n\r\n\r\n24.8\r\n\r\n\r\n19.024045\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9637376\r\n\r\n\r\n0.0352999\r\n\r\n\r\n0.0009626\r\n\r\n\r\n32\r\n\r\n\r\n30400\r\n\r\n\r\n44.3\r\n\r\n\r\n12.3\r\n\r\n\r\n18.1\r\n\r\n\r\n14.767255\r\n\r\n\r\n41.250\r\n\r\n\r\n8.247423\r\n\r\n\r\n27.8\r\n\r\n\r\n30.577849\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9820020\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0179980\r\n\r\n\r\n33\r\n\r\n\r\n30500\r\n\r\n\r\n29.1\r\n\r\n\r\n13.3\r\n\r\n\r\n2.7\r\n\r\n\r\n6.931383\r\n\r\n\r\n62.453\r\n\r\n\r\n11.087024\r\n\r\n\r\n15.7\r\n\r\n\r\n5.781957\r\n\r\n\r\n3\r\n\r\n\r\n0.0000003\r\n\r\n\r\n0.2383814\r\n\r\n\r\n0.7616182\r\n\r\n\r\n0.0000001\r\n\r\n\r\n34\r\n\r\n\r\n30601\r\n\r\n\r\n35.0\r\n\r\n\r\n6.8\r\n\r\n\r\n9.7\r\n\r\n\r\n2.740409\r\n\r\n\r\n38.822\r\n\r\n\r\n13.079255\r\n\r\n\r\n21.4\r\n\r\n\r\n15.695067\r\n\r\n\r\n2\r\n\r\n\r\n0.0000029\r\n\r\n\r\n0.9992705\r\n\r\n\r\n0.0004956\r\n\r\n\r\n0.0002309\r\n\r\n\r\n35\r\n\r\n\r\n30603\r\n\r\n\r\n29.2\r\n\r\n\r\n7.2\r\n\r\n\r\n10.5\r\n\r\n\r\n11.488251\r\n\r\n\r\n28.631\r\n\r\n\r\n17.159337\r\n\r\n\r\n35.5\r\n\r\n\r\n20.626632\r\n\r\n\r\n2\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.9999831\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000169\r\n\r\n\r\n36\r\n\r\n\r\n30604\r\n\r\n\r\n34.4\r\n\r\n\r\n4.1\r\n\r\n\r\n6.0\r\n\r\n\r\n3.826531\r\n\r\n\r\n32.069\r\n\r\n\r\n12.421805\r\n\r\n\r\n22.2\r\n\r\n\r\n12.551020\r\n\r\n\r\n2\r\n\r\n\r\n0.0148111\r\n\r\n\r\n0.9846784\r\n\r\n\r\n0.0004018\r\n\r\n\r\n0.0001087\r\n\r\n\r\n37\r\n\r\n\r\n30701\r\n\r\n\r\n31.7\r\n\r\n\r\n15.5\r\n\r\n\r\n15.1\r\n\r\n\r\n6.310160\r\n\r\n\r\n33.504\r\n\r\n\r\n3.409091\r\n\r\n\r\n31.0\r\n\r\n\r\n14.866310\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9996054\r\n\r\n\r\n0.0000713\r\n\r\n\r\n0.0003233\r\n\r\n\r\n38\r\n\r\n\r\n30702\r\n\r\n\r\n39.6\r\n\r\n\r\n1.5\r\n\r\n\r\n7.8\r\n\r\n\r\n3.538928\r\n\r\n\r\n48.870\r\n\r\n\r\n15.107296\r\n\r\n\r\n7.8\r\n\r\n\r\n1.718908\r\n\r\n\r\n1\r\n\r\n\r\n0.9065519\r\n\r\n\r\n0.0830127\r\n\r\n\r\n0.0098448\r\n\r\n\r\n0.0005907\r\n\r\n\r\n39\r\n\r\n\r\n30703\r\n\r\n\r\n35.6\r\n\r\n\r\n5.0\r\n\r\n\r\n6.3\r\n\r\n\r\n5.128205\r\n\r\n\r\n46.174\r\n\r\n\r\n12.883117\r\n\r\n\r\n13.7\r\n\r\n\r\n17.829457\r\n\r\n\r\n2\r\n\r\n\r\n0.0001535\r\n\r\n\r\n0.9958622\r\n\r\n\r\n0.0020986\r\n\r\n\r\n0.0018857\r\n\r\n\r\n40\r\n\r\n\r\n30706\r\n\r\n\r\n54.6\r\n\r\n\r\n8.8\r\n\r\n\r\n2.2\r\n\r\n\r\n5.118755\r\n\r\n\r\n36.362\r\n\r\n\r\n8.914584\r\n\r\n\r\n21.8\r\n\r\n\r\n11.916462\r\n\r\n\r\n2\r\n\r\n\r\n0.0004362\r\n\r\n\r\n0.9797384\r\n\r\n\r\n0.0005719\r\n\r\n\r\n0.0192536\r\n\r\n\r\n41\r\n\r\n\r\n30800\r\n\r\n\r\n39.2\r\n\r\n\r\n6.7\r\n\r\n\r\n9.5\r\n\r\n\r\n3.619909\r\n\r\n\r\n70.361\r\n\r\n\r\n12.262902\r\n\r\n\r\n7.5\r\n\r\n\r\n2.664656\r\n\r\n\r\n3\r\n\r\n\r\n0.0819534\r\n\r\n\r\n0.0953799\r\n\r\n\r\n0.8226666\r\n\r\n\r\n0.0000001\r\n\r\n\r\n42\r\n\r\n\r\n30900\r\n\r\n\r\n26.7\r\n\r\n\r\n7.2\r\n\r\n\r\n2.6\r\n\r\n\r\n9.686411\r\n\r\n\r\n80.657\r\n\r\n\r\n5.716163\r\n\r\n\r\n3.2\r\n\r\n\r\n10.871080\r\n\r\n\r\n3\r\n\r\n\r\n0.1331410\r\n\r\n\r\n0.0036901\r\n\r\n\r\n0.8631689\r\n\r\n\r\n0.0000000\r\n\r\n\r\n43\r\n\r\n\r\n31000\r\n\r\n\r\n30.1\r\n\r\n\r\n5.6\r\n\r\n\r\n5.9\r\n\r\n\r\n8.908202\r\n\r\n\r\n66.875\r\n\r\n\r\n11.191510\r\n\r\n\r\n5.8\r\n\r\n\r\n5.377512\r\n\r\n\r\n3\r\n\r\n\r\n0.0880318\r\n\r\n\r\n0.1053896\r\n\r\n\r\n0.8065776\r\n\r\n\r\n0.0000009\r\n\r\n\r\n44\r\n\r\n\r\n31100\r\n\r\n\r\n21.8\r\n\r\n\r\n12.9\r\n\r\n\r\n7.9\r\n\r\n\r\n5.386221\r\n\r\n\r\n57.994\r\n\r\n\r\n4.543643\r\n\r\n\r\n8.6\r\n\r\n\r\n7.891440\r\n\r\n\r\n3\r\n\r\n\r\n0.0000005\r\n\r\n\r\n0.0988186\r\n\r\n\r\n0.9011727\r\n\r\n\r\n0.0000082\r\n\r\n\r\n45\r\n\r\n\r\n31200\r\n\r\n\r\n23.6\r\n\r\n\r\n24.0\r\n\r\n\r\n10.9\r\n\r\n\r\n6.720122\r\n\r\n\r\n20.887\r\n\r\n\r\n16.111467\r\n\r\n\r\n34.9\r\n\r\n\r\n42.802596\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9878027\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0121973\r\n\r\n\r\n46\r\n\r\n\r\n31300\r\n\r\n\r\n57.4\r\n\r\n\r\n10.7\r\n\r\n\r\n10.0\r\n\r\n\r\n11.997074\r\n\r\n\r\n35.349\r\n\r\n\r\n3.392226\r\n\r\n\r\n23.6\r\n\r\n\r\n25.042672\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.8653997\r\n\r\n\r\n0.0003787\r\n\r\n\r\n0.1342215\r\n\r\n\r\n47\r\n\r\n\r\n31400\r\n\r\n\r\n27.4\r\n\r\n\r\n4.9\r\n\r\n\r\n4.0\r\n\r\n\r\n6.587404\r\n\r\n\r\n74.737\r\n\r\n\r\n9.508578\r\n\r\n\r\n8.1\r\n\r\n\r\n3.566838\r\n\r\n\r\n3\r\n\r\n\r\n0.3567877\r\n\r\n\r\n0.0026885\r\n\r\n\r\n0.6405238\r\n\r\n\r\n0.0000000\r\n\r\n\r\n48\r\n\r\n\r\n31501\r\n\r\n\r\n47.9\r\n\r\n\r\n15.1\r\n\r\n\r\n11.9\r\n\r\n\r\n10.254854\r\n\r\n\r\n24.643\r\n\r\n\r\n14.300572\r\n\r\n\r\n50.7\r\n\r\n\r\n36.165048\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9993333\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0006667\r\n\r\n\r\n49\r\n\r\n\r\n31502\r\n\r\n\r\n34.5\r\n\r\n\r\n7.6\r\n\r\n\r\n16.1\r\n\r\n\r\n6.619280\r\n\r\n\r\n21.265\r\n\r\n\r\n9.670947\r\n\r\n\r\n32.5\r\n\r\n\r\n32.119058\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9972245\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0027755\r\n\r\n\r\n50\r\n\r\n\r\n31700\r\n\r\n\r\n20.0\r\n\r\n\r\n8.2\r\n\r\n\r\n12.7\r\n\r\n\r\n9.429569\r\n\r\n\r\n40.129\r\n\r\n\r\n10.520833\r\n\r\n\r\n33.8\r\n\r\n\r\n22.002328\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9999976\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.0000023\r\n\r\n\r\nHere is the interesting point. Even though, we would see the mismatch among the clustering with the indicators, for example, cluster 4 and 3 may have the contradict between income, less than HS and poverty level. But overall, we will apply on the real cases, the clustering reflexes the correct situations. We can see below 2 applications on mapping and on classifying on race-ethnicity.\r\nApplying the Clustering\r\nMapping on Chicago Neighborhood map\r\nWith the level:\r\n1: not/mild disadvantage\r\n2, 3: medium disadvantage\r\n4: highly disadvantage\r\nWe can map on Chicago Neighborhood map (using ArcGIS) as such\r\n\r\nClassification by race-ethnicity\r\nSave for later on!!!\r\nRefs\r\nPugach, Oksana. 2018. “Latent Profile Analysis of Chicago Neighborhoods” Unpublished Work. Chalk Talk - Institute for Health Resereach; Policy - Methodology Research Core.\r\nUniversity of Virginia Library, Research Data Services + Sciences. “Getting Started with Factor Analysis” - https://data.library.virginia.edu/getting-started-with-factor-analysis/\r\nStats Stackexchange. “How to calculate the loading matrix from the score matrix and a data matrix X (PCA)?” - https://stats.stackexchange.com/questions/447952/how-to-calculate-the-loading-matrix-from-the-score-matrix-and-a-data-matrix-x-p\r\nLuca Scrucca. 2020. “A quick tour of mclust” - https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html\r\nLuca Scrucca, Michael Fop, T. Brendan Murphy,Adrian E. Raftery. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” R J. 2016 August ; 8(1): 289-317\r\n\r\n\r\n",
    "preview": "posts/2021-10-24-LPA Chicago Neighborhoods/distill-preview.png",
    "last_modified": "2021-10-25T11:17:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-25-selection-bias/",
    "title": "Selection bias",
    "description": "Understanding: definition, examples;    \nApplying in DAG;    \nGeneral solution;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-08-25",
    "categories": [
      "Causal Inference",
      "DiagrammeR"
    ],
    "contents": "\r\n\r\nContents\r\nSelection bias: set up\r\nSelection bias happens when? ~ Examples\r\nWhat’s the target population?\r\nWhy not?\r\nConditioning on a collider\r\nCommon structure\r\nIs the selected group different?\r\nConditioning on a collider\r\n\r\nA note about confounding\r\nMore examples\r\nWhat to do?\r\n\r\nSelection bias occurs when some part of the target population is not in the sampled population, or, more generally, when some population units are sampled at a different rate than intended by the investigator. A good sample will be as free from selection bias as possible.\r\n— Sharon L. Lohr, Sampling: Design and Analysis 2nd\r\nSelection bias happens in different fields. Everything can be “selection bias”\r\nModel selection\r\nProblems with statistical inference\r\n\r\nConfounding\r\nIn certain fields… “selection into treatment”\r\n\r\nNon-generalizability/transportability\r\nEffect in sample not the same as in target population\r\n\r\nCollider stratification (worst form of selection bias)\r\nBias for causal effects even within sample and under the null\r\n\r\nSome other names refer to “selection bias”\r\nBerkson’s bias\r\nHealthy worker effect\r\nCensoring/truncation\r\nNon-response bias\r\nPrevalent-user bias\r\nVolunteer bias\r\nIncidence-prevalence bias\r\nIndex-event bias\r\nSurvivor bias\r\nSelection bias: set up\r\n\\(X\\): exposure of interest\\(Y\\): outcome of interest\\(S\\): selection into study (S = 1 if selected)\r\nWe can estimate \\[ RR^s_{XY} = \\frac{Pr(Y = 1 \\mid X = 1; S = 1)}{Pr(Y = 1 \\mid X = 0; S = 1)} \\]\r\nwhich may not equal \\(RR^t_{XY}\\)\r\n\\(s\\): subject to bias\\(t\\): true\r\nWhat is \\(RR^t_{XY}\\)? \\(RR^t_{XY}\\) is the true causal effect in the target population.\r\n\r\nWe will assume that if we estimated \\(\\frac{Pr(Y =1\\mid X=1)}{Pr(Y =1\\mid X=0)}\\) , this is what we’d get\r\nSelection bias happens when? ~ Examples\r\n— (Hernán, Hernández-Díaz, and Robins 2004)\r\nConsider a randomized trial of anti-retroviral therapy (\\(X\\)) among people living with HIV, with a goal of preventing the development of AIDS (\\(Y\\))\r\n\\(\\frac{Pr(Y = 1 \\mid X = 1)}{Pr(Y = 1 \\mid X = 0)}\\) is the risk ratio among people randomized to the intervention arm vs. standard of care\r\nIf some people drop out of the study, we estimate \\(\\frac{Pr(Y = 1 \\mid X = 1; S = 1)}{Pr(Y = 1 \\mid X = 0; S = 1)}\\)\r\n\r\n\r\nhide\r\nlibrary(DiagrammeR) #grViz\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode[shape=none]\r\nX \r\n\r\nnode [shape = box,\r\n      fontname = Helvetica]\r\nS\r\n\r\nnode[shape=none]\r\nY\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode[shape=none]\\nX \\n\\nnode [shape = box,\\n      fontname = Helvetica]\\nS\\n\\nnode[shape=none]\\nY\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\nWhat’s the target population?\r\nTarget population The complete collection of observations we want to study. Defining the target population is an important and often difficult part of the study. For example, in a political poll, should the target population be all adults eligible to vote? All registered voters? All persons who voted in the last election? The choice of target population will profoundly affect the statistics that result.\r\n— Sharon L. Lohr, Sampling: Design and Analysis, 2nd\r\nThe study participants are not a random sample of all people living with HIV … is that a problem?\r\nPerhaps, if we’re trying to estimate how effective treatment would be in another context.\r\nBut not when it comes to estimating valid causal effects. With complete follow-up, we can estimate the effect of the drug in the target population from which the participants came.\r\nWithout loss to follow-up, we can’t even estimate that — not to mention generalize to another context.\r\nWhy not?\r\nThe participants who were lost to follow-up are not a random sample of all participants\r\nPerhaps the most severely immunocompromised people have trouble coming to study visits\r\nThey are also at higher risk of developing AIDS\r\nPerhaps people experiencing side effects of treatment no longer want to participate\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U;\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  S -> U\r\n  Y -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> S\r\n  S -> Y [color = white]\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U;\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  S -> U\\n  Y -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> S\\n  S -> Y [color = white]\\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConditioning on a collider\r\nSelection bias can occur when a non-causal X-Y path is opened by conditioning on \\(S\\)\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U;\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back, color = red]\r\n  S -> U \r\n  Y -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> S [color = red]\r\n  S -> Y [color = white]\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U;\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back, color = red]\\n  S -> U \\n  Y -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> S [color = red]\\n  S -> Y [color = white]\\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nCommon structure\r\nDoes Zika virus infection (\\(X\\)) increase the risk of microcephaly (\\(Y\\))?\r\nWe only assess microcephaly among live births (\\(S\\) = 1).\r\nElective terminations are not included (\\(S\\) = 0).\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode[shape=none]\r\nX \r\n\r\nnode [shape = box,\r\n      fontname = Helvetica]\r\nS\r\n\r\nnode[shape=none]\r\nY\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode[shape=none]\\nX \\n\\nnode [shape = box,\\n      fontname = Helvetica]\\nS\\n\\nnode[shape=none]\\nY\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nIs the selected group different?\r\nWe might assume that\r\nPeople who have more exposure to the virus are more likely to choose to end their pregnancies (worried about risks)\r\nPeople with less access to health care are less likely to have access to abortion services\r\nThere are factors that affect risk of microcephaly that are correlated with access to health care\r\nConditioning on a collider\r\nIntuitively, pregnancies are either in our study if:\r\nThey have lower-than-average probability of exposure to Zika virus (but average risk of microcephaly)\r\nThe have higher-than-average risk of microcephaly (but average risk of Zika exposure)\r\nThe already low-risk pregnancies also have lower exposure to the virus…. It looks like exposure to Zika virus is associated with microcephaly.\r\nA note about confounding\r\nThere are also confounders of the \\(X - Y\\) relationship, of course, since this study is observational\r\ni.e., other reasons why people might be simultaneously at high risk of Zika exposure and microcephaly\r\n\r\nSome of those might be the same factors causing selection bias\r\nIf we properly adjust for them to control confounding, we also control selection bias\r\n\r\nIf there are additional factors leading to selection bias that aren’t confounders, we may not plan to measure or adjust for them\r\nWe’ll tie confounding and selection bias (and misclassification!) together at the end\r\nMore examples\r\nConsider a pharmacoepidemiology study comparing outcomes (\\(Y\\)) in current users vs. never users of a drug (\\(X\\)):\r\nIf people more at risk (\\(U_2\\)) of outcome \\(Y\\) also have more side effects \\(U_1\\), they are more likely to discontinue the drug and not be included in the study (\\(S\\) = 0).\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U1; U2\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  U1 -> U2 \r\n  Y -> U2\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> U1 -> S\r\n  S -> Y [color = white]\r\n  \r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U1; U2\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  U1 -> U2 \\n  Y -> U2\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> U1 -> S\\n  S -> Y [color = white]\\n  \\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConsider a case-control study of coffee consumption (\\(X\\)) and pancreatic cancer (\\(Y\\)), in which controls are chosen from the same hospital:\r\nSelection is based on case status (\\(Y\\)). If controls with gastrointestinal disease are used (\\(U\\)), the fact that they are more likely to avoid coffee can make coffee look like it causes cancer.\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  X -> U \r\n  S -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  edge[color=gray]\r\n  X -> Y \r\n  edge[color=black]\r\n  Y -> S\r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  X -> U \\n  S -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  edge[color=gray]\\n  X -> Y \\n  edge[color=black]\\n  Y -> S\\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConsider a study of the effect of antidepressants (\\(X\\)) on lung cancer (\\(Y\\)) among people with coronary artery disease (\\(S\\)):\r\nIf depression (\\(U1\\)) causes \\(X\\) and \\(S\\), and smoking (\\(U_2\\)) causes \\(S\\) and \\(Y\\) , selection bias (“\\(M\\)-bias”) can result.\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U1; U2\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  X -> U1\r\n  Y -> U2\r\n  S -> U1\r\n  S -> U2\r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U1; U2\\n\\nsubgraph C{\\n  rank=same;\\n  edge[color=gray]\\n  X -> Y \\n}\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  X -> U1\\n  Y -> U2\\n  S -> U1\\n  S -> U2\\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n— (Hernán, Hernández-Díaz, and Robins 2004) and (Smith 2020)\r\nWhat to do?\r\nGo back to the drawing board\r\nDesign a better study …\r\nTrack down those lost to follow-up\r\n\r\nAttempt to correct for it\r\nMeasure the variables causing bias\r\nModeling assumptions\r\n\r\nRedefine question\r\nConsider interpretability\r\n\r\nOR\r\nSensitivity analysis!\r\n\r\n\r\nHernán, Miguel A., Sonia Hernández-Díaz, and James M. Robins. 2004. “A Structural Approach to Selection Bias.” Journal Article. Epidemiology (Cambridge, Mass.) 15 (5): 615–25. https://doi.org/10.1097/01.ede.0000135174.63482.43.\r\n\r\n\r\nSmith, Louisa H. 2020. “Selection Mechanisms and Their Consequences: Understanding and Addressing Selection Bias.” Journal Article. Current Epidemiology Reports 7 (4): 179–89. https://doi.org/10.1007/s40471-020-00241-6.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-26T09:32:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-03-matching in R-causal-inference/",
    "title": "Matching in R",
    "description": "Observational study;    \nCausal inference;    \nMatching;    \nR;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-07-03",
    "categories": [
      "Biostatistics",
      "Causal inference",
      "Tutorial",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nResearch questions\r\nGoal\r\nContext\r\nData\r\nCovariates with Missing Values\r\n\r\nPropensity Score\r\nDistance Matrix\r\nConstructing the Match (B. B. Hansen 2007)\r\nChecking Covariate Balance\r\nOutcomes\r\nFurther reading\r\n\r\n\r\nlibrary(DOS) # Design of Observational Studies\r\nlibrary(tidyverse)\r\nlibrary(kableExtra)\r\nlibrary(optmatch)\r\n\r\nMultivariate matching is involved in 3 steps:\r\n(i) creating a distance matrix,\r\n(ii) adding a propensity score caliper to the distance matrix, and\r\n(iii) finding an optimal match.\r\nMatching in R created largely due to the efforts of Ben Hansen (B. B. Hansen 2007), who created an R function, fullmatch, to do optimal matching from Fortan code created by Demetri Bertsekas D. P. Bertsekas (1981).\r\nResearch questions\r\nDoes financial aid increase college attendance?\r\nGoal\r\nFour matched samples will be constructed: The construction of one of the four matched samples is presented in step-by-step detail.\r\nContext\r\nDuring 1979-1981 Social Security Student Benefit Program provided a substantial tuition benefit for a child of a deceased father, before the cancellation in 1982.\r\nAnalyzing the impact on college attendance and completed schooling of the elimination of the Social Security Student Benefit Program in 1982\r\nFrom 1965 to 1982, the Social Security Administration paid for millions of students to go to college. Under this program, the 18- to 22-year-old children of deceased, disabled, or retired Social Security beneficiaries received monthly payments while enrolled full time in college.\r\nIn 1981, Congress voted to eliminate the program. Then was cancelled in 1982\r\nThe program’s demise provides an opportunity to measure the incentive effects of financial aid. Using difference-in-differences methodology, and proxying for benefit eligibility with the death of a parent during an individual’s childhood, found that ? the elimination of the Social Security student benefit program reduced college attendance probabilities\r\nData\r\nData Xb with 2820 rows and 8 columns:\r\n- faminc: family income in units of $10,000\r\n- incmiss: income missing (1 if family income is missing, 0 o.w.)\r\n- black: 1 if black, 0 o.w.\r\n- hispanic: 1 if hispanic, 0 o.w.\r\n- afqtpct: the Armed Forces Qualifications Test\r\n- edm: mother’s education (1 for less than high school, 2 for high school, 3 for some college, 4 for BA degree or more)\r\n- edmissm: mother’s education missing (1 if missing, 0 o.w.)\r\n- female: gender (1 for female, 0 for male)\r\n\r\nImputation: faminc is set to 2 when incmiss = 1 indicating that faminc is missing, and edm is set to 0 when edmissm=1 indicating that edm is missing\r\n\r\n\r\nAFQT was missing for less than 2% of subjects, and these subjects are not used in the matching. With this restriction, there are 131 high school seniors with deceased fathers and 2689 other high school seniors in the 1979-1981 cohort (131+2689 = 2820)\r\n\r\nThe treatment zb, where is 1 if the father is deceased, and 0 o.w.\r\ndata(\"dynarski\")\r\ndim(dynarski)\r\n[1] 2820   10\r\nkable(head(dynarski, n=20), caption=\"First 20 people in the 1979-1981 cohort, the treatment zb and the eight covariates Xb\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 1: First 20 people in the 1979-1981 cohort, the treatment zb and the eight covariates Xb\r\n\r\n\r\nid\r\n\r\n\r\nzb\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n5.3497560\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n71.901660\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2.4627060\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.158050\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n9.4876030\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.785250\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n0\r\n\r\n\r\n0.4388592\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90.617160\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n0\r\n\r\n\r\n10.4363600\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n81.761160\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n7\r\n\r\n\r\n0\r\n\r\n\r\n6.2694180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n97.917710\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\n3.2204620\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.916710\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\n0\r\n\r\n\r\n9.4719470\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n20.446560\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\n0\r\n\r\n\r\n2.7167480\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n57.175110\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n14\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n8.278976\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n15\r\n\r\n\r\n0\r\n\r\n\r\n7.1157020\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n50.928250\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n0\r\n\r\n\r\n7.7669970\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n71.149020\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\n0\r\n\r\n\r\n12.5388400\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n74.912190\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n18\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n98.394380\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n21\r\n\r\n\r\n0\r\n\r\n\r\n9.4041260\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n98.720520\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n23\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.724040\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n24\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n73.281490\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n29\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1.781234\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n31\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n42.900150\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n1\r\n\r\n\r\n2.6090910\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.623680\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n#write.csv(dynarski, file=\"dynarski.csv\")\r\n\r\n\r\nzb<-dynarski$zb\r\nzbf<-factor(zb,levels=c(1,0),labels=c(\"Father deceased\",\"Father not deceased\"))\r\ntable(zbf)\r\nzbf\r\n    Father deceased Father not deceased \r\n                131                2689 \r\nXb<-dynarski[,3:10]\r\n\r\n\\(\\Rightarrow\\) The 131 seniors with deceased fathers will each be matched to 10 controls whose fathers were not deceased.\r\nCovariates with Missing Values\r\n\r\nwith(dynarski, table(incmiss))\r\nincmiss\r\n   0    1 \r\n2282  538 \r\nincmiss.per<-round(table(dynarski$incmiss)[2]/dim(dynarski)[1]*100,1)\r\npaste0(\"Percentage of income missing: \", incmiss.per)\r\n[1] \"Percentage of income missing: 19.1\"\r\nwith(dynarski, table(edmissm))\r\nedmissm\r\n   0    1 \r\n2704  116 \r\nedmissm.per<-round(table(dynarski$edmissm)[2]/dim(dynarski)[1]*100,1)\r\npaste0(\"Percentage of mother's education missing: \", edmissm.per)\r\n[1] \"Percentage of mother's education missing: 4.1\"\r\n\r\nDiscussion of the missing covariate values will be later on another post.\r\nPropensity Score\r\nThe propensity score is estimated using a logit model.\r\n\r\n# Estimate the propensity score\r\np <- glm(zb ~ Xb$faminc + Xb$incmiss + Xb$black + Xb$hisp + Xb$afqtpct + Xb$edmissm + Xb$edm + Xb$female, family = binomial)$fitted.values\r\n\r\n\\(\\Rightarrow\\) The vector p contains the 2820 estimated propensity scores, \\(\\hat{e}(\\mathbf{x}_l)\\), \\(l\\) = 1, 2, …, 2820.\r\n\r\nIt is reasonable to be able to improve the model: perhaps including interaction terms or transformations or polynomials or whatnot.\r\n\r\nEstimated propensity scores for 131 seniors with deceased fathers and 2689 other seniors in the 1979-1981 cohort, before the Social Security Student Benefit Program was eliminated.\r\n\r\nboxplot(p~zbf, ylab=\"Propensity score\", main=\"1979-1981 Cohort\")\r\n\r\n\r\n\r\ndynarski$p <- p\r\nggplot(dynarski, aes(p, fill = zbf)) + \r\n  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity') +\r\n  ggtitle(\"Figure of Overlay Histograms of PS in 2 Groups\") +\r\n  theme_bw()\r\n\r\n\r\n\\(\\Longrightarrow\\) the median \\(\\hat{e}(\\mathbf{x})\\) in the treated group (0.064) is about equal to the upper quartile among potential controls (0.063). Nonetheless, the distributions in Figure of Overlay Histogram of PS in 2 Groups overlap substantially, so matching appears to be possible.\r\nDistance Matrix\r\nConstructing in 2 steps:\r\n- Step 1: computing the rankbased Mahalanobis distance \\(\\rightarrow\\) 131x2689 distance matrix\r\n#Robust Mahalanobis distance matrix, treated x control\r\ndmat<-smahal(zb,Xb)\r\ndim(dmat)\r\n[1]  131 2689\r\nkable(round(dmat[1:5,1:5],2), caption=\"First five rows and columns of the 131×2689 distance matrix using the rank-based Mahalanobis distance\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 2: First five rows and columns of the 131×2689 distance matrix using the rank-based Mahalanobis distance\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n3.86\r\n\r\n\r\n2.61\r\n\r\n\r\n5.06\r\n\r\n\r\n6.86\r\n\r\n\r\n6.72\r\n\r\n\r\n20\r\n\r\n\r\n3.47\r\n\r\n\r\n3.03\r\n\r\n\r\n7.58\r\n\r\n\r\n6.23\r\n\r\n\r\n5.82\r\n\r\n\r\n108\r\n\r\n\r\n9.60\r\n\r\n\r\n19.47\r\n\r\n\r\n20.02\r\n\r\n\r\n24.62\r\n\r\n\r\n13.03\r\n\r\n\r\n126\r\n\r\n\r\n6.81\r\n\r\n\r\n8.05\r\n\r\n\r\n12.93\r\n\r\n\r\n10.74\r\n\r\n\r\n9.88\r\n\r\n\r\n145\r\n\r\n\r\n8.70\r\n\r\n\r\n15.09\r\n\r\n\r\n17.74\r\n\r\n\r\n18.86\r\n\r\n\r\n12.37\r\n\r\n\r\nStep 2: adding the caliper on the propensity score. The caliper was at \\(0.2 \\times sd(\\hat{e}(\\mathbf{x}))\\).\r\n\r\n#Add a caliper on the propensity score using a penalty function\r\ndmat<-addcaliper(dmat,zb,p,caliper=.2)\r\ndim(dmat)\r\n[1]  131 2689\r\nkable(round(dmat[1:5,1:5],2), caption=\"First five rows and columns of the 131×2689 distance matrix after adding the propensity score calipers\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 3: First five rows and columns of the 131×2689 distance matrix after adding the propensity score calipers\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n18.60\r\n\r\n\r\n20.64\r\n\r\n\r\n42.04\r\n\r\n\r\n79.91\r\n\r\n\r\n46.10\r\n\r\n\r\n20\r\n\r\n\r\n46.32\r\n\r\n\r\n3.03\r\n\r\n\r\n72.66\r\n\r\n\r\n51.18\r\n\r\n\r\n73.30\r\n\r\n\r\n108\r\n\r\n\r\n82.94\r\n\r\n\r\n47.40\r\n\r\n\r\n115.60\r\n\r\n\r\n39.07\r\n\r\n\r\n111.01\r\n\r\n\r\n126\r\n\r\n\r\n57.81\r\n\r\n\r\n13.64\r\n\r\n\r\n86.16\r\n\r\n\r\n47.54\r\n\r\n\r\n85.51\r\n\r\n\r\n145\r\n\r\n\r\n8.70\r\n\r\n\r\n54.51\r\n\r\n\r\n33.32\r\n\r\n\r\n113.31\r\n\r\n\r\n30.34\r\n\r\n\r\n\\(\\Rightarrow\\) Among these 25 entries, only 2 respected the caliper and did not incur a penalty (3.03 and 8.70).\r\nConstructing the Match (B. B. Hansen 2007)\r\nMatching ten controls to each senior with a deceased father: The fullmatch function needs to know the distance matrix, here dmat, matching 10-to-1 means 10x131 will be used, omitting 2689−10x131 = 1379, which is the omit fraction be: 51%.\r\n\r\nm<-fullmatch(dmat, data=dynarski, min.controls=10, max.controls=10, omit.fraction=1379/2689)\r\n\r\nThere is an entry in m for each of the 2820 seniors\r\n\r\nlength(m)\r\n[1] 2820\r\n\r\nThe first ten entries in m are\r\n\r\nm[1:10]\r\n    1     2     3     4     5     6     7     8     9    10 \r\n1.129 1.100  <NA>  1.54  <NA> 1.121 1.114  <NA> 1.111  1.87 \r\n\r\ni.e. the first senior of the 2820 seniors is in matched set #34 and the second senior is in matched set #10. The third senior was one of the 1379 unmatched controls; this is the meaning of the zero in m.01. The fourth senior is in matched set #87, the fifth is unmatched, and so on.\r\nThe function matched(·) indicates who is matched. The first ten entries of matched(m) are\r\n\r\nmatched(m)[1:10]\r\n    1     2     3     4     5     6     7     8     9    10 \r\n TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE \r\n\r\nthe first two seniors were matched but the third was not, and so on.\r\nThere are 1441 matched seniors, where 1441 = 131×11, because the 131 seniors with deceased fathers were each matched to ten controls, making 131 matched sets each of size 11.\r\n\r\nsum(matched(m))\r\n[1] 1441\r\nsum(matched(m))/11\r\n[1] 131\r\n\r\nThe first 3 matched sets are in Table below. The first match consists of 11 female high school seniors, neither black nor hispanic, whose mothers had a high school education, with family incomes between $30,000 and $40,000, mostly with test scores between 59% and 77%. In the second matched set, incomes were lower but test scores were higher. And so on.\r\n\r\n# Housekeeping\r\nim<-as.integer(m) # matched set from 1 to 131\r\ndynarski<-cbind(dynarski,im) \r\ndm<-dynarski[matched(m),] # only select matched cases, note that matched(m): T\\F\r\ndm<-dm[order(dm$im,1-dm$zb),] # sort data according to matched set then zb decreasing\r\n\r\n# Table of matched set example\r\n#which(dm$id==10) # [1] 188\r\n#which(dm$id==396) # [1] 23\r\n#which(dm$id==3051) # [1] 1068\r\n\r\nkable(rbind(dm[188:198,-c(11,12)], dm[23:33,-c(11,12)],dm[1068:1078,-c(11,12)]), caption=\"The 3 of 131 matched sets, each set containing one treated subject and 10 matched controls\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:matched set ex)The 3 of 131 matched sets, each set containing one treated subject and 10 matched controls\r\n\r\n\r\n\r\n\r\nid\r\n\r\n\r\nzb\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\n7\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\n3.220462\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.91671\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n239\r\n\r\n\r\n350\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n59.58355\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n252\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n3.599340\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.23934\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n322\r\n\r\n\r\n465\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n56.37230\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n361\r\n\r\n\r\n518\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n67.00953\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n383\r\n\r\n\r\n550\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n63.49724\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n928\r\n\r\n\r\n1294\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n67.31059\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1481\r\n\r\n\r\n2072\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n64.97742\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1489\r\n\r\n\r\n2082\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n63.62268\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1560\r\n\r\n\r\n2183\r\n\r\n\r\n0\r\n\r\n\r\n3.970631\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n76.51781\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2810\r\n\r\n\r\n3965\r\n\r\n\r\n0\r\n\r\n\r\n3.761650\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n72.57903\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n268\r\n\r\n\r\n396\r\n\r\n\r\n1\r\n\r\n\r\n2.371901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n88.50979\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2.462706\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.15805\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n101\r\n\r\n\r\n147\r\n\r\n\r\n0\r\n\r\n\r\n2.273267\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n77.09483\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n375\r\n\r\n\r\n537\r\n\r\n\r\n0\r\n\r\n\r\n2.595314\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.96086\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n665\r\n\r\n\r\n933\r\n\r\n\r\n0\r\n\r\n\r\n2.846281\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n96.11139\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n698\r\n\r\n\r\n974\r\n\r\n\r\n0\r\n\r\n\r\n1.897521\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.59859\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n707\r\n\r\n\r\n987\r\n\r\n\r\n0\r\n\r\n\r\n2.134711\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n81.18414\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1394\r\n\r\n\r\n1947\r\n\r\n\r\n0\r\n\r\n\r\n2.050298\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n91.44506\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n1518\r\n\r\n\r\n2124\r\n\r\n\r\n0\r\n\r\n\r\n2.298786\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n72.40341\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1877\r\n\r\n\r\n2618\r\n\r\n\r\n0\r\n\r\n\r\n2.211560\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n68.91621\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2816\r\n\r\n\r\n3975\r\n\r\n\r\n0\r\n\r\n\r\n2.371901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90.74260\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2182\r\n\r\n\r\n3051\r\n\r\n\r\n1\r\n\r\n\r\n3.409901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n62.87004\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n427\r\n\r\n\r\n606\r\n\r\n\r\n0\r\n\r\n\r\n4.179612\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n81.73608\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n472\r\n\r\n\r\n664\r\n\r\n\r\n0\r\n\r\n\r\n4.388592\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n91.57050\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n628\r\n\r\n\r\n884\r\n\r\n\r\n0\r\n\r\n\r\n2.846281\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n48.77070\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n713\r\n\r\n\r\n995\r\n\r\n\r\n0\r\n\r\n\r\n3.134709\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n55.11791\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n724\r\n\r\n\r\n1008\r\n\r\n\r\n0\r\n\r\n\r\n3.320661\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n51.60562\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1006\r\n\r\n\r\n1399\r\n\r\n\r\n0\r\n\r\n\r\n3.439256\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n90.01505\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2087\r\n\r\n\r\n2908\r\n\r\n\r\n0\r\n\r\n\r\n3.795041\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n80.28098\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n2328\r\n\r\n\r\n3262\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n57.27546\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n2424\r\n\r\n\r\n3400\r\n\r\n\r\n0\r\n\r\n\r\n2.925728\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n80.88309\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2576\r\n\r\n\r\n3624\r\n\r\n\r\n0\r\n\r\n\r\n3.320661\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n74.08430\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nChecking Covariate Balance\r\nNote that: for covariate \\(k\\),\r\n- \\(\\bar{x}_{tk}\\) is the mean in the first group in the comparison,\r\n- \\(\\bar{x}_{ck}\\) is the mean in the second group in the comparison, and\r\n- \\(\\bar{x}_{mck}\\) is the mean in the matched comparison group formed from the second group\r\n- \\(sd_{bk}\\) is the standardized difference in means before matching and\r\n- \\(sd_{mk}\\) is the standardized difference in means after matching; they have the same denominator but different numerators\r\n\r\n#fd <- dynarski[dynarski$zb==1,-c(1,2,12)]\r\n#round(apply(fd, 2, FUN=mean),2)\r\n\r\nd.tk<-dynarski %>%\r\n  filter(zb==1) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im)\r\nx.tk<-d.tk %>% summarise_all(mean, na.rm=TRUE)\r\ns.tk<-d.tk %>% summarise_all(sd, na.rm=TRUE)\r\n\r\nd.ck<-dynarski %>%\r\n  filter(zb==0) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im)\r\nx.ck<-d.ck %>% summarise_all(mean, na.rm=TRUE)\r\ns.ck<-d.ck %>% summarise_all(sd, na.rm=TRUE)\r\n\r\nsd.bk<-abs(x.tk-x.ck)/sqrt((s.tk^2+s.ck^2)/2)\r\n\r\nd.cmk<-dm %>%\r\n  filter(zb==0) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im) \r\nx.cmk<-d.cmk %>% summarise_all(mean, na.rm=TRUE)\r\nsd.cmk<-abs(x.tk-x.cmk)/sqrt((s.tk^2+s.ck^2)/2)\r\n\r\nset<-round(rbind(x.tk,x.cmk,x.ck,sd.bk,sd.cmk),2)\r\nrow.names(set) <- c(\"x.tk\",\"x.cmk\",\"x.ck\",\"sd.bk\",\"sd.cmk\")\r\n\r\nkable(set, caption=\"Balance on covariates before and after matching\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 4: Balance on covariates before and after matching\r\n\r\n\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\np\r\n\r\n\r\nx.tk\r\n\r\n\r\n2.78\r\n\r\n\r\n0.15\r\n\r\n\r\n0.35\r\n\r\n\r\n0.15\r\n\r\n\r\n49.58\r\n\r\n\r\n0.08\r\n\r\n\r\n1.62\r\n\r\n\r\n0.49\r\n\r\n\r\n0.07\r\n\r\n\r\nx.cmk\r\n\r\n\r\n2.77\r\n\r\n\r\n0.15\r\n\r\n\r\n0.34\r\n\r\n\r\n0.15\r\n\r\n\r\n49.10\r\n\r\n\r\n0.04\r\n\r\n\r\n1.61\r\n\r\n\r\n0.50\r\n\r\n\r\n0.06\r\n\r\n\r\nx.ck\r\n\r\n\r\n4.58\r\n\r\n\r\n0.19\r\n\r\n\r\n0.29\r\n\r\n\r\n0.15\r\n\r\n\r\n52.39\r\n\r\n\r\n0.04\r\n\r\n\r\n1.91\r\n\r\n\r\n0.50\r\n\r\n\r\n0.05\r\n\r\n\r\nsd.bk\r\n\r\n\r\n0.71\r\n\r\n\r\n0.11\r\n\r\n\r\n0.13\r\n\r\n\r\n0.02\r\n\r\n\r\n0.10\r\n\r\n\r\n0.19\r\n\r\n\r\n0.33\r\n\r\n\r\n0.03\r\n\r\n\r\n0.67\r\n\r\n\r\nsd.cmk\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n0.02\r\n\r\n\r\n0.19\r\n\r\n\r\n0.02\r\n\r\n\r\n0.02\r\n\r\n\r\n0.09\r\n\r\n\r\n\\(\\Longrightarrow\\) They were quite different before matching but were much closer after matching. The family income of seniors with deceased fathers was lower, they were more often black, and their mothers had less education. Between 1979-1981, AFQT test scores decline. The imbalances are much smaller after matching.\r\nOutcomes\r\nThe outcomes were not provided in the Dynarski’s dataset. But I guessed we can report the proportions and MH-odds ratio between 2 groups after matching.\r\nIn short, during 1979-1981, when the Social Security Student Benefit Program provided tuition aid to students of a deceased Social Security beneficiary, seniors with deceased fathers were more likely to attend college and complete one year of college than were matched controls, with an odds ratio of about 1.65, but there is no sign of this in 1982-1983 after the program ended (the later can be checked because of lack the data for analysis).\r\nStandardized differences in covariate means before and after matching in matched comparisons. The boxplot displays standardized differences in means, for the nine covariates and the propensity score.\r\n\r\nm<-c(0,1)\r\nd.sd<-rbind(sd.bk,sd.cmk)\r\nd.sd<-cbind(d.sd,m)\r\nd.sd.long<-d.sd %>%\r\n  pivot_longer(-m,names_to=\"absstdzdiff\", values_to=\"sd\")\r\nd.sd.long$m<-factor(d.sd.long$m,levels=c(0,1),labels=c(\"Unmatched\",\"Matched\"))\r\nboxplot(sd~m, data=d.sd.long, xlab=\"\", ylab=\"Absolute Standardized Difference\", main=\"1979-1981 Cohort: FD vs. FND\")\r\n\r\n\r\nFurther reading\r\nDynarski (1999)’s fine study\r\nB. B. Hansen (2007)’s fullmatch function\r\nBen B. Hansen and Klopfer (2006), Paul R. Rosenbaum (1989), P. R. Rosenbaum (1991) showed how fullmatch is doing\r\nMing and Rosenbaum (2001) showed proc assign in SAS\r\n\r\n\r\nBertsekas. 1991. Linear Network Optimization. Book. Cambridge, MA: MIT Press. http://web.mit.edu/dimitrib/www/net.html.\r\n\r\n\r\nBertsekas, Dimitri P. 1981. “A New Algorithm for the Assignment Problem.” Journal Article. Mathematical Programming 21 (1): 152–71. https://doi.org/10.1007/BF01584237.\r\n\r\n\r\nDynarski, Susan M. 1999. “Does Aid Matter? Measuring the Effect of Student Aid on College Attendance and Completion.” Journal Article, 1 online resource. http://HZ9PJ6FE4T.search.serialssolutions.com/?V=1.0&L=HZ9PJ6FE4T&S=JCs&C=TC_008322359&T=marc&tab=BOOKS Available only to UIC users.\r\n\r\n\r\nHansen, B. B. 2007. “Optmatch: Flexible, Optimal Matching for Observational Studies.” Journal Article. R News 7: 18–24.\r\n\r\n\r\nHansen, Ben B., and Stephanie Olsen Klopfer. 2006. “Optimal Full Matching and Related Designs via Network Flows.” Journal Article. Journal of Computational and Graphical Statistics 15 (3): 609–27. www.jstor.org/stable/27594200.\r\n\r\n\r\nMing, Kewei, and Paul R. Rosenbaum. 2001. “A Note on Optimal Matching with Variable Controls Using the Assignment Algorithm.” Journal Article. Journal of Computational and Graphical Statistics 10 (3): 455–63. www.jstor.org/stable/1391099.\r\n\r\n\r\nRosenbaum, P. R. 1991. “A Characterization of Optimal Designs for Observational Studies.” Journal Article. Journal of the Royal Statistical Society. Series B (Methodological) 53 (3): 597–610. www.jstor.org/stable/2345589.\r\n\r\n\r\nRosenbaum, Paul R. 1989. “Optimal Matching for Observational Studies.” Journal Article. Journal of the American Statistical Association 84 (408): 1024–32. https://doi.org/10.2307/2290079.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-03-matching in R-causal-inference/distill-preview.png",
    "last_modified": "2021-10-31T01:26:11-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-06-09-mets-using-nhanesdata/",
    "title": "Metabolic Syndrome Prevalence across time using NHANES Data",
    "description": "How to correctly approach NHANES data to make estimates that are representative of the population    \nDefine Metabolic Syndrome based on ATP III  \nLook at the MetS prevalence over time",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [
      "Biostatistics",
      "NHANES approach",
      "R",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nNHANES Cycle 2013/2014\r\nImport data from NHANES cycle 2013/2014\r\nData Processing\r\nDefine metabolic syndrome\r\nDefine sub-population of interest\r\nMetS proportion in the random sample (unweighted)\r\nWith all age groups\r\nWith all age groups & none missing of MetS indicator\r\nWith age 20+ & none missing of MetS indicator\r\nWith age 20+ and ind.non_missing is TRUE:\r\n\r\nMetS prevalence with weighted\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n\r\nCycle 2015/2016\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\nCycle 2017/2018\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\nSummary\r\nReferences\r\n\r\nI need to use these packages:\r\ntidyverse: data management (tidyr, dplyr) and plots (ggplot2)\r\nforeign: reading xport files\r\nsurvey: for using survey weights # survey::svydesign, svymean, svyglm\r\n\r\nlibrary(tidyverse)\r\nlibrary(foreign) \r\nlibrary(survey) \r\noptions(survey.lonely.psu='adjust')\r\nlibrary(kableExtra)\r\n\r\nDisplay Version Information\r\n\r\n#cat(\"R package versions:\\n\")\r\nfor (p in c(\"base\", \"survey\",\"tidyverse\")) { \r\n  cat(p, \": \", as.character(packageVersion(p)), \"\\n\")\r\n}\r\nbase :  4.0.4 \r\nsurvey :  4.0 \r\ntidyverse :  1.3.1 \r\n\r\nNHANES Cycle 2013/2014\r\nImport data from NHANES cycle 2013/2014\r\nCode for data import was adapted from a NHANES tutorials example.\r\nfunction to download the required survey cycles for a component file\r\n\r\ndownloadNHANES <- function(fileprefix){\r\n  print (fileprefix)\r\n  outdf <- data.frame(NULL)\r\n  for (j in 1:length(letters)){\r\n    urlstring <- paste('https://wwwn.cdc.gov/nchs/nhanes/',yrs[j],'/',fileprefix,letters[j],'.XPT', sep='')\r\n    download.file(urlstring, tf <- tempfile(), mode=\"wb\")\r\n    tmpframe <- foreign::read.xport(tf)\r\n    outdf <- bind_rows(outdf, tmpframe)\r\n  }\r\n  return(outdf)\r\n}\r\n\r\nImport NHANES cycle 2013/2014\r\n\r\n# Specify the survey cycles required, with corresponding file suffixes\r\nyrs <- c('2013-2014')\r\nletters <- c('_h')\r\n\r\n# Download data for each component\r\n# (1) Demographic (DEMO)\r\nDEMO <- downloadNHANES('DEMO') # 10175\r\n# (2) Blood Pressure & Cholesterol\r\nBPQ <- downloadNHANES('BPQ') # 6464\r\n# (3) Diabetes \r\nDIQ <- downloadNHANES('DIQ') # 9770\r\n# (4) Blood Pressure\r\nBPX <- downloadNHANES('BPX') # 9813\r\n# (5) Body Measures\r\nBMX <- downloadNHANES('BMX') # 9813\r\n# (6) Cholesterol - High - Density Lipoprotein\r\nHDL <- downloadNHANES('HDL') # 8291\r\n# (7) Cholesterol - Low-Density Lipoproteins (LDL) & Triglycerides (TRIGLY_J)\r\nTRIGLY <- downloadNHANES('TRIGLY') # 3329\r\n# (8) Plasma Fasting Glucose (GLU_J)\r\nGLU <- downloadNHANES('GLU') # 3329\r\n\r\nMerge all datasets, preserve all observations\r\nNHANES Data Structure\r\n# Merging using \"piping\" \r\nnhanesdata.merged <- left_join(DEMO, BPQ, by=\"SEQN\") %>% \r\n  left_join(DIQ, by=\"SEQN\") %>% \r\n  left_join(BPX, by='SEQN') %>% \r\n  left_join(BMX, by='SEQN') %>%\r\n  left_join(HDL, by='SEQN') %>%\r\n  left_join(TRIGLY, by='SEQN') %>%\r\n  left_join(GLU, by='SEQN') # 10175 obs\r\n\r\nData Processing\r\nSelect necessary variables\r\nCreate indicator for overall summary one\r\nCreate age categories for adults aged 18 and over: ages 18-39, 40-59, 60 and over\r\nRe-code the levels of variables: gender, ethnicity/race, marital status, medication prescription due to ‘refuse to answer’ or ‘don’t know’ (to missing)\r\n\r\nnhanes.vbls <- nhanesdata.merged %>%\r\n  select(SEQN, # Respondent sequence number\r\n         RIDAGEYR, # Age in years at screening\r\n         RIAGENDR, # Gender\r\n\r\n         RIDRETH3, # Race/Hispanic origin w/ NH Asian\r\n         DMDMARTL, # Marital status\r\n         DMDEDUC2, # Education level - Adults 20+\r\n         \r\n         # Survey design variables\r\n         SDMVPSU, # Masked variance pseudo-PSU\r\n         SDMVSTRA, # Masked variance pseudo-stratum\r\n         WTMEC2YR, # Full sample 2 year MEC exam weight\r\n         WTINT2YR, # Full sample 2 year interview weight\r\n         \r\n         BPXSY1, # Systolic: Blood pres (1st rdg) mm Hg\r\n         BPXDI1, # Diastolic: Blood pres (1st rdg) mm Hg\r\n         BPXSY2, # Systolic: Blood pres (2nd rdg) mm Hg\r\n         BPXDI2, # Diastolic: Blood pres (2nd rdg) mm Hg\r\n         BPXSY3, # Systolic: Blood pres (3rd rdg) mm Hg\r\n         BPXDI3, # Diastolic: Blood pres (3rd rdg) mm Hg\r\n         BPXSY4, # Systolic: Blood pres (4th rdg) mm Hg\r\n         BPXDI4, # Diastolic: Blood pres (4th rdg) mm Hg\r\n         LBXGLU, # Fasting Glucose (mg/dL)\r\n         LBDHDD, # Direct HDL-Cholesterol (mg/dL)\r\n         LBXTR, # Triglyceride (mg/dL)\r\n         BMXWAIST, # Waist Circumference (cm)\r\n         \r\n         BPQ090D, # Told to take prescription for cholesterol\r\n         BPQ040A, # Taking prescription for hypertension\r\n         DIQ070 # Take diabetic pills to lower blood sugar\r\n         ) %>%\r\n  mutate(# create indicator for overall summary\r\n         one = 1,\r\n         id = SEQN,\r\n         psu = SDMVPSU, strata = SDMVSTRA, \r\n         persWeight = WTINT2YR, persWeightMEC= WTMEC2YR,\r\n         age = RIDAGEYR,\r\n         # Create age categories for adults aged 18 and over: ages 18-39, 40-59, 60 and over\r\n        ageCat = cut(RIDAGEYR,\r\n                    breaks = c(19, 29, 39, 49, 59, 69, Inf),\r\n                    labels = c('20-29','30-39', '40-49', '50-59', '60-69', '70+')),\r\n         female = factor(if_else(RIAGENDR == 1, 0, 1)), # 1:male ==> 0\r\n         ethnicity = case_when(\r\n           RIDRETH3 %in% c(1,2) ~ 0, # Hispanic\r\n           RIDRETH3 == 3 ~ 1, # Non-Hispanic White\r\n           RIDRETH3 == 4 ~ 2, # Non-Hispanic Black\r\n           RIDRETH3 == 6 ~ 3, # Non-Hispanic Asian\r\n           RIDRETH3 == 7 ~ 4, # Multi-Racial\r\n         is.na(RIDRETH3) ~ NA_real_\r\n         ),\r\n         chol.prsn = case_when(\r\n           BPQ090D == 2 ~ 0, # No\r\n           BPQ090D == 1 ~ 1, # Yes\r\n           BPQ090D %in% c(7,9) ~ NA_real_\r\n         ),\r\n         bp.prsn = case_when(\r\n           BPQ040A == 2 ~ 0, # No\r\n           BPQ040A == 1 ~ 1, # Yes\r\n           BPQ040A %in% c(7,9) ~ NA_real_\r\n         ),\r\n         glu.prsn = case_when(\r\n           DIQ070 == 2 ~ 0, # No\r\n           DIQ070 == 1 ~ 1, # Yes\r\n           DIQ070 %in% c(7,9) ~ NA_real_\r\n         ),\r\n         dia = rowMeans(.[, c(\"BPXDI1\",\"BPXDI2\",\"BPXDI3\",\"BPXDI4\")], na.rm = TRUE),\r\n         sys = rowMeans(.[, c(\"BPXSY1\",\"BPXSY2\",\"BPXSY3\",\"BPXSY4\")], na.rm = TRUE),\r\n         glu = LBXGLU,\r\n         hdl = LBDHDD,\r\n         tri = LBXTR,\r\n         waist = BMXWAIST\r\n        ) %>%\r\n  select(one, id, psu, strata, persWeight, persWeightMEC, age, ageCat, female, ethnicity, sys, dia, glu, hdl, tri, waist, bp.prsn, chol.prsn, glu.prsn) # 10175\r\n\r\nDefine metabolic syndrome\r\nAccording to Alberti et al. (2009), the presence of any 3 of 5 risk factors constitutes a diagnosis of metabolic syndrome:\r\nelevated waist circumference (≥88 cm for women and ≥102 cm for men),\r\nelevated triglycerides (≥150 mg/dL) or drug treatment for elevated triglycerides (there was no variable indicating the medication for lower triglycerides),\r\nlow HDL cholesterol (<40 mg/dL for men and <50 mg/dL for women) or drug treatment for low HDL cholesterol,\r\nelevated blood pressure (systolic ≥130 mm Hg, or diastolic ≥85 mm Hg, or both) or antihypertensive drug treatment for a history of hypertension, and\r\nelevated fasting glucose (≥100 mg/dL) or drug treatment for elevated glucose.” (Moore et al., 2017; Preventing Chronic Disease).\r\nBesides, I intentionally created the indicators for missing data: - return NA if all indicators are NA for mets.sum: when I summed up all 5 MetS indicators, if all are NA, so sum should be NA (not 0)\r\n- ind.non_missing: return TRUE if none of indicators are none-NA, otherwise FALSE\r\nFinally, mets.ind (MetS indicator) would be binary of 1 and 0 if sum of 5 indicators above greater or equal to 3.\r\n\r\nnhanes.MetS <- nhanes.vbls %>%\r\n  mutate(waist.ind = case_when(waist >= 88  & female == 1 ~ 1, #condition 1\r\n                               waist >= 102 & female == 0 ~ 1, #condition 2\r\n                               is.na(waist) | is.na(female) ~ NA_real_, #condition 3\r\n                               TRUE ~ 0), #all other cases\r\n         tri.ind = case_when(tri >= 150 | chol.prsn == 1 ~ 1,\r\n                             is.na(tri) ~ NA_real_,\r\n                             TRUE ~0),\r\n         hdl.ind = case_when((hdl < 40 & female == 0) | chol.prsn == 1 ~ 1,\r\n                             (hdl < 50 & female == 1) | chol.prsn == 1 ~ 1,\r\n                             is.na(hdl) | is.na(female) ~ NA_real_,\r\n                             TRUE ~ 0),\r\n         bp.ind = case_when(sys >= 130 | dia >= 85 | bp.prsn == 1 ~ 1,\r\n                            is.na(sys) & is.na(dia) ~ NA_real_,\r\n                            TRUE ~ 0\r\n                            ),\r\n         glu.ind = case_when(glu >= 100 | glu.prsn == 1 ~ 1,\r\n                             is.na(glu) ~ NA_real_,\r\n                             TRUE ~ 0)\r\n         ) # 10175 obs\r\n\r\n\r\n\r\nnhanes.MetS$mets.sum = rowSums(nhanes.MetS[20:24], na.rm = T) # from waist.ind to glu.ind # 20:24\r\nnhanes.MetS$mets.sum[rowSums(!is.na(nhanes.MetS[20:24])) == 0] <- NA # return NA if all indicators are NA\r\nnhanes.MetS$ind.non_missing <- !rowSums(!is.na(nhanes.MetS[20:24])==0) # create an indicator in which none of criteria is none NA\r\nnhanes.MetS$mets.ind <- ifelse(nhanes.MetS$mets.sum >= 3, 1, 0) # 10175 obs\r\n\r\nHere we can get the sense of data for all indicators created: \r\nDefine sub-population of interest\r\nage of 20+ Reasons for the consistency in comparison:Ford, Giles, and Dietz (2002) used NHANES III, 1988 to 1994 based on 2001 Adult Treatment Panel III (ATP III) criteria (same as Moore et al.) with age groups.\r\n\r\nSmiley, King, and Bidulescu (2019) did not specify the age range but they used the same criteria for adults (e.g. “Metabolic syndrome was defined according to NCEP ATPIII (National Cholesterol Education Program Adult Treatment Panel III) criteria.”).\r\ninAnalysis1: age of 20+ and none of missing of MetS indicatorinAnalysis2: age of 20+ and none of indicators are none-NA\r\n\r\n\r\nnhanes.MetS <- nhanes.MetS %>% \r\n  mutate(\r\n    #  Define sub-population of interest \r\n    inAnalysis1 = (age >=20 & !is.na(mets.ind)),\r\n    inAnalysis2 = (age >=20 & ind.non_missing)\r\n    ) # 10175 obs\r\n\r\n\r\nsaveRDS(nhanes.MetS, file = \"data/nhanesMetS13_14snd.rds\")\r\n\r\nOur sample size would be if I subset data on:\r\ninAnlysis1: 5649\r\ninAnlysis2: 2587\r\nMetS proportion in the random sample (unweighted)\r\nWith all age groups\r\n\r\nnhanes.MetS %>%\r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n6940\r\n\r\n\r\n68.2\r\n\r\n\r\n1\r\n\r\n\r\n2184\r\n\r\n\r\n21.5\r\n\r\n\r\nNA\r\n\r\n\r\n1051\r\n\r\n\r\n10.3\r\n\r\n\r\nWith all age groups & none missing of MetS indicator\r\n\r\nnhanes.MetS %>%\r\n  filter(!is.na(mets.ind)) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n6940\r\n\r\n\r\n76.1\r\n\r\n\r\n1\r\n\r\n\r\n2184\r\n\r\n\r\n23.9\r\n\r\n\r\nWith age 20+ & none missing of MetS indicator\r\n\r\nnhanes.MetS %>% \r\n  filter(age>=20 & !is.na(mets.ind)) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n3511\r\n\r\n\r\n62.2\r\n\r\n\r\n1\r\n\r\n\r\n2138\r\n\r\n\r\n37.8\r\n\r\n\r\nWith age 20+ and ind.non_missing is TRUE:\r\n\r\nNote: ind.non_missing was created an indicator in which none of criteria is none NA\r\n\r\n\r\nnhanes.MetS %>% \r\n  filter(age>=20 & ind.non_missing) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n1273\r\n\r\n\r\n49.2\r\n\r\n\r\n1\r\n\r\n\r\n1314\r\n\r\n\r\n50.8\r\n\r\n\r\nMetS prevalence with weighted\r\nNote:  \r\npsu = SDMVPSU, \r\nstrata = SDMVSTRA, \r\npersWeight = WTINT2YR, \r\npersWeightMEC= WTMEC2YR\r\nApply weights then subset data\r\n\r\n# Define survey design for overall dataset \r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\n# Create a survey design object for the subset of interest \r\n# Subsetting the original survey design object ensures we keep the design information about the number of clusters and strata\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1       36 0.8\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      7.0 1.0\r\n30-39  30-39     21.6 1.0\r\n40-49  40-49     30.0 1.3\r\n50-59  50-59     45.7 2.1\r\n60-69  60-69     59.8 2.8\r\n70+      70+     67.2 1.4\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     30.1 1.1\r\n1         1     39.0 1.1\r\n2         2     34.1 1.6\r\n3         3     23.4 2.8\r\n4         4     28.3 3.9\r\n\r\nCycle 2015/2016\r\nI processed the same procedures as the cycle 2013/2014. Then saved as RDS file. Here I just showed the prevalence after reading the RDS data into R using weights.\r\n\r\nnhanes.MetS <- readRDS(file = \"data/nhanesMetS15_16.rds\")\r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1     37.7 1.8\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      9.2 1.5\r\n30-39  30-39     19.3 1.4\r\n40-49  40-49     33.2 3.2\r\n50-59  50-59     48.1 2.5\r\n60-69  60-69     61.7 2.4\r\n70+      70+     66.7 2.5\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     34.0 1.8\r\n1         1     40.1 2.4\r\n2         2     34.0 1.2\r\n3         3     23.9 2.8\r\n4         4     43.9 4.1\r\n\r\nCycle 2017/2018\r\n\r\nnhanes.MetS <- readRDS(file = \"data/nhanesMetS17_18.rds\")\r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1     38.2 1.3\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      9.0 1.5\r\n30-39  30-39     18.4 1.6\r\n40-49  40-49     31.1 1.8\r\n50-59  50-59     49.2 3.4\r\n60-69  60-69     60.5 2.8\r\n70+      70+     71.3 1.4\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     34.5 1.7\r\n1         1     40.3 1.7\r\n2         2     33.5 1.7\r\n3         3     32.1 2.3\r\n4         4     42.5 4.4\r\n\r\nSummary\r\n\r\nCycle <- c(\"2013-2014\",\"2015-2016\",\"2017-2018\")\r\nPrevalence <- c(36, 37.7, 38.2)\r\nyear.prl <- as.tibble(data.frame(Cycle, Prevalence))\r\n\r\nAge.Categories <- c('20-29','30-39','40-49','50-59','60-69','70+')\r\nPrevalence.Cycle.13_14 <- c(7,21.6,30,45.7,59.8,67.2)\r\nPrevalence.Cycle.15_16 <- c(9.2,19.3,33.2,48.1,61.7,66.7)\r\nPrevalence.Cycle.17_18 <- c(9,18.4,31.1,49.2,60.5,71.3)\r\nage.g.p <- as.tibble(data.frame(Age.Categories,Prevalence.Cycle.13_14,Prevalence.Cycle.15_16,Prevalence.Cycle.17_18))\r\nage.g.p.l <- gather(age.g.p, Prevalence, Percent, Prevalence.Cycle.13_14:Prevalence.Cycle.17_18)\r\n\r\nRace.Ethnic.Categories <- c('Hispanic','White','Black','Asian','Multi-Racial')\r\nPrevalence.Cycle.13_14 <- c(34.0,40.1,34.0,23.9,43.9)\r\nPrevalence.Cycle.15_16 <- c(34.0,40.1,34.0,23.9,43.9)\r\nPrevalence.Cycle.17_18 <- c(34.5,40.3,33.5,32.1,42.5)\r\nr.e.g.p <- as.tibble(data.frame(Race.Ethnic.Categories,Prevalence.Cycle.13_14,Prevalence.Cycle.15_16,Prevalence.Cycle.17_18))\r\nr.e.g.p.l <- gather(r.e.g.p, Prevalence, Percent, Prevalence.Cycle.13_14:Prevalence.Cycle.17_18)\r\n\r\n\r\nyear.prl %>% ggplot(aes(x = Cycle, y = Prevalence)) + \r\n  geom_point() +\r\n  geom_line(aes(group=1)) +\r\n  geom_text(label=paste0(Prevalence,\"%\"), hjust=1.5, vjust=0) +\r\n  ylim(30, 45) +\r\n  theme_bw()\r\n\r\n\r\n\r\nage.g.p.l %>% ggplot(aes(fill=Prevalence, y=Percent, x=Age.Categories)) + \r\n  geom_bar(position=\"dodge\", stat=\"identity\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nr.e.g.p.l %>% ggplot(aes(fill=Prevalence, y=Percent, x=Race.Ethnic.Categories)) + \r\n  geom_bar(position=\"dodge\", stat=\"identity\") +\r\n  theme_bw()\r\n\r\n\r\nMetabolic syndrome at the cycle 2013-2014 was close to Smiley, King, and Bidulescu (2019) (NHANES 2013-2014 with 31.5% in which the authors defined MetS using the criteria established by Lee, Gurka and DeBoer (Expert Rev. Cardiovasc. Ther, 2016), which takes into account racial and age-specific differences in populations, resulting in a complex formula to determine a score so they perhaps got the lower MetS cases), and Moore, Chaudhary, and Akinyemiju (2017) reported 34.2% in 2007–2012 (also applying in NHANES data).\r\nMetabolic syndrome is on the rise.\r\nThe rise emphasized on the older age groups.\r\nMore recently, Asian groups rose the most in metabolic syndrome.\r\nReferences\r\n\r\n\r\nAlberti, K. G. M. M., Robert H. Eckel, Scott M. Grundy, Paul Z. Zimmet, James I. Cleeman, Karen A. Donato, Jean-Charles Fruchart, W. Philip T. James, Catherine M. Loria, and Jr. Smith Sidney C. 2009. “Harmonizing the Metabolic Syndrome: A Joint Interim Statement of the International Diabetes Federation Task Force on Epidemiology and Prevention; National Heart, Lung, and Blood Institute; American Heart Association; World Heart Federation; International Atherosclerosis Society; and International Association for the Study of Obesity.” Journal Article. Circulation 120 (16): 1640–45. https://doi.org/10.1161/CIRCULATIONAHA.109.192644.\r\n\r\n\r\nFord, Earl S., Wayne H. Giles, and William H. Dietz. 2002. “Prevalence of the Metabolic Syndrome Among US Adults: Findings from the Third National Health and Nutrition Examination Survey.” Journal Article. JAMA : The Journal of the American Medical Association 287 (3): 356–59. https://doi.org/10.1001/jama.287.3.356.\r\n\r\n\r\nMoore, Justin Xavier, Ninad Chaudhary, and Tomi Akinyemiju. 2017. “Metabolic Syndrome Prevalence by Race/Ethnicity and Sex in the United States, National Health and Nutrition Examination Survey, 1988-2012.” Journal Article. Preventing Chronic Disease 14: E24–24. https://doi.org/10.5888/pcd14.160287.\r\n\r\n\r\nSmiley, Abbas, David King, and Aurelian Bidulescu. 2019. “The Association Between Sleep Duration and Metabolic Syndrome: The NHANES 2013/2014.” Journal Article. Nutrients 11 (11): 2582. https://doi.org/10.3390/nu11112582.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-09-mets-using-nhanesdata/distill-preview.png",
    "last_modified": "2021-06-16T13:11:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-26-Mplus-as-a-knitr-engine/",
    "title": "Set up to run Mplus inside Rmarkdown",
    "description": "Mplus as a knitr engine in Rmarkdown  \nMplusAutomation: a brief guide",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Mplus",
      "R",
      "MplusAutomation",
      "Tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nGoals\r\nSet up Mplus in Rmarkdown\r\nGentle introduction to MplusAutomation\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nLatent Profile Analysis using Mplus\r\nRun direct in Rmarkdown\r\nUsing runModels\r\n\r\nCompare the model fit\r\n\r\n\r\nGoals\r\nThis post I aim to set up the Mplus engine using knitr package in Rmarkdown.\r\nAfter that, I run a simple model example to demonstrate how Mplus works within Rmarkdown. (Therefore, I do not intent to explain the model results and fit here.)\r\nSet up Mplus in Rmarkdown\r\nGlobally we can change the engine interpreters for one/multiple engine(s), we applied for Mplus:\r\n\r\nknitr::opts_chunk$set(engine.path = list(\r\n  mplus = \"C:/Program Files/Mplus/Mplus\"\r\n))\r\n\r\nNext step, we register a Mplus custom language engine:\r\n\r\nknitr::knit_engines$set(mplus = function(options) {\r\n    code <- paste(options$code, collapse = \"\\n\")\r\n    fileConn<-file(\"formplus.inp\")\r\n    writeLines(code, fileConn)\r\n    close(fileConn)\r\n    out  <- system2(\"C:/Program Files/Mplus/Mplus\", \"formplus.inp\")\r\n    fileConnOutput <- file(\"formplus.out\")\r\n    mplusOutput <- readLines(fileConnOutput)\r\n    knitr::engine_output(options, code, mplusOutput)\r\n})\r\n\r\nFor more language engines, we can refer demos from Yihui Xie’s post.\r\nFurther reading, we may read a Rich Jones’ post on rpubs\r\nGentle introduction to MplusAutomation\r\nThe MplusAutomation package for R (Hallquist and Wiley 2018):\r\nCreating many similar syntax files:\r\nSimulations with different sample sizes\r\nExcluding different parts of a sample\r\n\r\nRunning batches of input files\r\nExtracting and tabulating model parameters and test statistics.\r\nFour core routines support these aims:\r\ncreateModels\r\nrunModels\r\nreadModels\r\ncompareModels\r\nThe MplusAutomation package can be installed within R using the following call:\r\n\r\nif (!require(MplusAutomation)) install.packages(\"MplusAutomation\")\r\nlibrary(MplusAutomation)\r\n\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nI use Edgar Anderson’s Iris Data with 150 cases (rows) and 5 variables named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.\r\niris data set gives the measurements in cm of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris: Iris setosa, versicolor, and virginica.\r\n\r\n?iris\r\n\r\n\r\nhead(iris)\r\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1          5.1         3.5          1.4         0.2  setosa\r\n2          4.9         3.0          1.4         0.2  setosa\r\n3          4.7         3.2          1.3         0.2  setosa\r\n4          4.6         3.1          1.5         0.2  setosa\r\n5          5.0         3.6          1.4         0.2  setosa\r\n6          5.4         3.9          1.7         0.4  setosa\r\n\r\nprepareMPlusData() function prepares a data file in the MPlus’ format, namely, a tab-separated .dat file with no column names.\r\n\r\nprepareMplusData(iris[, -5], \"iris.dat\", inpfile=TRUE)\r\n\r\nLatent Profile Analysis using Mplus\r\nWe can directly run in Rmarkdown or apply runModels to run the .inp files.\r\nRun direct in Rmarkdown\r\nOne for which we estimate different means between 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:33 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n*** WARNING in MODEL command\r\n  All variables are uncorrelated with all other variables within class.\r\n  Check that this is what is intended.\r\n   2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -488.915  253358           2\r\n            -488.915  68985            17\r\n            -488.915  76974            16\r\n            -488.915  573096           20\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       13\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -488.915\r\n          H0 Scaling Correction Factor      0.9851\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                    1003.830\r\n          Bayesian (BIC)                  1042.968\r\n          Sample-Size Adjusted BIC        1001.825\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         0.991\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.003    0.997\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   0.995    0.005\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1      5.216    0.000\r\n    2    -13.816    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n Means\r\n    X1                 5.006      0.049    102.032      0.000\r\n    X2                 3.423      0.055     61.909      0.000\r\n    X3                 1.471      0.026     55.788      0.000\r\n    X4                 0.250      0.016     15.938      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nLatent Class 2\r\n\r\n Means\r\n    X1                 6.265      0.068     92.358      0.000\r\n    X2                 2.873      0.034     85.125      0.000\r\n    X3                 4.911      0.085     57.798      0.000\r\n    X4                 1.678      0.043     38.643      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.685      0.175     -3.924      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.810E-03\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -741.018\r\n          2 Times the Loglikelihood Difference            504.205\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             18.069\r\n          Standard Deviation                               25.180\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           484.852\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:33:15\r\n        Ending Time:  22:33:16\r\n       Elapsed Time:  00:00:01\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means between the 2 profiles and the model is specified to estimate the correlation (or covariance) for the variables\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n\r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:33 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      x1 WITH x2-x4;\r\n      x2 WITH x3-x4;\r\n      x3 WITH x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n   1 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -296.448  533738           11\r\n            -296.448  68985            17\r\n            -296.448  unperturbed      0\r\n            -296.448  27071            15\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       19\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -296.448\r\n          H0 Scaling Correction Factor      1.0304\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                     630.895\r\n          Bayesian (BIC)                   688.097\r\n          Sample-Size Adjusted BIC         627.966\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         1.000\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1     11.903    0.000\r\n    2    -12.706    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 5.006      0.049    101.442      0.000\r\n    X2                 3.428      0.053     64.589      0.000\r\n    X3                 1.462      0.024     60.137      0.000\r\n    X4                 0.246      0.015     16.674      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nLatent Class 2\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 6.262      0.066     94.947      0.000\r\n    X2                 2.872      0.033     86.743      0.000\r\n    X3                 4.906      0.082     59.719      0.000\r\n    X4                 1.676      0.042     39.652      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.693      0.173     -4.002      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.149E-04\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -379.915\r\n          2 Times the Loglikelihood Difference            166.934\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             11.054\r\n          Standard Deviation                                9.531\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           160.527\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:33:16\r\n        Ending Time:  22:33:17\r\n       Elapsed Time:  00:00:01\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means and the model is specified to different covariances (and variable variances) between the 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n\r\n    %c#1%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\n\r\n\r\nThe disadvantage is that Rmarkdown shows a lengthy output, making “eye strain” to track the necessary information.\r\n\r\nUsing runModels\r\nSave each chunk of the following models (either in a .txt file or in MPlus style using a .inp file type), then run with runModels\r\n\r\n# Model 1\r\nrunModels(\"2-iris-LPA_means.inp\")\r\n\r\nRunning model: 2-iris-LPA_means.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means.inp\" \r\n# Model 2\r\nrunModels(\"2-iris-LPA_means_correlated.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated.inp\" \r\n# Model 3\r\nrunModels(\"2-iris-LPA_means_correlated_free_variances.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated_free_variances.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated_free_variances.inp\" \r\n\r\n\r\nm1 <- readModels(\"2-iris-LPA_means.out\")\r\nReading model:  2-iris-LPA_means.out \r\nm2 <- readModels(\"2-iris-LPA_means_correlated.out\")\r\nReading model:  2-iris-LPA_means_correlated.out \r\nm3 <- readModels(\"2-iris-LPA_means_correlated_free_variances.out\")\r\nReading model:  2-iris-LPA_means_correlated_free_variances.out \r\n\r\nCompare the model fit\r\nNow, we inspect the fit statistics and other summary information for the three models:\r\n\r\nm1$summaries$BIC\r\n[1] 1042.968\r\nm2$summaries$BIC\r\n[1] 688.097\r\nm3$summaries$BIC\r\n[1] 574.018\r\n\r\nAnd examine parameters:\r\n\r\nm1$parameters[[1]][-nrow(m1$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1        Means    X1 5.006 0.049 102.032    0           1\r\n2        Means    X2 3.423 0.055  61.909    0           1\r\n3        Means    X3 1.471 0.026  55.788    0           1\r\n4        Means    X4 0.250 0.016  15.938    0           1\r\n5    Variances    X1 0.328 0.042   7.853    0           1\r\n6    Variances    X2 0.121 0.017   7.347    0           1\r\n7    Variances    X3 0.459 0.063   7.340    0           1\r\n8    Variances    X4 0.123 0.013   9.126    0           1\r\n9        Means    X1 6.265 0.068  92.358    0           2\r\n10       Means    X2 2.873 0.034  85.125    0           2\r\n11       Means    X3 4.911 0.085  57.798    0           2\r\n12       Means    X4 1.678 0.043  38.643    0           2\r\n13   Variances    X1 0.328 0.042   7.853    0           2\r\n14   Variances    X2 0.121 0.017   7.347    0           2\r\n15   Variances    X3 0.459 0.063   7.340    0           2\r\n16   Variances    X4 0.123 0.013   9.126    0           2\r\nm2$parameters[[1]][-nrow(m2$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1      X1.WITH    X2 0.113 0.019   5.805    0           1\r\n2      X1.WITH    X3 0.305 0.050   6.104    0           1\r\n3      X1.WITH    X4 0.114 0.019   6.112    0           1\r\n4      X2.WITH    X3 0.098 0.022   4.359    0           1\r\n5      X2.WITH    X4 0.056 0.010   5.330    0           1\r\n6      X3.WITH    X4 0.193 0.024   8.175    0           1\r\n7        Means    X1 5.006 0.049 101.442    0           1\r\n8        Means    X2 3.428 0.053  64.589    0           1\r\n9        Means    X3 1.462 0.024  60.137    0           1\r\n10       Means    X4 0.246 0.015  16.674    0           1\r\n11   Variances    X1 0.331 0.042   7.870    0           1\r\n12   Variances    X2 0.120 0.016   7.574    0           1\r\n13   Variances    X3 0.460 0.063   7.333    0           1\r\n14   Variances    X4 0.123 0.013   9.137    0           1\r\n15     X1.WITH    X2 0.113 0.019   5.805    0           2\r\n16     X1.WITH    X3 0.305 0.050   6.104    0           2\r\n17     X1.WITH    X4 0.114 0.019   6.112    0           2\r\n18     X2.WITH    X3 0.098 0.022   4.359    0           2\r\n19     X2.WITH    X4 0.056 0.010   5.330    0           2\r\n20     X3.WITH    X4 0.193 0.024   8.175    0           2\r\n21       Means    X1 6.262 0.066  94.947    0           2\r\n22       Means    X2 2.872 0.033  86.743    0           2\r\n23       Means    X3 4.906 0.082  59.719    0           2\r\n24       Means    X4 1.676 0.042  39.652    0           2\r\n25   Variances    X1 0.331 0.042   7.870    0           2\r\n26   Variances    X2 0.120 0.016   7.574    0           2\r\n27   Variances    X3 0.460 0.063   7.333    0           2\r\n28   Variances    X4 0.123 0.013   9.137    0           2\r\nm3$parameters[[1]][-nrow(m3$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se  pval LatentClass\r\n1      X1.WITH    X2 0.097 0.022   4.469 0.000           1\r\n2      X1.WITH    X3 0.016 0.010   1.655 0.098           1\r\n3      X1.WITH    X4 0.010 0.004   2.486 0.013           1\r\n4      X2.WITH    X3 0.011 0.008   1.418 0.156           1\r\n5      X2.WITH    X4 0.009 0.005   1.763 0.078           1\r\n6      X3.WITH    X4 0.006 0.003   2.316 0.021           1\r\n7        Means    X1 5.006 0.049 101.439 0.000           1\r\n8        Means    X2 3.428 0.053  64.591 0.000           1\r\n9        Means    X3 1.462 0.024  60.132 0.000           1\r\n10       Means    X4 0.246 0.015  16.673 0.000           1\r\n11   Variances    X1 0.122 0.022   5.498 0.000           1\r\n12   Variances    X2 0.141 0.033   4.267 0.000           1\r\n13   Variances    X3 0.030 0.007   4.222 0.000           1\r\n14   Variances    X4 0.011 0.003   3.816 0.000           1\r\n15     X1.WITH    X2 0.121 0.027   4.467 0.000           2\r\n16     X1.WITH    X3 0.449 0.070   6.377 0.000           2\r\n17     X1.WITH    X4 0.166 0.026   6.282 0.000           2\r\n18     X2.WITH    X3 0.141 0.033   4.330 0.000           2\r\n19     X2.WITH    X4 0.079 0.015   5.295 0.000           2\r\n20     X3.WITH    X4 0.286 0.031   9.107 0.000           2\r\n21       Means    X1 6.262 0.066  94.948 0.000           2\r\n22       Means    X2 2.872 0.033  86.743 0.000           2\r\n23       Means    X3 4.906 0.082  59.719 0.000           2\r\n24       Means    X4 1.676 0.042  39.652 0.000           2\r\n25   Variances    X1 0.435 0.059   7.332 0.000           2\r\n26   Variances    X2 0.110 0.017   6.442 0.000           2\r\n27   Variances    X3 0.675 0.086   7.822 0.000           2\r\n28   Variances    X4 0.179 0.018  10.148 0.000           2\r\n\r\nOne last thing, I want to mention about the createModels() which can creates a set of models using a template. We can save file as mplus_iris_lpa_template.txt\r\n[[init]]\r\niterators = classes;\r\nclasses = 1:9;\r\nfilename = \"[[classes]]-iris-LPA.inp\";\r\noutputDirectory = the_dir;\r\n[[/init]]\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1 x2 x3 x4;\r\n\r\n    Classes = c([[classes]]) ;\r\n\r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    [x1-x4];\r\n\r\n            \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nHere is an example that would create models with different numbers of profiles, from 1 to 9.\r\n\r\n#Set the folder containing mplus.txt as working\r\ncreateModels(\"mplus_iris_lpa_template.txt\")\r\n\r\n#Set the folder containing input files as working directory\r\nrunModels()\r\n\r\n#Set the folder containing output files as working directory\r\nmodels_list <- readModels()\r\noutputs<-extractModelParameters()\r\n\r\nFurther reading, we can look at a Josh Rosenberg’s post and the Mplus website\r\nMore information about the MplusAutomation package in GitHub.\r\n\r\n\r\nHallquist, M. N., and J. F. Wiley. 2018. “MplusAutomation: An r Package for Facilitating Large-Scale Latent Variable Analyses in Mplus.” Journal Article. Struct Equ Modeling 25 (4): 621–38. https://doi.org/10.1080/10705511.2017.1402334.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-26T22:33:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/",
    "title": "A Note on 5-day Workshop on Mplus ~ Day 1",
    "description": "A Crash Course on Social Psychology Research from Michael Zyphur: Regression - Pathway analysis - Model fit  \nCombination of using Mplus and R  \nDrawing a pathway graph/causal graph using `DiagrammeR` package",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Mplus",
      "Pathway/Causal Graph"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nCourse outline\r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nA variance \\(\\sigma^2\\)\r\nGraphical\r\nMultivariate Parameters\r\nCovariance \\(\\sigma_{xy}\\)\r\nA Linear Regression Model\r\nIn Terms of Distributions?\r\nWhy Regression? Causality\r\nControlling for variables\r\nWith Path Diagrams\r\nSummary\r\n\r\nPart 2: Mplus and Estimation\r\nData\r\nVariable\r\nDefine\r\nAnalysis\r\nEstimation and Inference\r\nModel\r\nOutput, Savedata, Plot, Montecarlo\r\nSummary\r\n\r\nPart 3: Path Analysis\r\nHistory\r\nShow case\r\nStructural Model Specification\r\nModel diagram\r\nReport\r\n\r\nWhat Does This Mean? How to Specify It?\r\nSummary\r\n\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nML Estimator\r\nModel Build\r\nModel Diagnostics\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nAbsolute Fit: SRMR\r\nAbsolute Fit: RMSEA\r\nRelative Fit: CFI\r\nRelative Fit: TLI/NNFI\r\nInformation Criteria: AIC\r\nInformation Criteria: BIC\r\n\r\nModification Indices\r\nBayesian Estimates of Fit\r\nPosterior Predictive Checking\r\nDeviance Information Criterion\r\n\r\nModel Selection\r\n\r\nFurther Readings\r\nPart 1\r\nPart 2\r\nPart 3\r\nPart 4\r\n\r\n\r\nMotivation\r\nWhile working on project Harmony at MRC-IHRP, my primary task is building an R package to build the tables and plots from alignment analysis designed for the multi-factor categorical case by extracting the Mplus output’s information. Since I have a chance to expose myself to Mplus, why don’t I self-learn a new tool and share it on my blog?\r\nThe Mplus and I see that a change to dive into Social Psychology Research. What on earth? The same concepts with statistics but a whole new world of terminology!!!\r\nThese can separate into five parts (equivalent to 5 days, then I divided into five topics) of taking notes on Structural Equation and Multilevel Modeling in the Mplus workshop of Prof. Michael Zyphur.\r\nHere’s a first part (1) of the so-called ‘A Crash Course on Social Psychology Research.’\r\n\r\nCourse outline\r\nStructural Equation and Multilevel Modeling in Mplus\r\nDay 1 - Introducing Mplus\r\nRegression, Covariation, and Statistical Models\r\nMplus and Parameter Estimation\r\nPath Analysis\r\nModel Fit and Model Selection\r\n\r\nDay 2 - Path Analysis\r\nMediation\r\nInstrumental Variable Methods\r\nModeration\r\nModerated Mediation\r\n\r\nDay 3 - SEM\r\nLatent Variables\r\nConfirmatory Factor Analysis\r\nStructural Equation Modeling\r\nModel Identification\r\n\r\nDay 4 - MLM\r\nMultilevel Data and Regression\r\nMultilevel Path Analysis\r\nMultilevel Confirmatory Factor Analysis and Structural Equation Modeling\r\nRandom Slopes\r\n\r\nDay 5 - LGM\r\nLongitudinal Data and Processes\r\nLatent Growth Models as Multilevel Models\r\nLatent Growth Models as Structural Equation Models\r\nDynamic Latent Growth Modeling\r\n\r\nThe material can be downloaded here\r\nReminder:\r\n  - Path analysis: regression for observed variables  \r\n  - CFA: regression from latent --> observed variables  \r\n  - SEM: regression among latent variables  \r\n  - Multilevel models: regression at multiple 'levels'  \r\n  - Letant growth: model change with latent variables  \r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nParameters depend on the distributions we model\r\nAssuming a normal distribution for y, we have:\r\nMean \\(\\mu_y\\): a location parameter\r\nVariance \\(\\sigma^2_y\\) : a scale parameter\r\n\r\n\r\np <- seq(-5,5,by=0.1)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(.2)), aes(colour = \"mu=0,sigma2=.2\")) + \r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(1.0)), aes(colour = \"mu=0,sigma2=1.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(5.0)), aes(colour = \"mu=0,sigma2=5.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=-2, sd=sqrt(2.0)), aes(colour = \"mu=-2,sigma2=2\")) +\r\n  scale_y_continuous(limits=c(0,1.0)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Normal Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\nA variance \\(\\sigma^2\\)\r\nTells us the spread of scores along a variable\r\nTo estimate a variance, we assume variation is ‘random’ or due to ‘chance’\r\nThis allows using distributions & their rules\r\nEither way, a variable varies (by definition)\r\nTo model a variable y, we model its variation\r\nTo speak of a variable is to speak of variance!\r\n\r\nGraphical\r\nWe’ll show models with diagrams\r\nSquares and rectangles are observed variables\r\nVariance is double-headed arrow on one variable\r\nFor concision, we will be selective about what we diagram\r\n\r\n\r\nTo refer to y’s variance in Mplus, we will write “y”\r\nMultivariate Parameters\r\nFor multiple variables x and y:\r\nMultivartiate mean vector \\(\\mu\\) for x and y\r\nCovariance \\(\\Sigma\\) as matrix of 2 variances and a covariance\r\n\r\n\r\n#generate the data\r\ngibbs<-function (n, rho) {\r\n    mat <- matrix(ncol = 2, nrow = n)\r\n    x <- 0\r\n    y <- 0\r\n    mat[1, ] <- c(x, y)\r\n    for (i in 2:n) {\r\n        x <- rnorm(1, rho * y, (1 - rho^2))\r\n        y <- rnorm(1, rho * x, (1 - rho^2))\r\n        mat[i, ] <- c(x, y)\r\n    }\r\n    mat\r\n}\r\nbvn <- gibbs(10000, 0.98)\r\n\r\n#setup\r\nlibrary(rgl) # plot3d, quads3d, lines3d, grid3d, par3d, axes3d, box3d, mtext3d\r\nlibrary(car) # dataEllipse\r\n\r\n#process the data\r\nhx <- hist(bvn[,2], plot=FALSE)\r\nhxs <- hx$density / sum(hx$density)\r\nhy <- hist(bvn[,1], plot=FALSE)\r\nhys <- hy$density / sum(hy$density)\r\n\r\n## [xy]max: so that there's no overlap in the adjoining corner\r\nxmax <- tail(hx$breaks, n=1) + diff(tail(hx$breaks, n=2))\r\nymax <- tail(hy$breaks, n=1) + diff(tail(hy$breaks, n=2))\r\nzmax <- max(hxs, hys)\r\n\r\n#Basic scatterplot on the floor\r\n## the base scatterplot\r\nplot3d(bvn[,2], bvn[,1], 0, zlim=c(0, zmax), pch='.',\r\n       xlab='X', ylab='Y', zlab='', axes=FALSE)\r\npar3d(scale=c(1,1,3))\r\n\r\n#Histograms on the back walls\r\n## manually create each histogram\r\nfor (ii in seq_along(hx$counts)) {\r\n    quads3d(hx$breaks[ii]*c(.9,.9,.1,.1) + hx$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            rep(ymax, 4),\r\n            hxs[ii]*c(0,1,1,0), color='gray80')\r\n}\r\nfor (ii in seq_along(hy$counts)) {\r\n    quads3d(rep(xmax, 4),\r\n            hy$breaks[ii]*c(.9,.9,.1,.1) + hy$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            hys[ii]*c(0,1,1,0), color='gray80')\r\n}\r\n\r\n#Summary Lines\r\n## I use these to ensure the lines are plotted \"in front of\" the\r\n## respective dot/hist\r\nbb <- par3d('bbox')\r\ninset <- 0.02 # percent off of the floor/wall for lines\r\nx1 <- bb[1] + (1-inset)*diff(bb[1:2])\r\ny1 <- bb[3] + (1-inset)*diff(bb[3:4])\r\nz1 <- bb[5] + inset*diff(bb[5:6])\r\n\r\n## even with draw=FALSE, dataEllipse still pops up a dev, so I create\r\n## a dummy dev and destroy it ... better way to do this?\r\n###dev.new()\r\nde <- dataEllipse(bvn[,1], bvn[,2], draw=FALSE, levels=0.95)\r\n###dev.off()\r\n\r\n## the ellipse\r\nlines3d(de[,2], de[,1], z1, color='green', lwd=3)\r\n\r\n## the two density curves, probability-style\r\ndenx <- density(bvn[,2])\r\nlines3d(denx$x, rep(y1, length(denx$x)), denx$y / sum(hx$density), col='red', lwd=3)\r\ndeny <- density(bvn[,1])\r\nlines3d(rep(x1, length(deny$x)), deny$x, deny$y / sum(hy$density), col='blue', lwd=3)\r\n\r\n#Beautifications\r\ngrid3d(c('x+', 'y+', 'z-'), n=10)\r\nbox3d()\r\naxes3d(edges=c('x-', 'y-', 'z+'))\r\noutset <- 1.2 # place text outside of bbox *this* percentage\r\nmtext3d('P(X)', edge='x+', pos=c(0, ymax, outset * zmax))\r\nmtext3d('P(Y)', edge='y+', pos=c(xmax, 0, outset * zmax))\r\n\r\n\r\nCovariance \\(\\sigma_{xy}\\)\r\nTells us how two variables ‘hang together’\r\nCorrelation is just a standardized covariance\r\n\r\nIf we assume variation is random …\r\nCovariance implies a non-causal relationship\r\nNo predictor or outcome, x and y simply co-vary\r\nBlue eyes & blond hair?\r\n\r\n\r\n\r\nIn Mplus we will write “y with x” or “x with y” If the relationship is non-causal, does it matter?\r\n\r\nA Linear Regression Model\r\nWhat is the rule \\(\\beta\\) ? Our linear model\r\n\\[ y_i = \\nu + \\beta x_i + \\epsilon_i \\]\r\n\\(y\\) and \\(x\\) are variables\r\n\\(\\epsilon\\) is a variable: unobserved residual/error\r\n\\(\\nu\\) and \\(\\beta\\) are constants/fixed that we estimate\r\nIn Terms of Distributions?\r\ny is decomposed into parts\r\nHow is the mean of \\(y\\) modeled?\\[ \\nu = \\mu_y — \\beta \\mu_x \\]\r\nHow is \\(y\\),\\(x\\) covariance modeled? \\[ \\beta = \\frac{\\sigma_{xy}}{\\sigma^2_x} \\]\r\nHow is the variance of \\(y\\) modeled? \\[  \\sigma^2_y =  \\sigma^2_y R^2 + \\sigma^2_{\\epsilon} \\]\r\nWhy Regression? Causality\r\nCausal statements are helpful for action\r\nConditions for \\(x \\rightarrow y\\) causality\r\n\\(x\\) precedes \\(y\\) in time\r\n\\(x\\) and \\(y\\) are related\r\n\\(x \\rightarrow y\\) effect isn’t due to a third variable z\r\n\r\nRegression assesses the second, and helps with the third\r\nControlling for variables\r\nRemoving the effect of \\(z\\) in \\(x \\rightarrow y\\) effect is\r\nControlling for \\(z\\)\r\nHolding \\(z\\) constant (it no longer varies)\r\nThe ‘independent effect’ of \\(x\\) on \\(y\\)\r\nWe ‘partial out’ the effect of \\(z\\)\r\n\r\nThese all mean the same thing:\r\nWe somehow make \\(z\\) irrelevant for \\(x \\rightarrow y\\)\r\nAllows estimating independent effects\r\n\\[ y_i = \\nu + \\beta_1 x_i + \\beta_2 z_i+ \\epsilon_i \\]\r\nEffects are independent and additive\r\nRead it left to right: \\(y\\) depends ON \\(x\\) \\(z\\)\r\nIn Mplus simply write “y ON x z”\r\nDoes it matter if “y ON x z” or “y ON z x” ?\r\nEach \\(\\beta\\) is just a slope, and \\(\\nu\\) is intercept on y-axis\r\nWith Path Diagrams\r\nRegression effects & residual as single-headed arrows\r\n\r\nResidual indicated as single-headed arrow w/out predictor\r\nRegress “y ON x z” reads the equation left-to-right\r\nNote: Predictors correlated by default in regression\r\nThe whole point is that the predictors are correlated\r\n\r\nSummary\r\nWe estimate rules/parameters to test theory\r\nMeans, (co)variances, & causal effects\r\n\r\nMplus has a simply language for these\r\nName of variable “y” refers to variance\r\n“With” refers to covariance\r\n“On” refers to a regression slope\r\n\r\nWe use multiple predictors to control for each (i.e., we estimate independent effects)\r\nPart 2: Mplus and Estimation\r\nMplus Team\r\nBengt Muthen\r\nFormer president of psychometrics society\r\nPh.D. in stats (Utrecht)\r\nAdviser: Joreskög\r\n\r\nLinda Muthen\r\nPh.D. in education (UCLA)\r\n\r\nTihomir Asparouhov\r\nPh.D. in stats (Cal Tech)\r\n\r\nStatmodel.com has a message board!\r\nstatmodel.com/discussion/messages/board-topics.html\r\n\r\nMplus Overview\r\nSolves for unknowns in regression equations:\r\nAll observed/manifest variables\r\nPath Analysis and traditional regression\r\n\r\n\r\nContinuous latent variables\r\nSEM (latent variables are factors)\r\nMultilevel/random effects/hierarchical linear models\r\nSurvival and other analyses with latent frailties/liabilities\r\n\r\nDiscrete latent variables (i.e., categorical)\r\nLatent profile analysis: continuous observed variables\r\nLatent class (cluster) analysis: discrete observed variables\r\nVarious forms of finite mixture models (LTA/Markov)\r\n\r\nMplus Features\r\nObserved variables\r\nContinuous: normal, skew-normal, t-distributed, skew-t, censored\r\nCategorical & Count: ordered, nominal, zero-inflated\r\n\r\nLinking functions: Identity, logit, probit\r\nEstimators\r\nULS, WLS, GLS, ML, MLR, MLF, BAYES, etc…\r\n\r\nSimulation with Monte Carlo & Bootstrap\r\nRead the manual – many examples!\r\nMplus Caveat\r\nInput is done by hand\r\nThere is a diagrammer, but we want a stick shift!\r\n\r\nVery little input is needed\r\nThis means many default options are used\r\n\r\nEach part of Mplus input has defaults\r\nCheck the manual, read model descriptions, look at output & parameter matrices (\\(TECH_1\\) option)\r\n\r\nExample\r\nTitle: \r\n    Start input section with the heading and a colon, then type specific commands/options and end each statement with a semi-colon;\r\n    * To ignore a command line, start it with an asteriks\r\nData: \r\n    File is data.dat;\r\nVariable: \r\n    Names are employ salary education height weight;\r\n    Usevariables are employ salary height education;\r\nAnalysis:\r\n    Estimator = ML;\r\nModel:\r\n    Employ salary education on height;\r\nOutput:\r\n    Standardized;\r\nInput Command Headings\r\nTitle: Ch. 15\r\nData: Ch. 15\r\nVariable: Ch. 15\r\nDefine: Ch. 15\r\nAnalysis: Ch. 16\r\nModel: Ch. 17\r\nModel Indirect, Model Constraint, Model Priors\r\n\r\nOutput: Ch. 18\r\nSavedata: Ch. 18\r\nPlot: Ch. 18\r\nMonteCarlo: Ch. 19\r\nData\r\nIndicates location of datafile\r\nStore in same folder as Mplus input file\r\n\r\nIndicates format of data\r\nIndividual data or summary data\r\nCorrelations, SDs, means\r\n\r\nDefault is individual data\r\n\r\nMplus requires numeric data only\r\nDo not save data with variable names\r\nFor missing data I use -999\r\nCan save from SPSS, Stata, Excel, etc…\r\nMust be tab-delimited or CSV\r\nEach variable is a column, separated by tabs or commas\r\nMissing data are -999\r\nWhere are the variables names?\r\n\r\n\r\nContains\r\nMplus input files\r\n\r\nIndividual example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Individual Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is individual.dat;\r\nAnalysis: \r\nEstimator = ml;\r\n!Estimator = Bayes;\r\nVariable: \r\nnames are y x1 x2;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1 TECH8;\r\n\r\nSummary example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Summary Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is summary.dat;\r\nType = means stdeviations correlation;\r\nNobservations = 500;\r\nVariable: \r\nnames are y x1 x2;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1;\r\n\r\nIndividual dataset\r\nIndividual.dat\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/individual.dat', n = 20), sep = '\\n')\r\n   -0.354517     0.573051    -0.175230\r\n    0.561655    -0.368095     1.090042\r\n    0.315551    -0.577052     0.425472\r\n    3.347049     1.088520     1.149353\r\n   -0.122389    -0.694153    -0.766538\r\n   -0.251276    -0.017487    -1.367410\r\n   -0.517996    -0.817974    -1.559255\r\n    1.888854    -0.658335     1.007614\r\n    0.461254     0.463916    -0.898300\r\n    2.237483     1.533398     0.180512\r\n    0.480991    -0.096545    -0.352276\r\n    0.165901    -1.341994    -1.445909\r\n    1.864947     1.027419     0.677408\r\n   -0.466245    -0.138712    -0.759287\r\n    2.567804     0.483444     0.959731\r\n   -0.024201    -0.507631    -0.517296\r\n   -1.912698     0.761720    -1.901134\r\n   -1.350069    -0.736562     2.318569\r\n    0.433773     0.723880     0.111837\r\n   -0.977083     0.155868    -0.897112\r\n\r\nSummary dataset\r\nSummary.dat\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/summary.dat'), sep = '\\n')\r\n.485    .001    -.042\r\n1.552   1.046   .978\r\n1.0\r\n.665    1.0\r\n.427    .028    1.0\r\n\r\n\r\nNote: Great! We can get the parameter estimates from the data with means, standard deviations and correlation matrix besides the `individual data. Furthermore, we can simulate data from ‘summary’ information (using Monte Carlo methods).\r\nOther options allowed (see manual)\r\nVariable\r\nAll info about our variables\r\nNames are arbitrary, but the number of names must match the number of columns in data file\r\nNames are y x1 x2;\r\n\r\nType of variable distribution\r\nContinuous normal assumed, but you can say other\r\n\r\nWhich variables will we use?\r\nUsevariables are y x1 x2;\r\n\r\nWeighting variables\r\nMany other options\r\n\r\nDefine\r\nCreates new variables that can be used\r\nYou must list them with Usevariables are\r\n\r\nLogical & arithmetic operators & functions\r\nConditional statements (to recode)\r\nIF ___ then ___;\r\n\r\nTransformations of various kinds\r\nCentering (grand-mean or group-mean)\r\nCluster_Mean, Sum, Mean, Cut\r\n\r\nDefine: x1x2 = x1*x2;\r\nAnalysis\r\nOptions\r\nType = nature of model desired\r\nType = General is default\r\nType = Twolevel or Threelevel means multilevel\r\nType = Random implies random slopes\r\n\r\n\r\nEstimators, Algorithms\r\nEstimator = ML, or Estimator = Bayes\r\n\r\nBootstrapping\r\nLinking functions (logit or probit)\r\nMany options for ML and Bayes estimation\r\nEstimation and Inference\r\nFrequentist: Estimation uses data & model\r\nTypical estimators, such as OLS or ML\r\nIterative process trying to find best parameter estimates\r\nIteratively alters estimates to find the ‘most likely’ values\r\nThis means probability of data is being maximized (i.e., ML)\r\n\r\nConvergence achieved when estimates change very little\r\nMplus gives the estimates, SE, p-values, & CIs upon request\r\nBut, even ‘most likely’ estimates may not be very likely\r\nYour model and its estimates may be the best of a bad lot!\r\nThis is why we’ll look at fit statistics\r\n\r\nBayes: Estimation uses data, model, & prior prob.\r\nDefault is ‘uninformative prior’, so results # ML\r\nIterative process that computes ‘posterior probabilities’\r\nIt uses MCMC estimation in at least two separate ‘chains’\r\nThe estimates produce a posterior distribution for each parameter\r\n\r\nConvergence achieved when chains agree (PSR < 1.05)\r\nMedian/mode of each distribution is like an ML estimate\r\nThe SD is like the SE\r\nGives Bayesian p-value and ‘credibility interval’\r\n\r\nAgain, bad models/estimates may agree across chains\r\nModel\r\nModel: ON Command\r\nON refers to a regression slope \\(\\beta\\)\r\ny ON x; \\[ y_i = \\nu + \\beta_1 x_i + \\epsilon_i \\]\r\nRegress y on x\r\nReads the equation from left to right\r\n\r\n\r\nThink of “y ON x” as \\(x \\rightarrow y\\)\r\nFreely estimates a \\(\\beta\\)\r\nThe single-headed arrow from nowhere is a residual\r\n\r\n\r\nModel: WITH Command\r\nWITH refers to covariance \\(\\Theta\\) or \\(\\Psi\\)\r\ny1 WITH y2;\r\nEstimates covariance among y1 and y2\r\n\r\nWhat if y1 and y2 are dependent variables?\r\nThen we estimate a residual covariance\r\n\r\n\r\n\r\nModel: BY Command\r\nBY refers to factor loadings (slopes)\r\nf1 BY y1 y2 y3;\r\nlatent variable “f1” is indicated by y1, y2, y3\r\n\r\nHuh BY y1 y2 y3\r\nlatent variable is called “Huh”… name is irrelevant\r\n\r\nFreely estimate \\(\\lambda\\) for each observed variable\r\nExcept first loading is fixed to 1.0 by default\r\n\r\n\r\n\r\nModel notation\r\nTypical SEM notation for vectors/matrices\r\n\\(B\\): matrix of regression coefficients, elements \\(\\beta\\)\r\n\\(\\Lambda\\): matrix of factor loadings, elements \\(\\lambda\\)\r\n\\(\\eta\\): vector of latents (factors/random effects)\r\n\\(\\Psi\\): matrix of latent (co)variances, elements ψ\r\n\\(\\Theta\\): matrix of observed (co)variances, elements \\(\\theta\\)\r\n\\(\\nu\\): intercepts, \\(\\alpha\\) is latent intercepts or means\r\n\r\nAll commands will tell Mplus to either:\r\nFreely estimate an element in vector/matrix\r\nThis is what our Model commands do\r\n\r\nFix an element in vector/matrix to some value\r\nMplus defaults often do this for us\r\n\r\nMplus notation for freeing and fixing estimates\r\ny ON x;\r\n* freely-estimated \\(\\beta\\)y ON x@.5;\r\n* \\(\\beta\\) is NOT estimated, but fixed to .5[y@0];\r\n* an intercept for y1 constrained to 0.0f1 BY y*;\r\n* * = freely estimate, so estimates factor loading for y on factor “f1”f1 BY y1-y5@1;\r\n* factor loadings for “f1” = 1 for variables y1, y2, y3, y4, y5\r\nLabeling and constraining/fixing estimates\r\nLabels are put in parenthesesy ON x (b1);\r\nNow, the slope is called “b1” and can be used later\r\n\r\nIf parameters have the same label, it’s like they’re the same thingy ON x z (b2);\r\nNow x\\(\\rightarrow\\)y and z\\(\\rightarrow\\)y slopes are constrained to equality\r\nMplus will estimate them but keep them equal\r\n\r\nModel Constraint\r\nHere we play with labeled parameters\r\nAllows linear/non-linear model constraints\r\nb1+b2=0;\r\n\r\nPlay with parameters labeled in Model command\r\n\r\nToo much creativity to describe, but:\r\nCan create new “phantom” parameters\r\nSee manual for the “New” command\r\nCheck example 5.21\r\n\r\n\r\nTITLE:     this is an example of a two-group twin\r\n           model for  continuous outcomes using parameter constraints\r\n\r\nDATA:      FILE = ex5.21.dat;\r\n\r\nVARIABLE:  NAMES = y1 y2 g;\r\n           GROUPING = g(1 = mz 2 = dz);\r\n\r\nMODEL:     [y1-y2]    (1);\r\n           y1-y2      (var);\r\n           y1 WITH y2 (covmz);\r\n\r\nMODEL dz:  y1 WITH y2 (covdz);\r\n\r\nMODEL CONSTRAINT:\r\n           NEW(a c e h);\r\n           var = a**2 + c**2 + e**2;\r\n           covmz = a**2 + c**2;\r\n           covdz = 0.5*a**2 + c**2;\r\n           h = a**2/(a**2 + c**2 + e**2);\r\nModel Priors\r\nFor Bayes, we can specify prior probabilities\r\nThese are distributions…\r\nModel:y2 ON y1 (b1);\r\nModel Priors:b1~N(.25, 1);\r\nWe say “b1” is distributed as (\\(\\sim\\)) normal (\\(N\\)) with mean and variance (\\(\\mu, \\sigma^2\\)) of .25 and 1\r\nMplus has defaults that are ‘diffuse priors’\r\nEg, regression coefficients are \\(\\sim N(0,10^10)\\)\r\n\r\nModel Indirect\r\nComputes mediation effects to show\r\n“Decomposition” of total and indirect effects\r\nUse bootstrapping in Analysis command with ML to get bootstrapped indirect effects\r\nBayesian estimation gives distribution of effects, so no bootstrapping required\r\n\r\nOutput, Savedata, Plot, Montecarlo\r\nOutput\r\nTECH, Standardized\r\n\r\nSavedata\r\nEstimated data: factor scores, co(var) matrices\r\n\r\nPlot\r\nHelpful for various models\r\n\r\nMonteCarlo\r\nData generation facility… allows parametric bootstrap\r\nMakes running simulations a snap\r\nEmpirically-derived estimates of power\r\nCan publish from this if you’re inclined\r\n\r\n\r\nSummary\r\nOur job is to\r\nTell Mplus about data and estimation method\r\nSpecify a statistical model to reflect our theory\r\nUse ML or Bayes to estimate parameters\r\nUse results to make inferences\r\nPart 3: Path Analysis\r\nHistory\r\nDeveloped by Sewell Wright in 1918 (1921)\r\nGeneticist modeling relations of family members\r\nBrother Philip borrows to create ‘simultaneous equations’ with IVs in 1928 (S&D of flaxseed)\r\n\r\nTaken up in health, biological, and social sciences to model complex relationships\r\nSubsumes simultaneous equation analysis\r\n\r\nShow case\r\nStructural Model Specification\r\nSpecify causal model of interest\r\nGrandey & Cropanzano (1999). The conservation of resources model applied to work–family conflict and strain. Journal of Vocational Behavior, 54, 350-370.\r\n\r\nCausal effects (regression)\r\nWork role stress \\(\\rightarrow\\) Job distress\r\nWork role stress \\(\\rightarrow\\) Work-family conflict\r\nWork-family conflict \\(\\rightarrow\\) Job distress\r\nJob distress \\(\\rightarrow\\) Turnover intentions\r\nJob distress \\(\\rightarrow\\) Life distress\r\n\r\nStochastic, non-causal relationships\r\nTurnover intentions\\(\\leftrightarrow\\)Life distress\r\n\r\nModel diagram\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>>]\r\nTI->LD[dir=both, label=<&psi;>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>>]\\nTI->LD[dir=both, label=<&psi;>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nVariable:\r\nNames are WRS FRS WFC FWC JD FD LD TI PPH SE;\r\nUsevariables are WRS TI WFC JD LD;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOR\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nTI WFC on WRS; LD on WFC;\r\n\r\nNote: for 2 dependent variables which are not predicting anything else, just the outcomes. Mplus will automatically covariance for us. If not estimate the residual covariance would only hurt the model fit. In other words, if there is a residual covariance there and we do not estimat them, it would drive the model fit down.\r\nCode in Mplus:\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS TI WFC JD LD;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS; ! left-side: dependent vbl; right-side: predictors\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\nNote: here’s again we use the ‘summary’ data from the article instead of ‘individual’ data (don’t have it!!!)\r\nMplus output\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.out'), sep = '\\n')\r\nMODEL RESULTS ! Unstandardized Output\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000 ! 1-unit increase on JD, .536 increase on TI\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880 ! Residual covariances\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.141E-02\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nSTANDARDIZED MODEL RESULTS \r\n\r\n\r\nSTDYX Standardization ! Standardized Output (STDYX)\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.075      0.092      0.813      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.376      0.073      5.178      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.490      0.066      7.408      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTDY Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.094      0.115      0.814      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.472      0.089      5.279      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.615      0.078      7.844      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTD Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nR-SQUARE\r\n\r\n    Observed                                        Two-Tailed\r\n    Variable        Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n    TI                 0.234      0.065      3.631      0.000\r\n    WFC                0.240      0.065      3.704      0.000\r\n    JD                 0.421      0.065      6.436      0.000\r\n    LD                 0.595      0.054     10.947      0.000\r\n\r\n\r\nTECHNICAL 1 OUTPUT\r\n\r\n     PARAMETER SPECIFICATION\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  0             0             0             0             0\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             0             0             0\r\n WFC                0             0             0             0             0\r\n JD                 0             0             0             0             0\r\n LD                 0             0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0\r\n WFC                0             0\r\n JD                 0             0             0\r\n LD                 0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  1             2             3             4             0\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             5             0             6\r\n WFC                0             0             0             0             7\r\n JD                 0             8             0             0             9\r\n LD                 0            10            11             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                12\r\n WFC                0            13\r\n JD                 0             0            14\r\n LD                15             0             0            16\r\n WRS                0             0             0             0             0\r\n\r\n     STARTING VALUES\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                0.000         0.000         0.000         0.000         0.000\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             1.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         1.000         0.000         0.000         0.000\r\n JD             0.000         0.000         1.000         0.000         0.000\r\n LD             0.000         0.000         0.000         1.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         1.000\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000\r\n WFC            0.000         0.000\r\n JD             0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                2.120         3.430         2.520         2.730         2.500\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         0.000         0.000         0.000         0.000\r\n JD             0.000         0.000         0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.490\r\n WFC            0.000         0.530\r\n JD             0.000         0.000         0.328\r\n LD             0.000         0.000         0.000         0.387\r\n WRS            0.000         0.000         0.000         0.000         0.635\r\n\r\nWhat’s \\(R^2\\) for our variables?\r\n(1 – standardized residual variance)\r\nCan also just look at bottom of output\r\nReport\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>=.63/.49**>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>=.09/.08>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>=.38/0.38**>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>=.30/.38**>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>=.19/.22**>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>=.54/.44**>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>=.68/.63**>]\r\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>=.63/.49**>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>=.09/.08>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>=.38/0.38**>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>=.30/.38**>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>=.19/.22**>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>=.54/.44**>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>=.68/.63**>]\\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nFor example: \\(\\beta_7\\) = .68/.63**, i.e. unstandardized/standardized both Y and X estimates and significance of p-value\r\nWhat Does This Mean? How to Specify It?\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (no covariance).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\n!Estimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nTI with LD@0;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model1).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nWRS on WFC JD TI;\r\nWFC on JD LD;\r\nJD on TI LD;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model2).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nWRS with WFC;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\nSummary\r\nPath models specify regression/covariation among observed variables\r\nRegression represents causality ON\r\nSo reduces magnitude of DV’s variance\r\n\r\nCovariation simply indicates WITH\r\nSo does not account for variance (there is no IV/DV)\r\n\r\nWe can draw and specify any diagram\r\nMplus then gives us parameter estimates\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nLots of literature & many indices for ML estimator\r\nIssue of Personality & Individual Differences (2007), lead by Barrett\r\nMany cite Hu & Bentler (1999)\r\n\r\nAbsolute fit\r\nHow well does our model explain our observed data?\r\n\r\nRelative/Incremental/Comparative fit\r\nCompares fit of different models using same dataset\r\n\r\nInformation Criteria\r\nMeasure info entropy, or info lost when using an estimated model rather than the data\r\n\r\nBayesians have less work on the topic …\r\nIn statistics programs we get fit from:\r\nAnalysis/Estimated model (\\(H_0\\) in Mplus output)\r\nModel we specify and estimate\r\n\r\nBaseline/Independence/Null model\r\nWorst fitting model (or at least all covariances = 0)\r\n\r\nUnrestricted/Saturated (\\(H_1\\) in Mplus output)\r\nBest fitting model (all parameters freely estimated)\r\n\r\n\r\nFor comparing models that we estimate:\r\nNull model (\\(H_0\\)): more-constrained model\r\nFewer parameters estimated, so more parsimonious\r\n\r\nAlternate model (\\(H_1\\)): less-constrained model\r\n\r\nML Estimator\r\nInformation we have vs. what’s estimated\r\nHow many parameters implied by our data?\r\nk means, k variances, and k\\(\\times\\)(k-1)/2 covariances\r\n\r\nHow many parameters are estimated?\r\nIntercepts, (residual) variances, slopes/covariances\r\n\r\n\r\nThe difference is our degrees of freedom (df)\r\nWhat about data/model fit ?\r\nFor ML, we have model log Likelihood (LL)\r\nIt turns out, -2\\(\\times\\)LL is \\(\\chi^2\\) distributed\r\nThe difference between two \\(\\chi^2\\) values is \\(\\chi^2\\) distributed\r\n\r\n\r\nModel Build\r\nAnalysis/Estimated model (\\(H_0\\))\r\nUnrestricted/Saturated/Alternate model (\\(H_1\\))\r\n\r\n\r\nModel:TI on JD WRS;LD on JD WFC;JD on WRS WFC;WFC on WRS;\r\nModel:JD LD TI WRS WFC with JD LD TI WRS WFC;\r\nBaseline/Null Model\r\n\r\nModel:\r\nModel Diagnostics\r\nContrasting Models\r\nChi-square testing of nested models\r\nCan use change in relative fit indices\r\nNon-nested model comparisons possible with AIC/BIC\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nUnrestricted vs Analysis model gives us \\(\\chi^2\\)\r\n\\(df\\) is difference in estimated model & unrestricted\r\n\r\nIf our model is good, the difference should be small\r\nConducting NHST with the \\(\\chi^2\\)\r\nWhat’s the null vs. alternate hypothesis?\r\n\r\nIssues\r\nSensitive to sample size\r\nRarely taken seriously… unless it’s non-significant!\r\nWe want to “accept the null” ???\r\n\r\nIf you want to look good on a relative basis… ??\r\nUsing \\(\\chi^2\\) to Compare Estimated Models\r\nGet \\(\\chi^2\\) and \\(df\\) for different estimated models\r\nSubtract \\(\\chi^2\\) and subtract \\(df\\), use diff. for NHST: \\(\\Delta \\chi^2\\) or \\(-2\\times LL\\)\r\nRecall the null is model with fewer parameters\r\n\r\nModel must be nested: null must be subset of alt.\r\nAbsolute Fit: SRMR\r\nStandardized Root Mean Square Residual\r\nStandardized difference between observed and model-implied data\r\nResidual in this case is the difference\r\n\r\nLess than .05 is good, Hu & Bentler say that .08 isn’t too bad…\r\nAbsolute Fit: RMSEA\r\nRoot mean squared error of approximation\r\nLike an adjusted root mean square standardized residual\r\n\r\nTakes model parsimony into account\r\nPenalized for estimating too many parameters\r\nPuts a positive value on df\r\n\r\n\\[ \\frac{\\sqrt{\\chi^2_{Estimated} - df_{Estimated}}}{\\sqrt{df\\times (N-1)}}\\]\r\nLess than .06 or .07 good (CI usually given)\r\nSteiger (2007), Understanding the limitations of global fit…\r\n\r\nRelative Fit: CFI\r\nComparative Fit Index\r\nCompares \\(\\chi^2\\) of estimated model to \\(\\chi^2\\) of baseline\r\nAdjusts for \\(df\\) to reward parsimony\r\n\r\n\\[ \\frac{\\sqrt{(\\chi^2_{Baseline} - df_{Baseline}) - (\\chi^2_{Estimated} - df_{Estimated})}}{\\sqrt{\\chi^2_{Baseline} - df_{Baseline}}}\\]\r\n.95 or higher generally accepted as cut-off\r\nRelative Fit: TLI/NNFI\r\nTucker-Lewis Index/Non-Normed Fit Index\r\nSimilar to CFI but different penalization\r\n\\[ \\frac{\\chi^2_{Baseline}/df_{Baseline} - \\chi^2_{Estimated}/df_{Estimated}}{\\chi^2_{Baseline}/ df_{Baseline} - 1}\\]\r\nWill always be smaller than CFI\r\nTo some decimal place\r\n\r\n.95 often thought of as cut-off\r\nInformation Criteria: AIC\r\nAkaikes Information Criterion\r\nk is number of estimated parameter\r\nA weighting of accuracy versus model complexity\r\nOnly useful for comparing two estimated models\r\nLower values are better\r\n\\[ \\chi^2_{Estimated} + k\\times (k-1) - 2\\times df_{Estimated} \\]\r\nInformation Criteria: BIC\r\nBayesian Information Criterion & Adjusted BIC\r\nSimilar to AIC, except weights for sample size\r\nOnly useful for comparing two estimated models\r\nSmaller values are better—BIC, then aBIC here:\r\n\\[ \\chi^2_{Estimated} + ln(N)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\n\\[ \\chi^2_{Estimated} + ln((N+2)/24)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\nFor both AIC and BIC, no significance tests… see recommendations in interpreting differences\r\nThese are all Global Measures\r\nThey collapse across all model parts\r\nSome authors call for separate fit measures\r\nWhatever… fit is necessary but insufficient\r\nThe model must make theoretical sense\r\n\r\nModification Indices\r\nMODINDICES command in Mplus\r\nIndicate change in model fit by estimating additional parameters\r\nHeated debates here\r\nFor frequentists this capitalizes on chance\r\nCan lead to nonsensical models, bias/variance tradeoff\r\n\r\nUseful for understanding sources of misfit\r\nLet’s try it with our path model…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (ML) modindices.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8 MODINDICES(1);\r\n\r\nBayesian Estimates of Fit\r\nNot as well developed\r\nBayes was unpopular for much of \\(20^{th}\\) Century\r\n\r\nLess obvious how to proceed\r\nThe idea of sampling from a population not formalized in the same way (so no \\(\\chi^2\\))\r\n\r\nMplus offers a few that are useful…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (Bayes).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = Bayes;\r\nfbiterations=10000;\r\nProcessors=2;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\nPosterior Predictive Checking\r\nSteps required during model estimation\r\nSample parameters from posteriors\r\n\r\nGenerate data with parameters\r\n\r\nEstimate fit of generated vs. observed data\r\n\r\n\r\nIf model is good, generated data should fit better/worse about half the time\r\nGet Predictive Posterior p-value (PPP)\r\nHow often the generated data fit better\r\nPPP < .05 bad, PPP ≈ .50 good, PPP > .95 weird\r\n\r\nDeviance Information Criterion\r\nLike other information criteria\r\nPenalizes for model complexity, rewards fit\r\nSmaller values = less deviance (i.e., better fit)\r\nCan compare models with different priors\r\nModel Selection\r\nHow do we choose the models we publish?\r\nSelecting models\r\nCan build them based on scripted steps9,10\r\nGive your model of interest a shot and look at fit\r\nIf that doesn’t work, try different theory-driven models\r\nLook at sources of misfit and sort out what’s going on\r\n\r\nIs this an ethical issue?\r\nScientific models are serious things\r\nPeople/groups use results for their own purposes\r\nWhen considering which model to estimate, consider the social & political implications\r\n\r\nFurther Readings\r\nPart 1\r\nGrace, J. B., & Bollen, K. A. (2005). Interpreting the results from multiple regression and structural equation models. Bulletin of the Ecological Society of America, 86, 283-295.\r\nCohen, Cohen, West, & Aiken (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (3rd). Lawrence Erlbaum & Associates.\r\nMyers, Well, & Lorch (2010). Research Design and Statistical Analysis (3rd). Routledge Academic.\r\nTabachnick & Fidell (2006). Using Multivariate Statistics (5th). Allyn & Bacon.\r\nFisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London, Series A 222, 309–368.\r\nPart 2\r\nMplus User’s Guide and Diagrammer Documentation\r\nPart 3\r\nStreiner, D. L. 2005. Finding Our Way: An Introduction to Path Analysis. Canadian Journal of Psychiatry, 50, 115-122.\r\nWright, S. (1921). Correlation and causation. J. Agricultural Research 20: 557–585.\r\nWright, S. (1934). The method of path coefficients. Annals of Mathematical Statistics 5: 161–215.\r\nAngrist, J. D., & Krueger, A. B. (2001). Instrumental variables and the search for identification: From supply and demand to natural experiments. Journal of Economics Perspectives, 15: 69-85.\r\nMuthén, B. (2002). Beyond SEM: General latent variable modeling. Behaviormetrika, 29, 81-117.\r\nPart 4\r\nHu & Bentler (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6(1), 1-55.\r\nPersonality and Individual Differences, Volume 42, Issue 5. 2007.\r\nNeyman, J., & Pearson, E. S. 1933. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society, A, 231, 289-337.\r\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111-16\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773-795.\r\nMcDonald, R. P. (2010). Structural models and the art of approximation. Perspectives on Psychological Science, 5, 675-686.\r\nCheung, G. W., & Rensvold, R. B. (1999). Testing factorial invariance across groups: a reconceptualization and proposed new method. Journal of Management, 25, 1–27.\r\nCheung, G. W., & Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. Structural Equation Modeling, 9, 233–255.\r\nAnderson, J. C., & Gerbing, D. W. (1988). Structural equation modeling in practice: A review and recommended two-step approach. Psychological Bulletin, 103, 411-423.\r\nMulaik, S. A., & Millsap, R. E. (2000). Doing the four-step right. Structural Equation Modeling, 7, 36-73. (see other papers in the same issue).\r\nShapin, S. 2008. The scientific life: A moral history of a late modern vocation. Chicago University Press.\r\nPorter, T. M. 1995. Trust in Numbers: The Pursuit of Objectivity in Science and Public Life.\r\nPoovey, M. 1997. A History of the Modern Fact: Problems of Knowledge in the Sciences of Wealth and Society. Chicago.\r\nMcCloskey, D. N. 1992. If you’re so smart: The narrative of economic expertise. University of Chicago Press.\r\nMcCloskey, D. N. 1992. The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives.\r\nAsparouhov, T., Muthén, B. & Morin, A. J. S. 2015. Bayesian structural equation modeling with cross-loadings and residual covariances: Comments on Stromeyer et al. Journal of Management, 41, 1561-1577.\r\n\r\n\r\n",
    "preview": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/distill-preview.png",
    "last_modified": "2021-05-26T22:32:56-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/",
    "title": "Fitting a Simple Linear Regression in Bayesian Context",
    "description": "How to fit a linear regression using Bayesian Methods  \nConsider a Bayesian model fit as a remedial measures for influential case",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nFit a model\r\nModel fit diagnostics\r\nMessage take-away\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\nMCMC in JAGS\r\nDescribe the model.\r\nExplore the MCMC object\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nExplore the MCMC object\r\n\r\nUsing fitted regression model for prediction\r\n\r\nCompare the fit between FA and BA\r\nFurther reading\r\n\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nI use the data in Kruschke (2015) which has\r\n- male\r\n- height\r\n- weight\r\nWe will use height to predict weight of a person\r\n\r\nDT::datatable(dta, \r\n          rownames = FALSE,\r\n          filter = list(position = \"top\"))\r\n\r\n{\"x\":{\"filter\":\"top\",\"filterHTML\":\"<tr>\\n  <td data-type=\\\"integer\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"54.6\\\" data-max=\\\"76\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"89\\\" data-max=\\\"356.8\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[0,0,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1],[64,62.3,67.9,64.2,64.8,57.5,65.6,70.2,63.9,71.1,66.5,68.1,62.9,75.1,64.6,69.2,68.1,72.6,63.2,64.1,64.1,71.5,76,69.7,73.3,61.7,66.4,65.7,68.3,66.9,62.4,64.5,60.6,70.8,61,66,59.6,70.1,66.6,59.8,68.5,61.4,64.7,67.4,68.3,67.3,62.5,72.4,64.4,70.6,66.3,65.9,61.1,67.8,64.4,71.2,64.5,65.4,67,70.9,62.4,70.4,64.6,69.6,65.9,70.6,65.4,73.9,70.1,64.8,59.7,69.2,61.4,70.4,71.1,60.8,68.3,67.4,63.5,66.8,73.4,64.8,68.9,66.5,64.7,66.2,69.1,66,60.9,72.4,67.5,71.6,70.6,64.7,72.7,64.7,67.9,65.9,54.6,64.1,68.3,72.9,64,62.1,67.3,60.4,63.4,63.3,61.4,67.7,65.1,67.5,67,65.6,70.9,61.5,59.4,59.7,69,71.7,65.7,67.2,65.9,64.9,73.6,70.5,63.6,64.7,69.4,69.2,64.5,70.9,63.6,73.7,67.6,65.9,68.1,64.1,61.6,71.2,66.7,71.1,68.6,62.8,67.6,63.1,65.2,67.4,64.3,59.9,61.9,63.4,59.8,67.7,64.5,67.4,66.1,75.1,67.9,65.7,64.9,64.8,55.3,63,64.1,64.3,63.8,65.1,71.8,66.7,64.5,60.7,59.6,68.4,63.2,66,62.5,67.9,70.2,70.3,67.1,66.2,62.1,72.4,65.2,72.1,65.6,67.2,66.2,66,63.6,65,57.5,63.4,67.2,62.8,65.3,66.7,70.3,68.6,66.4,62.9,57.6,65.3,70.9,68,65.4,67.4,62.8,73.3,64.6,70,68.1,69.4,71.9,67.8,63.5,69.8,66.1,62,71.5,71.9,72.6,64.3,71.7,70.1,64.3,63.9,74.6,65.5,63.1,66.7,65.2,62,71,69.4,69.8,66.6,71,74.7,63.6,69.8,58.9,67.9,65.2,70.6,59.8,71.2,71.5,63.7,65.1,64.8,66.8,64.5,68.6,65.4,66.5,68.8,70.4,57,60.6,62.8,73.3,73.2,61.9,66.4,69.9,60.8,65.9,61.8,66.5,64,68.2,71.8,70.4,63,60.1,66.3,67.1,61.5,66.4,63.8,69.9,66.1,66.2,66.5,70.2,64,69,65.7,69.5,64.1,61.2,62.6,67.9,68.5,69.2,65.9,68.3,70.2],[136.4,215.1,173.6,117.3,123.3,96.5,178.3,191.1,158,193.9,127.1,147.9,119,204.4,143.4,124.4,140.9,164.7,139.8,110.2,134.1,193.6,180,155,188.2,187.4,139.2,147.9,178.6,111.1,119.2,184.4,100.1,207.3,159.8,120.7,102.8,195.7,130.1,156.5,113.7,119.1,142.8,179.9,166.3,135.4,118.9,173.9,117.8,192.6,122,129.5,116.9,177.1,160.1,199.5,111,177.4,187.9,177.1,185.3,223.5,128.4,184.4,122.1,216.8,173.8,197.8,181.1,136.6,105,150.1,125,172.2,143.9,132,110.7,155.9,174.9,176.6,167.1,133.5,211.4,150.6,144.7,120.7,222.5,168.4,134.3,182.8,187.2,193.4,195.2,131.1,204.5,108.6,128.1,152.7,120.3,183.5,210.6,163.1,143.9,114.4,170.5,93.2,147.8,161.2,114.6,158,144.5,184,225,116.9,183.2,131.5,217.1,123.6,145.9,170.6,133.5,165.9,134.1,111.1,201.1,156.3,161.5,145,159.8,149,222,149.7,130.6,242.5,150,191.3,164.1,135.6,139.4,114.8,191.6,194.7,170,146.9,186.1,125.8,96.2,156.8,117.6,149.6,125.4,140.7,150.2,156.5,137.6,140.3,174.7,186.1,191.3,135.2,130.5,137.1,166.5,280.5,126,128.3,166.5,124,120.8,193.8,157,190.8,110.9,185.9,163.2,167.9,119.8,122,178.8,181.1,168.8,120.9,96.5,210.4,139.3,187,146.5,141.8,162,173.8,160.7,125.3,125.9,193.7,234.9,156.9,221.2,241.6,170.4,152.6,172.8,176.6,123.5,202.1,182.2,190.9,146.6,158.5,140.5,168,182,275.2,164.1,153.5,159.5,141.7,194.1,149.6,157.5,149.9,210.8,154.5,206.5,123.4,166,160.8,167.6,141.9,186.2,155.7,148.7,132.6,356.8,164.9,187.2,169.6,178.8,202.2,158.9,186.1,169.7,147.6,110.6,146.8,146.2,164.8,115.7,191.5,198.6,143.7,126.7,123.1,231.5,134.7,159.4,141.5,144.5,150.5,173.7,215.5,146.4,133.6,240.2,216.6,89,213.6,211.3,140.4,135.7,151.7,198.3,158.7,218,172.7,161.9,130.3,123,157,131.2,108.7,202.7,131.9,164.5,179.2,154.4,120.1,189.7,160.2,145.7,186.2,144.8,147.4,120.8,134.9,164.8,205.9,172.5,130.8,146.5,173.8]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>male<\\/th>\\n      <th>height<\\/th>\\n      <th>weight<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[0,1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\nFit a model\r\n\r\nfit <- lm(dta$weight ~ dta$height)\r\nsummary(fit)\r\n\r\nCall:\r\nlm(formula = dta$weight ~ dta$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-63.95 -21.17  -5.26  16.24 201.94 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -104.7832    31.5056  -3.326 0.000992 ***\r\ndta$height     3.9822     0.4737   8.406 1.77e-15 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.59 on 298 degrees of freedom\r\nMultiple R-squared:  0.1917,    Adjusted R-squared:  0.189 \r\nF-statistic: 70.66 on 1 and 298 DF,  p-value: 1.769e-15\r\n\r\nThen, we can see the fitted regression line overlay with data\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Scatter Plot between Height and Weight by Gender\",\r\n     pch = as.numeric(dta$male), col = as.factor(dta$male))\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\n\r\nModel fit diagnostics\r\nThere are four assumptions associated with a linear regression model:\r\nLinearity: The relationship between X and the mean of Y is linear.\r\nHomoscedasticity: The variance of residual is the same for any value of X.\r\nIndependence: Observations are independent of each other.\r\nNormality: For any fixed value of X, Y is normally distributed.\r\nI check the model fit by plotting:\r\nplot of residuals against predictor variable (how to check residuals have non-linear patterns or not)\r\nnormal probability plot (how to check residuals are normally distributed: residuals follow a straight line well or do they deviate severely; it’s good if residuals are lined well on the straight dashed line.)\r\nplot of square root of standardized residual absolute value (how to check the assumption of equal variance (homoscedasticity): if residuals are spread equally along the ranges of predictors; it’s good if we see a horizontal line with equally (randomly) spread points.)\r\nAnd to look at the outlier and leverage via\r\nplot standardized residuals vs leverage\r\nResiduals are the difference between the observed score and the predicted score.\r\n\\[ e_i = Y^{obs}_i - \\hat{Y}_i\\]\r\nResiduals come in three varieties:\r\nRaw Residuals: The difference between the raw observed score and the predicted score.\r\nStandardized Residuals: These are the raw residuals divided by the standard error of estimate.\r\nStudentized Residuals: These are raw residuals divided by the standard error of the residual with that case deleted. These are sometimes called studentized deleted residuals or studentized jackknifed residuals.\r\n\r\npar(mfrow=c(2,2))\r\nplot(fit)\r\n\r\n\r\nLastly, we look at the influential points\r\n\r\nn <- dim(dta)[1]\r\ncooksD <- cooks.distance(fit)\r\n#identify influential points\r\n(influential_obs <- as.numeric(names(cooksD)[(cooksD > (4/n))]))\r\n [1]   2 117 134 140 163 164 169 172 212 233 260 263\r\n#plot cooksD\r\nplot(cooksD, pch=\"*\", cex=2, main=\"Influential Obs by Cooks distance\")  \r\nabline(h = 4/n, col=\"red\")  # add cutoff line\r\ntext(x=1:length(cooksD), y=cooksD, labels=ifelse((cooksD>(4/n)),names(cooksD),\"\"), col=\"red\", pos = 4)\r\n\r\n\r\nTill now, we can say that the linear regression assumption is violated in this case, e.g. error is not following the normal distribution. Therefore, how about we delete the influential points and re-fit the model:\r\n\r\ndta.outliers_removed <- dta[-influential_obs, ]\r\n\r\nfit.outliers_removed <- lm(dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\nsummary(fit.outliers_removed)\r\n\r\nCall:\r\nlm(formula = dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-55.16 -18.87  -3.18  16.55  83.27 \r\n\r\nCoefficients:\r\n                             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)                 -155.9116    26.8300  -5.811 1.65e-08 ***\r\ndta.outliers_removed$height    4.7112     0.4032  11.685  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.36 on 286 degrees of freedom\r\nMultiple R-squared:  0.3231,    Adjusted R-squared:  0.3208 \r\nF-statistic: 136.5 on 1 and 286 DF,  p-value: < 2.2e-16\r\n\r\nSomehow, we saw the model assumption is satisfied when the influential cases removed.\r\n\r\npar(mfrow=c(1,2))\r\nplot(fit.outliers_removed, which=c(1,2))\r\n\r\n\r\nThe regression line changes when we remove the influential observations. Dangerous!!!\r\n\r\npar(mfrow=c(1,2))\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\nplot(dta.outliers_removed$height, dta.outliers_removed$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Outliers removed\")\r\nabline(fit.outliers_removed, col = \"orange\", lwd = 3)\r\n\r\n\r\nBut, the action of deleting of influential cases is often not the solution due to produce the bias estimates.\r\nThrough this case, we have reviewed:\r\nInfluential points = Outliers & Leverage\r\nA point that makes a lot of difference in a regression case, is called ‘an influential point.’ Usually influential points have two characteristics:\r\nThey are outliers, i.e. graphically they are far from the pattern described by the other points, that means that the relationship between x and y is different for that point than for the other points.\r\nThey are in a position of high leverage, meaning that the value of the variable x is far from the mean. Observations with very low or very high values of x are in positions of high leverage.\r\nRemind knowledge\r\nDiscrepancy: Difference between the predicted and observed value\r\n- Measured by Studentized Residuals\r\nLeverage: high leverage if it has “extreme” predictor x values\r\n- Measured by Hat Value\r\nInfluence: Assesses how much regression equation would change if an observation/potential outlier was dropped from the analysis\r\n- Measured by Cook’s Distance, Difference in Fit (DFFITS), or Difference in coefficients (DFBETAS)\r\nMessage take-away\r\nRemoving influential cases is not the optimal solution.\r\nIn the real-life analysis, we can use other types of regression:\r\nRidge regression\r\nRobust regression\r\nIRLS Robust regression\r\nLowess method\r\nRegression trees\r\nand etc…\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon \\]\r\nWith Bayesian approach distribution of \\(\\epsilon\\) does not have to be Gaussian (normal), we are going to use robust assumption.\r\nParameterization of the model for MCMC (adapted from Kruschke (2015))Bayes theorem for this model:\r\n\\[\r\np(\\beta_0, \\beta_1, \\sigma, \\gamma \\mid D) = \\frac{p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma)}{\\int \\int \\int \\int p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma) \\ d\\beta_0 \\ d\\beta_1 \\ d\\sigma \\ d\\gamma}\r\n\\]\r\nCreate the data list.\r\n\r\ny <- dta$weight\r\nx <- dta$height\r\ndataList <- list(x = x, y = y)\r\n\r\nMCMC in JAGS\r\nDescribe the model.\r\nBased on the Normal distribution (demonstrating purpose, not run)\r\n\r\nmodstring_norm = \"\r\n# Specify the Normal model for none-standardized data:\r\nmodel {\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ dnorm(mu[i], prec)\r\n        mu[i] = b[1] + b[2]*log_income[i] \r\n    }\r\n    \r\n    for (i in 1:2) {\r\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\r\n    }\r\n    \r\n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\r\n    sig2 = 1.0 / prec\r\n    sig = sqrt(sig2)\r\n} \"\r\n\r\nBased on the Student-t distribution, robust assumption\r\n\r\n modelString = \"\r\n# Standardize the data:\r\ndata {\r\n    Ntotal <- length(y)\r\n    xm <- mean(x)\r\n    ym <- mean(y)\r\n    xsd <- sd(x)\r\n    ysd <- sd(y)\r\n    for ( i in 1:length(y) ) {\r\n      zx[i] <- (x[i] - xm) / xsd\r\n      zy[i] <- (y[i] - ym) / ysd\r\n    }\r\n}\r\n# Specify the model for standardized data:\r\nmodel {\r\n    for ( i in 1:Ntotal ) {\r\n      zy[i] ~ dt( zbeta0 + zbeta1 * zx[i] , 1/zsigma^2 , nu )\r\n    }\r\n    # Priors vague on standardized scale:\r\n    zbeta0 ~ dnorm(0, 1/(10)^2 )  \r\n    zbeta1 ~ dnorm(0, 1/(10)^2 )\r\n    zsigma ~ dunif(1.0E-3, 1.0E+3 )\r\n    nu ~ dexp(1/30.0)\r\n    # Transform to original scale:\r\n    beta1 <- zbeta1 * ysd / xsd  \r\n    beta0 <- zbeta0 * ysd  + ym - zbeta1 * xm * ysd / xsd \r\n    sigma <- zsigma * ysd\r\n}\r\n\"\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"TEMPmodel.txt\")\r\n\r\nIn the tutorial, we just want to execute the model specified by t-student distribution.\r\nEvery arrow has a corresponding line in the descriptive diagram.\r\nVariable names starting with “z” mean that these variables are standardized (z-scores).\r\nThe intention of using z-scores in JAGS is to overcome a problem of correlation of the parameters (as the simulation the correlation between \\(\\beta_0\\) and \\(\\beta_1\\)).\r\nStrong correlation creates thin and long shape on scatter-plot of the variables which makes Gibbs sampling very slow and inefficient.\r\nBut remember to scale back to the original measures.\r\nHMC implemented in Stan does not have this problem. This can be applied to STAN in all situation !!!\r\n\r\nparameters = c(\"beta0\" ,  \"beta1\" ,  \"sigma\", \r\n              \"zbeta0\" , \"zbeta1\" , \"zsigma\", \"nu\")\r\nadaptSteps = 500  # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps=20000\r\nnIter = ceiling((numSavedSteps*thinSteps ) / nChains )\r\njagsModel = jags.model(\"TEMPmodel.txt\", data=dataList ,\r\n                      n.chains=nChains, n.adapt=adaptSteps)\r\nupdate(jagsModel, n.iter=burnInSteps)\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters, \r\n                          n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the MCMC object\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:6500\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 5000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n             Mean       SD  Naive SE Time-series SE\r\nbeta0  -139.96225 27.61760 0.1952859      0.2180717\r\nbeta1     4.46120  0.41330 0.0029225      0.0032290\r\nnu        5.37901  1.62796 0.0115114      0.0236101\r\nsigma    23.97943  1.65132 0.0116766      0.0204363\r\nzbeta0   -0.09632  0.04806 0.0003398      0.0004283\r\nzbeta1    0.49046  0.04544 0.0003213      0.0003550\r\nzsigma    0.68372  0.04708 0.0003329      0.0005827\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%       25%        50%       75%      97.5%\r\nbeta0  -193.3580 -158.7464 -140.16434 -121.3445 -8.624e+01\r\nbeta1     3.6527    4.1830    4.46491    4.7407  5.257e+00\r\nnu        3.1509    4.2578    5.05987    6.1485  9.410e+00\r\nsigma    20.8323   22.8476   23.94254   25.0519  2.728e+01\r\nzbeta0   -0.1888   -0.1282   -0.09682   -0.0644 -4.115e-04\r\nzbeta1    0.4016    0.4599    0.49087    0.5212  5.779e-01\r\nzsigma    0.5940    0.6514    0.68267    0.7143  7.779e-01\r\nplot(codaSamples, trace=TRUE, density=FALSE) # note: many graphs\r\n\r\nautocorr.plot(codaSamples, ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n    beta0     beta1        nu     sigma    zbeta0    zbeta1    zsigma \r\n16054.484 16401.066  4768.900  6535.965 12892.980 16401.066  6535.965 \r\n#gelman.diag(codaSamples)\r\ngelman.plot(codaSamples)   # lines may return error ==> Most likely reason: collinearity of parameters \r\n\r\n(HDIofChains <- lapply(codaSamples, function(z) cbind(Mu = hdi(codaSamples[[1]][,1]), Sd = hdi(codaSamples[[1]][,2]))))\r\n[[1]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[2]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[3]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[4]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\nLook at strong correlation between beta0 and beta1 which slows Gibbs sampling down.\r\n\r\nhead(as.matrix(codaSamples[[1]]))\r\n         beta0    beta1       nu    sigma      zbeta0    zbeta1\r\n[1,] -168.7337 4.890447 5.623225 23.90661 -0.10407253 0.5376553\r\n[2,] -158.6082 4.744414 6.371647 24.56065 -0.09181883 0.5216004\r\n[3,] -166.9662 4.873701 6.346842 22.92533 -0.08537616 0.5358143\r\n[4,] -161.6680 4.795882 3.209357 23.00506 -0.08162923 0.5272588\r\n[5,] -160.5706 4.801212 3.153090 22.15810 -0.04024801 0.5278448\r\n[6,] -155.9331 4.684888 4.394775 21.44653 -0.12823063 0.5150561\r\n        zsigma\r\n[1,] 0.6816415\r\n[2,] 0.7002899\r\n[3,] 0.6536626\r\n[4,] 0.6559359\r\n[5,] 0.6317868\r\n[6,] 0.6114980\r\npairs(as.matrix(codaSamples[[1]])[,1:4])\r\n\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nIn order to give a vague priors to slope and intercept consider the following arguments:\r\nThe largest possible value of slope is\r\n\\[ \\frac{\\sigma_y}{\\sigma_x} \\]\r\nwhen variables \\(x\\) and \\(y\\) are perfectly correlated.\r\nThen standard deviation of the slope parameter \\(\\beta_1\\) should be large enough to make the maximum value easily achievable.\r\nSize of intercept is defined by value of\r\n\\[ E[X] \\frac{\\sigma_y}{\\sigma_x} \\]\r\nSo, the prior should have enough width to include this value.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n    real meanX;\r\n    real sdX;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real expLambda;\r\n    real beta0sigma;\r\n    real beta1sigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    expLambda = 1/30.0;\r\n    beta1sigma = 10*fabs(sdY/sdX);\r\n    beta0sigma = 10*(sdY^2+sdX^2)    / 10*fabs(meanX*sdY/sdX);\r\n}\r\nparameters {\r\n    real beta0;\r\n    real beta1;\r\n    real<lower=0> nu; \r\n    real<lower=0> sigma; \r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi); \r\n    nu ~ exponential(expLambda);\r\n    beta0 ~ normal(0, beta0sigma);\r\n    beta1 ~ normal(0, beta1sigma);\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ student_t(nu, beta0 + beta1 * x[i], sigma);\r\n    }\r\n}\r\n\"\r\n\r\n\r\nstanDsoRobustReg <- stan_model(model_code=modelString) \r\n\r\n\r\ndat<-list(Ntotal=length(dta$weight), \r\n          y=dta$weight, \r\n          meanY=mean(dta$weight),\r\n          sdY=sd(dta$weight),\r\n          x=dta$height,\r\n          meanX=mean(dta$height),\r\n          sdX=sd(dta$height))\r\n\r\n\r\nfitSimRegStan <- sampling(stanDsoRobustReg, \r\n             data=dat, \r\n             pars=c('beta0', 'beta1', 'nu', 'sigma'),\r\n             iter=5000, chains = 4, cores = 4)\r\n\r\nSave the fitted object.\r\nExplore the MCMC object\r\n\r\nprint(fitSimRegStan)\r\nInference for Stan model: 99bd7b261ff9240bf0c6d7b1b21831d6.\r\n4 chains, each with iter=5000; warmup=2500; thin=1; \r\npost-warmup draws per chain=2500, total post-warmup draws=10000.\r\n\r\n          mean se_mean    sd     2.5%      25%      50%      75%\r\nbeta0  -139.35    0.50 27.89  -194.81  -157.61  -139.51  -120.74\r\nbeta1     4.45    0.01  0.42     3.64     4.17     4.45     4.72\r\nnu        5.37    0.03  1.61     3.15     4.25     5.07     6.16\r\nsigma    23.98    0.03  1.66    20.81    22.84    23.96    25.07\r\nlp__  -1265.00    0.02  1.43 -1268.55 -1265.74 -1264.68 -1263.94\r\n         97.5% n_eff Rhat\r\nbeta0   -85.12  3072    1\r\nbeta1     5.28  3117    1\r\nnu        9.39  3420    1\r\nsigma    27.31  3579    1\r\nlp__  -1263.20  3332    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Apr 22 15:10:38 2021.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(fitSimRegStan)\r\n\r\nrstan::traceplot(fitSimRegStan, ncol=1, inc_warmup=F)\r\n\r\npairs(fitSimRegStan, pars=c('nu','beta0','beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','beta1'))\r\n\r\nstan_scat(fitSimRegStan, c('beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('nu','sigma'))\r\n\r\nstan_dens(fitSimRegStan)\r\n\r\nstan_ac(fitSimRegStan, separate_chains = T)\r\n\r\nstan_diag(fitSimRegStan,information = \"sample\",chain=0)\r\n\r\nstan_diag(fitSimRegStan,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"divergence\",chain = 0)\r\n\r\n\r\nWork with shinystan object.\r\n\r\nlaunch_shinystan(fitSimRegStan)\r\n\r\nUsing fitted regression model for prediction\r\nRecall that the data in this example contains predictor height and output weight for a group of people from Ht-Wt.csv (data above).\r\nPlot all heights observed in the sample and check the summary of the variable.\r\n\r\nplot(1:length(dat$x),dat$x)\r\n\r\nsummary(dat$x)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  54.60   64.00   66.20   66.39   69.20   76.00 \r\n\r\nCan we predict weight of a person who is 50 or 80 inches tall?\r\nTo do this we can go through all pairs of simulated parameters (\\(\\beta_0\\), \\(\\beta_1\\)) and use them to simulate \\(y(50)\\) and \\(y(80)\\).\r\nThis gives distribution of predicted values.\r\n\r\nsummary(fitSimRegStan)\r\n$summary\r\n              mean     se_mean         sd         2.5%          25%\r\nbeta0  -139.354196 0.503202352 27.8915373  -194.808900  -157.605829\r\nbeta1     4.451593 0.007474065  0.4173043     3.636611     4.172879\r\nnu        5.366643 0.027556563  1.6115181     3.151177     4.246276\r\nsigma    23.982791 0.027670259  1.6554405    20.814300    22.838675\r\nlp__  -1265.002519 0.024734258  1.4276951 -1268.552672 -1265.738311\r\n               50%          75%        97.5%    n_eff     Rhat\r\nbeta0  -139.512830  -120.737878   -85.121838 3072.271 1.000770\r\nbeta1     4.454489     4.723609     5.282330 3117.396 1.000699\r\nnu        5.069790     6.157253     9.391004 3419.954 1.000994\r\nsigma    23.957637    25.072876    27.313013 3579.322 1.000550\r\nlp__  -1264.682923 -1263.938424 -1263.195323 3331.756 1.000423\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.496987 27.9128445  -195.452362  -159.028578\r\n    beta1     4.468322  0.4176474     3.643375     4.180768\r\n    nu        5.333266  1.6129391     3.146164     4.239934\r\n    sigma    23.996371  1.6756857    20.745408    22.833622\r\n    lp__  -1264.973734  1.3931068 -1268.262683 -1265.705478\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.690732  -121.306242   -85.591653\r\n    beta1     4.473340     4.745248     5.301112\r\n    nu        5.036853     6.074138     9.402810\r\n    sigma    23.966000    25.101448    27.242245\r\n    lp__  -1264.658401 -1263.936454 -1263.201543\r\n\r\n, , chains = chain:2\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.119919 27.7217798  -195.188356  -158.408294\r\n    beta1     4.462118  0.4146886     3.668582     4.177839\r\n    nu        5.396927  1.5988486     3.155521     4.277927\r\n    sigma    23.990640  1.6168350    21.056831    22.846187\r\n    lp__  -1264.946317  1.4072435 -1268.606024 -1265.657309\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.000666  -121.163812   -86.601618\r\n    beta1     4.462406     4.731267     5.288368\r\n    nu        5.129432     6.181671     9.348566\r\n    sigma    23.918413    25.056950    27.345459\r\n    lp__  -1264.644540 -1263.891813 -1263.183147\r\n\r\n, , chains = chain:3\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -137.758043 28.0473599  -193.976570  -156.709899\r\n    beta1     4.428479  0.4196264     3.620966     4.142890\r\n    nu        5.273822  1.5474425     3.130205     4.160287\r\n    sigma    23.905531  1.6471567    20.672028    22.808588\r\n    lp__  -1265.023664  1.3965268 -1268.555138 -1265.759590\r\n         stats\r\nparameter          50%         75%        97.5%\r\n    beta0  -137.963462  -118.57774   -83.786093\r\n    beta1     4.430414     4.70807     5.278970\r\n    nu        4.999793     6.07770     9.087928\r\n    sigma    23.906472    24.98220    27.219559\r\n    lp__  -1264.712841 -1263.99760 -1263.196266\r\n\r\n, , chains = chain:4\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -139.041834 27.8184750  -193.491107  -156.858089\r\n    beta1     4.447452  0.4163603     3.625069     4.186441\r\n    nu        5.462557  1.6789159     3.209953     4.299050\r\n    sigma    24.038623  1.6794892    20.805817    22.868034\r\n    lp__  -1265.066361  1.5085815 -1268.785015 -1265.858500\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -139.288564  -121.639438   -83.861266\r\n    beta1     4.449144     4.707789     5.268918\r\n    nu        5.106526     6.256394     9.576797\r\n    sigma    24.014382    25.132032    27.449061\r\n    lp__  -1264.738186 -1263.933356 -1263.202000\r\nregParam<-cbind(Beta0=rstan::extract(fitSimRegStan,pars=\"beta0\")$'beta0',\r\n                Beta1=rstan::extract(fitSimRegStan,pars=\"beta1\")$'beta1')\r\nhead(regParam)\r\n         Beta0    Beta1\r\n[1,] -174.7224 4.962250\r\n[2,] -118.6827 4.113129\r\n[3,] -152.0820 4.701561\r\n[4,] -147.0165 4.564773\r\n[5,] -159.9549 4.751269\r\n[6,] -126.6841 4.251516\r\npredX50<-apply(regParam,1,function(z) z%*%c(1,50))\r\npredX80<-apply(regParam,1,function(z) z%*%c(1,80))\r\n\r\nPlot both distributions, look at their summaries and HDIs.\r\n\r\nsuppressWarnings(library(HDInterval))\r\nden<-density(predX50)\r\nplot(density(predX80),xlim=c(60,240))\r\nlines(den$x,den$y)\r\n\r\nsummary(cbind(predX50,predX80))\r\n    predX50          predX80     \r\n Min.   : 55.42   Min.   :195.2  \r\n 1st Qu.: 78.48   1st Qu.:212.9  \r\n Median : 83.19   Median :216.8  \r\n Mean   : 83.23   Mean   :216.8  \r\n 3rd Qu.: 87.97   3rd Qu.:220.6  \r\n Max.   :111.77   Max.   :240.3  \r\nrbind(predX50=hdi(predX50),predX80=hdi(predX80))\r\n            lower     upper\r\npredX50  69.09173  97.20656\r\npredX80 205.32725 228.15029\r\n\r\nBoth JAGS and Stan produced the identical results.\r\nCompare the fit between FA and BA\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 2)\r\nabline(a=-139.96225, b= 4.46120, col = \"blue\", lwd = 1)\r\nabline(fit.outliers_removed, col = \"red\", lty=\"dashed\", lwd =1)\r\n\r\n\r\n\r\nWith influential\r\nWithout influential\r\nBayesian approach w/ robust assumption\r\n\r\norange, solid\r\nred, dashed\r\nblue, solid\r\nIntercept\r\n-104.78\r\n-155.91\r\n-139.96\r\nSlope\r\n3.98\r\n4.71\r\n4.46\r\n\r\nGreat! From the comparison, we can see that using Bayesian Methods to fit simple linear regression can be robust when the traditional regression has the influential points.\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., & Rubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition (3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th ed. ed.). Boston: McGraw-Hill Irwin.\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial with r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic Press is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/distill-preview.png",
    "last_modified": "2021-04-23T11:48:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-17-machine learning-discriminant-analysis/",
    "title": "Discriminant Analysis -- A Classification by Maximizing Class Separation",
    "description": "An Gentle Introduction of Discriminant Analysis & Its Applicant",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nLinear Discriminant Analysis\r\nWhy Discriminant Analysis\r\nQuadratic Discriminant Analysis\r\nLDA vs QDA\r\nBuilding our first linear and quadratic discriminant models\r\nCreating the task and learner, and training the LDA model\r\nCreating the task and learner, and training the QDA model\r\n\r\nReferences\r\n\r\nLinear Discriminant Analysis\r\nApproach for multiclass classification.\r\nA discriminant is a function that takes an input vector x and assigns to one of the multiple classes.\r\nModel the distribution of the predictors \\(X\\) in each response class\r\nUse Bayes theorem to flip these around into estimates for\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nWhen the distributions are assumed to be normal, the LDA model is very similar in form to logistic regression.\r\nWhy Discriminant Analysis\r\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\r\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\r\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\r\nQuadratic Discriminant Analysis\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nLDA = \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\sum\\) in each class.\r\nQDA = With Gaussians but different \\(\\sum_k\\) in each class, we get quadratic discriminant analysis.\r\nNOTE = By proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\r\nLDA vs QDA\r\nThe strengths of the LDA and QDA algorithms are:\r\nThey can reduce a high-dimensional feature space into a much more manageable number\r\nCan be used for classification or as a preprocessing (dimension reduction) technique to other classification algorithms that may perform better on the dataset\r\nQDA can learn curved decision boundaries between classes (this isn’t the case for LDA)\r\nThe weaknesses of the LDA and QDA algorithms are:\r\nThey can only handle continuous predictors (although recoding a categorical variable as numeric may help in some cases)\r\nThey assume the data are normally distributed across the predictors. If the data are not, performance will suffer\r\nLDA can only learn linear decision boundaries between classes (this isn’t the case for QDA)\r\nLDA assumes equal covariances of the classes and performance will suffer if this isn’t the case (this isn’t the case for QDA)\r\nQDA is more flexbile than LDA, and so can be more prone to overfitting\r\nWhen we should apply LDA vs QDA\r\nLDA needs lot less parameters than QDA.\r\nLDA is a much less flexible classifier than QDA \\(\\Rightarrow\\) substantially low variance.\r\nIf LDA’s assumption of common covariance matrix is poor, then LDA has high bias.\r\nLDA better bet if training set is small so reducing variance is important.\r\nQDA better bet if training set is large so variance of classifier not a major concern.\r\nBuilding our first linear and quadratic discriminant models\r\nWe have a tibble containing 178 cases and 14 variables of measurements made on various wine bottles: data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\r\nThe analysis determined the quantities of 13 constituents (Alcohol, Malic acid, Ash, Alcalinit of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline)found in each of the three types of wines.\r\n\r\n\r\n#install.packages(\"mlr\")\r\nlibrary(mlr)\r\nlibrary(tidyverse)\r\n#install.packages(\"HDclassif\")\r\ndata(wine, package = \"HDclassif\")\r\nwineTib <- as_tibble(wine)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   class    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10\r\n   <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1     1  14.2  1.71  2.43  15.6   127  2.8   3.06 0.28   2.29  5.64\r\n 2     1  13.2  1.78  2.14  11.2   100  2.65  2.76 0.26   1.28  4.38\r\n 3     1  13.2  2.36  2.67  18.6   101  2.8   3.24 0.3    2.81  5.68\r\n 4     1  14.4  1.95  2.5   16.8   113  3.85  3.49 0.24   2.18  7.8 \r\n 5     1  13.2  2.59  2.87  21     118  2.8   2.69 0.39   1.82  4.32\r\n 6     1  14.2  1.76  2.45  15.2   112  3.27  3.39 0.34   1.97  6.75\r\n 7     1  14.4  1.87  2.45  14.6    96  2.5   2.52 0.3    1.98  5.25\r\n 8     1  14.1  2.15  2.61  17.6   121  2.6   2.51 0.31   1.25  5.05\r\n 9     1  14.8  1.64  2.17  14      97  2.8   2.98 0.290  1.98  5.2 \r\n10     1  13.9  1.35  2.27  16      98  2.98  3.15 0.22   1.85  7.22\r\n# ... with 168 more rows, and 3 more variables: V11 <dbl>, V12 <dbl>,\r\n#   V13 <int>\r\n\r\n\r\n\r\nnames(wineTib) <- c(\"Class\", \"Alco\", \"Malic\", \"Ash\", \"Alk\", \"Mag\",\r\n                    \"Phe\", \"Flav\", \"Non_flav\", \"Proan\", \"Col\", \"Hue\",\r\n                    \"OD\", \"Prol\")\r\nwineTib$Class <- as.factor(wineTib$Class)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   Class  Alco Malic   Ash   Alk   Mag   Phe  Flav Non_flav Proan\r\n   <fct> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl>    <dbl> <dbl>\r\n 1 1      14.2  1.71  2.43  15.6   127  2.8   3.06    0.28   2.29\r\n 2 1      13.2  1.78  2.14  11.2   100  2.65  2.76    0.26   1.28\r\n 3 1      13.2  2.36  2.67  18.6   101  2.8   3.24    0.3    2.81\r\n 4 1      14.4  1.95  2.5   16.8   113  3.85  3.49    0.24   2.18\r\n 5 1      13.2  2.59  2.87  21     118  2.8   2.69    0.39   1.82\r\n 6 1      14.2  1.76  2.45  15.2   112  3.27  3.39    0.34   1.97\r\n 7 1      14.4  1.87  2.45  14.6    96  2.5   2.52    0.3    1.98\r\n 8 1      14.1  2.15  2.61  17.6   121  2.6   2.51    0.31   1.25\r\n 9 1      14.8  1.64  2.17  14      97  2.8   2.98    0.290  1.98\r\n10 1      13.9  1.35  2.27  16      98  2.98  3.15    0.22   1.85\r\n# ... with 168 more rows, and 4 more variables: Col <dbl>, Hue <dbl>,\r\n#   OD <dbl>, Prol <int>\r\n\r\nWe got:\r\n- 13 continuous measurements made on 178 bottles of wine, where each measurement is the amount of a different compound/element in the wine.\r\n- Class: vineyard the bottle comes from.\r\n\r\n\r\nwineUntidy <- gather(wineTib, \"Variable\", \"Value\", -Class)\r\nggplot(wineUntidy, aes(Class, Value)) +\r\n  facet_wrap(~ Variable, scales = \"free_y\") +\r\n  geom_boxplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nBox and whisker plots of each continuous variable in the data against vineyard number. For the box and whiskers: the thick horizontal line represents the median, the box represents the interquartile range (IQR), the whiskers represent the Tukey range (1.5 times the IQR above and below the quartiles), and the dots represent data outside of the Tukey range.   \r\nCreating the task and learner, and training the LDA model\r\n\r\n\r\nwineTask <- makeClassifTask(data = wineTib, target = \"Class\")\r\nlda <- makeLearner(\"classif.lda\")\r\nldaModel <- train(lda, wineTask)\r\n\r\n\r\n\r\nExtracting discriminant function values for each case\r\n\r\n\r\nldaModelData <- getLearnerModel(ldaModel)\r\nldaPreds <- predict(ldaModelData)$x\r\nhead(ldaPreds)\r\n\r\n\r\n        LD1       LD2\r\n1 -4.700244 1.9791383\r\n2 -4.301958 1.1704129\r\n3 -3.420720 1.4291014\r\n4 -4.205754 4.0028715\r\n5 -1.509982 0.4512239\r\n6 -4.518689 3.2131376\r\n\r\nPlotting the discriminant function values against each other\r\n\r\n\r\nwineTib %>%\r\n  mutate(LD1 = ldaPreds[, 1],\r\n         LD2 = ldaPreds[, 2]) %>%\r\n  ggplot(aes(LD1, LD2, col = Class)) + \r\n    geom_point() +\r\n    stat_ellipse() +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nCreating the task and learner, and training the QDA model\r\n\r\n\r\nqda <- makeLearner(\"classif.qda\")\r\nqdaModel <- train(qda, wineTask)\r\n\r\n\r\n\r\nCross-validating the LDA and QDA models\r\n\r\n\r\nkFold <- makeResampleDesc(method = \"RepCV\", folds = 10, reps = 50,\r\nstratify = TRUE)\r\n\r\nldaCV <- resample(learner = lda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nqdaCV <- resample(learner = qda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nldaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n    0.01133544     0.98866456 \r\n\r\nqdaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n   0.008314886    0.991685114 \r\n\r\nOur LDA model correctly classified 98.8% of wine bottles, on average! There isn’t much room for improvement here, but\r\nour QDA model managed to correctly classify 99.2% of cases!\r\nCalculating confusion matrices\r\n\r\n\r\ncalculateConfusionMatrix(ldaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.999/1e+00 0.001/9e-04 0.000/0e+00 0.001      \r\n  2      0.010/1e-02 0.977/1e+00 0.014/2e-02 0.023      \r\n  3      0.000/0e+00 0.007/5e-03 0.993/1e+00 0.007      \r\n  -err.-       0.011       0.005       0.020 0.01       \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2947    3    0      3\r\n  2        34 3468   48     82\r\n  3         0   16 2384     16\r\n  -err.-   34   19   48    101\r\n\r\ncalculateConfusionMatrix(qdaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.994/0.984 0.006/0.005 0.000/0.000 0.006      \r\n  2      0.014/0.016 0.986/0.993 0.000/0.000 0.014      \r\n  3      0.000/0.000 0.004/0.003 0.996/1.000 0.004      \r\n  -err.-       0.016       0.007       0.000 0.008      \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2933   17    0     17\r\n  2        48 3502    0     48\r\n  3         0    9 2391      9\r\n  -err.-   48   26    0     74\r\n\r\nPredicting which vineyard the poisoned wine came from\r\n\r\n\r\npoisoned <- tibble(Alco = 13, Malic = 2, Ash = 2.2, Alk = 19, Mag = 100,\r\n                   Phe = 2.3, Flav = 2.5, Non_flav = 0.35, Proan = 1.7,\r\n                   Col = 4, Hue = 1.1, OD = 3, Prol = 750)\r\npredict(qdaModel, newdata = poisoned)\r\n\r\n\r\nPrediction: 1 observations\r\npredict.type: response\r\nthreshold: \r\ntime: 0.00\r\n  response\r\n1        1\r\n\r\nThe model predicts that the poisoned bottle came from vineyard 1.\r\nHere’s we ends the analytic example.\r\nReferences\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-18T10:46:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-14-machine-learning-&-predictive-analytics/",
    "title": "Machine Learning & Predictive Analytics",
    "description": "An Overview of Machine Learning & Predictive Analytics",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [
      "Biostatistics",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Machine Learning\r\nLearning Types\r\nSupervised\r\nUnsupervised Learning\r\nSemi-supervised Learning\r\n\r\nActive Learning\r\nReinforcement Learning\r\nTransfer Learning\r\n\r\nUniversal Workflow of ML\r\nML Terminologies\r\nData Assumptions\r\nOverfitting and Underfitting\r\nParametric & Nonparametric Models\r\nRegression Analysis\r\n\r\nUse R or Python for machine learning?\r\nMachine Learning with mlr Package in R\r\n\r\nReferences\r\n\r\nWhat is Machine Learning\r\nMeaningful data transformations from input to output data.\r\nTransformations: represent or encode the data (RGB or HSV for color pixel).\r\nLearning is automatic search for better data representations.\r\nSearch through a predefined space of possibilities using guidance from feedback signal.\r\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E”\r\n\r\n-Tom Mitchell, Machine Learning, McGraw Hill, 1997\r\n\r\n\r\nExperience E, Task T, Performance P\r\n1. Chess: T: playing chess, P: % of games won, E: playing practice games against itself.\r\n2. Driving: T: driving a vehicle, P: avgdistance before error, E: sequence of images and steering commands recoded during manual driving.\r\n3. Handwriting Recognition: T: recognizing and classifying handwritten words in images, P: % of correctly classified words, E: DB of handwritten words with given classifications.\r\n\r\nLearning Types\r\nSupervised\r\nUnsupervised\r\nSemi-supervised\r\nReinforcement\r\nTransfer\r\nActive\r\nSupervised\r\nThe majority of practical machine learning uses supervised learning.\r\nSupervised learning is where you have input variables (\\(x\\)) and an output variable (\\(y\\)) and you use an algorithm to learn the mapping function from the input to the output.\\[ y = f(x) \\]\r\nThe goal is to approximate the mapping function so well that when you have new input data \\(x\\) that you can predict the output variables \\(y\\) for that data.\r\nIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\r\nWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\r\nSupervised Learning Examples:\r\nLinear Regression\r\nLogistic Regression\r\nK-NN (k-Nearest Neighbors)\r\nSupport Vector Machines (SVMs)\r\nDecision Tress and Random Forests\r\nNeural Networks\r\nUnsupervised Learning\r\nUnsupervised learning is where you only have input data \\(x\\) and no corresponding output variables.\r\nThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\r\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.\r\nUnsupervised Learning problems can be further grouped into Clustering and Association Problems.\r\nClustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\r\nAssociation: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy \\(A\\) also tend to buy \\(B\\).\r\nUnsupervised Learning Examples:\r\nClustering\r\nK-Means\r\nHierarchical Cluster Analysis (HCA)\r\nExpectation Maximization\r\n\r\nVisualization and Dimensionality Reduction\r\nPrincipal Component Analysis (PCA)\r\nKernel PCA\r\nt-distributed Stochastic Neighbor Embedding (t-SNE)\r\n\r\nAssociation Rule\r\nApriori\r\n\r\nNeural Networks\r\nAutoencoders\r\nBoltzmann machines\r\n\r\nSemi-supervised Learning\r\nSemi-supervised learning is halfway between supervised and unsupervised learning.\r\nTraditional classification methods use labeled data to build classifiers.\r\nThe labeled training sets used as input in Supervised learning is very certain and properly defined.\r\nHowever, they are limited, expensive and takes a lot of time to generate them.\r\nOn the other hand, unlabeled data is cheap and is readily available in large volumes.\r\nHence, semi-supervised learning is learning from a combination of both labeled and unlabeled data\r\nWhere we make use of a combination of small amount of labeled data and large amount of unlabeled data to increase the accuracy of our classifiers.\r\nActive Learning\r\nActive learning (sometimes called “query learning” or “optimal experimental design” in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence.\r\nThe key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns—to be “curious,” if you will—it will perform better with less training.\r\nActive learning is a special case of semi-supervised learning.\r\nReinforcement Learning\r\nReinforcement Learning is learning what to do and how to map situations to actions.\r\nThe end result is to maximize the numerical reward signal.\r\nThe learner is not told which action to take, but instead must discover which action will yield the maximum reward\r\nTransfer Learning\r\nA machine learning technique where a model trained on one task is re-purposed on a second related task.\r\nAn optimization that allows rapid progress or improved performance when modeling the second task.\r\nUniversal Workflow of ML\r\nDefine the problem\r\nAssemble dataset\r\nChoose a metric to quantify project outcome\r\nDecide on how to calculate the metric\r\nPrepare dataset\r\nDefine standard baseline\r\nDevelop model that beats baseline\r\nIdeal model is at the border of overfit and underfit–cross the border to know where it is so overfit model\r\nRegularize model and tune hyperparameters\r\nML Terminologies\r\nDataset\r\nTraining –Learn the parameters\r\nValidation –select hyperparameters\r\nTest –test the model aka generalization error\r\n\r\nBatch –set of examples used in one iteration of model training.\r\nMini-batch –A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference.\r\nEpoch –A full training pass over the entire data set such that each example has been seen once.\r\nIteration –A single update of a model’s weights during training.\r\nData Assumptions\r\nTraining and test data are from the same probability distribution.\r\nTraining and test data are iid.\r\nOverfitting and Underfitting\r\nOverfitting –model fits very well to the training data, aka detects patterns in the noise also\r\nDetect:\r\nLow training error, high generalization error.\r\n\r\nRemedies:\r\nReduce model capacity by removing features and/or parameters.\r\nGet more training data.\r\nImprove training data quality by reducing noise.\r\n\r\n\r\nUnderfitting–model too simple to detect patterns in the data\r\nDetect\r\nHigh training error.\r\n\r\nRemedies:\r\nIncrease model capacity by adding more parameters and/or features.\r\nReduce model constraints.\r\n\r\n\r\nParametric & Nonparametric Models\r\n\\[ 𝑦= 𝑓(𝑥) \\]\r\nEstimate the unknown function 𝑓 as \\(\\hat{f}\\)\r\nParametric Models:\r\nAssume the functional form or shape of 𝑓\r\nApply methodology to train model\r\nAdvantage –simple estimation\r\nDisadvantage – \\(\\hat{f}\\)may be far from true 𝑓\r\n\r\nNonparametric Models:\r\nNo assumption on the functional form or shape of 𝑓\r\nEstimate to fit as close as possible to the data\r\nAdvantage –can accurately fit a wide range of possible shapes of 𝑓\r\nDisadvantage –need large datasets (since there is no fixed # of params to estimate)\r\n\r\nRegression Analysis\r\nOLS\r\nMSE\r\nComputational Complexity of matrix inversion\r\nComplete training set\r\n\r\nBatch Gradient Descent\r\nCost function (MSE)\r\nLearning rate hyperparameter\r\nPartial derivative\r\nComplete training set\r\n\r\nStochastic Gradient Descent\r\nMini-batch Gradient Descent\r\nLinear Regression with OLS\r\n\\[ 𝑦= \\theta^T𝑋\\]\r\nThe cost function minimization is a closed-form solution called the Normal Equation: \\[ \\hat{\\theta} = (X^T . X)^{-1} X^T.y  \\]\r\nAdvantage –equation is linear with size of training set so it can handle large training sets efficiently.\r\nDisadvantage –\r\ncomputational complexity of inverting a matrix that increases with size of training set.\r\ndifficult to do online learning with new data arriving regularly (need to recalculate estimates), i.e. no iterative parameter updates.\r\n\r\nUse R or Python for machine learning?\r\nThere is something of a rivalry between the two most commonly used data science languages: R and Python. Of course, there are no machine learning tasks which are only possible to apply in one language or the other.\r\nR:\r\nR is geared specifically for mathematical and statistical applications, i.e. R can focus purely on data, but may feel restricted if they ever need to build applications based on their models.\r\nCurrently, there are modern tools in R designed specifically to make data science tasks simple and human-readable, such as those from the tidyverse.\r\nPreviously, ML algorithms in R were scattered across multiple packages, written by different authors. But R has now followed suit, with the caret and mlr packages (which stands for machine learning in R). While quite similar in purpose and functionality to caret, mlr package provides an interface for a large number of machine learning algorithms, and allows you to perform extremely complicated machine learning tasks with very little coding.\r\nPython:\r\nFirst of all, some of the more cutting-edge deep learning approaches are easier to apply in Python (they tend to be written in Python first and implemented in R later).\r\nPython, while very good for data science, is a more general purpose programming language.\r\nProponents of python could use this as en example of why it was better suited for machine learning, as it has the well known scikit-learn package which has a plethora of machine learning algorithms built into it.\r\nGoogle Trends demonstrates the search interest relative to the highest point on the chart for the given region and over the past 5 years.\r\n\r\n trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05z1_\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0212jm\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0jgqg\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/09gbxjr\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0j3djl7\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&q=%2Fm%2F05z1_,%2Fm%2F0212jm,%2Fm%2F0jgqg,%2Fm%2F09gbxjr,%2Fm%2F0j3djl7\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Machine Learning with mlr Package in R\r\nR users got mlr package similar to Scikit-Learn from Python. The package synthesizes all the ML functions from other packages in which we can perform most of ML tasks. mlr package has several algorithms in its bouquet. These algorithms have been categorized into regression, classification, clustering, survival, multiclassification and cost sensitive classification:\r\n\r\n\r\nlibrary(mlr)\r\nlistLearners(\"classif\")[c(\"class\",\"package\")]\r\n\r\n\r\n                            class                   package\r\n1                     classif.ada                 ada,rpart\r\n2              classif.adaboostm1                     RWeka\r\n3             classif.bartMachine               bartMachine\r\n4                classif.binomial                     stats\r\n5                classif.boosting              adabag,rpart\r\n6                     classif.bst                 bst,rpart\r\n7                     classif.C50                       C50\r\n8                 classif.cforest                     party\r\n9              classif.clusterSVM        SwarmSVM,LiblineaR\r\n10                  classif.ctree                     party\r\n11               classif.cvglmnet                    glmnet\r\n12                 classif.dbnDNN                   deepnet\r\n13                  classif.dcSVM            SwarmSVM,e1071\r\n14                  classif.earth               earth,stats\r\n15                 classif.evtree                    evtree\r\n16             classif.extraTrees                extraTrees\r\n17             classif.fdausc.glm                   fda.usc\r\n18          classif.fdausc.kernel                   fda.usc\r\n19             classif.fdausc.knn                   fda.usc\r\n20              classif.fdausc.np                   fda.usc\r\n21                classif.FDboost            FDboost,mboost\r\n22            classif.featureless                       mlr\r\n23                   classif.fgam                    refund\r\n24                    classif.fnn                       FNN\r\n25               classif.gamboost                    mboost\r\n26               classif.gaterSVM                  SwarmSVM\r\n27                classif.gausspr                   kernlab\r\n28                    classif.gbm                       gbm\r\n29                  classif.geoDA               DiscriMiner\r\n30               classif.glmboost                    mboost\r\n31                 classif.glmnet                    glmnet\r\n32       classif.h2o.deeplearning                       h2o\r\n33                classif.h2o.gbm                       h2o\r\n34                classif.h2o.glm                       h2o\r\n35       classif.h2o.randomForest                       h2o\r\n36                    classif.IBk                     RWeka\r\n37                    classif.J48                     RWeka\r\n38                   classif.JRip                     RWeka\r\n39                   classif.kknn                      kknn\r\n40                    classif.knn                     class\r\n41                   classif.ksvm                   kernlab\r\n42                    classif.lda                      MASS\r\n43       classif.LiblineaRL1L2SVC                 LiblineaR\r\n44      classif.LiblineaRL1LogReg                 LiblineaR\r\n45       classif.LiblineaRL2L1SVC                 LiblineaR\r\n46      classif.LiblineaRL2LogReg                 LiblineaR\r\n47         classif.LiblineaRL2SVC                 LiblineaR\r\n48 classif.LiblineaRMultiClassSVC                 LiblineaR\r\n49                  classif.linDA               DiscriMiner\r\n50                 classif.logreg                     stats\r\n51                  classif.lssvm                   kernlab\r\n52                   classif.lvq1                     class\r\n53                    classif.mda                       mda\r\n54                    classif.mlp                     RSNNS\r\n55               classif.multinom                      nnet\r\n56             classif.naiveBayes                     e1071\r\n57              classif.neuralnet                 neuralnet\r\n58                   classif.nnet                      nnet\r\n59                classif.nnTrain                   deepnet\r\n60            classif.nodeHarvest               nodeHarvest\r\n61                   classif.OneR                     RWeka\r\n62                   classif.pamr                      pamr\r\n63                   classif.PART                     RWeka\r\n64              classif.penalized                 penalized\r\n65                    classif.plr                   stepPlr\r\n66             classif.plsdaCaret                 caret,pls\r\n67                 classif.probit                     stats\r\n68                    classif.qda                      MASS\r\n69                  classif.quaDA               DiscriMiner\r\n70           classif.randomForest              randomForest\r\n71        classif.randomForestSRC           randomForestSRC\r\n72                 classif.ranger                    ranger\r\n73                    classif.rda                      klaR\r\n74                 classif.rFerns                    rFerns\r\n75                   classif.rknn                      rknn\r\n76         classif.rotationForest            rotationForest\r\n77                  classif.rpart                     rpart\r\n78                    classif.RRF                       RRF\r\n79                  classif.rrlda                     rrlda\r\n80                 classif.saeDNN                   deepnet\r\n81                    classif.sda                       sda\r\n82              classif.sparseLDA sparseLDA,MASS,elasticnet\r\n83                    classif.svm                     e1071\r\n84                classif.xgboost                   xgboost\r\n\r\n– ANALYTICS VIDHYA\r\nThe entire structure of this package relies on this premise:\r\n\\[\\text{Create a Task.   Make a Learner.   Train Them.}\\]\r\nCreating a task means loading data in the package (e.g., makeClassifTask).\r\nMaking a learner means choosing an algorithm (makeLearner) which learns from task (or data).\r\nFinally, train them (train).\r\nReferences\r\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R (1st ed. 2013. ed.). New York, NY: Springer New York.\r\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts: The MIT Press.\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nGéron, A. l. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems (First edition. ed.). Sebastopol, California: O’Reilly Media, Inc.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-17T14:33:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/",
    "title": "Beta Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Beta Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nDefinition\r\nIntuitive interpretation\r\nBeta function\r\nGamma function\r\nMain facts\r\nIn actions\r\nPlots in shiny\r\nFurther reading\r\n\r\nMotivation\r\n\r\nEven though I had learned the beta distribution from UIC’s Bayesian methods course and tutored it, such as setting up it as the prior distribution in conjugate distribution context. But it was easy to forget because of its dried content and too abstract. Here I try to combine the rigid theory (UC coursework’s content) and intuitive thought. By that way, I was able to ‘permenently stamp’ the concept to my brain.\r\n\r\nThe Beta distribution is a probability distribution on/of probabilities\r\nThe beta distribution describes a family of continuous probability distributions that are nonzero only on the interval (0 1).\r\nFor example, we can use it to model the probabilities: the Click-Through Rate of the advertisement, the batting averages, the 5-year survival chance for women with breast cancer, and so on.\r\nDefinition\r\nA continuous random variable \\(X_B \\sim Beta(\\alpha, \\beta)\\) has Beta distribution if its probability density function (PDF) is\r\n\\[ \r\nf_{X_B} (x; \\alpha, \\beta) = \\frac{1}{B(α,β)} x^{\\alpha − 1} (1−x)^{\\beta − 1}, \\ \\ \\text{for} \\ 0 < x < 1.\r\n\\]\r\nwhere \\(B(\\cdot)\\) is the Beta function and shape parameters \\(\\alpha, \\beta > 0\\).\r\nIntuitive interpretation\r\n\r\nPDF\r\nProbability as a …\r\nBinomial\r\n\\(f(x) = {n \\choose x} p^x (1-p)^{n-x}\\)\r\nparameter\r\n\r\n\\(\\rightarrow\\) the function of \\(x\\)\r\n\r\nBeta\r\n\\(f(p) = \\frac{1}{B(α,β)} p^{\\alpha − 1} (1−p)^{\\beta − 1}\\)\r\nrandom variable\r\n\r\n\\(\\rightarrow\\) the function of \\(p\\)\r\n\r\nThe beta distribution intuitively comes into play when we look at it in terms of numerator—\\(x/p\\) to the power of something multiplied by \\(1-x/1-p\\) to the power of something—from the lens of the binomial distribution.\r\nThe difference between the binomial and the beta is that the above models the number of successes (\\(x\\)), while the below models the probability (\\(p\\)) of success. In other words, the probability is a parameter in binomial; In the Beta, the probability is a random variable.\r\nIn this context, the shape parameters \\(\\alpha\\) and \\(\\beta\\) or \\(\\alpha-1\\) as the number of successes and \\(\\beta-1\\) as the number of failures\r\nWe can explore the beauty of beta distribution via the the calculator for Beta distribution—Dr. Bognar at the University of Iowa built it.\r\nBeta distribution is very flexible: bell-curve (The PDF of a beta distribution is approximately normal if \\(\\alpha + \\beta\\) is large enough and \\(\\alpha\\) & \\(\\beta\\) are approximately equal), U-shaped (when \\(\\alpha\\) < 1, \\(\\beta\\) < 1) and even straight line. Here’s an graph excerpt from wikipedia.\r\nThe very flexible of Beta distributionBeta function\r\nThe beta function is\r\n\\[ \r\nB(x,y) = \\int_0^1 t^{x−1} (1−t)^{y−1} dt = \\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)},\r\n\\]\r\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function.\r\nGamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\):\r\n\\[ \r\n\\Gamma (n) = (n−1)! = 1 \\times 2 \\times 3 \\times ... \\times (n−1)\r\n\\]\r\nThe gamma function is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\r\n\\Gamma (t) = \\int_0^{\\infty} x^{t-1} e^{-x} dx\r\n\\]\r\n\r\nSimplify the Beta function with the Gamma Function \\(\\Rightarrow\\) we saw the PDF of Beta written in terms of the Gamma function. The Beta function is the ratio of the product of the Gamma function of each parameter divided by the Gamma function of the sum of the parameters (proof refered the further reading topic).\r\n\r\nMain facts\r\n\\[\r\nE[X_B] = \\mu = \\frac{\\alpha}{\\alpha + \\beta}; \\ \\ V[X_B] = \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\r\n\\]\r\nThe standard uniform distribution \\(\\text{Unif} \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nThe mode is \\(\\omega = \\frac{\\alpha − 1}{\\alpha + \\beta − 2}\\) for \\(\\alpha, \\beta > 1\\).\r\nThe concentration is \\(\\kappa = \\alpha + \\beta\\).\r\nDefinitions of \\(\\mu, \\omega\\) and \\(\\kappa\\) can be inverted:\r\n\\[ \\alpha = \\mu\\kappa,  \\beta = (1 − \\mu)\\kappa \\]\r\n\\[ \\alpha = \\omega(\\kappa−2)+1,  \\beta = (1 − \\omega)(\\kappa−2)+1, \\ \\kappa > 2. \\]\r\nParameter \\(\\kappa\\) is a measure of number of observations needed to change our previous belief about \\(\\mu\\).\r\nIf \\(\\kappa\\) is small we need only a few new observations.\r\nExample. Concentration \\(\\kappa = 8\\) around \\(\\mu = 0.5\\) corresponds to \\(\\alpha = \\mu \\kappa = 4\\) and \\(\\beta = (1 − \\mu) \\kappa = 4\\).\r\nParameterization in terms of mean value and standard deviation is:\r\n\\[ \\alpha = \\mu [\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1]; \\ \\ \\beta = (1 - \\mu)[\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1] \\]\r\nStandard deviation is typically smaller than standard deviation of uniform distribution on \\([0,1]\\), i.e. \\(0.28867\\).\r\nExamples.\r\nFor \\(\\mu = 0.5\\), \\(\\sigma = 0.28867\\) the shape parameters are \\(\\alpha = 1\\), \\(\\beta = 1\\).\r\nFind shape parameters of beta distribution with \\(\\mu = 0.5\\), \\(\\sigma = 0.1\\).\r\nThe standard uniform distribution \\(Unif \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nIn actions\r\nKeep parameter \\(\\beta\\) fixed. Move \\(\\alpha\\) up or down. Observe how the mass of the distribution moves\r\n\r\n\r\np <- seq(0,1,by=0.2)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=2), aes(colour = \"alpha=1,beta=2\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=2), aes(colour = \"alpha=4,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=2), aes(colour = \"alpha=6,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=8, shape2=2), aes(colour = \"alpha=8,beta=2\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDo the same as above, but keep \\(\\alpha\\) constant and move \\(\\beta\\) up or down\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=1), aes(colour = \"alpha=2,beta=1\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=5), aes(colour = \"alpha=2,beta=5\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=6), aes(colour = \"alpha=2,beta=6\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=8), aes(colour = \"alpha=2,beta=8\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nMake \\(\\alpha = \\beta = 1\\). What does the shape of the distribution tell you about your knowledge about random variable \\(\\theta\\)?\\(\\Rightarrow\\) The standard uniform distribution \\(Unif(0,1)\\) is a special case of the beta distribution \\(Beta (1,1)\\), when \\(\\alpha\\)=\\(\\beta\\)=1.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"green\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nKeep \\(\\alpha = \\beta\\) , but move both of them up or down. Interpret the shape of the distribution\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=0.5, shape2=0.5), aes(colour = \"alpha=0.5,beta=0.5\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=4), aes(colour = \"alpha=4,beta=4\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=6), aes(colour = \"alpha=6,beta=6\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nVariance changes based on 2 shape parameters.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=400, shape2=80), aes(colour = \"alpha=400,beta=80\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=40, shape2=8), aes(colour = \"alpha=40,beta=8\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=30, shape2=70), aes(colour = \"alpha=30,beta=70\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=7), aes(colour = \"alpha=3,beta=7\")) +\r\n  scale_y_continuous(limits=c(0,25)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\", \"green\", \"orange\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWhen beta distribution is used as a prior for parameter of binomial distribution, parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as previously observed numbers of successes (\\(\\alpha\\)) or failures (\\(\\beta\\)). For example, if in 2 Bernoulli experiments there was 1 success and 1 failure you can express opinion about probability of success as \\(Beta(1,1)\\). What would you assume as prior if in 6 previously observed outcomes there were 3 successes and 3 failures? What is the likely value of the parameter? Do we have more or less information than in case of 1 success and 1 failure? \\(\\Rightarrow\\) Think of more\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=3), aes(colour = \"alpha=3,beta=3\")) +\r\n  stat_function(fun=dbinom, args=list(size=1, prob=0.5), aes(colour = \"Bernoulli w/ prob=0.5\")) + # bernoulli\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"red\",\"green\",\"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDefine distribution with mode \\(\\omega\\)=.8 and concentration \\(\\kappa = 12\\). To do that find shape parameters \\(\\alpha = \\omega (\\kappa − 2) + 1 = 9\\) and \\(\\beta = (1 − \\omega)(\\kappa − 2) + 1 = 3\\).\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=9, shape2=3), aes(colour = \"alpha=9,beta=3\")) +\r\n  scale_y_continuous(limits=c(0,3.4)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the actions we notify that:\r\nThe special case \\(a=b=1\\) is the uniform distribution.\r\nThe distribution is roughly centered on \\(a/(a+b)\\). Actually, it turns out that the mean is exactly \\(a/(a+b)\\). Thus the mean of the distribution is determined by the relative values of \\(a\\) and \\(b\\).\r\nThe larger the values of \\(a\\) and \\(b\\), the smaller the variance of the distribution about the mean.\r\nFor moderately large values of \\(a\\) and \\(b\\) the distribution looks visually “kind of normal”, although unlike the normal distribution the Beta distribution is restricted to [0,1].\r\nPlots in shiny\r\nPlanning to build an shiny app to plot beta distribution on the specification of shape parameter (“still being in the process”).\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nDavid Robinson (Principal Data Scientist at Heap, works in R and Python), Understanding the beta distribution (using baseball statistics), http://varianceexplained.org/statistics/beta_distribution_and_baseball/\r\nAerin Kim, Beta Distribution — Intuition, Examples, and Derivation, https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/Beta-Distribution-in-an-Intuitive-Explanation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-15T00:29:44-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/",
    "title": "Gamma Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Gamma Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "ggplot2",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nFact\r\nGamma function\r\nDefinition\r\nParameters of Gamma: a shape with a scale or a rate\r\nPlots\r\nFurther reading\r\n\r\nFact\r\nWhy did we invent Gamma distribution? Answer: To predict the wait time until future events. Hmmm ok, but I thought that’s what the exponential distribution is for. Then, what’s the difference between exponential distribution and gamma distribution? The exponential distribution predicts the wait time until the very first event. The gamma distribution, on the other hand, predicts the wait time until the k-th event occurs.\r\n– Aerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nExamples:\r\nThe time I wait to receive an interview might follow an exponential. Now, I am waiting not for my first interview offer but my third interview offer. How long must I wait? This waiting time can be described by a gamma.\r\nI missed the first and second CTA train to go to the campus. Now, how long I am able to catch the third train?\r\nGamma function\r\nIn the lecture series of Statistics 110, Lecture 24: Gamma distribution and Poisson process | Statistics 110, Prof. Joe Blitzstein had connected the \\(n!\\) function to the Gamma function. Why?\r\nLet’s see the Gamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\): \\[\\Gamma(n) = (n-1)! = 1 \\times 2 \\times 3 \\times ... \\times (n-1)\\]\r\nOne beautiful formula, Stirling formula to approximate the \\(n!\\), actually an extremely good approximation: \\[ n! \\approx \\sqrt{2\\pi n} \\Big( \\frac{n}{e}\\Big)^n\\]\r\n\r\n\r\nn <- c(1:6)\r\ny <- vector(mode = \"numeric\", length = length(n))\r\ny[1] <- 1\r\nfor(i in 2:length(n)) {\r\n  y[i] = y[i-1] * i\r\n}\r\ndta <- as.data.frame(cbind(n,y))\r\nlibrary(ggplot2)\r\nggplot(dta, aes(n, y)) + \r\n  geom_point() +\r\n  scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\")) +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nThen how we connect the dots. There are many ways to do it, but there’s a philosophical way to do it by Gamma function, which is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\\Gamma(t) = \\int_0^{\\infty} x^t e^{−x} \\frac{dx}{x} \\]\r\nDefinition\r\nFrom the Gamma function, how we got the PDF of Gamma distribution. We would normalize the Gamma distribution, which means from:\r\n\\[ \\Gamma(k) = \\int_0^{\\infty} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nto,\r\n\\[ 1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nThen, \\(X = \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{1}{x}\\) \\(\\sim\\) \\(Gamma(k, 1)\\) which has shape of \\(k\\) and scale of \\(1\\).\r\nHow we turn the scale of \\(1\\) to a general scale of \\(\\theta\\)?\r\nImagine that \\(Y \\sim \\frac{X}{\\theta}\\) where \\(X \\sim \\ Gamma(k,1)\\)\r\n\\(f_Y(y) = f_X(x) \\frac{dx}{dy} = \\frac{1}{\\Gamma(k)} (\\theta y)^{k} e^{−\\theta y} \\frac{1}{\\theta y} \\theta\\) where \\(\\frac{dx}{dy} = \\theta\\)\r\nThus, \\(f(y) = \\frac{1}{\\Gamma(k) \\theta^{k}} (y)^{k} e^{−\\theta y} \\frac{1}{y}\\)\r\nParameters of Gamma: a shape with a scale or a rate\r\n\r\n\r\nknitr::include_graphics(\"Gamma_scalevsrate_inwiki.png\") \r\n\r\n\r\n\r\n\r\n(#fig:model diagram)From https://en.wikipedia.org/wiki/Gamma_distribution\r\n\r\n\r\n\r\nFor (\\(\\alpha\\), \\(\\beta\\)) parameterization: Using our notation \\(k\\) (the # of events) & \\(\\lambda\\) (the rate of events), simply substitute \\(\\alpha\\) with \\(k\\), \\(\\beta\\) with \\(\\lambda\\). The PDF stays the same format as what we’ve derived.\r\nFor (\\(k\\), \\(\\theta\\)) parameterization: \\(\\theta\\) is a reciprocal of the event rate \\(\\lambda\\), which is the mean wait time (the average time between event arrivals).\r\nPlots\r\nI plotted the gamma distribution with the shape of \\(k\\), and constantly rate = \\(1\\)\r\n\r\n\r\nT <- seq(0,20,by=2.5)\r\n\r\ndf <- data.frame(T)\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=1, rate=1), aes(colour = \"k= 1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=5, rate=1), aes(colour = \"k= 5\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"k=10\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"yellowgreen\", \"olivedrab\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nI plotted the gamma distribution with the constantly shape of k = 10, and variant rate from 1 to 3.\r\n\r\n\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"r=1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=10, rate=2), aes(colour = \"r=2\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=3), aes(colour = \"r=3\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"gold\", \"burlywood\", \"darkorange\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution (k=10)\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFurther reading\r\nLecture 24: Gamma distribution and Poisson process | Statistics 110\r\nAerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nWiki\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/Gamma-distribution-in-an-intuitive-explanation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-23T11:53:12-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
