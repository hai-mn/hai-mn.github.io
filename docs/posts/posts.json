[
  {
    "path": "posts/2021-04-14-machine-learning-&-predictive-analytics/",
    "title": "Machine Learning & Predictive Analytics",
    "description": "An Overview of Machine Learning & Predictive Analytics",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [
      "Biostatistics",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Machine Learning\r\nLearning Types\r\nSupervised\r\nUnsupervised Learning\r\nSemi-supervised Learning\r\n\r\nActive Learning\r\nReinforcement Learning\r\nTransfer Learning\r\n\r\nUniversal Workflow of ML\r\nML Terminologies\r\nData Assumptions\r\nOverfitting and Underfitting\r\nParametric & Nonparametric Models\r\nRegression Analysis\r\n\r\nUse R or Python for machine learning?\r\nReferences\r\n\r\nWhat is Machine Learning\r\nMeaningful data transformations from input to output data.\r\nTransformations: represent or encode the data (RGB or HSV for color pixel).\r\nLearning is automatic search for better data representations.\r\nSearch through a predefined space of possibilities using guidance from feedback signal.\r\n‚ÄúA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E‚Äù\r\n\r\n-Tom Mitchell, Machine Learning, McGraw Hill, 1997\r\n\r\n\r\nExperience E, Task T, Performance P\r\n1. Chess: T: playing chess, P: % of games won, E: playing practice games against itself.\r\n2. Driving: T: driving a vehicle, P: avgdistance before error, E: sequence of images and steering commands recoded during manual driving.\r\n3. Handwriting Recognition: T: recognizing and classifying handwritten words in images, P: % of correctly classified words, E: DB of handwritten words with given classifications.\r\n\r\nLearning Types\r\nSupervised\r\nUnsupervised\r\nSemi-supervised\r\nReinforcement\r\nTransfer\r\nActive\r\nSupervised\r\nThe majority of practical machine learning uses supervised learning.\r\nSupervised learning is where you have input variables (\\(x\\)) and an output variable (\\(y\\)) and you use an algorithm to learn the mapping function from the input to the output.\\[ y = f(x) \\]\r\nThe goal is to approximate the mapping function so well that when you have new input data \\(x\\) that you can predict the output variables \\(y\\) for that data.\r\nIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\r\nWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\r\nSupervised Learning Examples:\r\nLinear Regression\r\nLogistic Regression\r\nK-NN (k-Nearest Neighbors)\r\nSupport Vector Machines (SVMs)\r\nDecision Tress and Random Forests\r\nNeural Networks\r\nUnsupervised Learning\r\nUnsupervised learning is where you only have input data \\(x\\) and no corresponding output variables.\r\nThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\r\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.\r\nUnsupervised Learning problems can be further grouped into Clustering and Association Problems.\r\nClustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\r\nAssociation: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy \\(A\\) also tend to buy \\(B\\).\r\nUnsupervised Learning Examples:\r\nClustering\r\nK-Means\r\nHierarchical Cluster Analysis (HCA)\r\nExpectation Maximization\r\n\r\nVisualization and Dimensionality Reduction\r\nPrincipal Component Analysis (PCA)\r\nKernel PCA\r\nt-distributed Stochastic Neighbor Embedding (t-SNE)\r\n\r\nAssociation Rule\r\nApriori\r\n\r\nNeural Networks\r\nAutoencoders\r\nBoltzmann machines\r\n\r\nSemi-supervised Learning\r\nSemi-supervised learning is halfway between supervised and unsupervised learning.\r\nTraditional classification methods use labeled data to build classifiers.\r\nThe labeled training sets used as input in Supervised learning is very certain and properly defined.\r\nHowever, they are limited, expensive and takes a lot of time to generate them.\r\nOn the other hand, unlabeled data is cheap and is readily available in large volumes.\r\nHence, semi-supervised learning is learning from a combination of both labeled and unlabeled data\r\nWhere we make use of a combination of small amount of labeled data and large amount of unlabeled data to increase the accuracy of our classifiers.\r\nActive Learning\r\nActive learning (sometimes called ‚Äúquery learning‚Äù or ‚Äúoptimal experimental design‚Äù in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence.\r\nThe key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns‚Äîto be ‚Äúcurious,‚Äù if you will‚Äîit will perform better with less training.\r\nActive learning is a special case of semi-supervised learning.\r\nReinforcement Learning\r\nReinforcement Learning is learning what to do and how to map situations to actions.\r\nThe end result is to maximize the numerical reward signal.\r\nThe learner is not told which action to take, but instead must discover which action will yield the maximum reward\r\nTransfer Learning\r\nA machine learning technique where a model trained on one task is re-purposed on a second related task.\r\nAn optimization that allows rapid progress or improved performance when modeling the second task.\r\nUniversal Workflow of ML\r\nDefine the problem\r\nAssemble dataset\r\nChoose a metric to quantify project outcome\r\nDecide on how to calculate the metric\r\nPrepare dataset\r\nDefine standard baseline\r\nDevelop model that beats baseline\r\nIdeal model is at the border of overfit and underfit‚Äìcross the border to know where it is so overfit model\r\nRegularize model and tune hyperparameters\r\nML Terminologies\r\nDataset\r\nTraining ‚ÄìLearn the parameters\r\nValidation ‚Äìselect hyperparameters\r\nTest ‚Äìtest the model aka generalization error\r\n\r\nBatch ‚Äìset of examples used in one iteration of model training.\r\nMini-batch ‚ÄìA small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference.\r\nEpoch ‚ÄìA full training pass over the entire data set such that each example has been seen once.\r\nIteration ‚ÄìA single update of a model‚Äôs weights during training.\r\nData Assumptions\r\nTraining and test data are from the same probability distribution.\r\nTraining and test data are iid.\r\nOverfitting and Underfitting\r\nOverfitting ‚Äìmodel fits very well to the training data, aka detects patterns in the noise also\r\nDetect:\r\nLow training error, high generalization error.\r\n\r\nRemedies:\r\nReduce model capacity by removing features and/or parameters.\r\nGet more training data.\r\nImprove training data quality by reducing noise.\r\n\r\n\r\nUnderfitting‚Äìmodel too simple to detect patterns in the data\r\nDetect\r\nHigh training error.\r\n\r\nRemedies:\r\nIncrease model capacity by adding more parameters and/or features.\r\nReduce model constraints.\r\n\r\n\r\nParametric & Nonparametric Models\r\n\\[ ùë¶= ùëì(ùë•) \\]\r\nEstimate the unknown function ùëì as \\(\\hat{f}\\)\r\nParametric Models:\r\nAssume the functional form or shape of ùëì\r\nApply methodology to train model\r\nAdvantage ‚Äìsimple estimation\r\nDisadvantage ‚Äì \\(\\hat{f}\\)may be far from true ùëì\r\n\r\nNonparametric Models:\r\nNo assumption on the functional form or shape of ùëì\r\nEstimate to fit as close as possible to the data\r\nAdvantage ‚Äìcan accurately fit a wide range of possible shapes of ùëì\r\nDisadvantage ‚Äìneed large datasets (since there is no fixed # of params to estimate)\r\n\r\nRegression Analysis\r\nOLS\r\nMSE\r\nComputational Complexity of matrix inversion\r\nComplete training set\r\n\r\nBatch Gradient Descent\r\nCost function (MSE)\r\nLearning rate hyperparameter\r\nPartial derivative\r\nComplete training set\r\n\r\nStochastic Gradient Descent\r\nMini-batch Gradient Descent\r\nLinear Regression with OLS\r\n\\[ ùë¶= \\theta^Tùëã\\]\r\nThe cost function minimization is a closed-form solution called the Normal Equation: \\[ \\hat{\\theta} = (X^T . X)^{-1} X^T.y  \\]\r\nAdvantage ‚Äìequation is linear with size of training set so it can handle large training sets efficiently.\r\nDisadvantage ‚Äì\r\ncomputational complexity of inverting a matrix that increases with size of training set.\r\ndifficult to do online learning with new data arriving regularly (need to recalculate estimates), i.e.¬†no iterative parameter updates.\r\n\r\nUse R or Python for machine learning?\r\nThere is something of a rivalry between the two most commonly used data science languages: R and Python. Of course, there are no machine learning tasks which are only possible to apply in one language or the other.\r\nR:\r\nR is geared specifically for mathematical and statistical applications, i.e.¬†R can focus purely on data, but may feel restricted if they ever need to build applications based on their models.\r\nCurrently, there are modern tools in R designed specifically to make data science tasks simple and human-readable, such as those from the tidyverse.\r\nPreviously, ML algorithms in R were scattered across multiple packages, written by different authors. But R has now followed suit, with the caret and mlr packages (which stands for machine learning in R). While quite similar in purpose and functionality to caret, mlr package provides an interface for a large number of machine learning algorithms, and allows you to perform extremely complicated machine learning tasks with very little coding.\r\nPython:\r\nFirst of all, some of the more cutting-edge deep learning approaches are easier to apply in Python (they tend to be written in Python first and implemented in R later).\r\nPython, while very good for data science, is a more general purpose programming language.\r\nProponents of python could use this as en example of why it was better suited for machine learning, as it has the well known scikit-learn package which has a plethora of machine learning algorithms built into it.\r\nReferences\r\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R (1st ed.¬†2013. ed.). New York, NY: Springer New York.\r\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts: The MIT Press.\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nGeÃÅron, A. l. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems (First edition. ed.). Sebastopol, California: O‚ÄôReilly Media, Inc.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-14T17:29:17-05:00",
    "input_file": "An-overview-of-ML-and-PA.utf8.md"
  },
  {
    "path": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/",
    "title": "Gamma Distribution in an Intuitive Explanation",
    "description": "Intuitively explain the Gamma Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "ggplot2",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nFact\r\nGamma function\r\nDefinition\r\nParameters of Gamma: a shape with a scale or a rate\r\nPlots\r\n\r\nFact\r\nWhy did we invent Gamma distribution? Answer: To predict the wait time until future events. Hmmm ok, but I thought that‚Äôs what the exponential distribution is for. Then, what‚Äôs the difference between exponential distribution and gamma distribution? The exponential distribution predicts the wait time until the very first event. The gamma distribution, on the other hand, predicts the wait time until the k-th event occurs.\r\n‚Äì Aerin Kim, Gamma Distribution ‚Äì Intuition, Derivation, and Examples\r\nExamples:\r\nThe time I wait to receive an interview might follow an exponential. Now, I am waiting not for my first interview offer but my third interview offer. How long must I wait? This waiting time can be described by a gamma.\r\nI missed the first and second CTA train to go to the campus. Now, how long I am able to catch the third train?\r\nGamma function\r\nIn the lecture series of Statistics 110, Lecture 24: Gamma distribution and Poisson process | Statistics 110, Prof.¬†Joe Blitzstein had connected the \\(n!\\) function to the Gamma function. Why?\r\nLet‚Äôs see the Gamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\): \\[\\Gamma(n) = (n-1)! = 1 \\times 2 \\times 3 \\times ... \\times (n-1)\\]\r\nOne beautiful formula, Stirling formula to approximate the \\(n!\\), actually an extremely good approximation: \\[ n! \\approx \\sqrt{2\\pi n} \\Big( \\frac{n}{e}\\Big)^n\\]\r\n\r\n\r\nn <- c(1:6)\r\ny <- vector(mode = \"numeric\", length = length(n))\r\ny[1] <- 1\r\nfor(i in 2:length(n)) {\r\n  y[i] = y[i-1] * i\r\n}\r\ndta <- as.data.frame(cbind(n,y))\r\nlibrary(ggplot2)\r\nggplot(dta, aes(n, y)) + \r\n  geom_point() +\r\n  scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\")) +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nThen how we connect the dots. There are many ways to do it, but there‚Äôs a philosophical way to do it by Gamma function, which is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\\Gamma(t) = \\int_0^{\\infty} x^t e^{‚àíx} \\frac{dx}{x} \\]\r\nDefinition\r\nFrom the Gamma function, how we got the PDF of Gamma distribution. We would normalize the Gamma distribution, which means from:\r\n\\[ \\Gamma(k) = \\int_0^{\\infty} x^{k} e^{‚àíx} \\frac{dx}{x} \\]\r\nto,\r\n\\[ 1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(k)} x^{k} e^{‚àíx} \\frac{dx}{x} \\]\r\nThen, \\(X = \\frac{1}{\\Gamma(k)} x^{k} e^{‚àíx} \\frac{1}{x}\\) \\(\\sim\\) \\(Gamma(k, 1)\\) which has shape of \\(k\\) and scale of \\(1\\).\r\nHow we turn the scale of \\(1\\) to a general scale of \\(\\theta\\)?\r\nImagine that \\(Y \\sim \\frac{X}{\\theta}\\) where \\(X \\sim \\ Gamma(k,1)\\)\r\n\\(f_Y(y) = f_X(x) \\frac{dx}{dy} = \\frac{1}{\\Gamma(k)} (\\theta y)^{k} e^{‚àí\\theta y} \\frac{1}{\\theta y} \\theta\\) where \\(\\frac{dx}{dy} = \\theta\\)\r\nThus, \\(f(y) = \\frac{1}{\\Gamma(k) \\theta^{k}} (y)^{k} e^{‚àí\\theta y} \\frac{1}{y}\\)\r\nParameters of Gamma: a shape with a scale or a rate\r\n\r\n\r\nknitr::include_graphics(\"Gamma_scalevsrate_inwiki.png\") \r\n\r\n\r\n\r\n\r\n(#fig:model diagram)From https://en.wikipedia.org/wiki/Gamma_distribution\r\n\r\n\r\n\r\nFor (\\(\\alpha\\), \\(\\beta\\)) parameterization: Using our notation \\(k\\) (the # of events) & \\(\\lambda\\) (the rate of events), simply substitute \\(\\alpha\\) with \\(k\\), \\(\\beta\\) with \\(\\lambda\\). The PDF stays the same format as what we‚Äôve derived.\r\nFor (\\(k\\), \\(\\theta\\)) parameterization: \\(\\theta\\) is a reciprocal of the event rate \\(\\lambda\\), which is the mean wait time (the average time between event arrivals).\r\nPlots\r\nI plotted the gamma distribution with the shape of \\(k\\), and constantly rate = \\(1\\)\r\n\r\n\r\nT <- seq(0,20,by=2.5)\r\n\r\ndf <- data.frame(T)\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=1, rate=1), aes(colour = \"k= 1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=5, rate=1), aes(colour = \"k= 5\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"k=10\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"yellowgreen\", \"olivedrab\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nI plotted the gamma distribution with the constantly shape of k = 10, and variant rate from 1 to 3.\r\n\r\n\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"r=1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=10, rate=2), aes(colour = \"r=2\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=3), aes(colour = \"r=3\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"gold\", \"burlywood\", \"darkorange\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution (k=10)\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/Gamma-distribution-in-an-intuitive-explanation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-14T17:29:02-05:00",
    "input_file": "Gamma-distribution-in-an-intuitive-explanation.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/",
    "title": "Beta Distribution in an Intuitive Explanation",
    "description": "Intuitively explain the Beta Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nDefinition\r\nIntuitive interpretation\r\nBeta function\r\nGamma function\r\nMain facts\r\nIn actions\r\nPlots in shiny\r\nFurther reading\r\n\r\nMotivation\r\n\r\nEven though I had learned the beta distribution from UIC‚Äôs Bayesian methods course and tutored it, such as setting up it as the prior distribution in conjugate distribution context. But it was easy to forget because of its dried content and too abstract. Here I try to combine the rigid theory (UC coursework‚Äôs content) and intuitive thought. By that way, I was able to ‚Äòpermenently stamp‚Äô the concept to my brain.\r\n\r\nThe Beta distribution is a probability distribution on/of probabilities\r\nThe beta distribution describes a family of continuous probability distributions that are nonzero only on the interval (0 1).\r\nFor example, we can use it to model the probabilities: the Click-Through Rate of the advertisement, the batting averages, the 5-year survival chance for women with breast cancer, and so on.\r\nDefinition\r\nA continuous random variable \\(X_B \\sim Beta(\\alpha, \\beta)\\) has Beta distribution if its probability density function (PDF) is\r\n\\[ \r\nf_{X_B} (x; \\alpha, \\beta) = \\frac{1}{B(Œ±,Œ≤)} x^{\\alpha ‚àí 1} (1‚àíx)^{\\beta ‚àí 1}, \\ \\ \\text{for} \\ 0 < x < 1.\r\n\\]\r\nwhere \\(B(\\cdot)\\) is the Beta function and shape parameters \\(\\alpha, \\beta > 0\\).\r\nIntuitive interpretation\r\n\r\nPDF\r\nProbability as a ‚Ä¶\r\nBinomial\r\n\\(f(x) = {n \\choose x} p^x (1-p)^{n-x}\\)\r\nparameter\r\n\r\n\\(\\rightarrow\\) the function of \\(x\\)\r\n\r\nBeta\r\n\\(f(p) = \\frac{1}{B(Œ±,Œ≤)} p^{\\alpha ‚àí 1} (1‚àíp)^{\\beta ‚àí 1}\\)\r\nrandom variable\r\n\r\n\\(\\rightarrow\\) the function of \\(p\\)\r\n\r\nThe beta distribution intuitively comes into play when we look at it in terms of numerator‚Äî\\(x/p\\) to the power of something multiplied by \\(1-x/1-p\\) to the power of something‚Äîfrom the lens of the binomial distribution.\r\nThe difference between the binomial and the beta is that the above models the number of successes (\\(x\\)), while the below models the probability (\\(p\\)) of success. In other words, the probability is a parameter in binomial; In the Beta, the probability is a random variable.\r\nIn this context, the shape parameters \\(\\alpha\\) and \\(\\beta\\) or \\(\\alpha-1\\) as the number of successes and \\(\\beta-1\\) as the number of failures\r\nWe can explore the beauty of beta distribution via the the calculator for Beta distribution‚ÄîDr.¬†Bognar at the University of Iowa built it.\r\nBeta distribution is very flexible: bell-curve (The PDF of a beta distribution is approximately normal if \\(\\alpha + \\beta\\) is large enough and \\(\\alpha\\) & \\(\\beta\\) are approximately equal), U-shaped (when \\(\\alpha\\) < 1, \\(\\beta\\) < 1) and even straight line. Here‚Äôs an graph excerpt from wikipedia.\r\nThe very flexible of Beta distributionBeta function\r\nThe beta function is\r\n\\[ \r\nB(x,y) = \\int_0^1 t^{x‚àí1} (1‚àít)^{y‚àí1} dt = \\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)},\r\n\\]\r\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function.\r\nGamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\):\r\n\\[ \r\n\\Gamma (n) = (n‚àí1)! = 1 \\times 2 \\times 3 \\times ... \\times (n‚àí1)\r\n\\]\r\nThe gamma function is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\r\n\\Gamma (t) = \\int_0^{\\infty} x^{t-1} e^{-x} dx\r\n\\]\r\n\r\nSimplify the Beta function with the Gamma Function \\(\\Rightarrow\\) we saw the PDF of Beta written in terms of the Gamma function. The Beta function is the ratio of the product of the Gamma function of each parameter divided by the Gamma function of the sum of the parameters (proof refered the further reading topic).\r\n\r\nMain facts\r\n\\[\r\nE[X_B] = \\mu = \\frac{\\alpha}{\\alpha + \\beta}; \\ \\ V[X_B] = \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\r\n\\]\r\nThe standard uniform distribution \\(\\text{Unif} \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nThe mode is \\(\\omega = \\frac{\\alpha ‚àí 1}{\\alpha + \\beta ‚àí 2}\\) for \\(\\alpha, \\beta > 1\\).\r\nThe concentration is \\(\\kappa = \\alpha + \\beta\\).\r\nDefinitions of \\(\\mu, \\omega\\) and \\(\\kappa\\) can be inverted:\r\n\\[ \\alpha = \\mu\\kappa,  \\beta = (1 ‚àí \\mu)\\kappa \\]\r\n\\[ \\alpha = \\omega(\\kappa‚àí2)+1,  \\beta = (1 ‚àí \\omega)(\\kappa‚àí2)+1, \\ \\kappa > 2. \\]\r\nParameter \\(\\kappa\\) is a measure of number of observations needed to change our previous belief about \\(\\mu\\).\r\nIf \\(\\kappa\\) is small we need only a few new observations.\r\nExample. Concentration \\(\\kappa = 8\\) around \\(\\mu = 0.5\\) corresponds to \\(\\alpha = \\mu \\kappa = 4\\) and \\(\\beta = (1 ‚àí \\mu) \\kappa = 4\\).\r\nParameterization in terms of mean value and standard deviation is:\r\n\\[ \\alpha = \\mu [\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1]; \\ \\ \\beta = (1 - \\mu)[\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1] \\]\r\nStandard deviation is typically smaller than standard deviation of uniform distribution on \\([0,1]\\), i.e.¬†\\(0.28867\\).\r\nExamples.\r\nFor \\(\\mu = 0.5\\), \\(\\sigma = 0.28867\\) the shape parameters are \\(\\alpha = 1\\), \\(\\beta = 1\\).\r\nFind shape parameters of beta distribution with \\(\\mu = 0.5\\), \\(\\sigma = 0.1\\).\r\nThe standard uniform distribution \\(Unif \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nIn actions\r\nKeep parameter \\(\\beta\\) fixed. Move \\(\\alpha\\) up or down. Observe how the mass of the distribution moves\r\n\r\n\r\nx <- seq(0,1,by=0.2)\r\n\r\ndf <- data.frame(x)\r\nggplot(data=df, aes(x=x))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=2), aes(colour = \"alpha=1,beta=2\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=2), aes(colour = \"alpha=4,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=2), aes(colour = \"alpha=6,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=8, shape2=2), aes(colour = \"alpha=8,beta=2\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDo the same as above, but keep \\(\\alpha\\) constant and move \\(\\beta\\) up or down\r\n\r\n\r\nggplot(data=df, aes(x=x))+\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=1), aes(colour = \"alpha=2,beta=1\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=5), aes(colour = \"alpha=2,beta=5\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=6), aes(colour = \"alpha=2,beta=6\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=8), aes(colour = \"alpha=2,beta=8\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nMake \\(\\alpha = \\beta = 1\\). What does the shape of the distribution tell you about your knowledge about random variable \\(\\theta\\)?\\(\\Rightarrow\\) The standard uniform distribution \\(Unif(0,1)\\) is a special case of the beta distribution \\(Beta (1,1)\\), when \\(\\alpha\\)=\\(\\beta\\)=1.\r\n\r\n\r\nggplot(data=df, aes(x=x))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"green\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nKeep \\(\\alpha = \\beta\\) , but move both of them up or down. Interpret the shape of the distribution\r\n\r\n\r\nggplot(data=df, aes(x=x))+\r\n  stat_function(fun=dbeta, args=list(shape1=0.5, shape2=0.5), aes(colour = \"alpha=0.5,beta=0.5\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=4), aes(colour = \"alpha=4,beta=4\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=6), aes(colour = \"alpha=6,beta=6\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWhen beta distribution is used as a prior for parameter of binomial distribution, parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as previously observed numbers of successes (\\(\\alpha\\)) or failures (\\(\\beta\\)). For example, if in 2 Bernoulli experiments there was 1 success and 1 failure you can express opinion about probability of success as \\(Beta(1,1)\\). What would you assume as prior if in 6 previously observed outcomes there were 3 successes and 3 failures? What is the likely value of the parameter? Do we have more or less information than in case of 1 success and 1 failure? \\(\\Rightarrow\\) Think of more\r\n\r\n\r\nggplot(data=df, aes(x=x))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=3), aes(colour = \"alpha=3,beta=3\")) +\r\n  stat_function(fun=dbinom, args=list(size=1, prob=0.5), aes(colour = \"Bernoulli w/ prob=0.5\")) + # bernoulli\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"red\",\"green\",\"black\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDefine distribution with mode \\(\\omega\\)=.8 and concentration \\(\\kappa = 12\\). To do that find shape parameters \\(\\alpha = \\omega (\\kappa ‚àí 2) + 1 = 9\\) and \\(\\beta = (1 ‚àí \\omega)(\\kappa ‚àí 2) + 1 = 3\\).\r\n\r\n\r\nggplot(data=df, aes(x=x))+\r\n  stat_function(fun=dbeta, args=list(shape1=9, shape2=3), aes(colour = \"alpha=9,beta=3\")) +\r\n  scale_y_continuous(limits=c(0,3.4)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nPlots in shiny\r\nPlanning to build an shiny app to plot beta distribution on the specification of shape parameter (‚Äústill being in the process‚Äù).\r\nFurther reading\r\nBayesian Methods, UC‚Äôs lecture\r\nDavid Robinson (Principal Data Scientist at Heap, works in R and Python), Understanding the beta distribution (using baseball statistics), http://varianceexplained.org/statistics/beta_distribution_and_baseball/\r\nAerin Kim, Beta Distribution ‚Äî Intuition, Examples, and Derivation, https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/Beta-Distribution-in-an-Intuitive-Explanation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-14T17:28:44-05:00",
    "input_file": "Beta-Distribution-in-an-Intuitive-Explanation.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
