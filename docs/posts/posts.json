[
  {
    "path": "posts/2021-05-26-Mplus-as-a-knitr-engine/",
    "title": "Set up to run Mplus inside Rmarkdown",
    "description": "Mplus as a knitr engine in Rmarkdown  \nMplusAutomation: a brief guide",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [
      "Biostatistics",
      "Psychology/Socialogy",
      "Mplus",
      "R",
      "MplusAutomation",
      "Tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nGoals\r\nSet up Mplus in Rmarkdown\r\nGentle introduction to MplusAutomation\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nLatent Profile Analysis using Mplus\r\nRun direct in Rmarkdown\r\nUsing runModels\r\n\r\nCompare the model fit\r\n\r\n\r\nGoals\r\nThis post I aim to set up the Mplus engine using knitr package in Rmarkdown.\r\nAfter that, I run a simple model example to demonstrate how Mplus works within Rmarkdown. (Therefore, I do not intent to explain the model results and fit here.)\r\nSet up Mplus in Rmarkdown\r\nGlobally we can change the engine interpreters for one/multiple engine(s), we applied for Mplus:\r\n\r\nknitr::opts_chunk$set(engine.path = list(\r\n  mplus = \"C:/Program Files/Mplus/Mplus\"\r\n))\r\n\r\nNext step, we register a Mplus custom language engine:\r\n\r\nknitr::knit_engines$set(mplus = function(options) {\r\n    code <- paste(options$code, collapse = \"\\n\")\r\n    fileConn<-file(\"formplus.inp\")\r\n    writeLines(code, fileConn)\r\n    close(fileConn)\r\n    out  <- system2(\"C:/Program Files/Mplus/Mplus\", \"formplus.inp\")\r\n    fileConnOutput <- file(\"formplus.out\")\r\n    mplusOutput <- readLines(fileConnOutput)\r\n    knitr::engine_output(options, code, mplusOutput)\r\n})\r\n\r\nFor more language engines, we can refer demos from Yihui Xie’s post.\r\nFurther reading, we may read a Rich Jones’ post on rpubs\r\nGentle introduction to MplusAutomation\r\nThe MplusAutomation package for R (Hallquist and Wiley 2018):\r\nCreating many similar syntax files:\r\nSimulations with different sample sizes\r\nExcluding different parts of a sample\r\n\r\nRunning batches of input files\r\nExtracting and tabulating model parameters and test statistics.\r\nFour core routines support these aims:\r\ncreateModels\r\nrunModels\r\nreadModels\r\ncompareModels\r\nThe MplusAutomation package can be installed within R using the following call:\r\n\r\nif (!require(MplusAutomation)) install.packages(\"MplusAutomation\")\r\nlibrary(MplusAutomation)\r\n\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nI use Edgar Anderson’s Iris Data with 150 cases (rows) and 5 variables named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.\r\niris data set gives the measurements in cm of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris: Iris setosa, versicolor, and virginica.\r\n\r\n?iris\r\n\r\n\r\nhead(iris)\r\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1          5.1         3.5          1.4         0.2  setosa\r\n2          4.9         3.0          1.4         0.2  setosa\r\n3          4.7         3.2          1.3         0.2  setosa\r\n4          4.6         3.1          1.5         0.2  setosa\r\n5          5.0         3.6          1.4         0.2  setosa\r\n6          5.4         3.9          1.7         0.4  setosa\r\n\r\nprepareMPlusData() function prepares a data file in the MPlus’ format, namely, a tab-separated .dat file with no column names.\r\n\r\nprepareMplusData(iris[, -5], \"iris.dat\", inpfile=TRUE)\r\n\r\nLatent Profile Analysis using Mplus\r\nWe can directly run in Rmarkdown or apply runModels to run the .inp files.\r\nRun direct in Rmarkdown\r\nOne for which we estimate different means between 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:28 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n*** WARNING in MODEL command\r\n  All variables are uncorrelated with all other variables within class.\r\n  Check that this is what is intended.\r\n   2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -488.915  253358           2\r\n            -488.915  68985            17\r\n            -488.915  76974            16\r\n            -488.915  573096           20\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       13\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -488.915\r\n          H0 Scaling Correction Factor      0.9851\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                    1003.830\r\n          Bayesian (BIC)                  1042.968\r\n          Sample-Size Adjusted BIC        1001.825\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         0.991\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.003    0.997\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   0.995    0.005\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1      5.216    0.000\r\n    2    -13.816    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n Means\r\n    X1                 5.006      0.049    102.032      0.000\r\n    X2                 3.423      0.055     61.909      0.000\r\n    X3                 1.471      0.026     55.788      0.000\r\n    X4                 0.250      0.016     15.938      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nLatent Class 2\r\n\r\n Means\r\n    X1                 6.265      0.068     92.358      0.000\r\n    X2                 2.873      0.034     85.125      0.000\r\n    X3                 4.911      0.085     57.798      0.000\r\n    X4                 1.678      0.043     38.643      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.685      0.175     -3.924      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.810E-03\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -741.018\r\n          2 Times the Loglikelihood Difference            504.205\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             18.069\r\n          Standard Deviation                               25.180\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           484.852\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:28:08\r\n        Ending Time:  22:28:08\r\n       Elapsed Time:  00:00:00\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means between the 2 profiles and the model is specified to estimate the correlation (or covariance) for the variables\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n\r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:28 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      x1 WITH x2-x4;\r\n      x2 WITH x3-x4;\r\n      x3 WITH x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n   1 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -296.448  533738           11\r\n            -296.448  68985            17\r\n            -296.448  unperturbed      0\r\n            -296.448  27071            15\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       19\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -296.448\r\n          H0 Scaling Correction Factor      1.0304\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                     630.895\r\n          Bayesian (BIC)                   688.097\r\n          Sample-Size Adjusted BIC         627.966\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         1.000\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1     11.903    0.000\r\n    2    -12.706    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 5.006      0.049    101.442      0.000\r\n    X2                 3.428      0.053     64.589      0.000\r\n    X3                 1.462      0.024     60.137      0.000\r\n    X4                 0.246      0.015     16.674      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nLatent Class 2\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 6.262      0.066     94.947      0.000\r\n    X2                 2.872      0.033     86.743      0.000\r\n    X3                 4.906      0.082     59.719      0.000\r\n    X4                 1.676      0.042     39.652      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.693      0.173     -4.002      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.149E-04\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -379.915\r\n          2 Times the Loglikelihood Difference            166.934\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             11.054\r\n          Standard Deviation                                9.531\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           160.527\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:28:09\r\n        Ending Time:  22:28:09\r\n       Elapsed Time:  00:00:00\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means and the model is specified to different covariances (and variable variances) between the 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n\r\n    %c#1%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\n\r\n\r\nThe disadvantage is that Rmarkdown shows a lengthy output, making “eye strain” to track the necessary information.\r\n\r\nUsing runModels\r\nSave each chunk of the following models (either in a .txt file or in MPlus style using a .inp file type), then run with runModels\r\n\r\n# Model 1\r\nrunModels(\"2-iris-LPA_means.inp\")\r\n\r\nRunning model: 2-iris-LPA_means.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means.inp\" \r\n# Model 2\r\nrunModels(\"2-iris-LPA_means_correlated.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated.inp\" \r\n# Model 3\r\nrunModels(\"2-iris-LPA_means_correlated_free_variances.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated_free_variances.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated_free_variances.inp\" \r\n\r\n\r\nm1 <- readModels(\"2-iris-LPA_means.out\")\r\nReading model:  2-iris-LPA_means.out \r\nm2 <- readModels(\"2-iris-LPA_means_correlated.out\")\r\nReading model:  2-iris-LPA_means_correlated.out \r\nm3 <- readModels(\"2-iris-LPA_means_correlated_free_variances.out\")\r\nReading model:  2-iris-LPA_means_correlated_free_variances.out \r\n\r\nCompare the model fit\r\nNow, we inspect the fit statistics and other summary information for the three models:\r\n\r\nm1$summaries$BIC\r\n[1] 1042.968\r\nm2$summaries$BIC\r\n[1] 688.097\r\nm3$summaries$BIC\r\n[1] 574.018\r\n\r\nAnd examine parameters:\r\n\r\nm1$parameters[[1]][-nrow(m1$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1        Means    X1 5.006 0.049 102.032    0           1\r\n2        Means    X2 3.423 0.055  61.909    0           1\r\n3        Means    X3 1.471 0.026  55.788    0           1\r\n4        Means    X4 0.250 0.016  15.938    0           1\r\n5    Variances    X1 0.328 0.042   7.853    0           1\r\n6    Variances    X2 0.121 0.017   7.347    0           1\r\n7    Variances    X3 0.459 0.063   7.340    0           1\r\n8    Variances    X4 0.123 0.013   9.126    0           1\r\n9        Means    X1 6.265 0.068  92.358    0           2\r\n10       Means    X2 2.873 0.034  85.125    0           2\r\n11       Means    X3 4.911 0.085  57.798    0           2\r\n12       Means    X4 1.678 0.043  38.643    0           2\r\n13   Variances    X1 0.328 0.042   7.853    0           2\r\n14   Variances    X2 0.121 0.017   7.347    0           2\r\n15   Variances    X3 0.459 0.063   7.340    0           2\r\n16   Variances    X4 0.123 0.013   9.126    0           2\r\nm2$parameters[[1]][-nrow(m2$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1      X1.WITH    X2 0.113 0.019   5.805    0           1\r\n2      X1.WITH    X3 0.305 0.050   6.104    0           1\r\n3      X1.WITH    X4 0.114 0.019   6.112    0           1\r\n4      X2.WITH    X3 0.098 0.022   4.359    0           1\r\n5      X2.WITH    X4 0.056 0.010   5.330    0           1\r\n6      X3.WITH    X4 0.193 0.024   8.175    0           1\r\n7        Means    X1 5.006 0.049 101.442    0           1\r\n8        Means    X2 3.428 0.053  64.589    0           1\r\n9        Means    X3 1.462 0.024  60.137    0           1\r\n10       Means    X4 0.246 0.015  16.674    0           1\r\n11   Variances    X1 0.331 0.042   7.870    0           1\r\n12   Variances    X2 0.120 0.016   7.574    0           1\r\n13   Variances    X3 0.460 0.063   7.333    0           1\r\n14   Variances    X4 0.123 0.013   9.137    0           1\r\n15     X1.WITH    X2 0.113 0.019   5.805    0           2\r\n16     X1.WITH    X3 0.305 0.050   6.104    0           2\r\n17     X1.WITH    X4 0.114 0.019   6.112    0           2\r\n18     X2.WITH    X3 0.098 0.022   4.359    0           2\r\n19     X2.WITH    X4 0.056 0.010   5.330    0           2\r\n20     X3.WITH    X4 0.193 0.024   8.175    0           2\r\n21       Means    X1 6.262 0.066  94.947    0           2\r\n22       Means    X2 2.872 0.033  86.743    0           2\r\n23       Means    X3 4.906 0.082  59.719    0           2\r\n24       Means    X4 1.676 0.042  39.652    0           2\r\n25   Variances    X1 0.331 0.042   7.870    0           2\r\n26   Variances    X2 0.120 0.016   7.574    0           2\r\n27   Variances    X3 0.460 0.063   7.333    0           2\r\n28   Variances    X4 0.123 0.013   9.137    0           2\r\nm3$parameters[[1]][-nrow(m3$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se  pval LatentClass\r\n1      X1.WITH    X2 0.097 0.022   4.469 0.000           1\r\n2      X1.WITH    X3 0.016 0.010   1.655 0.098           1\r\n3      X1.WITH    X4 0.010 0.004   2.486 0.013           1\r\n4      X2.WITH    X3 0.011 0.008   1.418 0.156           1\r\n5      X2.WITH    X4 0.009 0.005   1.763 0.078           1\r\n6      X3.WITH    X4 0.006 0.003   2.316 0.021           1\r\n7        Means    X1 5.006 0.049 101.439 0.000           1\r\n8        Means    X2 3.428 0.053  64.591 0.000           1\r\n9        Means    X3 1.462 0.024  60.132 0.000           1\r\n10       Means    X4 0.246 0.015  16.673 0.000           1\r\n11   Variances    X1 0.122 0.022   5.498 0.000           1\r\n12   Variances    X2 0.141 0.033   4.267 0.000           1\r\n13   Variances    X3 0.030 0.007   4.222 0.000           1\r\n14   Variances    X4 0.011 0.003   3.816 0.000           1\r\n15     X1.WITH    X2 0.121 0.027   4.467 0.000           2\r\n16     X1.WITH    X3 0.449 0.070   6.377 0.000           2\r\n17     X1.WITH    X4 0.166 0.026   6.282 0.000           2\r\n18     X2.WITH    X3 0.141 0.033   4.330 0.000           2\r\n19     X2.WITH    X4 0.079 0.015   5.295 0.000           2\r\n20     X3.WITH    X4 0.286 0.031   9.107 0.000           2\r\n21       Means    X1 6.262 0.066  94.948 0.000           2\r\n22       Means    X2 2.872 0.033  86.743 0.000           2\r\n23       Means    X3 4.906 0.082  59.719 0.000           2\r\n24       Means    X4 1.676 0.042  39.652 0.000           2\r\n25   Variances    X1 0.435 0.059   7.332 0.000           2\r\n26   Variances    X2 0.110 0.017   6.442 0.000           2\r\n27   Variances    X3 0.675 0.086   7.822 0.000           2\r\n28   Variances    X4 0.179 0.018  10.148 0.000           2\r\n\r\nOne last thing, I want to mention about the createModels() which can creates a set of models using a template. We can save file as mplus_iris_lpa_template.txt\r\n[[init]]\r\niterators = classes;\r\nclasses = 1:9;\r\nfilename = \"[[classes]]-iris-LPA.inp\";\r\noutputDirectory = the_dir;\r\n[[/init]]\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1 x2 x3 x4;\r\n\r\n    Classes = c([[classes]]) ;\r\n\r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    [x1-x4];\r\n\r\n            \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nHere is an example that would create models with different numbers of profiles, from 1 to 9.\r\n\r\n#Set the folder containing mplus.txt as working\r\ncreateModels(\"mplus_iris_lpa_template.txt\")\r\n\r\n#Set the folder containing input files as working directory\r\nrunModels()\r\n\r\n#Set the folder containing output files as working directory\r\nmodels_list <- readModels()\r\noutputs<-extractModelParameters()\r\n\r\nFurther reading, we can look at a Josh Rosenberg’s post and the Mplus website\r\nMore information about the MplusAutomation package in GitHub.\r\n\r\n\r\nHallquist, M. N., and J. F. Wiley. 2018. “MplusAutomation: An r Package for Facilitating Large-Scale Latent Variable Analyses in Mplus.” Journal Article. Struct Equ Modeling 25 (4): 621–38. https://doi.org/10.1080/10705511.2017.1402334.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-26T22:28:17-05:00",
    "input_file": "Mplus-as-a-knitr-engine-in-Rmarkdown.utf8.md"
  },
  {
    "path": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/",
    "title": "A Note on 5-day Workshop on Mplus ~ Day 1",
    "description": "A Crash Course on Social Psychology Research from Michael Zyphur: Regression - Pathway analysis - Model fit  \nCombination of using Mplus and R  \nDrawing a pathway graph/causal graph using `DiagrammeR` package",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [
      "Biostatistics",
      "Social Sciences",
      "Mplus",
      "Pathway/causal graph"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nCourse outline\r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nA variance \\(\\sigma^2\\)\r\nGraphical\r\nMultivariate Parameters\r\nCovariance \\(\\sigma_{xy}\\)\r\nA Linear Regression Model\r\nIn Terms of Distributions?\r\nWhy Regression? Causality\r\nControlling for variables\r\nWith Path Diagrams\r\nSummary\r\n\r\nPart 2: Mplus and Estimation\r\nData\r\nVariable\r\nDefine\r\nAnalysis\r\nEstimation and Inference\r\nModel\r\nOutput, Savedata, Plot, Montecarlo\r\nSummary\r\n\r\nPart 3: Path Analysis\r\nHistory\r\nShow case\r\nStructural Model Specification\r\nModel diagram\r\nReport\r\n\r\nWhat Does This Mean? How to Specify It?\r\nSummary\r\n\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nML Estimator\r\nModel Build\r\nModel Diagnostics\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nAbsolute Fit: SRMR\r\nAbsolute Fit: RMSEA\r\nRelative Fit: CFI\r\nRelative Fit: TLI/NNFI\r\nInformation Criteria: AIC\r\nInformation Criteria: BIC\r\n\r\nModification Indices\r\nBayesian Estimates of Fit\r\nPosterior Predictive Checking\r\nDeviance Information Criterion\r\n\r\nModel Selection\r\n\r\nFurther Readings\r\nPart 1\r\nPart 2\r\nPart 3\r\nPart 4\r\n\r\n\r\nMotivation\r\nWhile working on project Harmony at MRC-IHRP, my primary task is building an R package to build the tables and plots from alignment analysis designed for the multi-factor categorical case by extracting the Mplus output’s information. Since I have a chance to expose myself to Mplus, why don’t I self-learn a new tool and share it on my blog?\r\nThe Mplus and I see that a change to dive into Social Psychology Research. What on earth? The same concepts with statistics but a whole new world of terminology!!!\r\nThese can separate into five parts (equivalent to 5 days, then I divided into five topics) of taking notes on Structural Equation and Multilevel Modeling in the Mplus workshop of Prof. Michael Zyphur.\r\nHere’s a first part (1) of the so-called ‘A Crash Course on Social Psychology Research.’\r\n\r\nCourse outline\r\nStructural Equation and Multilevel Modeling in Mplus\r\nDay 1 - Introducing Mplus\r\nRegression, Covariation, and Statistical Models\r\nMplus and Parameter Estimation\r\nPath Analysis\r\nModel Fit and Model Selection\r\n\r\nDay 2 - Path Analysis\r\nMediation\r\nInstrumental Variable Methods\r\nModeration\r\nModerated Mediation\r\n\r\nDay 3 - SEM\r\nLatent Variables\r\nConfirmatory Factor Analysis\r\nStructural Equation Modeling\r\nModel Identification\r\n\r\nDay 4 - MLM\r\nMultilevel Data and Regression\r\nMultilevel Path Analysis\r\nMultilevel Confirmatory Factor Analysis and Structural Equation Modeling\r\nRandom Slopes\r\n\r\nDay 5 - LGM\r\nLongitudinal Data and Processes\r\nLatent Growth Models as Multilevel Models\r\nLatent Growth Models as Structural Equation Models\r\nDynamic Latent Growth Modeling\r\n\r\nThe material can be downloaded here\r\nReminder:\r\n  - Path analysis: regression for observed variables  \r\n  - CFA: regression from latent --> observed variables  \r\n  - SEM: regression among latent variables  \r\n  - Multilevel models: regression at multiple 'levels'  \r\n  - Letant growth: model change with latent variables  \r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nParameters depend on the distributions we model\r\nAssuming a normal distribution for y, we have:\r\nMean \\(\\mu_y\\): a location parameter\r\nVariance \\(\\sigma^2_y\\) : a scale parameter\r\n\r\n\r\np <- seq(-5,5,by=0.1)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(.2)), aes(colour = \"mu=0,sigma2=.2\")) + \r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(1.0)), aes(colour = \"mu=0,sigma2=1.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(5.0)), aes(colour = \"mu=0,sigma2=5.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=-2, sd=sqrt(2.0)), aes(colour = \"mu=-2,sigma2=2\")) +\r\n  scale_y_continuous(limits=c(0,1.0)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Normal Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\nA variance \\(\\sigma^2\\)\r\nTells us the spread of scores along a variable\r\nTo estimate a variance, we assume variation is ‘random’ or due to ‘chance’\r\nThis allows using distributions & their rules\r\nEither way, a variable varies (by definition)\r\nTo model a variable y, we model its variation\r\nTo speak of a variable is to speak of variance!\r\n\r\nGraphical\r\nWe’ll show models with diagrams\r\nSquares and rectangles are observed variables\r\nVariance is double-headed arrow on one variable\r\nFor concision, we will be selective about what we diagram\r\n\r\n\r\nTo refer to y’s variance in Mplus, we will write “y”\r\nMultivariate Parameters\r\nFor multiple variables x and y:\r\nMultivartiate mean vector \\(\\mu\\) for x and y\r\nCovariance \\(\\Sigma\\) as matrix of 2 variances and a covariance\r\n\r\n\r\n#generate the data\r\ngibbs<-function (n, rho) {\r\n    mat <- matrix(ncol = 2, nrow = n)\r\n    x <- 0\r\n    y <- 0\r\n    mat[1, ] <- c(x, y)\r\n    for (i in 2:n) {\r\n        x <- rnorm(1, rho * y, (1 - rho^2))\r\n        y <- rnorm(1, rho * x, (1 - rho^2))\r\n        mat[i, ] <- c(x, y)\r\n    }\r\n    mat\r\n}\r\nbvn <- gibbs(10000, 0.98)\r\n\r\n#setup\r\nlibrary(rgl) # plot3d, quads3d, lines3d, grid3d, par3d, axes3d, box3d, mtext3d\r\nlibrary(car) # dataEllipse\r\n\r\n#process the data\r\nhx <- hist(bvn[,2], plot=FALSE)\r\nhxs <- hx$density / sum(hx$density)\r\nhy <- hist(bvn[,1], plot=FALSE)\r\nhys <- hy$density / sum(hy$density)\r\n\r\n## [xy]max: so that there's no overlap in the adjoining corner\r\nxmax <- tail(hx$breaks, n=1) + diff(tail(hx$breaks, n=2))\r\nymax <- tail(hy$breaks, n=1) + diff(tail(hy$breaks, n=2))\r\nzmax <- max(hxs, hys)\r\n\r\n#Basic scatterplot on the floor\r\n## the base scatterplot\r\nplot3d(bvn[,2], bvn[,1], 0, zlim=c(0, zmax), pch='.',\r\n       xlab='X', ylab='Y', zlab='', axes=FALSE)\r\npar3d(scale=c(1,1,3))\r\n\r\n#Histograms on the back walls\r\n## manually create each histogram\r\nfor (ii in seq_along(hx$counts)) {\r\n    quads3d(hx$breaks[ii]*c(.9,.9,.1,.1) + hx$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            rep(ymax, 4),\r\n            hxs[ii]*c(0,1,1,0), color='gray80')\r\n}\r\nfor (ii in seq_along(hy$counts)) {\r\n    quads3d(rep(xmax, 4),\r\n            hy$breaks[ii]*c(.9,.9,.1,.1) + hy$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            hys[ii]*c(0,1,1,0), color='gray80')\r\n}\r\n\r\n#Summary Lines\r\n## I use these to ensure the lines are plotted \"in front of\" the\r\n## respective dot/hist\r\nbb <- par3d('bbox')\r\ninset <- 0.02 # percent off of the floor/wall for lines\r\nx1 <- bb[1] + (1-inset)*diff(bb[1:2])\r\ny1 <- bb[3] + (1-inset)*diff(bb[3:4])\r\nz1 <- bb[5] + inset*diff(bb[5:6])\r\n\r\n## even with draw=FALSE, dataEllipse still pops up a dev, so I create\r\n## a dummy dev and destroy it ... better way to do this?\r\n###dev.new()\r\nde <- dataEllipse(bvn[,1], bvn[,2], draw=FALSE, levels=0.95)\r\n###dev.off()\r\n\r\n## the ellipse\r\nlines3d(de[,2], de[,1], z1, color='green', lwd=3)\r\n\r\n## the two density curves, probability-style\r\ndenx <- density(bvn[,2])\r\nlines3d(denx$x, rep(y1, length(denx$x)), denx$y / sum(hx$density), col='red', lwd=3)\r\ndeny <- density(bvn[,1])\r\nlines3d(rep(x1, length(deny$x)), deny$x, deny$y / sum(hy$density), col='blue', lwd=3)\r\n\r\n#Beautifications\r\ngrid3d(c('x+', 'y+', 'z-'), n=10)\r\nbox3d()\r\naxes3d(edges=c('x-', 'y-', 'z+'))\r\noutset <- 1.2 # place text outside of bbox *this* percentage\r\nmtext3d('P(X)', edge='x+', pos=c(0, ymax, outset * zmax))\r\nmtext3d('P(Y)', edge='y+', pos=c(xmax, 0, outset * zmax))\r\n\r\n\r\nCovariance \\(\\sigma_{xy}\\)\r\nTells us how two variables ‘hang together’\r\nCorrelation is just a standardized covariance\r\n\r\nIf we assume variation is random …\r\nCovariance implies a non-causal relationship\r\nNo predictor or outcome, x and y simply co-vary\r\nBlue eyes & blond hair?\r\n\r\n\r\n\r\nIn Mplus we will write “y with x” or “x with y” If the relationship is non-causal, does it matter?\r\n\r\nA Linear Regression Model\r\nWhat is the rule \\(\\beta\\) ? Our linear model\r\n\\[ y_i = \\nu + \\beta x_i + \\epsilon_i \\]\r\n\\(y\\) and \\(x\\) are variables\r\n\\(\\epsilon\\) is a variable: unobserved residual/error\r\n\\(\\nu\\) and \\(\\beta\\) are constants/fixed that we estimate\r\nIn Terms of Distributions?\r\ny is decomposed into parts\r\nHow is the mean of \\(y\\) modeled?\\[ \\nu = \\mu_y — \\beta \\mu_x \\]\r\nHow is \\(y\\),\\(x\\) covariance modeled? \\[ \\beta = \\frac{\\sigma_{xy}}{\\sigma^2_x} \\]\r\nHow is the variance of \\(y\\) modeled? \\[  \\sigma^2_y =  \\sigma^2_y R^2 + \\sigma^2_{\\epsilon} \\]\r\nWhy Regression? Causality\r\nCausal statements are helpful for action\r\nConditions for \\(x \\rightarrow y\\) causality\r\n\\(x\\) precedes \\(y\\) in time\r\n\\(x\\) and \\(y\\) are related\r\n\\(x \\rightarrow y\\) effect isn’t due to a third variable z\r\n\r\nRegression assesses the second, and helps with the third\r\nControlling for variables\r\nRemoving the effect of \\(z\\) in \\(x \\rightarrow y\\) effect is\r\nControlling for \\(z\\)\r\nHolding \\(z\\) constant (it no longer varies)\r\nThe ‘independent effect’ of \\(x\\) on \\(y\\)\r\nWe ‘partial out’ the effect of \\(z\\)\r\n\r\nThese all mean the same thing:\r\nWe somehow make \\(z\\) irrelevant for \\(x \\rightarrow y\\)\r\nAllows estimating independent effects\r\n\\[ y_i = \\nu + \\beta_1 x_i + \\beta_2 z_i+ \\epsilon_i \\]\r\nEffects are independent and additive\r\nRead it left to right: \\(y\\) depends ON \\(x\\) \\(z\\)\r\nIn Mplus simply write “y ON x z”\r\nDoes it matter if “y ON x z” or “y ON z x” ?\r\nEach \\(\\beta\\) is just a slope, and \\(\\nu\\) is intercept on y-axis\r\nWith Path Diagrams\r\nRegression effects & residual as single-headed arrows\r\n\r\nResidual indicated as single-headed arrow w/out predictor\r\nRegress “y ON x z” reads the equation left-to-right\r\nNote: Predictors correlated by default in regression\r\nThe whole point is that the predictors are correlated\r\n\r\nSummary\r\nWe estimate rules/parameters to test theory\r\nMeans, (co)variances, & causal effects\r\n\r\nMplus has a simply language for these\r\nName of variable “y” refers to variance\r\n“With” refers to covariance\r\n“On” refers to a regression slope\r\n\r\nWe use multiple predictors to control for each (i.e., we estimate independent effects)\r\nPart 2: Mplus and Estimation\r\nMplus Team\r\nBengt Muthen\r\nFormer president of psychometrics society\r\nPh.D. in stats (Utrecht)\r\nAdviser: Joreskög\r\n\r\nLinda Muthen\r\nPh.D. in education (UCLA)\r\n\r\nTihomir Asparouhov\r\nPh.D. in stats (Cal Tech)\r\n\r\nStatmodel.com has a message board!\r\nstatmodel.com/discussion/messages/board-topics.html\r\n\r\nMplus Overview\r\nSolves for unknowns in regression equations:\r\nAll observed/manifest variables\r\nPath Analysis and traditional regression\r\n\r\n\r\nContinuous latent variables\r\nSEM (latent variables are factors)\r\nMultilevel/random effects/hierarchical linear models\r\nSurvival and other analyses with latent frailties/liabilities\r\n\r\nDiscrete latent variables (i.e., categorical)\r\nLatent profile analysis: continuous observed variables\r\nLatent class (cluster) analysis: discrete observed variables\r\nVarious forms of finite mixture models (LTA/Markov)\r\n\r\nMplus Features\r\nObserved variables\r\nContinuous: normal, skew-normal, t-distributed, skew-t, censored\r\nCategorical & Count: ordered, nominal, zero-inflated\r\n\r\nLinking functions: Identity, logit, probit\r\nEstimators\r\nULS, WLS, GLS, ML, MLR, MLF, BAYES, etc…\r\n\r\nSimulation with Monte Carlo & Bootstrap\r\nRead the manual – many examples!\r\nMplus Caveat\r\nInput is done by hand\r\nThere is a diagrammer, but we want a stick shift!\r\n\r\nVery little input is needed\r\nThis means many default options are used\r\n\r\nEach part of Mplus input has defaults\r\nCheck the manual, read model descriptions, look at output & parameter matrices (\\(TECH_1\\) option)\r\n\r\nExample\r\nTitle: \r\n    Start input section with the heading and a colon, then type specific commands/options and end each statement with a semi-colon;\r\n    * To ignore a command line, start it with an asteriks\r\nData: \r\n    File is data.dat;\r\nVariable: \r\n    Names are employ salary education height weight;\r\n    Usevariables are employ salary height education;\r\nAnalysis:\r\n    Estimator = ML;\r\nModel:\r\n    Employ salary education on height;\r\nOutput:\r\n    Standardized;\r\nInput Command Headings\r\nTitle: Ch. 15\r\nData: Ch. 15\r\nVariable: Ch. 15\r\nDefine: Ch. 15\r\nAnalysis: Ch. 16\r\nModel: Ch. 17\r\nModel Indirect, Model Constraint, Model Priors\r\n\r\nOutput: Ch. 18\r\nSavedata: Ch. 18\r\nPlot: Ch. 18\r\nMonteCarlo: Ch. 19\r\nData\r\nIndicates location of datafile\r\nStore in same folder as Mplus input file\r\n\r\nIndicates format of data\r\nIndividual data or summary data\r\nCorrelations, SDs, means\r\n\r\nDefault is individual data\r\n\r\nMplus requires numeric data only\r\nDo not save data with variable names\r\nFor missing data I use -999\r\nCan save from SPSS, Stata, Excel, etc…\r\nMust be tab-delimited or CSV\r\nEach variable is a column, separated by tabs or commas\r\nMissing data are -999\r\nWhere are the variables names?\r\n\r\n\r\nContains\r\nMplus input files\r\n\r\nIndividual example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Individual Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is individual.dat;\r\nAnalysis: \r\nEstimator = ml;\r\n!Estimator = Bayes;\r\nVariable: \r\nnames are y x1 x2;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1 TECH8;\r\n\r\nSummary example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Summary Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is summary.dat;\r\nType = means stdeviations correlation;\r\nNobservations = 500;\r\nVariable: \r\nnames are y x1 x2;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1;\r\n\r\nIndividual dataset\r\nIndividual.dat\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/individual.dat', n = 20), sep = '\\n')\r\n   -0.354517     0.573051    -0.175230\r\n    0.561655    -0.368095     1.090042\r\n    0.315551    -0.577052     0.425472\r\n    3.347049     1.088520     1.149353\r\n   -0.122389    -0.694153    -0.766538\r\n   -0.251276    -0.017487    -1.367410\r\n   -0.517996    -0.817974    -1.559255\r\n    1.888854    -0.658335     1.007614\r\n    0.461254     0.463916    -0.898300\r\n    2.237483     1.533398     0.180512\r\n    0.480991    -0.096545    -0.352276\r\n    0.165901    -1.341994    -1.445909\r\n    1.864947     1.027419     0.677408\r\n   -0.466245    -0.138712    -0.759287\r\n    2.567804     0.483444     0.959731\r\n   -0.024201    -0.507631    -0.517296\r\n   -1.912698     0.761720    -1.901134\r\n   -1.350069    -0.736562     2.318569\r\n    0.433773     0.723880     0.111837\r\n   -0.977083     0.155868    -0.897112\r\n\r\nSummary dataset\r\nSummary.dat\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/summary.dat'), sep = '\\n')\r\n.485    .001    -.042\r\n1.552   1.046   .978\r\n1.0\r\n.665    1.0\r\n.427    .028    1.0\r\n\r\n\r\nNote: Great! We can get the parameter estimates from the data with means, standard deviations and correlation matrix besides the `individual data. Furthermore, we can simulate data from ‘summary’ information (using Monte Carlo methods).\r\nOther options allowed (see manual)\r\nVariable\r\nAll info about our variables\r\nNames are arbitrary, but the number of names must match the number of columns in data file\r\nNames are y x1 x2;\r\n\r\nType of variable distribution\r\nContinuous normal assumed, but you can say other\r\n\r\nWhich variables will we use?\r\nUsevariables are y x1 x2;\r\n\r\nWeighting variables\r\nMany other options\r\n\r\nDefine\r\nCreates new variables that can be used\r\nYou must list them with Usevariables are\r\n\r\nLogical & arithmetic operators & functions\r\nConditional statements (to recode)\r\nIF ___ then ___;\r\n\r\nTransformations of various kinds\r\nCentering (grand-mean or group-mean)\r\nCluster_Mean, Sum, Mean, Cut\r\n\r\nDefine: x1x2 = x1*x2;\r\nAnalysis\r\nOptions\r\nType = nature of model desired\r\nType = General is default\r\nType = Twolevel or Threelevel means multilevel\r\nType = Random implies random slopes\r\n\r\n\r\nEstimators, Algorithms\r\nEstimator = ML, or Estimator = Bayes\r\n\r\nBootstrapping\r\nLinking functions (logit or probit)\r\nMany options for ML and Bayes estimation\r\nEstimation and Inference\r\nFrequentist: Estimation uses data & model\r\nTypical estimators, such as OLS or ML\r\nIterative process trying to find best parameter estimates\r\nIteratively alters estimates to find the ‘most likely’ values\r\nThis means probability of data is being maximized (i.e., ML)\r\n\r\nConvergence achieved when estimates change very little\r\nMplus gives the estimates, SE, p-values, & CIs upon request\r\nBut, even ‘most likely’ estimates may not be very likely\r\nYour model and its estimates may be the best of a bad lot!\r\nThis is why we’ll look at fit statistics\r\n\r\nBayes: Estimation uses data, model, & prior prob.\r\nDefault is ‘uninformative prior’, so results # ML\r\nIterative process that computes ‘posterior probabilities’\r\nIt uses MCMC estimation in at least two separate ‘chains’\r\nThe estimates produce a posterior distribution for each parameter\r\n\r\nConvergence achieved when chains agree (PSR < 1.05)\r\nMedian/mode of each distribution is like an ML estimate\r\nThe SD is like the SE\r\nGives Bayesian p-value and ‘credibility interval’\r\n\r\nAgain, bad models/estimates may agree across chains\r\nModel\r\nModel: ON Command\r\nON refers to a regression slope \\(\\beta\\)\r\ny ON x; \\[ y_i = \\nu + \\beta_1 x_i + \\epsilon_i \\]\r\nRegress y on x\r\nReads the equation from left to right\r\n\r\n\r\nThink of “y ON x” as \\(x \\rightarrow y\\)\r\nFreely estimates a \\(\\beta\\)\r\nThe single-headed arrow from nowhere is a residual\r\n\r\n\r\nModel: WITH Command\r\nWITH refers to covariance \\(\\Theta\\) or \\(\\Psi\\)\r\ny1 WITH y2;\r\nEstimates covariance among y1 and y2\r\n\r\nWhat if y1 and y2 are dependent variables?\r\nThen we estimate a residual covariance\r\n\r\n\r\n\r\nModel: BY Command\r\nBY refers to factor loadings (slopes)\r\nf1 BY y1 y2 y3;\r\nlatent variable “f1” is indicated by y1, y2, y3\r\n\r\nHuh BY y1 y2 y3\r\nlatent variable is called “Huh”… name is irrelevant\r\n\r\nFreely estimate \\(\\lambda\\) for each observed variable\r\nExcept first loading is fixed to 1.0 by default\r\n\r\n\r\n\r\nModel notation\r\nTypical SEM notation for vectors/matrices\r\n\\(B\\): matrix of regression coefficients, elements \\(\\beta\\)\r\n\\(\\Lambda\\): matrix of factor loadings, elements \\(\\lambda\\)\r\n\\(\\eta\\): vector of latents (factors/random effects)\r\n\\(\\Psi\\): matrix of latent (co)variances, elements ψ\r\n\\(\\Theta\\): matrix of observed (co)variances, elements \\(\\theta\\)\r\n\\(\\nu\\): intercepts, \\(\\alpha\\) is latent intercepts or means\r\n\r\nAll commands will tell Mplus to either:\r\nFreely estimate an element in vector/matrix\r\nThis is what our Model commands do\r\n\r\nFix an element in vector/matrix to some value\r\nMplus defaults often do this for us\r\n\r\nMplus notation for freeing and fixing estimates\r\ny ON x;\r\n* freely-estimated \\(\\beta\\)y ON x@.5;\r\n* \\(\\beta\\) is NOT estimated, but fixed to .5[y@0];\r\n* an intercept for y1 constrained to 0.0f1 BY y*;\r\n* * = freely estimate, so estimates factor loading for y on factor “f1”f1 BY y1-y5@1;\r\n* factor loadings for “f1” = 1 for variables y1, y2, y3, y4, y5\r\nLabeling and constraining/fixing estimates\r\nLabels are put in parenthesesy ON x (b1);\r\nNow, the slope is called “b1” and can be used later\r\n\r\nIf parameters have the same label, it’s like they’re the same thingy ON x z (b2);\r\nNow x\\(\\rightarrow\\)y and z\\(\\rightarrow\\)y slopes are constrained to equality\r\nMplus will estimate them but keep them equal\r\n\r\nModel Constraint\r\nHere we play with labeled parameters\r\nAllows linear/non-linear model constraints\r\nb1+b2=0;\r\n\r\nPlay with parameters labeled in Model command\r\n\r\nToo much creativity to describe, but:\r\nCan create new “phantom” parameters\r\nSee manual for the “New” command\r\nCheck example 5.21\r\n\r\n\r\nTITLE:     this is an example of a two-group twin\r\n           model for  continuous outcomes using parameter constraints\r\n\r\nDATA:      FILE = ex5.21.dat;\r\n\r\nVARIABLE:  NAMES = y1 y2 g;\r\n           GROUPING = g(1 = mz 2 = dz);\r\n\r\nMODEL:     [y1-y2]    (1);\r\n           y1-y2      (var);\r\n           y1 WITH y2 (covmz);\r\n\r\nMODEL dz:  y1 WITH y2 (covdz);\r\n\r\nMODEL CONSTRAINT:\r\n           NEW(a c e h);\r\n           var = a**2 + c**2 + e**2;\r\n           covmz = a**2 + c**2;\r\n           covdz = 0.5*a**2 + c**2;\r\n           h = a**2/(a**2 + c**2 + e**2);\r\nModel Priors\r\nFor Bayes, we can specify prior probabilities\r\nThese are distributions…\r\nModel:y2 ON y1 (b1);\r\nModel Priors:b1~N(.25, 1);\r\nWe say “b1” is distributed as (\\(\\sim\\)) normal (\\(N\\)) with mean and variance (\\(\\mu, \\sigma^2\\)) of .25 and 1\r\nMplus has defaults that are ‘diffuse priors’\r\nEg, regression coefficients are \\(\\sim N(0,10^10)\\)\r\n\r\nModel Indirect\r\nComputes mediation effects to show\r\n“Decomposition” of total and indirect effects\r\nUse bootstrapping in Analysis command with ML to get bootstrapped indirect effects\r\nBayesian estimation gives distribution of effects, so no bootstrapping required\r\n\r\nOutput, Savedata, Plot, Montecarlo\r\nOutput\r\nTECH, Standardized\r\n\r\nSavedata\r\nEstimated data: factor scores, co(var) matrices\r\n\r\nPlot\r\nHelpful for various models\r\n\r\nMonteCarlo\r\nData generation facility… allows parametric bootstrap\r\nMakes running simulations a snap\r\nEmpirically-derived estimates of power\r\nCan publish from this if you’re inclined\r\n\r\n\r\nSummary\r\nOur job is to\r\nTell Mplus about data and estimation method\r\nSpecify a statistical model to reflect our theory\r\nUse ML or Bayes to estimate parameters\r\nUse results to make inferences\r\nPart 3: Path Analysis\r\nHistory\r\nDeveloped by Sewell Wright in 1918 (1921)\r\nGeneticist modeling relations of family members\r\nBrother Philip borrows to create ‘simultaneous equations’ with IVs in 1928 (S&D of flaxseed)\r\n\r\nTaken up in health, biological, and social sciences to model complex relationships\r\nSubsumes simultaneous equation analysis\r\n\r\nShow case\r\nStructural Model Specification\r\nSpecify causal model of interest\r\nGrandey & Cropanzano (1999). The conservation of resources model applied to work–family conflict and strain. Journal of Vocational Behavior, 54, 350-370.\r\n\r\nCausal effects (regression)\r\nWork role stress \\(\\rightarrow\\) Job distress\r\nWork role stress \\(\\rightarrow\\) Work-family conflict\r\nWork-family conflict \\(\\rightarrow\\) Job distress\r\nJob distress \\(\\rightarrow\\) Turnover intentions\r\nJob distress \\(\\rightarrow\\) Life distress\r\n\r\nStochastic, non-causal relationships\r\nTurnover intentions\\(\\leftrightarrow\\)Life distress\r\n\r\nModel diagram\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>>]\r\nTI->LD[dir=both, label=<&psi;>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>>]\\nTI->LD[dir=both, label=<&psi;>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nVariable:\r\nNames are WRS FRS WFC FWC JD FD LD TI PPH SE;\r\nUsevariables are WRS TI WFC JD LD;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOR\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nTI WFC on WRS; LD on WFC;\r\n\r\nNote: for 2 dependent variables which are not predicting anything else, just the outcomes. Mplus will automatically covariance for us. If not estimate the residual covariance would only hurt the model fit. In other words, if there is a residual covariance there and we do not estimat them, it would drive the model fit down.\r\nCode in Mplus:\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS TI WFC JD LD;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS; ! left-side: dependent vbl; right-side: predictors\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\nNote: here’s again we use the ‘summary’ data from the article instead of ‘individual’ data (don’t have it!!!)\r\nMplus output\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.out'), sep = '\\n')\r\nMODEL RESULTS ! Unstandardized Output\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000 ! 1-unit increase on JD, .536 increase on TI\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880 ! Residual covariances\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.141E-02\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nSTANDARDIZED MODEL RESULTS \r\n\r\n\r\nSTDYX Standardization ! Standardized Output (STDYX)\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.075      0.092      0.813      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.376      0.073      5.178      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.490      0.066      7.408      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTDY Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.094      0.115      0.814      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.472      0.089      5.279      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.615      0.078      7.844      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTD Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nR-SQUARE\r\n\r\n    Observed                                        Two-Tailed\r\n    Variable        Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n    TI                 0.234      0.065      3.631      0.000\r\n    WFC                0.240      0.065      3.704      0.000\r\n    JD                 0.421      0.065      6.436      0.000\r\n    LD                 0.595      0.054     10.947      0.000\r\n\r\n\r\nTECHNICAL 1 OUTPUT\r\n\r\n     PARAMETER SPECIFICATION\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  0             0             0             0             0\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             0             0             0\r\n WFC                0             0             0             0             0\r\n JD                 0             0             0             0             0\r\n LD                 0             0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0\r\n WFC                0             0\r\n JD                 0             0             0\r\n LD                 0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  1             2             3             4             0\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             5             0             6\r\n WFC                0             0             0             0             7\r\n JD                 0             8             0             0             9\r\n LD                 0            10            11             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                12\r\n WFC                0            13\r\n JD                 0             0            14\r\n LD                15             0             0            16\r\n WRS                0             0             0             0             0\r\n\r\n     STARTING VALUES\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                0.000         0.000         0.000         0.000         0.000\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             1.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         1.000         0.000         0.000         0.000\r\n JD             0.000         0.000         1.000         0.000         0.000\r\n LD             0.000         0.000         0.000         1.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         1.000\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000\r\n WFC            0.000         0.000\r\n JD             0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                2.120         3.430         2.520         2.730         2.500\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         0.000         0.000         0.000         0.000\r\n JD             0.000         0.000         0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.490\r\n WFC            0.000         0.530\r\n JD             0.000         0.000         0.328\r\n LD             0.000         0.000         0.000         0.387\r\n WRS            0.000         0.000         0.000         0.000         0.635\r\n\r\nWhat’s \\(R^2\\) for our variables?\r\n(1 – standardized residual variance)\r\nCan also just look at bottom of output\r\nReport\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>=.63/.49**>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>=.09/.08>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>=.38/0.38**>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>=.30/.38**>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>=.19/.22**>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>=.54/.44**>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>=.68/.63**>]\r\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>=.63/.49**>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>=.09/.08>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>=.38/0.38**>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>=.30/.38**>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>=.19/.22**>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>=.54/.44**>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>=.68/.63**>]\\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nFor example: \\(\\beta_7\\) = .68/.63**, i.e. unstandardized/standardized both Y and X estimates and significance of p-value\r\nWhat Does This Mean? How to Specify It?\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (no covariance).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\n!Estimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nTI with LD@0;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model1).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nWRS on WFC JD TI;\r\nWFC on JD LD;\r\nJD on TI LD;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model2).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nWRS with WFC;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\nSummary\r\nPath models specify regression/covariation among observed variables\r\nRegression represents causality ON\r\nSo reduces magnitude of DV’s variance\r\n\r\nCovariation simply indicates WITH\r\nSo does not account for variance (there is no IV/DV)\r\n\r\nWe can draw and specify any diagram\r\nMplus then gives us parameter estimates\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nLots of literature & many indices for ML estimator\r\nIssue of Personality & Individual Differences (2007), lead by Barrett\r\nMany cite Hu & Bentler (1999)\r\n\r\nAbsolute fit\r\nHow well does our model explain our observed data?\r\n\r\nRelative/Incremental/Comparative fit\r\nCompares fit of different models using same dataset\r\n\r\nInformation Criteria\r\nMeasure info entropy, or info lost when using an estimated model rather than the data\r\n\r\nBayesians have less work on the topic …\r\nIn statistics programs we get fit from:\r\nAnalysis/Estimated model (\\(H_0\\) in Mplus output)\r\nModel we specify and estimate\r\n\r\nBaseline/Independence/Null model\r\nWorst fitting model (or at least all covariances = 0)\r\n\r\nUnrestricted/Saturated (\\(H_1\\) in Mplus output)\r\nBest fitting model (all parameters freely estimated)\r\n\r\n\r\nFor comparing models that we estimate:\r\nNull model (\\(H_0\\)): more-constrained model\r\nFewer parameters estimated, so more parsimonious\r\n\r\nAlternate model (\\(H_1\\)): less-constrained model\r\n\r\nML Estimator\r\nInformation we have vs. what’s estimated\r\nHow many parameters implied by our data?\r\nk means, k variances, and k\\(\\times\\)(k-1)/2 covariances\r\n\r\nHow many parameters are estimated?\r\nIntercepts, (residual) variances, slopes/covariances\r\n\r\n\r\nThe difference is our degrees of freedom (df)\r\nWhat about data/model fit ?\r\nFor ML, we have model log Likelihood (LL)\r\nIt turns out, -2\\(\\times\\)LL is \\(\\chi^2\\) distributed\r\nThe difference between two \\(\\chi^2\\) values is \\(\\chi^2\\) distributed\r\n\r\n\r\nModel Build\r\nAnalysis/Estimated model (\\(H_0\\))\r\nUnrestricted/Saturated/Alternate model (\\(H_1\\))\r\n\r\n\r\nModel:TI on JD WRS;LD on JD WFC;JD on WRS WFC;WFC on WRS;\r\nModel:JD LD TI WRS WFC with JD LD TI WRS WFC;\r\nBaseline/Null Model\r\n\r\nModel:\r\nModel Diagnostics\r\nContrasting Models\r\nChi-square testing of nested models\r\nCan use change in relative fit indices\r\nNon-nested model comparisons possible with AIC/BIC\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nUnrestricted vs Analysis model gives us \\(\\chi^2\\)\r\n\\(df\\) is difference in estimated model & unrestricted\r\n\r\nIf our model is good, the difference should be small\r\nConducting NHST with the \\(\\chi^2\\)\r\nWhat’s the null vs. alternate hypothesis?\r\n\r\nIssues\r\nSensitive to sample size\r\nRarely taken seriously… unless it’s non-significant!\r\nWe want to “accept the null” ???\r\n\r\nIf you want to look good on a relative basis… ??\r\nUsing \\(\\chi^2\\) to Compare Estimated Models\r\nGet \\(\\chi^2\\) and \\(df\\) for different estimated models\r\nSubtract \\(\\chi^2\\) and subtract \\(df\\), use diff. for NHST: \\(\\Delta \\chi^2\\) or \\(-2\\times LL\\)\r\nRecall the null is model with fewer parameters\r\n\r\nModel must be nested: null must be subset of alt.\r\nAbsolute Fit: SRMR\r\nStandardized Root Mean Square Residual\r\nStandardized difference between observed and model-implied data\r\nResidual in this case is the difference\r\n\r\nLess than .05 is good, Hu & Bentler say that .08 isn’t too bad…\r\nAbsolute Fit: RMSEA\r\nRoot mean squared error of approximation\r\nLike an adjusted root mean square standardized residual\r\n\r\nTakes model parsimony into account\r\nPenalized for estimating too many parameters\r\nPuts a positive value on df\r\n\r\n\\[ \\frac{\\sqrt{\\chi^2_{Estimated} - df_{Estimated}}}{\\sqrt{df\\times (N-1)}}\\]\r\nLess than .06 or .07 good (CI usually given)\r\nSteiger (2007), Understanding the limitations of global fit…\r\n\r\nRelative Fit: CFI\r\nComparative Fit Index\r\nCompares \\(\\chi^2\\) of estimated model to \\(\\chi^2\\) of baseline\r\nAdjusts for \\(df\\) to reward parsimony\r\n\r\n\\[ \\frac{\\sqrt{(\\chi^2_{Baseline} - df_{Baseline}) - (\\chi^2_{Estimated} - df_{Estimated})}}{\\sqrt{\\chi^2_{Baseline} - df_{Baseline}}}\\]\r\n.95 or higher generally accepted as cut-off\r\nRelative Fit: TLI/NNFI\r\nTucker-Lewis Index/Non-Normed Fit Index\r\nSimilar to CFI but different penalization\r\n\\[ \\frac{\\chi^2_{Baseline}/df_{Baseline} - \\chi^2_{Estimated}/df_{Estimated}}{\\chi^2_{Baseline}/ df_{Baseline} - 1}\\]\r\nWill always be smaller than CFI\r\nTo some decimal place\r\n\r\n.95 often thought of as cut-off\r\nInformation Criteria: AIC\r\nAkaikes Information Criterion\r\nk is number of estimated parameter\r\nA weighting of accuracy versus model complexity\r\nOnly useful for comparing two estimated models\r\nLower values are better\r\n\\[ \\chi^2_{Estimated} + k\\times (k-1) - 2\\times df_{Estimated} \\]\r\nInformation Criteria: BIC\r\nBayesian Information Criterion & Adjusted BIC\r\nSimilar to AIC, except weights for sample size\r\nOnly useful for comparing two estimated models\r\nSmaller values are better—BIC, then aBIC here:\r\n\\[ \\chi^2_{Estimated} + ln(N)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\n\\[ \\chi^2_{Estimated} + ln((N+2)/24)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\nFor both AIC and BIC, no significance tests… see recommendations in interpreting differences\r\nThese are all Global Measures\r\nThey collapse across all model parts\r\nSome authors call for separate fit measures\r\nWhatever… fit is necessary but insufficient\r\nThe model must make theoretical sense\r\n\r\nModification Indices\r\nMODINDICES command in Mplus\r\nIndicate change in model fit by estimating additional parameters\r\nHeated debates here\r\nFor frequentists this capitalizes on chance\r\nCan lead to nonsensical models, bias/variance tradeoff\r\n\r\nUseful for understanding sources of misfit\r\nLet’s try it with our path model…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (ML) modindices.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8 MODINDICES(1);\r\n\r\nBayesian Estimates of Fit\r\nNot as well developed\r\nBayes was unpopular for much of \\(20^{th}\\) Century\r\n\r\nLess obvious how to proceed\r\nThe idea of sampling from a population not formalized in the same way (so no \\(\\chi^2\\))\r\n\r\nMplus offers a few that are useful…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (Bayes).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = Bayes;\r\nfbiterations=10000;\r\nProcessors=2;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\nPosterior Predictive Checking\r\nSteps required during model estimation\r\nSample parameters from posteriors\r\n\r\nGenerate data with parameters\r\n\r\nEstimate fit of generated vs. observed data\r\n\r\n\r\nIf model is good, generated data should fit better/worse about half the time\r\nGet Predictive Posterior p-value (PPP)\r\nHow often the generated data fit better\r\nPPP < .05 bad, PPP ≈ .50 good, PPP > .95 weird\r\n\r\nDeviance Information Criterion\r\nLike other information criteria\r\nPenalizes for model complexity, rewards fit\r\nSmaller values = less deviance (i.e., better fit)\r\nCan compare models with different priors\r\nModel Selection\r\nHow do we choose the models we publish?\r\nSelecting models\r\nCan build them based on scripted steps9,10\r\nGive your model of interest a shot and look at fit\r\nIf that doesn’t work, try different theory-driven models\r\nLook at sources of misfit and sort out what’s going on\r\n\r\nIs this an ethical issue?\r\nScientific models are serious things\r\nPeople/groups use results for their own purposes\r\nWhen considering which model to estimate, consider the social & political implications\r\n\r\nFurther Readings\r\nPart 1\r\nGrace, J. B., & Bollen, K. A. (2005). Interpreting the results from multiple regression and structural equation models. Bulletin of the Ecological Society of America, 86, 283-295.\r\nCohen, Cohen, West, & Aiken (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (3rd). Lawrence Erlbaum & Associates.\r\nMyers, Well, & Lorch (2010). Research Design and Statistical Analysis (3rd). Routledge Academic.\r\nTabachnick & Fidell (2006). Using Multivariate Statistics (5th). Allyn & Bacon.\r\nFisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London, Series A 222, 309–368.\r\nPart 2\r\nMplus User’s Guide and Diagrammer Documentation\r\nPart 3\r\nStreiner, D. L. 2005. Finding Our Way: An Introduction to Path Analysis. Canadian Journal of Psychiatry, 50, 115-122.\r\nWright, S. (1921). Correlation and causation. J. Agricultural Research 20: 557–585.\r\nWright, S. (1934). The method of path coefficients. Annals of Mathematical Statistics 5: 161–215.\r\nAngrist, J. D., & Krueger, A. B. (2001). Instrumental variables and the search for identification: From supply and demand to natural experiments. Journal of Economics Perspectives, 15: 69-85.\r\nMuthén, B. (2002). Beyond SEM: General latent variable modeling. Behaviormetrika, 29, 81-117.\r\nPart 4\r\nHu & Bentler (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6(1), 1-55.\r\nPersonality and Individual Differences, Volume 42, Issue 5. 2007.\r\nNeyman, J., & Pearson, E. S. 1933. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society, A, 231, 289-337.\r\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111-16\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773-795.\r\nMcDonald, R. P. (2010). Structural models and the art of approximation. Perspectives on Psychological Science, 5, 675-686.\r\nCheung, G. W., & Rensvold, R. B. (1999). Testing factorial invariance across groups: a reconceptualization and proposed new method. Journal of Management, 25, 1–27.\r\nCheung, G. W., & Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. Structural Equation Modeling, 9, 233–255.\r\nAnderson, J. C., & Gerbing, D. W. (1988). Structural equation modeling in practice: A review and recommended two-step approach. Psychological Bulletin, 103, 411-423.\r\nMulaik, S. A., & Millsap, R. E. (2000). Doing the four-step right. Structural Equation Modeling, 7, 36-73. (see other papers in the same issue).\r\nShapin, S. 2008. The scientific life: A moral history of a late modern vocation. Chicago University Press.\r\nPorter, T. M. 1995. Trust in Numbers: The Pursuit of Objectivity in Science and Public Life.\r\nPoovey, M. 1997. A History of the Modern Fact: Problems of Knowledge in the Sciences of Wealth and Society. Chicago.\r\nMcCloskey, D. N. 1992. If you’re so smart: The narrative of economic expertise. University of Chicago Press.\r\nMcCloskey, D. N. 1992. The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives.\r\nAsparouhov, T., Muthén, B. & Morin, A. J. S. 2015. Bayesian structural equation modeling with cross-loadings and residual covariances: Comments on Stromeyer et al. Journal of Management, 41, 1561-1577.\r\n\r\n\r\n",
    "preview": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/distill-preview.png",
    "last_modified": "2021-05-11T11:14:26-05:00",
    "input_file": "5-day-Mplus-Workshop-from-Michael-Zyphur.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/",
    "title": "Fitting a Simple Linear Regression in Bayesian Context",
    "description": "How to fit a linear regression using Bayesian Methods  \nConsider a Bayesian model fit as a remedial measures for influential case",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nFit a model\r\nModel fit diagnostics\r\nMessage take-away\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\nMCMC in JAGS\r\nDescribe the model.\r\nExplore the MCMC object\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nExplore the MCMC object\r\n\r\nUsing fitted regression model for prediction\r\n\r\nCompare the fit between FA and BA\r\nFurther reading\r\n\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nI use the data in Kruschke (2015) which has\r\n- male\r\n- height\r\n- weight\r\nWe will use height to predict weight of a person\r\n\r\nDT::datatable(dta, \r\n          rownames = FALSE,\r\n          filter = list(position = \"top\"))\r\n\r\n{\"x\":{\"filter\":\"top\",\"filterHTML\":\"<tr>\\n  <td data-type=\\\"integer\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"54.6\\\" data-max=\\\"76\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"89\\\" data-max=\\\"356.8\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[0,0,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1],[64,62.3,67.9,64.2,64.8,57.5,65.6,70.2,63.9,71.1,66.5,68.1,62.9,75.1,64.6,69.2,68.1,72.6,63.2,64.1,64.1,71.5,76,69.7,73.3,61.7,66.4,65.7,68.3,66.9,62.4,64.5,60.6,70.8,61,66,59.6,70.1,66.6,59.8,68.5,61.4,64.7,67.4,68.3,67.3,62.5,72.4,64.4,70.6,66.3,65.9,61.1,67.8,64.4,71.2,64.5,65.4,67,70.9,62.4,70.4,64.6,69.6,65.9,70.6,65.4,73.9,70.1,64.8,59.7,69.2,61.4,70.4,71.1,60.8,68.3,67.4,63.5,66.8,73.4,64.8,68.9,66.5,64.7,66.2,69.1,66,60.9,72.4,67.5,71.6,70.6,64.7,72.7,64.7,67.9,65.9,54.6,64.1,68.3,72.9,64,62.1,67.3,60.4,63.4,63.3,61.4,67.7,65.1,67.5,67,65.6,70.9,61.5,59.4,59.7,69,71.7,65.7,67.2,65.9,64.9,73.6,70.5,63.6,64.7,69.4,69.2,64.5,70.9,63.6,73.7,67.6,65.9,68.1,64.1,61.6,71.2,66.7,71.1,68.6,62.8,67.6,63.1,65.2,67.4,64.3,59.9,61.9,63.4,59.8,67.7,64.5,67.4,66.1,75.1,67.9,65.7,64.9,64.8,55.3,63,64.1,64.3,63.8,65.1,71.8,66.7,64.5,60.7,59.6,68.4,63.2,66,62.5,67.9,70.2,70.3,67.1,66.2,62.1,72.4,65.2,72.1,65.6,67.2,66.2,66,63.6,65,57.5,63.4,67.2,62.8,65.3,66.7,70.3,68.6,66.4,62.9,57.6,65.3,70.9,68,65.4,67.4,62.8,73.3,64.6,70,68.1,69.4,71.9,67.8,63.5,69.8,66.1,62,71.5,71.9,72.6,64.3,71.7,70.1,64.3,63.9,74.6,65.5,63.1,66.7,65.2,62,71,69.4,69.8,66.6,71,74.7,63.6,69.8,58.9,67.9,65.2,70.6,59.8,71.2,71.5,63.7,65.1,64.8,66.8,64.5,68.6,65.4,66.5,68.8,70.4,57,60.6,62.8,73.3,73.2,61.9,66.4,69.9,60.8,65.9,61.8,66.5,64,68.2,71.8,70.4,63,60.1,66.3,67.1,61.5,66.4,63.8,69.9,66.1,66.2,66.5,70.2,64,69,65.7,69.5,64.1,61.2,62.6,67.9,68.5,69.2,65.9,68.3,70.2],[136.4,215.1,173.6,117.3,123.3,96.5,178.3,191.1,158,193.9,127.1,147.9,119,204.4,143.4,124.4,140.9,164.7,139.8,110.2,134.1,193.6,180,155,188.2,187.4,139.2,147.9,178.6,111.1,119.2,184.4,100.1,207.3,159.8,120.7,102.8,195.7,130.1,156.5,113.7,119.1,142.8,179.9,166.3,135.4,118.9,173.9,117.8,192.6,122,129.5,116.9,177.1,160.1,199.5,111,177.4,187.9,177.1,185.3,223.5,128.4,184.4,122.1,216.8,173.8,197.8,181.1,136.6,105,150.1,125,172.2,143.9,132,110.7,155.9,174.9,176.6,167.1,133.5,211.4,150.6,144.7,120.7,222.5,168.4,134.3,182.8,187.2,193.4,195.2,131.1,204.5,108.6,128.1,152.7,120.3,183.5,210.6,163.1,143.9,114.4,170.5,93.2,147.8,161.2,114.6,158,144.5,184,225,116.9,183.2,131.5,217.1,123.6,145.9,170.6,133.5,165.9,134.1,111.1,201.1,156.3,161.5,145,159.8,149,222,149.7,130.6,242.5,150,191.3,164.1,135.6,139.4,114.8,191.6,194.7,170,146.9,186.1,125.8,96.2,156.8,117.6,149.6,125.4,140.7,150.2,156.5,137.6,140.3,174.7,186.1,191.3,135.2,130.5,137.1,166.5,280.5,126,128.3,166.5,124,120.8,193.8,157,190.8,110.9,185.9,163.2,167.9,119.8,122,178.8,181.1,168.8,120.9,96.5,210.4,139.3,187,146.5,141.8,162,173.8,160.7,125.3,125.9,193.7,234.9,156.9,221.2,241.6,170.4,152.6,172.8,176.6,123.5,202.1,182.2,190.9,146.6,158.5,140.5,168,182,275.2,164.1,153.5,159.5,141.7,194.1,149.6,157.5,149.9,210.8,154.5,206.5,123.4,166,160.8,167.6,141.9,186.2,155.7,148.7,132.6,356.8,164.9,187.2,169.6,178.8,202.2,158.9,186.1,169.7,147.6,110.6,146.8,146.2,164.8,115.7,191.5,198.6,143.7,126.7,123.1,231.5,134.7,159.4,141.5,144.5,150.5,173.7,215.5,146.4,133.6,240.2,216.6,89,213.6,211.3,140.4,135.7,151.7,198.3,158.7,218,172.7,161.9,130.3,123,157,131.2,108.7,202.7,131.9,164.5,179.2,154.4,120.1,189.7,160.2,145.7,186.2,144.8,147.4,120.8,134.9,164.8,205.9,172.5,130.8,146.5,173.8]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>male<\\/th>\\n      <th>height<\\/th>\\n      <th>weight<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[0,1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\nFit a model\r\n\r\nfit <- lm(dta$weight ~ dta$height)\r\nsummary(fit)\r\n\r\nCall:\r\nlm(formula = dta$weight ~ dta$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-63.95 -21.17  -5.26  16.24 201.94 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -104.7832    31.5056  -3.326 0.000992 ***\r\ndta$height     3.9822     0.4737   8.406 1.77e-15 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.59 on 298 degrees of freedom\r\nMultiple R-squared:  0.1917,    Adjusted R-squared:  0.189 \r\nF-statistic: 70.66 on 1 and 298 DF,  p-value: 1.769e-15\r\n\r\nThen, we can see the fitted regression line overlay with data\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Scatter Plot between Height and Weight by Gender\",\r\n     pch = as.numeric(dta$male), col = as.factor(dta$male))\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\n\r\nModel fit diagnostics\r\nThere are four assumptions associated with a linear regression model:\r\nLinearity: The relationship between X and the mean of Y is linear.\r\nHomoscedasticity: The variance of residual is the same for any value of X.\r\nIndependence: Observations are independent of each other.\r\nNormality: For any fixed value of X, Y is normally distributed.\r\nI check the model fit by plotting:\r\nplot of residuals against predictor variable (how to check residuals have non-linear patterns or not)\r\nnormal probability plot (how to check residuals are normally distributed: residuals follow a straight line well or do they deviate severely; it’s good if residuals are lined well on the straight dashed line.)\r\nplot of square root of standardized residual absolute value (how to check the assumption of equal variance (homoscedasticity): if residuals are spread equally along the ranges of predictors; it’s good if we see a horizontal line with equally (randomly) spread points.)\r\nAnd to look at the outlier and leverage via\r\nplot standardized residuals vs leverage\r\nResiduals are the difference between the observed score and the predicted score.\r\n\\[ e_i = Y^{obs}_i - \\hat{Y}_i\\]\r\nResiduals come in three varieties:\r\nRaw Residuals: The difference between the raw observed score and the predicted score.\r\nStandardized Residuals: These are the raw residuals divided by the standard error of estimate.\r\nStudentized Residuals: These are raw residuals divided by the standard error of the residual with that case deleted. These are sometimes called studentized deleted residuals or studentized jackknifed residuals.\r\n\r\npar(mfrow=c(2,2))\r\nplot(fit)\r\n\r\n\r\nLastly, we look at the influential points\r\n\r\nn <- dim(dta)[1]\r\ncooksD <- cooks.distance(fit)\r\n#identify influential points\r\n(influential_obs <- as.numeric(names(cooksD)[(cooksD > (4/n))]))\r\n [1]   2 117 134 140 163 164 169 172 212 233 260 263\r\n#plot cooksD\r\nplot(cooksD, pch=\"*\", cex=2, main=\"Influential Obs by Cooks distance\")  \r\nabline(h = 4/n, col=\"red\")  # add cutoff line\r\ntext(x=1:length(cooksD), y=cooksD, labels=ifelse((cooksD>(4/n)),names(cooksD),\"\"), col=\"red\", pos = 4)\r\n\r\n\r\nTill now, we can say that the linear regression assumption is violated in this case, e.g. error is not following the normal distribution. Therefore, how about we delete the influential points and re-fit the model:\r\n\r\ndta.outliers_removed <- dta[-influential_obs, ]\r\n\r\nfit.outliers_removed <- lm(dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\nsummary(fit.outliers_removed)\r\n\r\nCall:\r\nlm(formula = dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-55.16 -18.87  -3.18  16.55  83.27 \r\n\r\nCoefficients:\r\n                             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)                 -155.9116    26.8300  -5.811 1.65e-08 ***\r\ndta.outliers_removed$height    4.7112     0.4032  11.685  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.36 on 286 degrees of freedom\r\nMultiple R-squared:  0.3231,    Adjusted R-squared:  0.3208 \r\nF-statistic: 136.5 on 1 and 286 DF,  p-value: < 2.2e-16\r\n\r\nSomehow, we saw the model assumption is satisfied when the influential cases removed.\r\n\r\npar(mfrow=c(1,2))\r\nplot(fit.outliers_removed, which=c(1,2))\r\n\r\n\r\nThe regression line changes when we remove the influential observations. Dangerous!!!\r\n\r\npar(mfrow=c(1,2))\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\nplot(dta.outliers_removed$height, dta.outliers_removed$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Outliers removed\")\r\nabline(fit.outliers_removed, col = \"orange\", lwd = 3)\r\n\r\n\r\nBut, the action of deleting of influential cases is often not the solution due to produce the bias estimates.\r\nThrough this case, we have reviewed:\r\nInfluential points = Outliers & Leverage\r\nA point that makes a lot of difference in a regression case, is called ‘an influential point.’ Usually influential points have two characteristics:\r\nThey are outliers, i.e. graphically they are far from the pattern described by the other points, that means that the relationship between x and y is different for that point than for the other points.\r\nThey are in a position of high leverage, meaning that the value of the variable x is far from the mean. Observations with very low or very high values of x are in positions of high leverage.\r\nRemind knowledge\r\nDiscrepancy: Difference between the predicted and observed value\r\n- Measured by Studentized Residuals\r\nLeverage: high leverage if it has “extreme” predictor x values\r\n- Measured by Hat Value\r\nInfluence: Assesses how much regression equation would change if an observation/potential outlier was dropped from the analysis\r\n- Measured by Cook’s Distance, Difference in Fit (DFFITS), or Difference in coefficients (DFBETAS)\r\nMessage take-away\r\nRemoving influential cases is not the optimal solution.\r\nIn the real-life analysis, we can use other types of regression:\r\nRidge regression\r\nRobust regression\r\nIRLS Robust regression\r\nLowess method\r\nRegression trees\r\nand etc…\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon \\]\r\nWith Bayesian approach distribution of \\(\\epsilon\\) does not have to be Gaussian (normal), we are going to use robust assumption.\r\nParameterization of the model for MCMC (adapted from Kruschke (2015))Bayes theorem for this model:\r\n\\[\r\np(\\beta_0, \\beta_1, \\sigma, \\gamma \\mid D) = \\frac{p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma)}{\\int \\int \\int \\int p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma) \\ d\\beta_0 \\ d\\beta_1 \\ d\\sigma \\ d\\gamma}\r\n\\]\r\nCreate the data list.\r\n\r\ny <- dta$weight\r\nx <- dta$height\r\ndataList <- list(x = x, y = y)\r\n\r\nMCMC in JAGS\r\nDescribe the model.\r\nBased on the Normal distribution (demonstrating purpose, not run)\r\n\r\nmodstring_norm = \"\r\n# Specify the Normal model for none-standardized data:\r\nmodel {\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ dnorm(mu[i], prec)\r\n        mu[i] = b[1] + b[2]*log_income[i] \r\n    }\r\n    \r\n    for (i in 1:2) {\r\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\r\n    }\r\n    \r\n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\r\n    sig2 = 1.0 / prec\r\n    sig = sqrt(sig2)\r\n} \"\r\n\r\nBased on the Student-t distribution, robust assumption\r\n\r\n modelString = \"\r\n# Standardize the data:\r\ndata {\r\n    Ntotal <- length(y)\r\n    xm <- mean(x)\r\n    ym <- mean(y)\r\n    xsd <- sd(x)\r\n    ysd <- sd(y)\r\n    for ( i in 1:length(y) ) {\r\n      zx[i] <- (x[i] - xm) / xsd\r\n      zy[i] <- (y[i] - ym) / ysd\r\n    }\r\n}\r\n# Specify the model for standardized data:\r\nmodel {\r\n    for ( i in 1:Ntotal ) {\r\n      zy[i] ~ dt( zbeta0 + zbeta1 * zx[i] , 1/zsigma^2 , nu )\r\n    }\r\n    # Priors vague on standardized scale:\r\n    zbeta0 ~ dnorm(0, 1/(10)^2 )  \r\n    zbeta1 ~ dnorm(0, 1/(10)^2 )\r\n    zsigma ~ dunif(1.0E-3, 1.0E+3 )\r\n    nu ~ dexp(1/30.0)\r\n    # Transform to original scale:\r\n    beta1 <- zbeta1 * ysd / xsd  \r\n    beta0 <- zbeta0 * ysd  + ym - zbeta1 * xm * ysd / xsd \r\n    sigma <- zsigma * ysd\r\n}\r\n\"\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"TEMPmodel.txt\")\r\n\r\nIn the tutorial, we just want to execute the model specified by t-student distribution.\r\nEvery arrow has a corresponding line in the descriptive diagram.\r\nVariable names starting with “z” mean that these variables are standardized (z-scores).\r\nThe intention of using z-scores in JAGS is to overcome a problem of correlation of the parameters (as the simulation the correlation between \\(\\beta_0\\) and \\(\\beta_1\\)).\r\nStrong correlation creates thin and long shape on scatter-plot of the variables which makes Gibbs sampling very slow and inefficient.\r\nBut remember to scale back to the original measures.\r\nHMC implemented in Stan does not have this problem. This can be applied to STAN in all situation !!!\r\n\r\nparameters = c(\"beta0\" ,  \"beta1\" ,  \"sigma\", \r\n              \"zbeta0\" , \"zbeta1\" , \"zsigma\", \"nu\")\r\nadaptSteps = 500  # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps=20000\r\nnIter = ceiling((numSavedSteps*thinSteps ) / nChains )\r\njagsModel = jags.model(\"TEMPmodel.txt\", data=dataList ,\r\n                      n.chains=nChains, n.adapt=adaptSteps)\r\nupdate(jagsModel, n.iter=burnInSteps)\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters, \r\n                          n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the MCMC object\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:6500\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 5000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n             Mean       SD  Naive SE Time-series SE\r\nbeta0  -139.96225 27.61760 0.1952859      0.2180717\r\nbeta1     4.46120  0.41330 0.0029225      0.0032290\r\nnu        5.37901  1.62796 0.0115114      0.0236101\r\nsigma    23.97943  1.65132 0.0116766      0.0204363\r\nzbeta0   -0.09632  0.04806 0.0003398      0.0004283\r\nzbeta1    0.49046  0.04544 0.0003213      0.0003550\r\nzsigma    0.68372  0.04708 0.0003329      0.0005827\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%       25%        50%       75%      97.5%\r\nbeta0  -193.3580 -158.7464 -140.16434 -121.3445 -8.624e+01\r\nbeta1     3.6527    4.1830    4.46491    4.7407  5.257e+00\r\nnu        3.1509    4.2578    5.05987    6.1485  9.410e+00\r\nsigma    20.8323   22.8476   23.94254   25.0519  2.728e+01\r\nzbeta0   -0.1888   -0.1282   -0.09682   -0.0644 -4.115e-04\r\nzbeta1    0.4016    0.4599    0.49087    0.5212  5.779e-01\r\nzsigma    0.5940    0.6514    0.68267    0.7143  7.779e-01\r\nplot(codaSamples, trace=TRUE, density=FALSE) # note: many graphs\r\n\r\nautocorr.plot(codaSamples, ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n    beta0     beta1        nu     sigma    zbeta0    zbeta1    zsigma \r\n16054.484 16401.066  4768.900  6535.965 12892.980 16401.066  6535.965 \r\n#gelman.diag(codaSamples)\r\ngelman.plot(codaSamples)   # lines may return error ==> Most likely reason: collinearity of parameters \r\n\r\n(HDIofChains <- lapply(codaSamples, function(z) cbind(Mu = hdi(codaSamples[[1]][,1]), Sd = hdi(codaSamples[[1]][,2]))))\r\n[[1]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[2]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[3]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[4]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\nLook at strong correlation between beta0 and beta1 which slows Gibbs sampling down.\r\n\r\nhead(as.matrix(codaSamples[[1]]))\r\n         beta0    beta1       nu    sigma      zbeta0    zbeta1\r\n[1,] -168.7337 4.890447 5.623225 23.90661 -0.10407253 0.5376553\r\n[2,] -158.6082 4.744414 6.371647 24.56065 -0.09181883 0.5216004\r\n[3,] -166.9662 4.873701 6.346842 22.92533 -0.08537616 0.5358143\r\n[4,] -161.6680 4.795882 3.209357 23.00506 -0.08162923 0.5272588\r\n[5,] -160.5706 4.801212 3.153090 22.15810 -0.04024801 0.5278448\r\n[6,] -155.9331 4.684888 4.394775 21.44653 -0.12823063 0.5150561\r\n        zsigma\r\n[1,] 0.6816415\r\n[2,] 0.7002899\r\n[3,] 0.6536626\r\n[4,] 0.6559359\r\n[5,] 0.6317868\r\n[6,] 0.6114980\r\npairs(as.matrix(codaSamples[[1]])[,1:4])\r\n\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nIn order to give a vague priors to slope and intercept consider the following arguments:\r\nThe largest possible value of slope is\r\n\\[ \\frac{\\sigma_y}{\\sigma_x} \\]\r\nwhen variables \\(x\\) and \\(y\\) are perfectly correlated.\r\nThen standard deviation of the slope parameter \\(\\beta_1\\) should be large enough to make the maximum value easily achievable.\r\nSize of intercept is defined by value of\r\n\\[ E[X] \\frac{\\sigma_y}{\\sigma_x} \\]\r\nSo, the prior should have enough width to include this value.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n    real meanX;\r\n    real sdX;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real expLambda;\r\n    real beta0sigma;\r\n    real beta1sigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    expLambda = 1/30.0;\r\n    beta1sigma = 10*fabs(sdY/sdX);\r\n    beta0sigma = 10*(sdY^2+sdX^2)    / 10*fabs(meanX*sdY/sdX);\r\n}\r\nparameters {\r\n    real beta0;\r\n    real beta1;\r\n    real<lower=0> nu; \r\n    real<lower=0> sigma; \r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi); \r\n    nu ~ exponential(expLambda);\r\n    beta0 ~ normal(0, beta0sigma);\r\n    beta1 ~ normal(0, beta1sigma);\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ student_t(nu, beta0 + beta1 * x[i], sigma);\r\n    }\r\n}\r\n\"\r\n\r\n\r\nstanDsoRobustReg <- stan_model(model_code=modelString) \r\n\r\n\r\ndat<-list(Ntotal=length(dta$weight), \r\n          y=dta$weight, \r\n          meanY=mean(dta$weight),\r\n          sdY=sd(dta$weight),\r\n          x=dta$height,\r\n          meanX=mean(dta$height),\r\n          sdX=sd(dta$height))\r\n\r\n\r\nfitSimRegStan <- sampling(stanDsoRobustReg, \r\n             data=dat, \r\n             pars=c('beta0', 'beta1', 'nu', 'sigma'),\r\n             iter=5000, chains = 4, cores = 4)\r\n\r\nSave the fitted object.\r\nExplore the MCMC object\r\n\r\nprint(fitSimRegStan)\r\nInference for Stan model: 99bd7b261ff9240bf0c6d7b1b21831d6.\r\n4 chains, each with iter=5000; warmup=2500; thin=1; \r\npost-warmup draws per chain=2500, total post-warmup draws=10000.\r\n\r\n          mean se_mean    sd     2.5%      25%      50%      75%\r\nbeta0  -139.35    0.50 27.89  -194.81  -157.61  -139.51  -120.74\r\nbeta1     4.45    0.01  0.42     3.64     4.17     4.45     4.72\r\nnu        5.37    0.03  1.61     3.15     4.25     5.07     6.16\r\nsigma    23.98    0.03  1.66    20.81    22.84    23.96    25.07\r\nlp__  -1265.00    0.02  1.43 -1268.55 -1265.74 -1264.68 -1263.94\r\n         97.5% n_eff Rhat\r\nbeta0   -85.12  3072    1\r\nbeta1     5.28  3117    1\r\nnu        9.39  3420    1\r\nsigma    27.31  3579    1\r\nlp__  -1263.20  3332    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Apr 22 15:10:38 2021.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(fitSimRegStan)\r\n\r\nrstan::traceplot(fitSimRegStan, ncol=1, inc_warmup=F)\r\n\r\npairs(fitSimRegStan, pars=c('nu','beta0','beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','beta1'))\r\n\r\nstan_scat(fitSimRegStan, c('beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('nu','sigma'))\r\n\r\nstan_dens(fitSimRegStan)\r\n\r\nstan_ac(fitSimRegStan, separate_chains = T)\r\n\r\nstan_diag(fitSimRegStan,information = \"sample\",chain=0)\r\n\r\nstan_diag(fitSimRegStan,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"divergence\",chain = 0)\r\n\r\n\r\nWork with shinystan object.\r\n\r\nlaunch_shinystan(fitSimRegStan)\r\n\r\nUsing fitted regression model for prediction\r\nRecall that the data in this example contains predictor height and output weight for a group of people from Ht-Wt.csv (data above).\r\nPlot all heights observed in the sample and check the summary of the variable.\r\n\r\nplot(1:length(dat$x),dat$x)\r\n\r\nsummary(dat$x)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  54.60   64.00   66.20   66.39   69.20   76.00 \r\n\r\nCan we predict weight of a person who is 50 or 80 inches tall?\r\nTo do this we can go through all pairs of simulated parameters (\\(\\beta_0\\), \\(\\beta_1\\)) and use them to simulate \\(y(50)\\) and \\(y(80)\\).\r\nThis gives distribution of predicted values.\r\n\r\nsummary(fitSimRegStan)\r\n$summary\r\n              mean     se_mean         sd         2.5%          25%\r\nbeta0  -139.354196 0.503202352 27.8915373  -194.808900  -157.605829\r\nbeta1     4.451593 0.007474065  0.4173043     3.636611     4.172879\r\nnu        5.366643 0.027556563  1.6115181     3.151177     4.246276\r\nsigma    23.982791 0.027670259  1.6554405    20.814300    22.838675\r\nlp__  -1265.002519 0.024734258  1.4276951 -1268.552672 -1265.738311\r\n               50%          75%        97.5%    n_eff     Rhat\r\nbeta0  -139.512830  -120.737878   -85.121838 3072.271 1.000770\r\nbeta1     4.454489     4.723609     5.282330 3117.396 1.000699\r\nnu        5.069790     6.157253     9.391004 3419.954 1.000994\r\nsigma    23.957637    25.072876    27.313013 3579.322 1.000550\r\nlp__  -1264.682923 -1263.938424 -1263.195323 3331.756 1.000423\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.496987 27.9128445  -195.452362  -159.028578\r\n    beta1     4.468322  0.4176474     3.643375     4.180768\r\n    nu        5.333266  1.6129391     3.146164     4.239934\r\n    sigma    23.996371  1.6756857    20.745408    22.833622\r\n    lp__  -1264.973734  1.3931068 -1268.262683 -1265.705478\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.690732  -121.306242   -85.591653\r\n    beta1     4.473340     4.745248     5.301112\r\n    nu        5.036853     6.074138     9.402810\r\n    sigma    23.966000    25.101448    27.242245\r\n    lp__  -1264.658401 -1263.936454 -1263.201543\r\n\r\n, , chains = chain:2\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.119919 27.7217798  -195.188356  -158.408294\r\n    beta1     4.462118  0.4146886     3.668582     4.177839\r\n    nu        5.396927  1.5988486     3.155521     4.277927\r\n    sigma    23.990640  1.6168350    21.056831    22.846187\r\n    lp__  -1264.946317  1.4072435 -1268.606024 -1265.657309\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.000666  -121.163812   -86.601618\r\n    beta1     4.462406     4.731267     5.288368\r\n    nu        5.129432     6.181671     9.348566\r\n    sigma    23.918413    25.056950    27.345459\r\n    lp__  -1264.644540 -1263.891813 -1263.183147\r\n\r\n, , chains = chain:3\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -137.758043 28.0473599  -193.976570  -156.709899\r\n    beta1     4.428479  0.4196264     3.620966     4.142890\r\n    nu        5.273822  1.5474425     3.130205     4.160287\r\n    sigma    23.905531  1.6471567    20.672028    22.808588\r\n    lp__  -1265.023664  1.3965268 -1268.555138 -1265.759590\r\n         stats\r\nparameter          50%         75%        97.5%\r\n    beta0  -137.963462  -118.57774   -83.786093\r\n    beta1     4.430414     4.70807     5.278970\r\n    nu        4.999793     6.07770     9.087928\r\n    sigma    23.906472    24.98220    27.219559\r\n    lp__  -1264.712841 -1263.99760 -1263.196266\r\n\r\n, , chains = chain:4\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -139.041834 27.8184750  -193.491107  -156.858089\r\n    beta1     4.447452  0.4163603     3.625069     4.186441\r\n    nu        5.462557  1.6789159     3.209953     4.299050\r\n    sigma    24.038623  1.6794892    20.805817    22.868034\r\n    lp__  -1265.066361  1.5085815 -1268.785015 -1265.858500\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -139.288564  -121.639438   -83.861266\r\n    beta1     4.449144     4.707789     5.268918\r\n    nu        5.106526     6.256394     9.576797\r\n    sigma    24.014382    25.132032    27.449061\r\n    lp__  -1264.738186 -1263.933356 -1263.202000\r\nregParam<-cbind(Beta0=rstan::extract(fitSimRegStan,pars=\"beta0\")$'beta0',\r\n                Beta1=rstan::extract(fitSimRegStan,pars=\"beta1\")$'beta1')\r\nhead(regParam)\r\n         Beta0    Beta1\r\n[1,] -174.7224 4.962250\r\n[2,] -118.6827 4.113129\r\n[3,] -152.0820 4.701561\r\n[4,] -147.0165 4.564773\r\n[5,] -159.9549 4.751269\r\n[6,] -126.6841 4.251516\r\npredX50<-apply(regParam,1,function(z) z%*%c(1,50))\r\npredX80<-apply(regParam,1,function(z) z%*%c(1,80))\r\n\r\nPlot both distributions, look at their summaries and HDIs.\r\n\r\nsuppressWarnings(library(HDInterval))\r\nden<-density(predX50)\r\nplot(density(predX80),xlim=c(60,240))\r\nlines(den$x,den$y)\r\n\r\nsummary(cbind(predX50,predX80))\r\n    predX50          predX80     \r\n Min.   : 55.42   Min.   :195.2  \r\n 1st Qu.: 78.48   1st Qu.:212.9  \r\n Median : 83.19   Median :216.8  \r\n Mean   : 83.23   Mean   :216.8  \r\n 3rd Qu.: 87.97   3rd Qu.:220.6  \r\n Max.   :111.77   Max.   :240.3  \r\nrbind(predX50=hdi(predX50),predX80=hdi(predX80))\r\n            lower     upper\r\npredX50  69.09173  97.20656\r\npredX80 205.32725 228.15029\r\n\r\nBoth JAGS and Stan produced the identical results.\r\nCompare the fit between FA and BA\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 2)\r\nabline(a=-139.96225, b= 4.46120, col = \"blue\", lwd = 1)\r\nabline(fit.outliers_removed, col = \"red\", lty=\"dashed\", lwd =1)\r\n\r\n\r\n\r\nWith influential\r\nWithout influential\r\nBayesian approach w/ robust assumption\r\n\r\norange, solid\r\nred, dashed\r\nblue, solid\r\nIntercept\r\n-104.78\r\n-155.91\r\n-139.96\r\nSlope\r\n3.98\r\n4.71\r\n4.46\r\n\r\nGreat! From the comparison, we can see that using Bayesian Methods to fit simple linear regression can be robust when the traditional regression has the influential points.\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., & Rubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition (3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th ed. ed.). Boston: McGraw-Hill Irwin.\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial with r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic Press is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/distill-preview.png",
    "last_modified": "2021-04-23T11:48:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-17-machine learning-discriminant-analysis/",
    "title": "Discriminant Analysis -- A Classification by Maximizing Class Separation",
    "description": "An Gentle Introduction of Discriminant Analysis & Its Applicant",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nLinear Discriminant Analysis\r\nWhy Discriminant Analysis\r\nQuadratic Discriminant Analysis\r\nLDA vs QDA\r\nBuilding our first linear and quadratic discriminant models\r\nCreating the task and learner, and training the LDA model\r\nCreating the task and learner, and training the QDA model\r\n\r\nReferences\r\n\r\nLinear Discriminant Analysis\r\nApproach for multiclass classification.\r\nA discriminant is a function that takes an input vector x and assigns to one of the multiple classes.\r\nModel the distribution of the predictors \\(X\\) in each response class\r\nUse Bayes theorem to flip these around into estimates for\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nWhen the distributions are assumed to be normal, the LDA model is very similar in form to logistic regression.\r\nWhy Discriminant Analysis\r\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\r\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\r\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\r\nQuadratic Discriminant Analysis\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nLDA = \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\sum\\) in each class.\r\nQDA = With Gaussians but different \\(\\sum_k\\) in each class, we get quadratic discriminant analysis.\r\nNOTE = By proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\r\nLDA vs QDA\r\nThe strengths of the LDA and QDA algorithms are:\r\nThey can reduce a high-dimensional feature space into a much more manageable number\r\nCan be used for classification or as a preprocessing (dimension reduction) technique to other classification algorithms that may perform better on the dataset\r\nQDA can learn curved decision boundaries between classes (this isn’t the case for LDA)\r\nThe weaknesses of the LDA and QDA algorithms are:\r\nThey can only handle continuous predictors (although recoding a categorical variable as numeric may help in some cases)\r\nThey assume the data are normally distributed across the predictors. If the data are not, performance will suffer\r\nLDA can only learn linear decision boundaries between classes (this isn’t the case for QDA)\r\nLDA assumes equal covariances of the classes and performance will suffer if this isn’t the case (this isn’t the case for QDA)\r\nQDA is more flexbile than LDA, and so can be more prone to overfitting\r\nWhen we should apply LDA vs QDA\r\nLDA needs lot less parameters than QDA.\r\nLDA is a much less flexible classifier than QDA \\(\\Rightarrow\\) substantially low variance.\r\nIf LDA’s assumption of common covariance matrix is poor, then LDA has high bias.\r\nLDA better bet if training set is small so reducing variance is important.\r\nQDA better bet if training set is large so variance of classifier not a major concern.\r\nBuilding our first linear and quadratic discriminant models\r\nWe have a tibble containing 178 cases and 14 variables of measurements made on various wine bottles: data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\r\nThe analysis determined the quantities of 13 constituents (Alcohol, Malic acid, Ash, Alcalinit of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline)found in each of the three types of wines.\r\n\r\n\r\n#install.packages(\"mlr\")\r\nlibrary(mlr)\r\nlibrary(tidyverse)\r\n#install.packages(\"HDclassif\")\r\ndata(wine, package = \"HDclassif\")\r\nwineTib <- as_tibble(wine)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   class    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10\r\n   <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1     1  14.2  1.71  2.43  15.6   127  2.8   3.06 0.28   2.29  5.64\r\n 2     1  13.2  1.78  2.14  11.2   100  2.65  2.76 0.26   1.28  4.38\r\n 3     1  13.2  2.36  2.67  18.6   101  2.8   3.24 0.3    2.81  5.68\r\n 4     1  14.4  1.95  2.5   16.8   113  3.85  3.49 0.24   2.18  7.8 \r\n 5     1  13.2  2.59  2.87  21     118  2.8   2.69 0.39   1.82  4.32\r\n 6     1  14.2  1.76  2.45  15.2   112  3.27  3.39 0.34   1.97  6.75\r\n 7     1  14.4  1.87  2.45  14.6    96  2.5   2.52 0.3    1.98  5.25\r\n 8     1  14.1  2.15  2.61  17.6   121  2.6   2.51 0.31   1.25  5.05\r\n 9     1  14.8  1.64  2.17  14      97  2.8   2.98 0.290  1.98  5.2 \r\n10     1  13.9  1.35  2.27  16      98  2.98  3.15 0.22   1.85  7.22\r\n# ... with 168 more rows, and 3 more variables: V11 <dbl>, V12 <dbl>,\r\n#   V13 <int>\r\n\r\n\r\n\r\nnames(wineTib) <- c(\"Class\", \"Alco\", \"Malic\", \"Ash\", \"Alk\", \"Mag\",\r\n                    \"Phe\", \"Flav\", \"Non_flav\", \"Proan\", \"Col\", \"Hue\",\r\n                    \"OD\", \"Prol\")\r\nwineTib$Class <- as.factor(wineTib$Class)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   Class  Alco Malic   Ash   Alk   Mag   Phe  Flav Non_flav Proan\r\n   <fct> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl>    <dbl> <dbl>\r\n 1 1      14.2  1.71  2.43  15.6   127  2.8   3.06    0.28   2.29\r\n 2 1      13.2  1.78  2.14  11.2   100  2.65  2.76    0.26   1.28\r\n 3 1      13.2  2.36  2.67  18.6   101  2.8   3.24    0.3    2.81\r\n 4 1      14.4  1.95  2.5   16.8   113  3.85  3.49    0.24   2.18\r\n 5 1      13.2  2.59  2.87  21     118  2.8   2.69    0.39   1.82\r\n 6 1      14.2  1.76  2.45  15.2   112  3.27  3.39    0.34   1.97\r\n 7 1      14.4  1.87  2.45  14.6    96  2.5   2.52    0.3    1.98\r\n 8 1      14.1  2.15  2.61  17.6   121  2.6   2.51    0.31   1.25\r\n 9 1      14.8  1.64  2.17  14      97  2.8   2.98    0.290  1.98\r\n10 1      13.9  1.35  2.27  16      98  2.98  3.15    0.22   1.85\r\n# ... with 168 more rows, and 4 more variables: Col <dbl>, Hue <dbl>,\r\n#   OD <dbl>, Prol <int>\r\n\r\nWe got:\r\n- 13 continuous measurements made on 178 bottles of wine, where each measurement is the amount of a different compound/element in the wine.\r\n- Class: vineyard the bottle comes from.\r\n\r\n\r\nwineUntidy <- gather(wineTib, \"Variable\", \"Value\", -Class)\r\nggplot(wineUntidy, aes(Class, Value)) +\r\n  facet_wrap(~ Variable, scales = \"free_y\") +\r\n  geom_boxplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nBox and whisker plots of each continuous variable in the data against vineyard number. For the box and whiskers: the thick horizontal line represents the median, the box represents the interquartile range (IQR), the whiskers represent the Tukey range (1.5 times the IQR above and below the quartiles), and the dots represent data outside of the Tukey range.   \r\nCreating the task and learner, and training the LDA model\r\n\r\n\r\nwineTask <- makeClassifTask(data = wineTib, target = \"Class\")\r\nlda <- makeLearner(\"classif.lda\")\r\nldaModel <- train(lda, wineTask)\r\n\r\n\r\n\r\nExtracting discriminant function values for each case\r\n\r\n\r\nldaModelData <- getLearnerModel(ldaModel)\r\nldaPreds <- predict(ldaModelData)$x\r\nhead(ldaPreds)\r\n\r\n\r\n        LD1       LD2\r\n1 -4.700244 1.9791383\r\n2 -4.301958 1.1704129\r\n3 -3.420720 1.4291014\r\n4 -4.205754 4.0028715\r\n5 -1.509982 0.4512239\r\n6 -4.518689 3.2131376\r\n\r\nPlotting the discriminant function values against each other\r\n\r\n\r\nwineTib %>%\r\n  mutate(LD1 = ldaPreds[, 1],\r\n         LD2 = ldaPreds[, 2]) %>%\r\n  ggplot(aes(LD1, LD2, col = Class)) + \r\n    geom_point() +\r\n    stat_ellipse() +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nCreating the task and learner, and training the QDA model\r\n\r\n\r\nqda <- makeLearner(\"classif.qda\")\r\nqdaModel <- train(qda, wineTask)\r\n\r\n\r\n\r\nCross-validating the LDA and QDA models\r\n\r\n\r\nkFold <- makeResampleDesc(method = \"RepCV\", folds = 10, reps = 50,\r\nstratify = TRUE)\r\n\r\nldaCV <- resample(learner = lda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nqdaCV <- resample(learner = qda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nldaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n    0.01133544     0.98866456 \r\n\r\nqdaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n   0.008314886    0.991685114 \r\n\r\nOur LDA model correctly classified 98.8% of wine bottles, on average! There isn’t much room for improvement here, but\r\nour QDA model managed to correctly classify 99.2% of cases!\r\nCalculating confusion matrices\r\n\r\n\r\ncalculateConfusionMatrix(ldaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.999/1e+00 0.001/9e-04 0.000/0e+00 0.001      \r\n  2      0.010/1e-02 0.977/1e+00 0.014/2e-02 0.023      \r\n  3      0.000/0e+00 0.007/5e-03 0.993/1e+00 0.007      \r\n  -err.-       0.011       0.005       0.020 0.01       \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2947    3    0      3\r\n  2        34 3468   48     82\r\n  3         0   16 2384     16\r\n  -err.-   34   19   48    101\r\n\r\ncalculateConfusionMatrix(qdaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.994/0.984 0.006/0.005 0.000/0.000 0.006      \r\n  2      0.014/0.016 0.986/0.993 0.000/0.000 0.014      \r\n  3      0.000/0.000 0.004/0.003 0.996/1.000 0.004      \r\n  -err.-       0.016       0.007       0.000 0.008      \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2933   17    0     17\r\n  2        48 3502    0     48\r\n  3         0    9 2391      9\r\n  -err.-   48   26    0     74\r\n\r\nPredicting which vineyard the poisoned wine came from\r\n\r\n\r\npoisoned <- tibble(Alco = 13, Malic = 2, Ash = 2.2, Alk = 19, Mag = 100,\r\n                   Phe = 2.3, Flav = 2.5, Non_flav = 0.35, Proan = 1.7,\r\n                   Col = 4, Hue = 1.1, OD = 3, Prol = 750)\r\npredict(qdaModel, newdata = poisoned)\r\n\r\n\r\nPrediction: 1 observations\r\npredict.type: response\r\nthreshold: \r\ntime: 0.00\r\n  response\r\n1        1\r\n\r\nThe model predicts that the poisoned bottle came from vineyard 1.\r\nHere’s we ends the analytic example.\r\nReferences\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-18T10:46:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-14-machine-learning-&-predictive-analytics/",
    "title": "Machine Learning & Predictive Analytics",
    "description": "An Overview of Machine Learning & Predictive Analytics",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [
      "Biostatistics",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Machine Learning\r\nLearning Types\r\nSupervised\r\nUnsupervised Learning\r\nSemi-supervised Learning\r\n\r\nActive Learning\r\nReinforcement Learning\r\nTransfer Learning\r\n\r\nUniversal Workflow of ML\r\nML Terminologies\r\nData Assumptions\r\nOverfitting and Underfitting\r\nParametric & Nonparametric Models\r\nRegression Analysis\r\n\r\nUse R or Python for machine learning?\r\nMachine Learning with mlr Package in R\r\n\r\nReferences\r\n\r\nWhat is Machine Learning\r\nMeaningful data transformations from input to output data.\r\nTransformations: represent or encode the data (RGB or HSV for color pixel).\r\nLearning is automatic search for better data representations.\r\nSearch through a predefined space of possibilities using guidance from feedback signal.\r\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E”\r\n\r\n-Tom Mitchell, Machine Learning, McGraw Hill, 1997\r\n\r\n\r\nExperience E, Task T, Performance P\r\n1. Chess: T: playing chess, P: % of games won, E: playing practice games against itself.\r\n2. Driving: T: driving a vehicle, P: avgdistance before error, E: sequence of images and steering commands recoded during manual driving.\r\n3. Handwriting Recognition: T: recognizing and classifying handwritten words in images, P: % of correctly classified words, E: DB of handwritten words with given classifications.\r\n\r\nLearning Types\r\nSupervised\r\nUnsupervised\r\nSemi-supervised\r\nReinforcement\r\nTransfer\r\nActive\r\nSupervised\r\nThe majority of practical machine learning uses supervised learning.\r\nSupervised learning is where you have input variables (\\(x\\)) and an output variable (\\(y\\)) and you use an algorithm to learn the mapping function from the input to the output.\\[ y = f(x) \\]\r\nThe goal is to approximate the mapping function so well that when you have new input data \\(x\\) that you can predict the output variables \\(y\\) for that data.\r\nIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\r\nWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\r\nSupervised Learning Examples:\r\nLinear Regression\r\nLogistic Regression\r\nK-NN (k-Nearest Neighbors)\r\nSupport Vector Machines (SVMs)\r\nDecision Tress and Random Forests\r\nNeural Networks\r\nUnsupervised Learning\r\nUnsupervised learning is where you only have input data \\(x\\) and no corresponding output variables.\r\nThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\r\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.\r\nUnsupervised Learning problems can be further grouped into Clustering and Association Problems.\r\nClustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\r\nAssociation: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy \\(A\\) also tend to buy \\(B\\).\r\nUnsupervised Learning Examples:\r\nClustering\r\nK-Means\r\nHierarchical Cluster Analysis (HCA)\r\nExpectation Maximization\r\n\r\nVisualization and Dimensionality Reduction\r\nPrincipal Component Analysis (PCA)\r\nKernel PCA\r\nt-distributed Stochastic Neighbor Embedding (t-SNE)\r\n\r\nAssociation Rule\r\nApriori\r\n\r\nNeural Networks\r\nAutoencoders\r\nBoltzmann machines\r\n\r\nSemi-supervised Learning\r\nSemi-supervised learning is halfway between supervised and unsupervised learning.\r\nTraditional classification methods use labeled data to build classifiers.\r\nThe labeled training sets used as input in Supervised learning is very certain and properly defined.\r\nHowever, they are limited, expensive and takes a lot of time to generate them.\r\nOn the other hand, unlabeled data is cheap and is readily available in large volumes.\r\nHence, semi-supervised learning is learning from a combination of both labeled and unlabeled data\r\nWhere we make use of a combination of small amount of labeled data and large amount of unlabeled data to increase the accuracy of our classifiers.\r\nActive Learning\r\nActive learning (sometimes called “query learning” or “optimal experimental design” in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence.\r\nThe key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns—to be “curious,” if you will—it will perform better with less training.\r\nActive learning is a special case of semi-supervised learning.\r\nReinforcement Learning\r\nReinforcement Learning is learning what to do and how to map situations to actions.\r\nThe end result is to maximize the numerical reward signal.\r\nThe learner is not told which action to take, but instead must discover which action will yield the maximum reward\r\nTransfer Learning\r\nA machine learning technique where a model trained on one task is re-purposed on a second related task.\r\nAn optimization that allows rapid progress or improved performance when modeling the second task.\r\nUniversal Workflow of ML\r\nDefine the problem\r\nAssemble dataset\r\nChoose a metric to quantify project outcome\r\nDecide on how to calculate the metric\r\nPrepare dataset\r\nDefine standard baseline\r\nDevelop model that beats baseline\r\nIdeal model is at the border of overfit and underfit–cross the border to know where it is so overfit model\r\nRegularize model and tune hyperparameters\r\nML Terminologies\r\nDataset\r\nTraining –Learn the parameters\r\nValidation –select hyperparameters\r\nTest –test the model aka generalization error\r\n\r\nBatch –set of examples used in one iteration of model training.\r\nMini-batch –A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference.\r\nEpoch –A full training pass over the entire data set such that each example has been seen once.\r\nIteration –A single update of a model’s weights during training.\r\nData Assumptions\r\nTraining and test data are from the same probability distribution.\r\nTraining and test data are iid.\r\nOverfitting and Underfitting\r\nOverfitting –model fits very well to the training data, aka detects patterns in the noise also\r\nDetect:\r\nLow training error, high generalization error.\r\n\r\nRemedies:\r\nReduce model capacity by removing features and/or parameters.\r\nGet more training data.\r\nImprove training data quality by reducing noise.\r\n\r\n\r\nUnderfitting–model too simple to detect patterns in the data\r\nDetect\r\nHigh training error.\r\n\r\nRemedies:\r\nIncrease model capacity by adding more parameters and/or features.\r\nReduce model constraints.\r\n\r\n\r\nParametric & Nonparametric Models\r\n\\[ 𝑦= 𝑓(𝑥) \\]\r\nEstimate the unknown function 𝑓 as \\(\\hat{f}\\)\r\nParametric Models:\r\nAssume the functional form or shape of 𝑓\r\nApply methodology to train model\r\nAdvantage –simple estimation\r\nDisadvantage – \\(\\hat{f}\\)may be far from true 𝑓\r\n\r\nNonparametric Models:\r\nNo assumption on the functional form or shape of 𝑓\r\nEstimate to fit as close as possible to the data\r\nAdvantage –can accurately fit a wide range of possible shapes of 𝑓\r\nDisadvantage –need large datasets (since there is no fixed # of params to estimate)\r\n\r\nRegression Analysis\r\nOLS\r\nMSE\r\nComputational Complexity of matrix inversion\r\nComplete training set\r\n\r\nBatch Gradient Descent\r\nCost function (MSE)\r\nLearning rate hyperparameter\r\nPartial derivative\r\nComplete training set\r\n\r\nStochastic Gradient Descent\r\nMini-batch Gradient Descent\r\nLinear Regression with OLS\r\n\\[ 𝑦= \\theta^T𝑋\\]\r\nThe cost function minimization is a closed-form solution called the Normal Equation: \\[ \\hat{\\theta} = (X^T . X)^{-1} X^T.y  \\]\r\nAdvantage –equation is linear with size of training set so it can handle large training sets efficiently.\r\nDisadvantage –\r\ncomputational complexity of inverting a matrix that increases with size of training set.\r\ndifficult to do online learning with new data arriving regularly (need to recalculate estimates), i.e. no iterative parameter updates.\r\n\r\nUse R or Python for machine learning?\r\nThere is something of a rivalry between the two most commonly used data science languages: R and Python. Of course, there are no machine learning tasks which are only possible to apply in one language or the other.\r\nR:\r\nR is geared specifically for mathematical and statistical applications, i.e. R can focus purely on data, but may feel restricted if they ever need to build applications based on their models.\r\nCurrently, there are modern tools in R designed specifically to make data science tasks simple and human-readable, such as those from the tidyverse.\r\nPreviously, ML algorithms in R were scattered across multiple packages, written by different authors. But R has now followed suit, with the caret and mlr packages (which stands for machine learning in R). While quite similar in purpose and functionality to caret, mlr package provides an interface for a large number of machine learning algorithms, and allows you to perform extremely complicated machine learning tasks with very little coding.\r\nPython:\r\nFirst of all, some of the more cutting-edge deep learning approaches are easier to apply in Python (they tend to be written in Python first and implemented in R later).\r\nPython, while very good for data science, is a more general purpose programming language.\r\nProponents of python could use this as en example of why it was better suited for machine learning, as it has the well known scikit-learn package which has a plethora of machine learning algorithms built into it.\r\nGoogle Trends demonstrates the search interest relative to the highest point on the chart for the given region and over the past 5 years.\r\n\r\n trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05z1_\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0212jm\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0jgqg\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/09gbxjr\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0j3djl7\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&q=%2Fm%2F05z1_,%2Fm%2F0212jm,%2Fm%2F0jgqg,%2Fm%2F09gbxjr,%2Fm%2F0j3djl7\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Machine Learning with mlr Package in R\r\nR users got mlr package similar to Scikit-Learn from Python. The package synthesizes all the ML functions from other packages in which we can perform most of ML tasks. mlr package has several algorithms in its bouquet. These algorithms have been categorized into regression, classification, clustering, survival, multiclassification and cost sensitive classification:\r\n\r\n\r\nlibrary(mlr)\r\nlistLearners(\"classif\")[c(\"class\",\"package\")]\r\n\r\n\r\n                            class                   package\r\n1                     classif.ada                 ada,rpart\r\n2              classif.adaboostm1                     RWeka\r\n3             classif.bartMachine               bartMachine\r\n4                classif.binomial                     stats\r\n5                classif.boosting              adabag,rpart\r\n6                     classif.bst                 bst,rpart\r\n7                     classif.C50                       C50\r\n8                 classif.cforest                     party\r\n9              classif.clusterSVM        SwarmSVM,LiblineaR\r\n10                  classif.ctree                     party\r\n11               classif.cvglmnet                    glmnet\r\n12                 classif.dbnDNN                   deepnet\r\n13                  classif.dcSVM            SwarmSVM,e1071\r\n14                  classif.earth               earth,stats\r\n15                 classif.evtree                    evtree\r\n16             classif.extraTrees                extraTrees\r\n17             classif.fdausc.glm                   fda.usc\r\n18          classif.fdausc.kernel                   fda.usc\r\n19             classif.fdausc.knn                   fda.usc\r\n20              classif.fdausc.np                   fda.usc\r\n21                classif.FDboost            FDboost,mboost\r\n22            classif.featureless                       mlr\r\n23                   classif.fgam                    refund\r\n24                    classif.fnn                       FNN\r\n25               classif.gamboost                    mboost\r\n26               classif.gaterSVM                  SwarmSVM\r\n27                classif.gausspr                   kernlab\r\n28                    classif.gbm                       gbm\r\n29                  classif.geoDA               DiscriMiner\r\n30               classif.glmboost                    mboost\r\n31                 classif.glmnet                    glmnet\r\n32       classif.h2o.deeplearning                       h2o\r\n33                classif.h2o.gbm                       h2o\r\n34                classif.h2o.glm                       h2o\r\n35       classif.h2o.randomForest                       h2o\r\n36                    classif.IBk                     RWeka\r\n37                    classif.J48                     RWeka\r\n38                   classif.JRip                     RWeka\r\n39                   classif.kknn                      kknn\r\n40                    classif.knn                     class\r\n41                   classif.ksvm                   kernlab\r\n42                    classif.lda                      MASS\r\n43       classif.LiblineaRL1L2SVC                 LiblineaR\r\n44      classif.LiblineaRL1LogReg                 LiblineaR\r\n45       classif.LiblineaRL2L1SVC                 LiblineaR\r\n46      classif.LiblineaRL2LogReg                 LiblineaR\r\n47         classif.LiblineaRL2SVC                 LiblineaR\r\n48 classif.LiblineaRMultiClassSVC                 LiblineaR\r\n49                  classif.linDA               DiscriMiner\r\n50                 classif.logreg                     stats\r\n51                  classif.lssvm                   kernlab\r\n52                   classif.lvq1                     class\r\n53                    classif.mda                       mda\r\n54                    classif.mlp                     RSNNS\r\n55               classif.multinom                      nnet\r\n56             classif.naiveBayes                     e1071\r\n57              classif.neuralnet                 neuralnet\r\n58                   classif.nnet                      nnet\r\n59                classif.nnTrain                   deepnet\r\n60            classif.nodeHarvest               nodeHarvest\r\n61                   classif.OneR                     RWeka\r\n62                   classif.pamr                      pamr\r\n63                   classif.PART                     RWeka\r\n64              classif.penalized                 penalized\r\n65                    classif.plr                   stepPlr\r\n66             classif.plsdaCaret                 caret,pls\r\n67                 classif.probit                     stats\r\n68                    classif.qda                      MASS\r\n69                  classif.quaDA               DiscriMiner\r\n70           classif.randomForest              randomForest\r\n71        classif.randomForestSRC           randomForestSRC\r\n72                 classif.ranger                    ranger\r\n73                    classif.rda                      klaR\r\n74                 classif.rFerns                    rFerns\r\n75                   classif.rknn                      rknn\r\n76         classif.rotationForest            rotationForest\r\n77                  classif.rpart                     rpart\r\n78                    classif.RRF                       RRF\r\n79                  classif.rrlda                     rrlda\r\n80                 classif.saeDNN                   deepnet\r\n81                    classif.sda                       sda\r\n82              classif.sparseLDA sparseLDA,MASS,elasticnet\r\n83                    classif.svm                     e1071\r\n84                classif.xgboost                   xgboost\r\n\r\n– ANALYTICS VIDHYA\r\nThe entire structure of this package relies on this premise:\r\n\\[\\text{Create a Task.   Make a Learner.   Train Them.}\\]\r\nCreating a task means loading data in the package (e.g., makeClassifTask).\r\nMaking a learner means choosing an algorithm (makeLearner) which learns from task (or data).\r\nFinally, train them (train).\r\nReferences\r\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R (1st ed. 2013. ed.). New York, NY: Springer New York.\r\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts: The MIT Press.\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nGéron, A. l. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems (First edition. ed.). Sebastopol, California: O’Reilly Media, Inc.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-17T14:33:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/",
    "title": "Beta Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Beta Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nDefinition\r\nIntuitive interpretation\r\nBeta function\r\nGamma function\r\nMain facts\r\nIn actions\r\nPlots in shiny\r\nFurther reading\r\n\r\nMotivation\r\n\r\nEven though I had learned the beta distribution from UIC’s Bayesian methods course and tutored it, such as setting up it as the prior distribution in conjugate distribution context. But it was easy to forget because of its dried content and too abstract. Here I try to combine the rigid theory (UC coursework’s content) and intuitive thought. By that way, I was able to ‘permenently stamp’ the concept to my brain.\r\n\r\nThe Beta distribution is a probability distribution on/of probabilities\r\nThe beta distribution describes a family of continuous probability distributions that are nonzero only on the interval (0 1).\r\nFor example, we can use it to model the probabilities: the Click-Through Rate of the advertisement, the batting averages, the 5-year survival chance for women with breast cancer, and so on.\r\nDefinition\r\nA continuous random variable \\(X_B \\sim Beta(\\alpha, \\beta)\\) has Beta distribution if its probability density function (PDF) is\r\n\\[ \r\nf_{X_B} (x; \\alpha, \\beta) = \\frac{1}{B(α,β)} x^{\\alpha − 1} (1−x)^{\\beta − 1}, \\ \\ \\text{for} \\ 0 < x < 1.\r\n\\]\r\nwhere \\(B(\\cdot)\\) is the Beta function and shape parameters \\(\\alpha, \\beta > 0\\).\r\nIntuitive interpretation\r\n\r\nPDF\r\nProbability as a …\r\nBinomial\r\n\\(f(x) = {n \\choose x} p^x (1-p)^{n-x}\\)\r\nparameter\r\n\r\n\\(\\rightarrow\\) the function of \\(x\\)\r\n\r\nBeta\r\n\\(f(p) = \\frac{1}{B(α,β)} p^{\\alpha − 1} (1−p)^{\\beta − 1}\\)\r\nrandom variable\r\n\r\n\\(\\rightarrow\\) the function of \\(p\\)\r\n\r\nThe beta distribution intuitively comes into play when we look at it in terms of numerator—\\(x/p\\) to the power of something multiplied by \\(1-x/1-p\\) to the power of something—from the lens of the binomial distribution.\r\nThe difference between the binomial and the beta is that the above models the number of successes (\\(x\\)), while the below models the probability (\\(p\\)) of success. In other words, the probability is a parameter in binomial; In the Beta, the probability is a random variable.\r\nIn this context, the shape parameters \\(\\alpha\\) and \\(\\beta\\) or \\(\\alpha-1\\) as the number of successes and \\(\\beta-1\\) as the number of failures\r\nWe can explore the beauty of beta distribution via the the calculator for Beta distribution—Dr. Bognar at the University of Iowa built it.\r\nBeta distribution is very flexible: bell-curve (The PDF of a beta distribution is approximately normal if \\(\\alpha + \\beta\\) is large enough and \\(\\alpha\\) & \\(\\beta\\) are approximately equal), U-shaped (when \\(\\alpha\\) < 1, \\(\\beta\\) < 1) and even straight line. Here’s an graph excerpt from wikipedia.\r\nThe very flexible of Beta distributionBeta function\r\nThe beta function is\r\n\\[ \r\nB(x,y) = \\int_0^1 t^{x−1} (1−t)^{y−1} dt = \\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)},\r\n\\]\r\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function.\r\nGamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\):\r\n\\[ \r\n\\Gamma (n) = (n−1)! = 1 \\times 2 \\times 3 \\times ... \\times (n−1)\r\n\\]\r\nThe gamma function is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\r\n\\Gamma (t) = \\int_0^{\\infty} x^{t-1} e^{-x} dx\r\n\\]\r\n\r\nSimplify the Beta function with the Gamma Function \\(\\Rightarrow\\) we saw the PDF of Beta written in terms of the Gamma function. The Beta function is the ratio of the product of the Gamma function of each parameter divided by the Gamma function of the sum of the parameters (proof refered the further reading topic).\r\n\r\nMain facts\r\n\\[\r\nE[X_B] = \\mu = \\frac{\\alpha}{\\alpha + \\beta}; \\ \\ V[X_B] = \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\r\n\\]\r\nThe standard uniform distribution \\(\\text{Unif} \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nThe mode is \\(\\omega = \\frac{\\alpha − 1}{\\alpha + \\beta − 2}\\) for \\(\\alpha, \\beta > 1\\).\r\nThe concentration is \\(\\kappa = \\alpha + \\beta\\).\r\nDefinitions of \\(\\mu, \\omega\\) and \\(\\kappa\\) can be inverted:\r\n\\[ \\alpha = \\mu\\kappa,  \\beta = (1 − \\mu)\\kappa \\]\r\n\\[ \\alpha = \\omega(\\kappa−2)+1,  \\beta = (1 − \\omega)(\\kappa−2)+1, \\ \\kappa > 2. \\]\r\nParameter \\(\\kappa\\) is a measure of number of observations needed to change our previous belief about \\(\\mu\\).\r\nIf \\(\\kappa\\) is small we need only a few new observations.\r\nExample. Concentration \\(\\kappa = 8\\) around \\(\\mu = 0.5\\) corresponds to \\(\\alpha = \\mu \\kappa = 4\\) and \\(\\beta = (1 − \\mu) \\kappa = 4\\).\r\nParameterization in terms of mean value and standard deviation is:\r\n\\[ \\alpha = \\mu [\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1]; \\ \\ \\beta = (1 - \\mu)[\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1] \\]\r\nStandard deviation is typically smaller than standard deviation of uniform distribution on \\([0,1]\\), i.e. \\(0.28867\\).\r\nExamples.\r\nFor \\(\\mu = 0.5\\), \\(\\sigma = 0.28867\\) the shape parameters are \\(\\alpha = 1\\), \\(\\beta = 1\\).\r\nFind shape parameters of beta distribution with \\(\\mu = 0.5\\), \\(\\sigma = 0.1\\).\r\nThe standard uniform distribution \\(Unif \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nIn actions\r\nKeep parameter \\(\\beta\\) fixed. Move \\(\\alpha\\) up or down. Observe how the mass of the distribution moves\r\n\r\n\r\np <- seq(0,1,by=0.2)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=2), aes(colour = \"alpha=1,beta=2\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=2), aes(colour = \"alpha=4,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=2), aes(colour = \"alpha=6,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=8, shape2=2), aes(colour = \"alpha=8,beta=2\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDo the same as above, but keep \\(\\alpha\\) constant and move \\(\\beta\\) up or down\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=1), aes(colour = \"alpha=2,beta=1\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=5), aes(colour = \"alpha=2,beta=5\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=6), aes(colour = \"alpha=2,beta=6\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=8), aes(colour = \"alpha=2,beta=8\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nMake \\(\\alpha = \\beta = 1\\). What does the shape of the distribution tell you about your knowledge about random variable \\(\\theta\\)?\\(\\Rightarrow\\) The standard uniform distribution \\(Unif(0,1)\\) is a special case of the beta distribution \\(Beta (1,1)\\), when \\(\\alpha\\)=\\(\\beta\\)=1.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"green\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nKeep \\(\\alpha = \\beta\\) , but move both of them up or down. Interpret the shape of the distribution\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=0.5, shape2=0.5), aes(colour = \"alpha=0.5,beta=0.5\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=4), aes(colour = \"alpha=4,beta=4\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=6), aes(colour = \"alpha=6,beta=6\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nVariance changes based on 2 shape parameters.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=400, shape2=80), aes(colour = \"alpha=400,beta=80\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=40, shape2=8), aes(colour = \"alpha=40,beta=8\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=30, shape2=70), aes(colour = \"alpha=30,beta=70\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=7), aes(colour = \"alpha=3,beta=7\")) +\r\n  scale_y_continuous(limits=c(0,25)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\", \"green\", \"orange\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWhen beta distribution is used as a prior for parameter of binomial distribution, parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as previously observed numbers of successes (\\(\\alpha\\)) or failures (\\(\\beta\\)). For example, if in 2 Bernoulli experiments there was 1 success and 1 failure you can express opinion about probability of success as \\(Beta(1,1)\\). What would you assume as prior if in 6 previously observed outcomes there were 3 successes and 3 failures? What is the likely value of the parameter? Do we have more or less information than in case of 1 success and 1 failure? \\(\\Rightarrow\\) Think of more\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=3), aes(colour = \"alpha=3,beta=3\")) +\r\n  stat_function(fun=dbinom, args=list(size=1, prob=0.5), aes(colour = \"Bernoulli w/ prob=0.5\")) + # bernoulli\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"red\",\"green\",\"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDefine distribution with mode \\(\\omega\\)=.8 and concentration \\(\\kappa = 12\\). To do that find shape parameters \\(\\alpha = \\omega (\\kappa − 2) + 1 = 9\\) and \\(\\beta = (1 − \\omega)(\\kappa − 2) + 1 = 3\\).\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=9, shape2=3), aes(colour = \"alpha=9,beta=3\")) +\r\n  scale_y_continuous(limits=c(0,3.4)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the actions we notify that:\r\nThe special case \\(a=b=1\\) is the uniform distribution.\r\nThe distribution is roughly centered on \\(a/(a+b)\\). Actually, it turns out that the mean is exactly \\(a/(a+b)\\). Thus the mean of the distribution is determined by the relative values of \\(a\\) and \\(b\\).\r\nThe larger the values of \\(a\\) and \\(b\\), the smaller the variance of the distribution about the mean.\r\nFor moderately large values of \\(a\\) and \\(b\\) the distribution looks visually “kind of normal”, although unlike the normal distribution the Beta distribution is restricted to [0,1].\r\nPlots in shiny\r\nPlanning to build an shiny app to plot beta distribution on the specification of shape parameter (“still being in the process”).\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nDavid Robinson (Principal Data Scientist at Heap, works in R and Python), Understanding the beta distribution (using baseball statistics), http://varianceexplained.org/statistics/beta_distribution_and_baseball/\r\nAerin Kim, Beta Distribution — Intuition, Examples, and Derivation, https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/Beta-Distribution-in-an-Intuitive-Explanation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-15T00:29:44-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/",
    "title": "Gamma Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Gamma Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "ggplot2",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nFact\r\nGamma function\r\nDefinition\r\nParameters of Gamma: a shape with a scale or a rate\r\nPlots\r\nFurther reading\r\n\r\nFact\r\nWhy did we invent Gamma distribution? Answer: To predict the wait time until future events. Hmmm ok, but I thought that’s what the exponential distribution is for. Then, what’s the difference between exponential distribution and gamma distribution? The exponential distribution predicts the wait time until the very first event. The gamma distribution, on the other hand, predicts the wait time until the k-th event occurs.\r\n– Aerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nExamples:\r\nThe time I wait to receive an interview might follow an exponential. Now, I am waiting not for my first interview offer but my third interview offer. How long must I wait? This waiting time can be described by a gamma.\r\nI missed the first and second CTA train to go to the campus. Now, how long I am able to catch the third train?\r\nGamma function\r\nIn the lecture series of Statistics 110, Lecture 24: Gamma distribution and Poisson process | Statistics 110, Prof. Joe Blitzstein had connected the \\(n!\\) function to the Gamma function. Why?\r\nLet’s see the Gamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\): \\[\\Gamma(n) = (n-1)! = 1 \\times 2 \\times 3 \\times ... \\times (n-1)\\]\r\nOne beautiful formula, Stirling formula to approximate the \\(n!\\), actually an extremely good approximation: \\[ n! \\approx \\sqrt{2\\pi n} \\Big( \\frac{n}{e}\\Big)^n\\]\r\n\r\n\r\nn <- c(1:6)\r\ny <- vector(mode = \"numeric\", length = length(n))\r\ny[1] <- 1\r\nfor(i in 2:length(n)) {\r\n  y[i] = y[i-1] * i\r\n}\r\ndta <- as.data.frame(cbind(n,y))\r\nlibrary(ggplot2)\r\nggplot(dta, aes(n, y)) + \r\n  geom_point() +\r\n  scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\")) +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nThen how we connect the dots. There are many ways to do it, but there’s a philosophical way to do it by Gamma function, which is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\\Gamma(t) = \\int_0^{\\infty} x^t e^{−x} \\frac{dx}{x} \\]\r\nDefinition\r\nFrom the Gamma function, how we got the PDF of Gamma distribution. We would normalize the Gamma distribution, which means from:\r\n\\[ \\Gamma(k) = \\int_0^{\\infty} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nto,\r\n\\[ 1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nThen, \\(X = \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{1}{x}\\) \\(\\sim\\) \\(Gamma(k, 1)\\) which has shape of \\(k\\) and scale of \\(1\\).\r\nHow we turn the scale of \\(1\\) to a general scale of \\(\\theta\\)?\r\nImagine that \\(Y \\sim \\frac{X}{\\theta}\\) where \\(X \\sim \\ Gamma(k,1)\\)\r\n\\(f_Y(y) = f_X(x) \\frac{dx}{dy} = \\frac{1}{\\Gamma(k)} (\\theta y)^{k} e^{−\\theta y} \\frac{1}{\\theta y} \\theta\\) where \\(\\frac{dx}{dy} = \\theta\\)\r\nThus, \\(f(y) = \\frac{1}{\\Gamma(k) \\theta^{k}} (y)^{k} e^{−\\theta y} \\frac{1}{y}\\)\r\nParameters of Gamma: a shape with a scale or a rate\r\n\r\n\r\nknitr::include_graphics(\"Gamma_scalevsrate_inwiki.png\") \r\n\r\n\r\n\r\n\r\n(#fig:model diagram)From https://en.wikipedia.org/wiki/Gamma_distribution\r\n\r\n\r\n\r\nFor (\\(\\alpha\\), \\(\\beta\\)) parameterization: Using our notation \\(k\\) (the # of events) & \\(\\lambda\\) (the rate of events), simply substitute \\(\\alpha\\) with \\(k\\), \\(\\beta\\) with \\(\\lambda\\). The PDF stays the same format as what we’ve derived.\r\nFor (\\(k\\), \\(\\theta\\)) parameterization: \\(\\theta\\) is a reciprocal of the event rate \\(\\lambda\\), which is the mean wait time (the average time between event arrivals).\r\nPlots\r\nI plotted the gamma distribution with the shape of \\(k\\), and constantly rate = \\(1\\)\r\n\r\n\r\nT <- seq(0,20,by=2.5)\r\n\r\ndf <- data.frame(T)\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=1, rate=1), aes(colour = \"k= 1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=5, rate=1), aes(colour = \"k= 5\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"k=10\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"yellowgreen\", \"olivedrab\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nI plotted the gamma distribution with the constantly shape of k = 10, and variant rate from 1 to 3.\r\n\r\n\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"r=1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=10, rate=2), aes(colour = \"r=2\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=3), aes(colour = \"r=3\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"gold\", \"burlywood\", \"darkorange\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution (k=10)\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFurther reading\r\nLecture 24: Gamma distribution and Poisson process | Statistics 110\r\nAerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nWiki\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/Gamma-distribution-in-an-intuitive-explanation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-23T11:53:12-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
