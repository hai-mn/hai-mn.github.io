[
  {
    "path": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/",
    "title": "Fitting a Simple Linear Regression in Bayesian Context",
    "description": "How to fit a linear regression using Bayesian Methods  \nConsider a Bayesian model fit as a remedial measures for influential case",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nFit a model\r\nModel fit diagnostics\r\nMessage take-away\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\nMCMC in JAGS\r\nDescribe the model.\r\nExplore the MCMC object\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nExplore the MCMC object\r\n\r\nUsing fitted regression model for prediction\r\n\r\nCompare the fit between FA and BA\r\nFurther reading\r\n\r\nFrequentist approach in simple linear regression\r\nAn example with data\r\nI use the data in Kruschke (2015) which has\r\n- male\r\n- height\r\n- weight\r\nWe will use height to predict weight of a person\r\n\r\nDT::datatable(dta, \r\n          rownames = FALSE,\r\n          filter = list(position = \"top\"))\r\n\r\n{\"x\":{\"filter\":\"top\",\"filterHTML\":\"<tr>\\n  <td data-type=\\\"integer\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"54.6\\\" data-max=\\\"76\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"89\\\" data-max=\\\"356.8\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[0,0,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1],[64,62.3,67.9,64.2,64.8,57.5,65.6,70.2,63.9,71.1,66.5,68.1,62.9,75.1,64.6,69.2,68.1,72.6,63.2,64.1,64.1,71.5,76,69.7,73.3,61.7,66.4,65.7,68.3,66.9,62.4,64.5,60.6,70.8,61,66,59.6,70.1,66.6,59.8,68.5,61.4,64.7,67.4,68.3,67.3,62.5,72.4,64.4,70.6,66.3,65.9,61.1,67.8,64.4,71.2,64.5,65.4,67,70.9,62.4,70.4,64.6,69.6,65.9,70.6,65.4,73.9,70.1,64.8,59.7,69.2,61.4,70.4,71.1,60.8,68.3,67.4,63.5,66.8,73.4,64.8,68.9,66.5,64.7,66.2,69.1,66,60.9,72.4,67.5,71.6,70.6,64.7,72.7,64.7,67.9,65.9,54.6,64.1,68.3,72.9,64,62.1,67.3,60.4,63.4,63.3,61.4,67.7,65.1,67.5,67,65.6,70.9,61.5,59.4,59.7,69,71.7,65.7,67.2,65.9,64.9,73.6,70.5,63.6,64.7,69.4,69.2,64.5,70.9,63.6,73.7,67.6,65.9,68.1,64.1,61.6,71.2,66.7,71.1,68.6,62.8,67.6,63.1,65.2,67.4,64.3,59.9,61.9,63.4,59.8,67.7,64.5,67.4,66.1,75.1,67.9,65.7,64.9,64.8,55.3,63,64.1,64.3,63.8,65.1,71.8,66.7,64.5,60.7,59.6,68.4,63.2,66,62.5,67.9,70.2,70.3,67.1,66.2,62.1,72.4,65.2,72.1,65.6,67.2,66.2,66,63.6,65,57.5,63.4,67.2,62.8,65.3,66.7,70.3,68.6,66.4,62.9,57.6,65.3,70.9,68,65.4,67.4,62.8,73.3,64.6,70,68.1,69.4,71.9,67.8,63.5,69.8,66.1,62,71.5,71.9,72.6,64.3,71.7,70.1,64.3,63.9,74.6,65.5,63.1,66.7,65.2,62,71,69.4,69.8,66.6,71,74.7,63.6,69.8,58.9,67.9,65.2,70.6,59.8,71.2,71.5,63.7,65.1,64.8,66.8,64.5,68.6,65.4,66.5,68.8,70.4,57,60.6,62.8,73.3,73.2,61.9,66.4,69.9,60.8,65.9,61.8,66.5,64,68.2,71.8,70.4,63,60.1,66.3,67.1,61.5,66.4,63.8,69.9,66.1,66.2,66.5,70.2,64,69,65.7,69.5,64.1,61.2,62.6,67.9,68.5,69.2,65.9,68.3,70.2],[136.4,215.1,173.6,117.3,123.3,96.5,178.3,191.1,158,193.9,127.1,147.9,119,204.4,143.4,124.4,140.9,164.7,139.8,110.2,134.1,193.6,180,155,188.2,187.4,139.2,147.9,178.6,111.1,119.2,184.4,100.1,207.3,159.8,120.7,102.8,195.7,130.1,156.5,113.7,119.1,142.8,179.9,166.3,135.4,118.9,173.9,117.8,192.6,122,129.5,116.9,177.1,160.1,199.5,111,177.4,187.9,177.1,185.3,223.5,128.4,184.4,122.1,216.8,173.8,197.8,181.1,136.6,105,150.1,125,172.2,143.9,132,110.7,155.9,174.9,176.6,167.1,133.5,211.4,150.6,144.7,120.7,222.5,168.4,134.3,182.8,187.2,193.4,195.2,131.1,204.5,108.6,128.1,152.7,120.3,183.5,210.6,163.1,143.9,114.4,170.5,93.2,147.8,161.2,114.6,158,144.5,184,225,116.9,183.2,131.5,217.1,123.6,145.9,170.6,133.5,165.9,134.1,111.1,201.1,156.3,161.5,145,159.8,149,222,149.7,130.6,242.5,150,191.3,164.1,135.6,139.4,114.8,191.6,194.7,170,146.9,186.1,125.8,96.2,156.8,117.6,149.6,125.4,140.7,150.2,156.5,137.6,140.3,174.7,186.1,191.3,135.2,130.5,137.1,166.5,280.5,126,128.3,166.5,124,120.8,193.8,157,190.8,110.9,185.9,163.2,167.9,119.8,122,178.8,181.1,168.8,120.9,96.5,210.4,139.3,187,146.5,141.8,162,173.8,160.7,125.3,125.9,193.7,234.9,156.9,221.2,241.6,170.4,152.6,172.8,176.6,123.5,202.1,182.2,190.9,146.6,158.5,140.5,168,182,275.2,164.1,153.5,159.5,141.7,194.1,149.6,157.5,149.9,210.8,154.5,206.5,123.4,166,160.8,167.6,141.9,186.2,155.7,148.7,132.6,356.8,164.9,187.2,169.6,178.8,202.2,158.9,186.1,169.7,147.6,110.6,146.8,146.2,164.8,115.7,191.5,198.6,143.7,126.7,123.1,231.5,134.7,159.4,141.5,144.5,150.5,173.7,215.5,146.4,133.6,240.2,216.6,89,213.6,211.3,140.4,135.7,151.7,198.3,158.7,218,172.7,161.9,130.3,123,157,131.2,108.7,202.7,131.9,164.5,179.2,154.4,120.1,189.7,160.2,145.7,186.2,144.8,147.4,120.8,134.9,164.8,205.9,172.5,130.8,146.5,173.8]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>male<\\/th>\\n      <th>height<\\/th>\\n      <th>weight<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[0,1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\nFit a model\r\n\r\nfit <- lm(dta$weight ~ dta$height)\r\nsummary(fit)\r\n\r\nCall:\r\nlm(formula = dta$weight ~ dta$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-63.95 -21.17  -5.26  16.24 201.94 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -104.7832    31.5056  -3.326 0.000992 ***\r\ndta$height     3.9822     0.4737   8.406 1.77e-15 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.59 on 298 degrees of freedom\r\nMultiple R-squared:  0.1917,    Adjusted R-squared:  0.189 \r\nF-statistic: 70.66 on 1 and 298 DF,  p-value: 1.769e-15\r\n\r\nThen, we can see the fitted regression line overlay with data\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Scatter Plot between Height and Weight by Gender\",\r\n     pch = as.numeric(dta$male), col = as.factor(dta$male))\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\n\r\nModel fit diagnostics\r\nThere are four assumptions associated with a linear regression model:\r\nLinearity: The relationship between X and the mean of Y is linear.\r\nHomoscedasticity: The variance of residual is the same for any value of X.\r\nIndependence: Observations are independent of each other.\r\nNormality: For any fixed value of X, Y is normally distributed.\r\nI check the model fit by plotting:\r\nplot of residuals against predictor variable (how to check residuals have non-linear patterns or not)\r\nnormal probability plot (how to check residuals are normally distributed: residuals follow a straight line well or do they deviate severely; it’s good if residuals are lined well on the straight dashed line.)\r\nplot of square root of standardized residual absolute value (how to check the assumption of equal variance (homoscedasticity): if residuals are spread equally along the ranges of predictors; it’s good if we see a horizontal line with equally (randomly) spread points.)\r\nAnd to look at the outlier and leverage via\r\nplot standardized residuals vs leverage\r\nResiduals are the difference between the observed score and the predicted score.\r\n\\[ e_i = Y^{obs}_i - \\hat{Y}_i\\]\r\nResiduals come in three varieties:\r\nRaw Residuals: The difference between the raw observed score and the predicted score.\r\nStandardized Residuals: These are the raw residuals divided by the standard error of estimate.\r\nStudentized Residuals: These are raw residuals divided by the standard error of the residual with that case deleted. These are sometimes called studentized deleted residuals or studentized jackknifed residuals.\r\n\r\npar(mfrow=c(2,2))\r\nplot(fit)\r\n\r\n\r\nLastly, we look at the influential points\r\n\r\nn <- dim(dta)[1]\r\ncooksD <- cooks.distance(fit)\r\n#identify influential points\r\n(influential_obs <- as.numeric(names(cooksD)[(cooksD > (4/n))]))\r\n [1]   2 117 134 140 163 164 169 172 212 233 260 263\r\n#plot cooksD\r\nplot(cooksD, pch=\"*\", cex=2, main=\"Influential Obs by Cooks distance\")  \r\nabline(h = 4/n, col=\"red\")  # add cutoff line\r\ntext(x=1:length(cooksD), y=cooksD, labels=ifelse((cooksD>(4/n)),names(cooksD),\"\"), col=\"red\", pos = 4)\r\n\r\n\r\nTill now, we can say that the linear regression assumption is violated in this case, e.g. error is not following the normal distribution. Therefore, how about we delete the influential points and re-fit the model:\r\n\r\ndta.outliers_removed <- dta[-influential_obs, ]\r\n\r\nfit.outliers_removed <- lm(dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\nsummary(fit.outliers_removed)\r\n\r\nCall:\r\nlm(formula = dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-55.16 -18.87  -3.18  16.55  83.27 \r\n\r\nCoefficients:\r\n                             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)                 -155.9116    26.8300  -5.811 1.65e-08 ***\r\ndta.outliers_removed$height    4.7112     0.4032  11.685  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.36 on 286 degrees of freedom\r\nMultiple R-squared:  0.3231,    Adjusted R-squared:  0.3208 \r\nF-statistic: 136.5 on 1 and 286 DF,  p-value: < 2.2e-16\r\n\r\nSomehow, we saw the model assumption is satisfied when the influential cases removed.\r\n\r\npar(mfrow=c(1,2))\r\nplot(fit.outliers_removed, which=c(1,2))\r\n\r\n\r\nThe regression line changes when we remove the influential observations. Dangerous!!!\r\n\r\npar(mfrow=c(1,2))\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\nplot(dta.outliers_removed$height, dta.outliers_removed$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Outliers removed\")\r\nabline(fit.outliers_removed, col = \"orange\", lwd = 3)\r\n\r\n\r\nBut, the action of deleting of influential cases is often not the solution due to produce the bias estimates.\r\nThrough this case, we have reviewed:\r\nInfluential points = Outliers & Leverage\r\nA point that makes a lot of difference in a regression case, is called ‘an influential point.’ Usually influential points have two characteristics:\r\nThey are outliers, i.e. graphically they are far from the pattern described by the other points, that means that the relationship between x and y is different for that point than for the other points.\r\nThey are in a position of high leverage, meaning that the value of the variable x is far from the mean. Observations with very low or very high values of x are in positions of high leverage.\r\nRemind knowledge\r\nDiscrepancy: Difference between the predicted and observed value\r\n- Measured by Studentized Residuals\r\nLeverage: high leverage if it has “extreme” predictor x values\r\n- Measured by Hat Value\r\nInfluence: Assesses how much regression equation would change if an observation/potential outlier was dropped from the analysis\r\n- Measured by Cook’s Distance, Difference in Fit (DFFITS), or Difference in coefficients (DFBETAS)\r\nMessage take-away\r\nRemoving influential cases is not the optimal solution.\r\nIn the real-life analysis, we can use other types of regression:\r\nRidge regression\r\nRobust regression\r\nIRLS Robust regression\r\nLowess method\r\nRegression trees\r\nand etc…\r\n\r\nBayesian approach\r\nIntroduction to a regression model in Bayesian way\r\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon \\]\r\nWith Bayesian approach distribution of \\(\\epsilon\\) does not have to be Gaussian (normal), we are going to use robust assumption.\r\nParameterization of the model for MCMC (adapted from Kruschke (2015))Bayes theorem for this model:\r\n\\[\r\np(\\beta_0, \\beta_1, \\sigma, \\gamma \\mid D) = \\frac{p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma)}{\\int \\int \\int \\int p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma) \\ d\\beta_0 \\ d\\beta_1 \\ d\\sigma \\ d\\gamma}\r\n\\]\r\nCreate the data list.\r\n\r\ny <- dta$weight\r\nx <- dta$height\r\ndataList <- list(x = x, y = y)\r\n\r\nMCMC in JAGS\r\nDescribe the model.\r\nBased on the Normal distribution (demonstrating purpose, not run)\r\n\r\nmodstring_norm = \"\r\n# Specify the Normal model for none-standardized data:\r\nmodel {\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ dnorm(mu[i], prec)\r\n        mu[i] = b[1] + b[2]*log_income[i] \r\n    }\r\n    \r\n    for (i in 1:2) {\r\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\r\n    }\r\n    \r\n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\r\n    sig2 = 1.0 / prec\r\n    sig = sqrt(sig2)\r\n} \"\r\n\r\nBased on the Student-t distribution, robust assumption\r\n\r\n modelString = \"\r\n# Standardize the data:\r\ndata {\r\n    Ntotal <- length(y)\r\n    xm <- mean(x)\r\n    ym <- mean(y)\r\n    xsd <- sd(x)\r\n    ysd <- sd(y)\r\n    for ( i in 1:length(y) ) {\r\n      zx[i] <- (x[i] - xm) / xsd\r\n      zy[i] <- (y[i] - ym) / ysd\r\n    }\r\n}\r\n# Specify the model for standardized data:\r\nmodel {\r\n    for ( i in 1:Ntotal ) {\r\n      zy[i] ~ dt( zbeta0 + zbeta1 * zx[i] , 1/zsigma^2 , nu )\r\n    }\r\n    # Priors vague on standardized scale:\r\n    zbeta0 ~ dnorm(0, 1/(10)^2 )  \r\n    zbeta1 ~ dnorm(0, 1/(10)^2 )\r\n    zsigma ~ dunif(1.0E-3, 1.0E+3 )\r\n    nu ~ dexp(1/30.0)\r\n    # Transform to original scale:\r\n    beta1 <- zbeta1 * ysd / xsd  \r\n    beta0 <- zbeta0 * ysd  + ym - zbeta1 * xm * ysd / xsd \r\n    sigma <- zsigma * ysd\r\n}\r\n\"\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"TEMPmodel.txt\")\r\n\r\nIn the tutorial, we just want to execute the model specified by t-student distribution.\r\nEvery arrow has a corresponding line in the descriptive diagram.\r\nVariable names starting with “z” mean that these variables are standardized (z-scores).\r\nThe intention of using z-scores in JAGS is to overcome a problem of correlation of the parameters (as the simulation the correlation between \\(\\beta_0\\) and \\(\\beta_1\\)).\r\nStrong correlation creates thin and long shape on scatter-plot of the variables which makes Gibbs sampling very slow and inefficient.\r\nBut remember to scale back to the original measures.\r\nHMC implemented in Stan does not have this problem. This can be applied to STAN in all situation !!!\r\n\r\nparameters = c(\"beta0\" ,  \"beta1\" ,  \"sigma\", \r\n              \"zbeta0\" , \"zbeta1\" , \"zsigma\", \"nu\")\r\nadaptSteps = 500  # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps=20000\r\nnIter = ceiling((numSavedSteps*thinSteps ) / nChains )\r\njagsModel = jags.model(\"TEMPmodel.txt\", data=dataList ,\r\n                      n.chains=nChains, n.adapt=adaptSteps)\r\nupdate(jagsModel, n.iter=burnInSteps)\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters, \r\n                          n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the MCMC object\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:6500\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 5000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n             Mean       SD  Naive SE Time-series SE\r\nbeta0  -139.96225 27.61760 0.1952859      0.2180717\r\nbeta1     4.46120  0.41330 0.0029225      0.0032290\r\nnu        5.37901  1.62796 0.0115114      0.0236101\r\nsigma    23.97943  1.65132 0.0116766      0.0204363\r\nzbeta0   -0.09632  0.04806 0.0003398      0.0004283\r\nzbeta1    0.49046  0.04544 0.0003213      0.0003550\r\nzsigma    0.68372  0.04708 0.0003329      0.0005827\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%       25%        50%       75%      97.5%\r\nbeta0  -193.3580 -158.7464 -140.16434 -121.3445 -8.624e+01\r\nbeta1     3.6527    4.1830    4.46491    4.7407  5.257e+00\r\nnu        3.1509    4.2578    5.05987    6.1485  9.410e+00\r\nsigma    20.8323   22.8476   23.94254   25.0519  2.728e+01\r\nzbeta0   -0.1888   -0.1282   -0.09682   -0.0644 -4.115e-04\r\nzbeta1    0.4016    0.4599    0.49087    0.5212  5.779e-01\r\nzsigma    0.5940    0.6514    0.68267    0.7143  7.779e-01\r\nplot(codaSamples, trace=TRUE, density=FALSE) # note: many graphs\r\n\r\nautocorr.plot(codaSamples, ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n    beta0     beta1        nu     sigma    zbeta0    zbeta1    zsigma \r\n16054.484 16401.066  4768.900  6535.965 12892.980 16401.066  6535.965 \r\n#gelman.diag(codaSamples)\r\ngelman.plot(codaSamples)   # lines may return error ==> Most likely reason: collinearity of parameters \r\n\r\n(HDIofChains <- lapply(codaSamples, function(z) cbind(Mu = hdi(codaSamples[[1]][,1]), Sd = hdi(codaSamples[[1]][,2]))))\r\n[[1]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[2]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[3]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[4]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\nLook at strong correlation between beta0 and beta1 which slows Gibbs sampling down.\r\n\r\nhead(as.matrix(codaSamples[[1]]))\r\n         beta0    beta1       nu    sigma      zbeta0    zbeta1\r\n[1,] -168.7337 4.890447 5.623225 23.90661 -0.10407253 0.5376553\r\n[2,] -158.6082 4.744414 6.371647 24.56065 -0.09181883 0.5216004\r\n[3,] -166.9662 4.873701 6.346842 22.92533 -0.08537616 0.5358143\r\n[4,] -161.6680 4.795882 3.209357 23.00506 -0.08162923 0.5272588\r\n[5,] -160.5706 4.801212 3.153090 22.15810 -0.04024801 0.5278448\r\n[6,] -155.9331 4.684888 4.394775 21.44653 -0.12823063 0.5150561\r\n        zsigma\r\n[1,] 0.6816415\r\n[2,] 0.7002899\r\n[3,] 0.6536626\r\n[4,] 0.6559359\r\n[5,] 0.6317868\r\n[6,] 0.6114980\r\npairs(as.matrix(codaSamples[[1]])[,1:4])\r\n\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nIn order to give a vague priors to slope and intercept consider the following arguments:\r\nThe largest possible value of slope is\r\n\\[ \\frac{\\sigma_y}{\\sigma_x} \\]\r\nwhen variables \\(x\\) and \\(y\\) are perfectly correlated.\r\nThen standard deviation of the slope parameter \\(\\beta_1\\) should be large enough to make the maximum value easily achievable.\r\nSize of intercept is defined by value of\r\n\\[ E[X] \\frac{\\sigma_y}{\\sigma_x} \\]\r\nSo, the prior should have enough width to include this value.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n    real meanX;\r\n    real sdX;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real expLambda;\r\n    real beta0sigma;\r\n    real beta1sigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    expLambda = 1/30.0;\r\n    beta1sigma = 10*fabs(sdY/sdX);\r\n    beta0sigma = 10*(sdY^2+sdX^2)    / 10*fabs(meanX*sdY/sdX);\r\n}\r\nparameters {\r\n    real beta0;\r\n    real beta1;\r\n    real<lower=0> nu; \r\n    real<lower=0> sigma; \r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi); \r\n    nu ~ exponential(expLambda);\r\n    beta0 ~ normal(0, beta0sigma);\r\n    beta1 ~ normal(0, beta1sigma);\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ student_t(nu, beta0 + beta1 * x[i], sigma);\r\n    }\r\n}\r\n\"\r\n\r\n\r\nstanDsoRobustReg <- stan_model(model_code=modelString) \r\n\r\n\r\ndat<-list(Ntotal=length(dta$weight), \r\n          y=dta$weight, \r\n          meanY=mean(dta$weight),\r\n          sdY=sd(dta$weight),\r\n          x=dta$height,\r\n          meanX=mean(dta$height),\r\n          sdX=sd(dta$height))\r\n\r\n\r\nfitSimRegStan <- sampling(stanDsoRobustReg, \r\n             data=dat, \r\n             pars=c('beta0', 'beta1', 'nu', 'sigma'),\r\n             iter=5000, chains = 4, cores = 4)\r\n\r\nSave the fitted object.\r\nExplore the MCMC object\r\n\r\nprint(fitSimRegStan)\r\nInference for Stan model: 99bd7b261ff9240bf0c6d7b1b21831d6.\r\n4 chains, each with iter=5000; warmup=2500; thin=1; \r\npost-warmup draws per chain=2500, total post-warmup draws=10000.\r\n\r\n          mean se_mean    sd     2.5%      25%      50%      75%\r\nbeta0  -139.35    0.50 27.89  -194.81  -157.61  -139.51  -120.74\r\nbeta1     4.45    0.01  0.42     3.64     4.17     4.45     4.72\r\nnu        5.37    0.03  1.61     3.15     4.25     5.07     6.16\r\nsigma    23.98    0.03  1.66    20.81    22.84    23.96    25.07\r\nlp__  -1265.00    0.02  1.43 -1268.55 -1265.74 -1264.68 -1263.94\r\n         97.5% n_eff Rhat\r\nbeta0   -85.12  3072    1\r\nbeta1     5.28  3117    1\r\nnu        9.39  3420    1\r\nsigma    27.31  3579    1\r\nlp__  -1263.20  3332    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Apr 22 15:10:38 2021.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(fitSimRegStan)\r\n\r\nrstan::traceplot(fitSimRegStan, ncol=1, inc_warmup=F)\r\n\r\npairs(fitSimRegStan, pars=c('nu','beta0','beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','beta1'))\r\n\r\nstan_scat(fitSimRegStan, c('beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('nu','sigma'))\r\n\r\nstan_dens(fitSimRegStan)\r\n\r\nstan_ac(fitSimRegStan, separate_chains = T)\r\n\r\nstan_diag(fitSimRegStan,information = \"sample\",chain=0)\r\n\r\nstan_diag(fitSimRegStan,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"divergence\",chain = 0)\r\n\r\n\r\nWork with shinystan object.\r\n\r\nlaunch_shinystan(fitSimRegStan)\r\n\r\nUsing fitted regression model for prediction\r\nRecall that the data in this example contains predictor height and output weight for a group of people from Ht-Wt.csv (data above).\r\nPlot all heights observed in the sample and check the summary of the variable.\r\n\r\nplot(1:length(dat$x),dat$x)\r\n\r\nsummary(dat$x)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  54.60   64.00   66.20   66.39   69.20   76.00 \r\n\r\nCan we predict weight of a person who is 50 or 80 inches tall?\r\nTo do this we can go through all pairs of simulated parameters (\\(\\beta_0\\), \\(\\beta_1\\)) and use them to simulate \\(y(50)\\) and \\(y(80)\\).\r\nThis gives distribution of predicted values.\r\n\r\nsummary(fitSimRegStan)\r\n$summary\r\n              mean     se_mean         sd         2.5%          25%\r\nbeta0  -139.354196 0.503202352 27.8915373  -194.808900  -157.605829\r\nbeta1     4.451593 0.007474065  0.4173043     3.636611     4.172879\r\nnu        5.366643 0.027556563  1.6115181     3.151177     4.246276\r\nsigma    23.982791 0.027670259  1.6554405    20.814300    22.838675\r\nlp__  -1265.002519 0.024734258  1.4276951 -1268.552672 -1265.738311\r\n               50%          75%        97.5%    n_eff     Rhat\r\nbeta0  -139.512830  -120.737878   -85.121838 3072.271 1.000770\r\nbeta1     4.454489     4.723609     5.282330 3117.396 1.000699\r\nnu        5.069790     6.157253     9.391004 3419.954 1.000994\r\nsigma    23.957637    25.072876    27.313013 3579.322 1.000550\r\nlp__  -1264.682923 -1263.938424 -1263.195323 3331.756 1.000423\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.496987 27.9128445  -195.452362  -159.028578\r\n    beta1     4.468322  0.4176474     3.643375     4.180768\r\n    nu        5.333266  1.6129391     3.146164     4.239934\r\n    sigma    23.996371  1.6756857    20.745408    22.833622\r\n    lp__  -1264.973734  1.3931068 -1268.262683 -1265.705478\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.690732  -121.306242   -85.591653\r\n    beta1     4.473340     4.745248     5.301112\r\n    nu        5.036853     6.074138     9.402810\r\n    sigma    23.966000    25.101448    27.242245\r\n    lp__  -1264.658401 -1263.936454 -1263.201543\r\n\r\n, , chains = chain:2\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.119919 27.7217798  -195.188356  -158.408294\r\n    beta1     4.462118  0.4146886     3.668582     4.177839\r\n    nu        5.396927  1.5988486     3.155521     4.277927\r\n    sigma    23.990640  1.6168350    21.056831    22.846187\r\n    lp__  -1264.946317  1.4072435 -1268.606024 -1265.657309\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.000666  -121.163812   -86.601618\r\n    beta1     4.462406     4.731267     5.288368\r\n    nu        5.129432     6.181671     9.348566\r\n    sigma    23.918413    25.056950    27.345459\r\n    lp__  -1264.644540 -1263.891813 -1263.183147\r\n\r\n, , chains = chain:3\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -137.758043 28.0473599  -193.976570  -156.709899\r\n    beta1     4.428479  0.4196264     3.620966     4.142890\r\n    nu        5.273822  1.5474425     3.130205     4.160287\r\n    sigma    23.905531  1.6471567    20.672028    22.808588\r\n    lp__  -1265.023664  1.3965268 -1268.555138 -1265.759590\r\n         stats\r\nparameter          50%         75%        97.5%\r\n    beta0  -137.963462  -118.57774   -83.786093\r\n    beta1     4.430414     4.70807     5.278970\r\n    nu        4.999793     6.07770     9.087928\r\n    sigma    23.906472    24.98220    27.219559\r\n    lp__  -1264.712841 -1263.99760 -1263.196266\r\n\r\n, , chains = chain:4\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -139.041834 27.8184750  -193.491107  -156.858089\r\n    beta1     4.447452  0.4163603     3.625069     4.186441\r\n    nu        5.462557  1.6789159     3.209953     4.299050\r\n    sigma    24.038623  1.6794892    20.805817    22.868034\r\n    lp__  -1265.066361  1.5085815 -1268.785015 -1265.858500\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -139.288564  -121.639438   -83.861266\r\n    beta1     4.449144     4.707789     5.268918\r\n    nu        5.106526     6.256394     9.576797\r\n    sigma    24.014382    25.132032    27.449061\r\n    lp__  -1264.738186 -1263.933356 -1263.202000\r\nregParam<-cbind(Beta0=rstan::extract(fitSimRegStan,pars=\"beta0\")$'beta0',\r\n                Beta1=rstan::extract(fitSimRegStan,pars=\"beta1\")$'beta1')\r\nhead(regParam)\r\n         Beta0    Beta1\r\n[1,] -174.7224 4.962250\r\n[2,] -118.6827 4.113129\r\n[3,] -152.0820 4.701561\r\n[4,] -147.0165 4.564773\r\n[5,] -159.9549 4.751269\r\n[6,] -126.6841 4.251516\r\npredX50<-apply(regParam,1,function(z) z%*%c(1,50))\r\npredX80<-apply(regParam,1,function(z) z%*%c(1,80))\r\n\r\nPlot both distributions, look at their summaries and HDIs.\r\n\r\nsuppressWarnings(library(HDInterval))\r\nden<-density(predX50)\r\nplot(density(predX80),xlim=c(60,240))\r\nlines(den$x,den$y)\r\n\r\nsummary(cbind(predX50,predX80))\r\n    predX50          predX80     \r\n Min.   : 55.42   Min.   :195.2  \r\n 1st Qu.: 78.48   1st Qu.:212.9  \r\n Median : 83.19   Median :216.8  \r\n Mean   : 83.23   Mean   :216.8  \r\n 3rd Qu.: 87.97   3rd Qu.:220.6  \r\n Max.   :111.77   Max.   :240.3  \r\nrbind(predX50=hdi(predX50),predX80=hdi(predX80))\r\n            lower     upper\r\npredX50  69.09173  97.20656\r\npredX80 205.32725 228.15029\r\n\r\nBoth JAGS and Stan produced the identical results.\r\nCompare the fit between FA and BA\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 2)\r\nabline(a=-139.96225, b= 4.46120, col = \"blue\", lwd = 1)\r\nabline(fit.outliers_removed, col = \"red\", lty=\"dashed\", lwd =1)\r\n\r\n\r\n\r\nWith influential\r\nWithout influential\r\nBayesian approach w/ robust assumption\r\n\r\norange, solid\r\nred, dashed\r\nblue, solid\r\nIntercept\r\n-104.78\r\n-155.91\r\n-139.96\r\nSlope\r\n3.98\r\n4.71\r\n4.46\r\n\r\nGreat! From the comparison, we can see that using Bayesian Methods to fit simple linear regression can be robust when the traditional regression has the influential points.\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., & Rubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition (3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th ed. ed.). Boston: McGraw-Hill Irwin.\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial with r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic Press is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/distill-preview.png",
    "last_modified": "2021-04-23T11:48:13-05:00",
    "input_file": "Simple-Linear-Regression-in-Bayes.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-17-machine learning-discriminant-analysis/",
    "title": "Discriminant Analysis -- A Classification by Maximizing Class Separation",
    "description": "An Gentle Introduction of Discriminant Analysis & Its Applicant",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nLinear Discriminant Analysis\r\nWhy Discriminant Analysis\r\nQuadratic Discriminant Analysis\r\nLDA vs QDA\r\nBuilding our first linear and quadratic discriminant models\r\nCreating the task and learner, and training the LDA model\r\nCreating the task and learner, and training the QDA model\r\n\r\nReferences\r\n\r\nLinear Discriminant Analysis\r\nApproach for multiclass classification.\r\nA discriminant is a function that takes an input vector x and assigns to one of the multiple classes.\r\nModel the distribution of the predictors \\(X\\) in each response class\r\nUse Bayes theorem to flip these around into estimates for\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nWhen the distributions are assumed to be normal, the LDA model is very similar in form to logistic regression.\r\nWhy Discriminant Analysis\r\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\r\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\r\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\r\nQuadratic Discriminant Analysis\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nLDA = \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\sum\\) in each class.\r\nQDA = With Gaussians but different \\(\\sum_k\\) in each class, we get quadratic discriminant analysis.\r\nNOTE = By proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\r\nLDA vs QDA\r\nThe strengths of the LDA and QDA algorithms are:\r\nThey can reduce a high-dimensional feature space into a much more manageable number\r\nCan be used for classification or as a preprocessing (dimension reduction) technique to other classification algorithms that may perform better on the dataset\r\nQDA can learn curved decision boundaries between classes (this isn’t the case for LDA)\r\nThe weaknesses of the LDA and QDA algorithms are:\r\nThey can only handle continuous predictors (although recoding a categorical variable as numeric may help in some cases)\r\nThey assume the data are normally distributed across the predictors. If the data are not, performance will suffer\r\nLDA can only learn linear decision boundaries between classes (this isn’t the case for QDA)\r\nLDA assumes equal covariances of the classes and performance will suffer if this isn’t the case (this isn’t the case for QDA)\r\nQDA is more flexbile than LDA, and so can be more prone to overfitting\r\nWhen we should apply LDA vs QDA\r\nLDA needs lot less parameters than QDA.\r\nLDA is a much less flexible classifier than QDA \\(\\Rightarrow\\) substantially low variance.\r\nIf LDA’s assumption of common covariance matrix is poor, then LDA has high bias.\r\nLDA better bet if training set is small so reducing variance is important.\r\nQDA better bet if training set is large so variance of classifier not a major concern.\r\nBuilding our first linear and quadratic discriminant models\r\nWe have a tibble containing 178 cases and 14 variables of measurements made on various wine bottles: data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\r\nThe analysis determined the quantities of 13 constituents (Alcohol, Malic acid, Ash, Alcalinit of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline)found in each of the three types of wines.\r\n\r\n\r\n#install.packages(\"mlr\")\r\nlibrary(mlr)\r\nlibrary(tidyverse)\r\n#install.packages(\"HDclassif\")\r\ndata(wine, package = \"HDclassif\")\r\nwineTib <- as_tibble(wine)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   class    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10\r\n   <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1     1  14.2  1.71  2.43  15.6   127  2.8   3.06 0.28   2.29  5.64\r\n 2     1  13.2  1.78  2.14  11.2   100  2.65  2.76 0.26   1.28  4.38\r\n 3     1  13.2  2.36  2.67  18.6   101  2.8   3.24 0.3    2.81  5.68\r\n 4     1  14.4  1.95  2.5   16.8   113  3.85  3.49 0.24   2.18  7.8 \r\n 5     1  13.2  2.59  2.87  21     118  2.8   2.69 0.39   1.82  4.32\r\n 6     1  14.2  1.76  2.45  15.2   112  3.27  3.39 0.34   1.97  6.75\r\n 7     1  14.4  1.87  2.45  14.6    96  2.5   2.52 0.3    1.98  5.25\r\n 8     1  14.1  2.15  2.61  17.6   121  2.6   2.51 0.31   1.25  5.05\r\n 9     1  14.8  1.64  2.17  14      97  2.8   2.98 0.290  1.98  5.2 \r\n10     1  13.9  1.35  2.27  16      98  2.98  3.15 0.22   1.85  7.22\r\n# ... with 168 more rows, and 3 more variables: V11 <dbl>, V12 <dbl>,\r\n#   V13 <int>\r\n\r\n\r\n\r\nnames(wineTib) <- c(\"Class\", \"Alco\", \"Malic\", \"Ash\", \"Alk\", \"Mag\",\r\n                    \"Phe\", \"Flav\", \"Non_flav\", \"Proan\", \"Col\", \"Hue\",\r\n                    \"OD\", \"Prol\")\r\nwineTib$Class <- as.factor(wineTib$Class)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   Class  Alco Malic   Ash   Alk   Mag   Phe  Flav Non_flav Proan\r\n   <fct> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl>    <dbl> <dbl>\r\n 1 1      14.2  1.71  2.43  15.6   127  2.8   3.06    0.28   2.29\r\n 2 1      13.2  1.78  2.14  11.2   100  2.65  2.76    0.26   1.28\r\n 3 1      13.2  2.36  2.67  18.6   101  2.8   3.24    0.3    2.81\r\n 4 1      14.4  1.95  2.5   16.8   113  3.85  3.49    0.24   2.18\r\n 5 1      13.2  2.59  2.87  21     118  2.8   2.69    0.39   1.82\r\n 6 1      14.2  1.76  2.45  15.2   112  3.27  3.39    0.34   1.97\r\n 7 1      14.4  1.87  2.45  14.6    96  2.5   2.52    0.3    1.98\r\n 8 1      14.1  2.15  2.61  17.6   121  2.6   2.51    0.31   1.25\r\n 9 1      14.8  1.64  2.17  14      97  2.8   2.98    0.290  1.98\r\n10 1      13.9  1.35  2.27  16      98  2.98  3.15    0.22   1.85\r\n# ... with 168 more rows, and 4 more variables: Col <dbl>, Hue <dbl>,\r\n#   OD <dbl>, Prol <int>\r\n\r\nWe got:\r\n- 13 continuous measurements made on 178 bottles of wine, where each measurement is the amount of a different compound/element in the wine.\r\n- Class: vineyard the bottle comes from.\r\n\r\n\r\nwineUntidy <- gather(wineTib, \"Variable\", \"Value\", -Class)\r\nggplot(wineUntidy, aes(Class, Value)) +\r\n  facet_wrap(~ Variable, scales = \"free_y\") +\r\n  geom_boxplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nBox and whisker plots of each continuous variable in the data against vineyard number. For the box and whiskers: the thick horizontal line represents the median, the box represents the interquartile range (IQR), the whiskers represent the Tukey range (1.5 times the IQR above and below the quartiles), and the dots represent data outside of the Tukey range.   \r\nCreating the task and learner, and training the LDA model\r\n\r\n\r\nwineTask <- makeClassifTask(data = wineTib, target = \"Class\")\r\nlda <- makeLearner(\"classif.lda\")\r\nldaModel <- train(lda, wineTask)\r\n\r\n\r\n\r\nExtracting discriminant function values for each case\r\n\r\n\r\nldaModelData <- getLearnerModel(ldaModel)\r\nldaPreds <- predict(ldaModelData)$x\r\nhead(ldaPreds)\r\n\r\n\r\n        LD1       LD2\r\n1 -4.700244 1.9791383\r\n2 -4.301958 1.1704129\r\n3 -3.420720 1.4291014\r\n4 -4.205754 4.0028715\r\n5 -1.509982 0.4512239\r\n6 -4.518689 3.2131376\r\n\r\nPlotting the discriminant function values against each other\r\n\r\n\r\nwineTib %>%\r\n  mutate(LD1 = ldaPreds[, 1],\r\n         LD2 = ldaPreds[, 2]) %>%\r\n  ggplot(aes(LD1, LD2, col = Class)) + \r\n    geom_point() +\r\n    stat_ellipse() +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nCreating the task and learner, and training the QDA model\r\n\r\n\r\nqda <- makeLearner(\"classif.qda\")\r\nqdaModel <- train(qda, wineTask)\r\n\r\n\r\n\r\nCross-validating the LDA and QDA models\r\n\r\n\r\nkFold <- makeResampleDesc(method = \"RepCV\", folds = 10, reps = 50,\r\nstratify = TRUE)\r\n\r\nldaCV <- resample(learner = lda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nqdaCV <- resample(learner = qda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nldaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n    0.01133544     0.98866456 \r\n\r\nqdaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n   0.008314886    0.991685114 \r\n\r\nOur LDA model correctly classified 98.8% of wine bottles, on average! There isn’t much room for improvement here, but\r\nour QDA model managed to correctly classify 99.2% of cases!\r\nCalculating confusion matrices\r\n\r\n\r\ncalculateConfusionMatrix(ldaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.999/1e+00 0.001/9e-04 0.000/0e+00 0.001      \r\n  2      0.010/1e-02 0.977/1e+00 0.014/2e-02 0.023      \r\n  3      0.000/0e+00 0.007/5e-03 0.993/1e+00 0.007      \r\n  -err.-       0.011       0.005       0.020 0.01       \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2947    3    0      3\r\n  2        34 3468   48     82\r\n  3         0   16 2384     16\r\n  -err.-   34   19   48    101\r\n\r\ncalculateConfusionMatrix(qdaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.994/0.984 0.006/0.005 0.000/0.000 0.006      \r\n  2      0.014/0.016 0.986/0.993 0.000/0.000 0.014      \r\n  3      0.000/0.000 0.004/0.003 0.996/1.000 0.004      \r\n  -err.-       0.016       0.007       0.000 0.008      \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2933   17    0     17\r\n  2        48 3502    0     48\r\n  3         0    9 2391      9\r\n  -err.-   48   26    0     74\r\n\r\nPredicting which vineyard the poisoned wine came from\r\n\r\n\r\npoisoned <- tibble(Alco = 13, Malic = 2, Ash = 2.2, Alk = 19, Mag = 100,\r\n                   Phe = 2.3, Flav = 2.5, Non_flav = 0.35, Proan = 1.7,\r\n                   Col = 4, Hue = 1.1, OD = 3, Prol = 750)\r\npredict(qdaModel, newdata = poisoned)\r\n\r\n\r\nPrediction: 1 observations\r\npredict.type: response\r\nthreshold: \r\ntime: 0.00\r\n  response\r\n1        1\r\n\r\nThe model predicts that the poisoned bottle came from vineyard 1.\r\nHere’s we ends the analytic example.\r\nReferences\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-18T10:46:57-05:00",
    "input_file": "ml-discriminant-analysis.utf8.md"
  },
  {
    "path": "posts/2021-04-14-machine-learning-&-predictive-analytics/",
    "title": "Machine Learning & Predictive Analytics",
    "description": "An Overview of Machine Learning & Predictive Analytics",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [
      "Biostatistics",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Machine Learning\r\nLearning Types\r\nSupervised\r\nUnsupervised Learning\r\nSemi-supervised Learning\r\n\r\nActive Learning\r\nReinforcement Learning\r\nTransfer Learning\r\n\r\nUniversal Workflow of ML\r\nML Terminologies\r\nData Assumptions\r\nOverfitting and Underfitting\r\nParametric & Nonparametric Models\r\nRegression Analysis\r\n\r\nUse R or Python for machine learning?\r\nMachine Learning with mlr Package in R\r\n\r\nReferences\r\n\r\nWhat is Machine Learning\r\nMeaningful data transformations from input to output data.\r\nTransformations: represent or encode the data (RGB or HSV for color pixel).\r\nLearning is automatic search for better data representations.\r\nSearch through a predefined space of possibilities using guidance from feedback signal.\r\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E”\r\n\r\n-Tom Mitchell, Machine Learning, McGraw Hill, 1997\r\n\r\n\r\nExperience E, Task T, Performance P\r\n1. Chess: T: playing chess, P: % of games won, E: playing practice games against itself.\r\n2. Driving: T: driving a vehicle, P: avgdistance before error, E: sequence of images and steering commands recoded during manual driving.\r\n3. Handwriting Recognition: T: recognizing and classifying handwritten words in images, P: % of correctly classified words, E: DB of handwritten words with given classifications.\r\n\r\nLearning Types\r\nSupervised\r\nUnsupervised\r\nSemi-supervised\r\nReinforcement\r\nTransfer\r\nActive\r\nSupervised\r\nThe majority of practical machine learning uses supervised learning.\r\nSupervised learning is where you have input variables (\\(x\\)) and an output variable (\\(y\\)) and you use an algorithm to learn the mapping function from the input to the output.\\[ y = f(x) \\]\r\nThe goal is to approximate the mapping function so well that when you have new input data \\(x\\) that you can predict the output variables \\(y\\) for that data.\r\nIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\r\nWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\r\nSupervised Learning Examples:\r\nLinear Regression\r\nLogistic Regression\r\nK-NN (k-Nearest Neighbors)\r\nSupport Vector Machines (SVMs)\r\nDecision Tress and Random Forests\r\nNeural Networks\r\nUnsupervised Learning\r\nUnsupervised learning is where you only have input data \\(x\\) and no corresponding output variables.\r\nThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\r\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.\r\nUnsupervised Learning problems can be further grouped into Clustering and Association Problems.\r\nClustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\r\nAssociation: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy \\(A\\) also tend to buy \\(B\\).\r\nUnsupervised Learning Examples:\r\nClustering\r\nK-Means\r\nHierarchical Cluster Analysis (HCA)\r\nExpectation Maximization\r\n\r\nVisualization and Dimensionality Reduction\r\nPrincipal Component Analysis (PCA)\r\nKernel PCA\r\nt-distributed Stochastic Neighbor Embedding (t-SNE)\r\n\r\nAssociation Rule\r\nApriori\r\n\r\nNeural Networks\r\nAutoencoders\r\nBoltzmann machines\r\n\r\nSemi-supervised Learning\r\nSemi-supervised learning is halfway between supervised and unsupervised learning.\r\nTraditional classification methods use labeled data to build classifiers.\r\nThe labeled training sets used as input in Supervised learning is very certain and properly defined.\r\nHowever, they are limited, expensive and takes a lot of time to generate them.\r\nOn the other hand, unlabeled data is cheap and is readily available in large volumes.\r\nHence, semi-supervised learning is learning from a combination of both labeled and unlabeled data\r\nWhere we make use of a combination of small amount of labeled data and large amount of unlabeled data to increase the accuracy of our classifiers.\r\nActive Learning\r\nActive learning (sometimes called “query learning” or “optimal experimental design” in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence.\r\nThe key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns—to be “curious,” if you will—it will perform better with less training.\r\nActive learning is a special case of semi-supervised learning.\r\nReinforcement Learning\r\nReinforcement Learning is learning what to do and how to map situations to actions.\r\nThe end result is to maximize the numerical reward signal.\r\nThe learner is not told which action to take, but instead must discover which action will yield the maximum reward\r\nTransfer Learning\r\nA machine learning technique where a model trained on one task is re-purposed on a second related task.\r\nAn optimization that allows rapid progress or improved performance when modeling the second task.\r\nUniversal Workflow of ML\r\nDefine the problem\r\nAssemble dataset\r\nChoose a metric to quantify project outcome\r\nDecide on how to calculate the metric\r\nPrepare dataset\r\nDefine standard baseline\r\nDevelop model that beats baseline\r\nIdeal model is at the border of overfit and underfit–cross the border to know where it is so overfit model\r\nRegularize model and tune hyperparameters\r\nML Terminologies\r\nDataset\r\nTraining –Learn the parameters\r\nValidation –select hyperparameters\r\nTest –test the model aka generalization error\r\n\r\nBatch –set of examples used in one iteration of model training.\r\nMini-batch –A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference.\r\nEpoch –A full training pass over the entire data set such that each example has been seen once.\r\nIteration –A single update of a model’s weights during training.\r\nData Assumptions\r\nTraining and test data are from the same probability distribution.\r\nTraining and test data are iid.\r\nOverfitting and Underfitting\r\nOverfitting –model fits very well to the training data, aka detects patterns in the noise also\r\nDetect:\r\nLow training error, high generalization error.\r\n\r\nRemedies:\r\nReduce model capacity by removing features and/or parameters.\r\nGet more training data.\r\nImprove training data quality by reducing noise.\r\n\r\n\r\nUnderfitting–model too simple to detect patterns in the data\r\nDetect\r\nHigh training error.\r\n\r\nRemedies:\r\nIncrease model capacity by adding more parameters and/or features.\r\nReduce model constraints.\r\n\r\n\r\nParametric & Nonparametric Models\r\n\\[ 𝑦= 𝑓(𝑥) \\]\r\nEstimate the unknown function 𝑓 as \\(\\hat{f}\\)\r\nParametric Models:\r\nAssume the functional form or shape of 𝑓\r\nApply methodology to train model\r\nAdvantage –simple estimation\r\nDisadvantage – \\(\\hat{f}\\)may be far from true 𝑓\r\n\r\nNonparametric Models:\r\nNo assumption on the functional form or shape of 𝑓\r\nEstimate to fit as close as possible to the data\r\nAdvantage –can accurately fit a wide range of possible shapes of 𝑓\r\nDisadvantage –need large datasets (since there is no fixed # of params to estimate)\r\n\r\nRegression Analysis\r\nOLS\r\nMSE\r\nComputational Complexity of matrix inversion\r\nComplete training set\r\n\r\nBatch Gradient Descent\r\nCost function (MSE)\r\nLearning rate hyperparameter\r\nPartial derivative\r\nComplete training set\r\n\r\nStochastic Gradient Descent\r\nMini-batch Gradient Descent\r\nLinear Regression with OLS\r\n\\[ 𝑦= \\theta^T𝑋\\]\r\nThe cost function minimization is a closed-form solution called the Normal Equation: \\[ \\hat{\\theta} = (X^T . X)^{-1} X^T.y  \\]\r\nAdvantage –equation is linear with size of training set so it can handle large training sets efficiently.\r\nDisadvantage –\r\ncomputational complexity of inverting a matrix that increases with size of training set.\r\ndifficult to do online learning with new data arriving regularly (need to recalculate estimates), i.e. no iterative parameter updates.\r\n\r\nUse R or Python for machine learning?\r\nThere is something of a rivalry between the two most commonly used data science languages: R and Python. Of course, there are no machine learning tasks which are only possible to apply in one language or the other.\r\nR:\r\nR is geared specifically for mathematical and statistical applications, i.e. R can focus purely on data, but may feel restricted if they ever need to build applications based on their models.\r\nCurrently, there are modern tools in R designed specifically to make data science tasks simple and human-readable, such as those from the tidyverse.\r\nPreviously, ML algorithms in R were scattered across multiple packages, written by different authors. But R has now followed suit, with the caret and mlr packages (which stands for machine learning in R). While quite similar in purpose and functionality to caret, mlr package provides an interface for a large number of machine learning algorithms, and allows you to perform extremely complicated machine learning tasks with very little coding.\r\nPython:\r\nFirst of all, some of the more cutting-edge deep learning approaches are easier to apply in Python (they tend to be written in Python first and implemented in R later).\r\nPython, while very good for data science, is a more general purpose programming language.\r\nProponents of python could use this as en example of why it was better suited for machine learning, as it has the well known scikit-learn package which has a plethora of machine learning algorithms built into it.\r\nGoogle Trends demonstrates the search interest relative to the highest point on the chart for the given region and over the past 5 years.\r\n\r\n trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05z1_\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0212jm\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0jgqg\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/09gbxjr\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0j3djl7\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&q=%2Fm%2F05z1_,%2Fm%2F0212jm,%2Fm%2F0jgqg,%2Fm%2F09gbxjr,%2Fm%2F0j3djl7\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Machine Learning with mlr Package in R\r\nR users got mlr package similar to Scikit-Learn from Python. The package synthesizes all the ML functions from other packages in which we can perform most of ML tasks. mlr package has several algorithms in its bouquet. These algorithms have been categorized into regression, classification, clustering, survival, multiclassification and cost sensitive classification:\r\n\r\n\r\nlibrary(mlr)\r\nlistLearners(\"classif\")[c(\"class\",\"package\")]\r\n\r\n\r\n                            class                   package\r\n1                     classif.ada                 ada,rpart\r\n2              classif.adaboostm1                     RWeka\r\n3             classif.bartMachine               bartMachine\r\n4                classif.binomial                     stats\r\n5                classif.boosting              adabag,rpart\r\n6                     classif.bst                 bst,rpart\r\n7                     classif.C50                       C50\r\n8                 classif.cforest                     party\r\n9              classif.clusterSVM        SwarmSVM,LiblineaR\r\n10                  classif.ctree                     party\r\n11               classif.cvglmnet                    glmnet\r\n12                 classif.dbnDNN                   deepnet\r\n13                  classif.dcSVM            SwarmSVM,e1071\r\n14                  classif.earth               earth,stats\r\n15                 classif.evtree                    evtree\r\n16             classif.extraTrees                extraTrees\r\n17             classif.fdausc.glm                   fda.usc\r\n18          classif.fdausc.kernel                   fda.usc\r\n19             classif.fdausc.knn                   fda.usc\r\n20              classif.fdausc.np                   fda.usc\r\n21                classif.FDboost            FDboost,mboost\r\n22            classif.featureless                       mlr\r\n23                   classif.fgam                    refund\r\n24                    classif.fnn                       FNN\r\n25               classif.gamboost                    mboost\r\n26               classif.gaterSVM                  SwarmSVM\r\n27                classif.gausspr                   kernlab\r\n28                    classif.gbm                       gbm\r\n29                  classif.geoDA               DiscriMiner\r\n30               classif.glmboost                    mboost\r\n31                 classif.glmnet                    glmnet\r\n32       classif.h2o.deeplearning                       h2o\r\n33                classif.h2o.gbm                       h2o\r\n34                classif.h2o.glm                       h2o\r\n35       classif.h2o.randomForest                       h2o\r\n36                    classif.IBk                     RWeka\r\n37                    classif.J48                     RWeka\r\n38                   classif.JRip                     RWeka\r\n39                   classif.kknn                      kknn\r\n40                    classif.knn                     class\r\n41                   classif.ksvm                   kernlab\r\n42                    classif.lda                      MASS\r\n43       classif.LiblineaRL1L2SVC                 LiblineaR\r\n44      classif.LiblineaRL1LogReg                 LiblineaR\r\n45       classif.LiblineaRL2L1SVC                 LiblineaR\r\n46      classif.LiblineaRL2LogReg                 LiblineaR\r\n47         classif.LiblineaRL2SVC                 LiblineaR\r\n48 classif.LiblineaRMultiClassSVC                 LiblineaR\r\n49                  classif.linDA               DiscriMiner\r\n50                 classif.logreg                     stats\r\n51                  classif.lssvm                   kernlab\r\n52                   classif.lvq1                     class\r\n53                    classif.mda                       mda\r\n54                    classif.mlp                     RSNNS\r\n55               classif.multinom                      nnet\r\n56             classif.naiveBayes                     e1071\r\n57              classif.neuralnet                 neuralnet\r\n58                   classif.nnet                      nnet\r\n59                classif.nnTrain                   deepnet\r\n60            classif.nodeHarvest               nodeHarvest\r\n61                   classif.OneR                     RWeka\r\n62                   classif.pamr                      pamr\r\n63                   classif.PART                     RWeka\r\n64              classif.penalized                 penalized\r\n65                    classif.plr                   stepPlr\r\n66             classif.plsdaCaret                 caret,pls\r\n67                 classif.probit                     stats\r\n68                    classif.qda                      MASS\r\n69                  classif.quaDA               DiscriMiner\r\n70           classif.randomForest              randomForest\r\n71        classif.randomForestSRC           randomForestSRC\r\n72                 classif.ranger                    ranger\r\n73                    classif.rda                      klaR\r\n74                 classif.rFerns                    rFerns\r\n75                   classif.rknn                      rknn\r\n76         classif.rotationForest            rotationForest\r\n77                  classif.rpart                     rpart\r\n78                    classif.RRF                       RRF\r\n79                  classif.rrlda                     rrlda\r\n80                 classif.saeDNN                   deepnet\r\n81                    classif.sda                       sda\r\n82              classif.sparseLDA sparseLDA,MASS,elasticnet\r\n83                    classif.svm                     e1071\r\n84                classif.xgboost                   xgboost\r\n\r\n– ANALYTICS VIDHYA\r\nThe entire structure of this package relies on this premise:\r\n\\[\\text{Create a Task.   Make a Learner.   Train Them.}\\]\r\nCreating a task means loading data in the package (e.g., makeClassifTask).\r\nMaking a learner means choosing an algorithm (makeLearner) which learns from task (or data).\r\nFinally, train them (train).\r\nReferences\r\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R (1st ed. 2013. ed.). New York, NY: Springer New York.\r\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts: The MIT Press.\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nGéron, A. l. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems (First edition. ed.). Sebastopol, California: O’Reilly Media, Inc.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-17T14:33:51-05:00",
    "input_file": "An-overview-of-ML-and-PA.utf8.md"
  },
  {
    "path": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/",
    "title": "Beta Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Beta Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nDefinition\r\nIntuitive interpretation\r\nBeta function\r\nGamma function\r\nMain facts\r\nIn actions\r\nPlots in shiny\r\nFurther reading\r\n\r\nMotivation\r\n\r\nEven though I had learned the beta distribution from UIC’s Bayesian methods course and tutored it, such as setting up it as the prior distribution in conjugate distribution context. But it was easy to forget because of its dried content and too abstract. Here I try to combine the rigid theory (UC coursework’s content) and intuitive thought. By that way, I was able to ‘permenently stamp’ the concept to my brain.\r\n\r\nThe Beta distribution is a probability distribution on/of probabilities\r\nThe beta distribution describes a family of continuous probability distributions that are nonzero only on the interval (0 1).\r\nFor example, we can use it to model the probabilities: the Click-Through Rate of the advertisement, the batting averages, the 5-year survival chance for women with breast cancer, and so on.\r\nDefinition\r\nA continuous random variable \\(X_B \\sim Beta(\\alpha, \\beta)\\) has Beta distribution if its probability density function (PDF) is\r\n\\[ \r\nf_{X_B} (x; \\alpha, \\beta) = \\frac{1}{B(α,β)} x^{\\alpha − 1} (1−x)^{\\beta − 1}, \\ \\ \\text{for} \\ 0 < x < 1.\r\n\\]\r\nwhere \\(B(\\cdot)\\) is the Beta function and shape parameters \\(\\alpha, \\beta > 0\\).\r\nIntuitive interpretation\r\n\r\nPDF\r\nProbability as a …\r\nBinomial\r\n\\(f(x) = {n \\choose x} p^x (1-p)^{n-x}\\)\r\nparameter\r\n\r\n\\(\\rightarrow\\) the function of \\(x\\)\r\n\r\nBeta\r\n\\(f(p) = \\frac{1}{B(α,β)} p^{\\alpha − 1} (1−p)^{\\beta − 1}\\)\r\nrandom variable\r\n\r\n\\(\\rightarrow\\) the function of \\(p\\)\r\n\r\nThe beta distribution intuitively comes into play when we look at it in terms of numerator—\\(x/p\\) to the power of something multiplied by \\(1-x/1-p\\) to the power of something—from the lens of the binomial distribution.\r\nThe difference between the binomial and the beta is that the above models the number of successes (\\(x\\)), while the below models the probability (\\(p\\)) of success. In other words, the probability is a parameter in binomial; In the Beta, the probability is a random variable.\r\nIn this context, the shape parameters \\(\\alpha\\) and \\(\\beta\\) or \\(\\alpha-1\\) as the number of successes and \\(\\beta-1\\) as the number of failures\r\nWe can explore the beauty of beta distribution via the the calculator for Beta distribution—Dr. Bognar at the University of Iowa built it.\r\nBeta distribution is very flexible: bell-curve (The PDF of a beta distribution is approximately normal if \\(\\alpha + \\beta\\) is large enough and \\(\\alpha\\) & \\(\\beta\\) are approximately equal), U-shaped (when \\(\\alpha\\) < 1, \\(\\beta\\) < 1) and even straight line. Here’s an graph excerpt from wikipedia.\r\nThe very flexible of Beta distributionBeta function\r\nThe beta function is\r\n\\[ \r\nB(x,y) = \\int_0^1 t^{x−1} (1−t)^{y−1} dt = \\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)},\r\n\\]\r\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function.\r\nGamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\):\r\n\\[ \r\n\\Gamma (n) = (n−1)! = 1 \\times 2 \\times 3 \\times ... \\times (n−1)\r\n\\]\r\nThe gamma function is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\r\n\\Gamma (t) = \\int_0^{\\infty} x^{t-1} e^{-x} dx\r\n\\]\r\n\r\nSimplify the Beta function with the Gamma Function \\(\\Rightarrow\\) we saw the PDF of Beta written in terms of the Gamma function. The Beta function is the ratio of the product of the Gamma function of each parameter divided by the Gamma function of the sum of the parameters (proof refered the further reading topic).\r\n\r\nMain facts\r\n\\[\r\nE[X_B] = \\mu = \\frac{\\alpha}{\\alpha + \\beta}; \\ \\ V[X_B] = \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\r\n\\]\r\nThe standard uniform distribution \\(\\text{Unif} \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nThe mode is \\(\\omega = \\frac{\\alpha − 1}{\\alpha + \\beta − 2}\\) for \\(\\alpha, \\beta > 1\\).\r\nThe concentration is \\(\\kappa = \\alpha + \\beta\\).\r\nDefinitions of \\(\\mu, \\omega\\) and \\(\\kappa\\) can be inverted:\r\n\\[ \\alpha = \\mu\\kappa,  \\beta = (1 − \\mu)\\kappa \\]\r\n\\[ \\alpha = \\omega(\\kappa−2)+1,  \\beta = (1 − \\omega)(\\kappa−2)+1, \\ \\kappa > 2. \\]\r\nParameter \\(\\kappa\\) is a measure of number of observations needed to change our previous belief about \\(\\mu\\).\r\nIf \\(\\kappa\\) is small we need only a few new observations.\r\nExample. Concentration \\(\\kappa = 8\\) around \\(\\mu = 0.5\\) corresponds to \\(\\alpha = \\mu \\kappa = 4\\) and \\(\\beta = (1 − \\mu) \\kappa = 4\\).\r\nParameterization in terms of mean value and standard deviation is:\r\n\\[ \\alpha = \\mu [\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1]; \\ \\ \\beta = (1 - \\mu)[\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1] \\]\r\nStandard deviation is typically smaller than standard deviation of uniform distribution on \\([0,1]\\), i.e. \\(0.28867\\).\r\nExamples.\r\nFor \\(\\mu = 0.5\\), \\(\\sigma = 0.28867\\) the shape parameters are \\(\\alpha = 1\\), \\(\\beta = 1\\).\r\nFind shape parameters of beta distribution with \\(\\mu = 0.5\\), \\(\\sigma = 0.1\\).\r\nThe standard uniform distribution \\(Unif \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nIn actions\r\nKeep parameter \\(\\beta\\) fixed. Move \\(\\alpha\\) up or down. Observe how the mass of the distribution moves\r\n\r\n\r\np <- seq(0,1,by=0.2)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=2), aes(colour = \"alpha=1,beta=2\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=2), aes(colour = \"alpha=4,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=2), aes(colour = \"alpha=6,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=8, shape2=2), aes(colour = \"alpha=8,beta=2\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDo the same as above, but keep \\(\\alpha\\) constant and move \\(\\beta\\) up or down\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=1), aes(colour = \"alpha=2,beta=1\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=5), aes(colour = \"alpha=2,beta=5\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=6), aes(colour = \"alpha=2,beta=6\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=8), aes(colour = \"alpha=2,beta=8\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nMake \\(\\alpha = \\beta = 1\\). What does the shape of the distribution tell you about your knowledge about random variable \\(\\theta\\)?\\(\\Rightarrow\\) The standard uniform distribution \\(Unif(0,1)\\) is a special case of the beta distribution \\(Beta (1,1)\\), when \\(\\alpha\\)=\\(\\beta\\)=1.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"green\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nKeep \\(\\alpha = \\beta\\) , but move both of them up or down. Interpret the shape of the distribution\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=0.5, shape2=0.5), aes(colour = \"alpha=0.5,beta=0.5\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=4), aes(colour = \"alpha=4,beta=4\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=6), aes(colour = \"alpha=6,beta=6\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nVariance changes based on 2 shape parameters.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=400, shape2=80), aes(colour = \"alpha=400,beta=80\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=40, shape2=8), aes(colour = \"alpha=40,beta=8\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=30, shape2=70), aes(colour = \"alpha=30,beta=70\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=7), aes(colour = \"alpha=3,beta=7\")) +\r\n  scale_y_continuous(limits=c(0,25)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\", \"green\", \"orange\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWhen beta distribution is used as a prior for parameter of binomial distribution, parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as previously observed numbers of successes (\\(\\alpha\\)) or failures (\\(\\beta\\)). For example, if in 2 Bernoulli experiments there was 1 success and 1 failure you can express opinion about probability of success as \\(Beta(1,1)\\). What would you assume as prior if in 6 previously observed outcomes there were 3 successes and 3 failures? What is the likely value of the parameter? Do we have more or less information than in case of 1 success and 1 failure? \\(\\Rightarrow\\) Think of more\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=3), aes(colour = \"alpha=3,beta=3\")) +\r\n  stat_function(fun=dbinom, args=list(size=1, prob=0.5), aes(colour = \"Bernoulli w/ prob=0.5\")) + # bernoulli\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"red\",\"green\",\"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDefine distribution with mode \\(\\omega\\)=.8 and concentration \\(\\kappa = 12\\). To do that find shape parameters \\(\\alpha = \\omega (\\kappa − 2) + 1 = 9\\) and \\(\\beta = (1 − \\omega)(\\kappa − 2) + 1 = 3\\).\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=9, shape2=3), aes(colour = \"alpha=9,beta=3\")) +\r\n  scale_y_continuous(limits=c(0,3.4)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the actions we notify that:\r\nThe special case \\(a=b=1\\) is the uniform distribution.\r\nThe distribution is roughly centered on \\(a/(a+b)\\). Actually, it turns out that the mean is exactly \\(a/(a+b)\\). Thus the mean of the distribution is determined by the relative values of \\(a\\) and \\(b\\).\r\nThe larger the values of \\(a\\) and \\(b\\), the smaller the variance of the distribution about the mean.\r\nFor moderately large values of \\(a\\) and \\(b\\) the distribution looks visually “kind of normal”, although unlike the normal distribution the Beta distribution is restricted to [0,1].\r\nPlots in shiny\r\nPlanning to build an shiny app to plot beta distribution on the specification of shape parameter (“still being in the process”).\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nDavid Robinson (Principal Data Scientist at Heap, works in R and Python), Understanding the beta distribution (using baseball statistics), http://varianceexplained.org/statistics/beta_distribution_and_baseball/\r\nAerin Kim, Beta Distribution — Intuition, Examples, and Derivation, https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/Beta-Distribution-in-an-Intuitive-Explanation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-15T00:29:44-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/",
    "title": "Gamma Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Gamma Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "ggplot2",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nFact\r\nGamma function\r\nDefinition\r\nParameters of Gamma: a shape with a scale or a rate\r\nPlots\r\nFurther reading\r\n\r\nFact\r\nWhy did we invent Gamma distribution? Answer: To predict the wait time until future events. Hmmm ok, but I thought that’s what the exponential distribution is for. Then, what’s the difference between exponential distribution and gamma distribution? The exponential distribution predicts the wait time until the very first event. The gamma distribution, on the other hand, predicts the wait time until the k-th event occurs.\r\n– Aerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nExamples:\r\nThe time I wait to receive an interview might follow an exponential. Now, I am waiting not for my first interview offer but my third interview offer. How long must I wait? This waiting time can be described by a gamma.\r\nI missed the first and second CTA train to go to the campus. Now, how long I am able to catch the third train?\r\nGamma function\r\nIn the lecture series of Statistics 110, Lecture 24: Gamma distribution and Poisson process | Statistics 110, Prof. Joe Blitzstein had connected the \\(n!\\) function to the Gamma function. Why?\r\nLet’s see the Gamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\): \\[\\Gamma(n) = (n-1)! = 1 \\times 2 \\times 3 \\times ... \\times (n-1)\\]\r\nOne beautiful formula, Stirling formula to approximate the \\(n!\\), actually an extremely good approximation: \\[ n! \\approx \\sqrt{2\\pi n} \\Big( \\frac{n}{e}\\Big)^n\\]\r\n\r\n\r\nn <- c(1:6)\r\ny <- vector(mode = \"numeric\", length = length(n))\r\ny[1] <- 1\r\nfor(i in 2:length(n)) {\r\n  y[i] = y[i-1] * i\r\n}\r\ndta <- as.data.frame(cbind(n,y))\r\nlibrary(ggplot2)\r\nggplot(dta, aes(n, y)) + \r\n  geom_point() +\r\n  scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\")) +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nThen how we connect the dots. There are many ways to do it, but there’s a philosophical way to do it by Gamma function, which is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\\Gamma(t) = \\int_0^{\\infty} x^t e^{−x} \\frac{dx}{x} \\]\r\nDefinition\r\nFrom the Gamma function, how we got the PDF of Gamma distribution. We would normalize the Gamma distribution, which means from:\r\n\\[ \\Gamma(k) = \\int_0^{\\infty} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nto,\r\n\\[ 1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nThen, \\(X = \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{1}{x}\\) \\(\\sim\\) \\(Gamma(k, 1)\\) which has shape of \\(k\\) and scale of \\(1\\).\r\nHow we turn the scale of \\(1\\) to a general scale of \\(\\theta\\)?\r\nImagine that \\(Y \\sim \\frac{X}{\\theta}\\) where \\(X \\sim \\ Gamma(k,1)\\)\r\n\\(f_Y(y) = f_X(x) \\frac{dx}{dy} = \\frac{1}{\\Gamma(k)} (\\theta y)^{k} e^{−\\theta y} \\frac{1}{\\theta y} \\theta\\) where \\(\\frac{dx}{dy} = \\theta\\)\r\nThus, \\(f(y) = \\frac{1}{\\Gamma(k) \\theta^{k}} (y)^{k} e^{−\\theta y} \\frac{1}{y}\\)\r\nParameters of Gamma: a shape with a scale or a rate\r\n\r\n\r\nknitr::include_graphics(\"Gamma_scalevsrate_inwiki.png\") \r\n\r\n\r\n\r\n\r\n(#fig:model diagram)From https://en.wikipedia.org/wiki/Gamma_distribution\r\n\r\n\r\n\r\nFor (\\(\\alpha\\), \\(\\beta\\)) parameterization: Using our notation \\(k\\) (the # of events) & \\(\\lambda\\) (the rate of events), simply substitute \\(\\alpha\\) with \\(k\\), \\(\\beta\\) with \\(\\lambda\\). The PDF stays the same format as what we’ve derived.\r\nFor (\\(k\\), \\(\\theta\\)) parameterization: \\(\\theta\\) is a reciprocal of the event rate \\(\\lambda\\), which is the mean wait time (the average time between event arrivals).\r\nPlots\r\nI plotted the gamma distribution with the shape of \\(k\\), and constantly rate = \\(1\\)\r\n\r\n\r\nT <- seq(0,20,by=2.5)\r\n\r\ndf <- data.frame(T)\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=1, rate=1), aes(colour = \"k= 1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=5, rate=1), aes(colour = \"k= 5\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"k=10\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"yellowgreen\", \"olivedrab\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nI plotted the gamma distribution with the constantly shape of k = 10, and variant rate from 1 to 3.\r\n\r\n\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"r=1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=10, rate=2), aes(colour = \"r=2\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=3), aes(colour = \"r=3\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"gold\", \"burlywood\", \"darkorange\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution (k=10)\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFurther reading\r\nLecture 24: Gamma distribution and Poisson process | Statistics 110\r\nAerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nWiki\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/Gamma-distribution-in-an-intuitive-explanation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-23T11:53:11-05:00",
    "input_file": "Gamma-distribution-in-an-intuitive-explanation.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
