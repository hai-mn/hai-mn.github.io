[
  {
    "path": "posts/2022-09-30-FA in Multivariate Regression/",
    "title": "Factor Analysis applied in Multivariate Regression: Machine Learning's Perspective",
    "description": "FA vs. PCA;   \nConducting an Exploratory Factor Analysis Example;  \nFactor Score;  \nFA applied in Regression;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-09-30",
    "categories": [
      "Biostatistics",
      "Machine Learning",
      "PCA",
      "EFA",
      "Regression"
    ],
    "contents": "\r\n\r\nContents\r\nQuestions\r\nEFA vs. PCA\r\nAn Example: Data\r\nExploration\r\nMultiple Linear Regression\r\nVariable Inflation Factor\r\n(VIF)\r\nFactor\r\nAnalysis\r\nFind the optimal clusters\r\nConducting the Factor\r\nAnalysis\r\nGraph Factor Loading\r\nMatrices\r\nLabeling and\r\ninterpretation of the factors\r\nScores for all the rows\r\n\r\nRegression\r\nanalysis using the factors scores as the independent variable\r\nSplitting the data to\r\ntrain and test set\r\nRegression Model using train\r\ndata\r\nCheck vif\r\nCheck\r\nprediction of the model in the test dataset\r\nTry the\r\ninteraction?!\r\n\r\nConclusion\r\nFurther\r\nReading\r\n\r\nQuestions\r\n1- What is difference between EFA and PCA?\r\n2- Is it possible that we can run the regression analysis on factors\r\ngenerating from Factor Analysis?\r\n3- If so, how can we run it?\r\nThe demonstration is to use the dataset Factor-Hair-Revised.csv\r\nto build a regression model to predict satisfaction.\r\nEFA vs. PCA\r\n\r\nFA\r\nPCA\r\nMeaning\r\nA factor (or latent) is a common or\r\nunderlying element with which several other variables are\r\ncorrelated\\(X_1 = L_1\\times F +\r\n\\epsilon_1\\)\\(X_2 = L_2\\times F +\r\n\\epsilon_2\\)\\(X_3 = L_3\\times F +\r\n\\epsilon_3\\) where, F is the factor, W is are the loading\r\nA component is a derived new dimension (or\r\nvariable) so that the derived variables are linearly independent of each\r\nother\\(Y = W_1\\times PC_1 + W_2\\times PC_2\r\n+ ...+ W_{10}\\times PC_{10} + C\\) where, PCs are the components\r\n& W is are the weights\r\nProcess\r\nThe original variables are defined as the\r\nlinear combinations of the factors\r\nThe components are calculated as the\r\nlinear combinations of the original variables\r\nObjective\r\nFA focuses on explaining the covariances\r\nor the correlations between the variables\r\nThe aim of PCA is to explain as much of\r\nthe cumulative variance in the predictors (or variables) as\r\npossible\r\nPurpose\r\nFactor Analysis is used to understand the\r\nunderlying ‘cause’ which these factors (latent or constituents) capture\r\nmuch of the information of a set of variables in the dataset data\r\nPCA is used to decompose the data into a\r\nsmaller number of components and therefore is a type of Singular Value\r\nDecomposition (SVD)\r\nInterpretation of coefficients\r\nThe coefficients (loadings) in the factor\r\nanalysis express the relationship or association of each variable (X) to\r\nthe underlying factor (F)\r\nThe coefficients indicate which component\r\ncontributes more to the target variable, Y, as the independent variables\r\nare standardized\r\nAn Example: Data Exploration\r\n\r\nprodsurvey <- read_csv(\"Factor-Hair-Revised.csv\")\r\nhead(prodsurvey)\r\n# A tibble: 6 × 13\r\n     ID ProdQual  Ecom TechSup CompRes Adver…¹ ProdL…² Sales…³ ComPr…⁴\r\n  <dbl>    <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\r\n1     1      8.5   3.9     2.5     5.9     4.8     4.9     6       6.8\r\n2     2      8.2   2.7     5.1     7.2     3.4     7.9     3.1     5.3\r\n3     3      9.2   3.4     5.6     5.6     5.4     7.4     5.8     4.5\r\n4     4      6.4   3.3     7       3.7     4.7     4.7     4.5     8.8\r\n5     5      9     3.4     5.2     4.6     2.2     6       4.5     6.8\r\n6     6      6.5   2.8     3.1     4.1     4       4.3     3.7     8.5\r\n# … with 4 more variables: WartyClaim <dbl>, OrdBilling <dbl>,\r\n#   DelSpeed <dbl>, Satisfaction <dbl>, and abbreviated variable\r\n#   names ¹​Advertising, ²​ProdLine, ³​SalesFImage, ⁴​ComPricing\r\ndim(prodsurvey)\r\n[1] 100  13\r\nstr(prodsurvey)\r\nspc_tbl_ [100 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ ID          : num [1:100] 1 2 3 4 5 6 7 8 9 10 ...\r\n $ ProdQual    : num [1:100] 8.5 8.2 9.2 6.4 9 6.5 6.9 6.2 5.8 6.4 ...\r\n $ Ecom        : num [1:100] 3.9 2.7 3.4 3.3 3.4 2.8 3.7 3.3 3.6 4.5 ...\r\n $ TechSup     : num [1:100] 2.5 5.1 5.6 7 5.2 3.1 5 3.9 5.1 5.1 ...\r\n $ CompRes     : num [1:100] 5.9 7.2 5.6 3.7 4.6 4.1 2.6 4.8 6.7 6.1 ...\r\n $ Advertising : num [1:100] 4.8 3.4 5.4 4.7 2.2 4 2.1 4.6 3.7 4.7 ...\r\n $ ProdLine    : num [1:100] 4.9 7.9 7.4 4.7 6 4.3 2.3 3.6 5.9 5.7 ...\r\n $ SalesFImage : num [1:100] 6 3.1 5.8 4.5 4.5 3.7 5.4 5.1 5.8 5.7 ...\r\n $ ComPricing  : num [1:100] 6.8 5.3 4.5 8.8 6.8 8.5 8.9 6.9 9.3 8.4 ...\r\n $ WartyClaim  : num [1:100] 4.7 5.5 6.2 7 6.1 5.1 4.8 5.4 5.9 5.4 ...\r\n $ OrdBilling  : num [1:100] 5 3.9 5.4 4.3 4.5 3.6 2.1 4.3 4.4 4.1 ...\r\n $ DelSpeed    : num [1:100] 3.7 4.9 4.5 3 3.5 3.3 2 3.7 4.6 4.4 ...\r\n $ Satisfaction: num [1:100] 8.2 5.7 8.9 4.8 7.1 4.7 5.7 6.3 7 5.5 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   ID = col_double(),\r\n  ..   ProdQual = col_double(),\r\n  ..   Ecom = col_double(),\r\n  ..   TechSup = col_double(),\r\n  ..   CompRes = col_double(),\r\n  ..   Advertising = col_double(),\r\n  ..   ProdLine = col_double(),\r\n  ..   SalesFImage = col_double(),\r\n  ..   ComPricing = col_double(),\r\n  ..   WartyClaim = col_double(),\r\n  ..   OrdBilling = col_double(),\r\n  ..   DelSpeed = col_double(),\r\n  ..   Satisfaction = col_double()\r\n  .. )\r\n - attr(*, \"problems\")=<externalptr> \r\nnames(prodsurvey)\r\n [1] \"ID\"           \"ProdQual\"     \"Ecom\"         \"TechSup\"     \r\n [5] \"CompRes\"      \"Advertising\"  \"ProdLine\"     \"SalesFImage\" \r\n [9] \"ComPricing\"   \"WartyClaim\"   \"OrdBilling\"   \"DelSpeed\"    \r\n[13] \"Satisfaction\"\r\ndescribe(prodsurvey)\r\n             vars   n  mean    sd median trimmed   mad min   max\r\nID              1 100 50.50 29.01  50.50   50.50 37.06 1.0 100.0\r\nProdQual        2 100  7.81  1.40   8.00    7.85  1.78 5.0  10.0\r\nEcom            3 100  3.67  0.70   3.60    3.63  0.52 2.2   5.7\r\nTechSup         4 100  5.36  1.53   5.40    5.40  1.85 1.3   8.5\r\nCompRes         5 100  5.44  1.21   5.45    5.46  1.26 2.6   7.8\r\nAdvertising     6 100  4.01  1.13   4.00    4.00  1.19 1.9   6.5\r\nProdLine        7 100  5.80  1.32   5.75    5.81  1.56 2.3   8.4\r\nSalesFImage     8 100  5.12  1.07   4.90    5.09  0.89 2.9   8.2\r\nComPricing      9 100  6.97  1.55   7.10    7.02  1.93 3.7   9.9\r\nWartyClaim     10 100  6.04  0.82   6.10    6.04  0.89 4.1   8.1\r\nOrdBilling     11 100  4.28  0.93   4.40    4.31  0.74 2.0   6.7\r\nDelSpeed       12 100  3.89  0.73   3.90    3.92  0.74 1.6   5.5\r\nSatisfaction   13 100  6.92  1.19   7.05    6.90  1.33 4.7   9.9\r\n             range  skew kurtosis   se\r\nID            99.0  0.00    -1.24 2.90\r\nProdQual       5.0 -0.24    -1.17 0.14\r\nEcom           3.5  0.64     0.57 0.07\r\nTechSup        7.2 -0.20    -0.63 0.15\r\nCompRes        5.2 -0.13    -0.66 0.12\r\nAdvertising    4.6  0.04    -0.94 0.11\r\nProdLine       6.1 -0.09    -0.60 0.13\r\nSalesFImage    5.3  0.37     0.26 0.11\r\nComPricing     6.2 -0.23    -0.96 0.15\r\nWartyClaim     4.0  0.01    -0.53 0.08\r\nOrdBilling     4.7 -0.32     0.11 0.09\r\nDelSpeed       3.9 -0.45     0.09 0.07\r\nSatisfaction   5.2  0.08    -0.86 0.12\r\n\r\n\r\ndatamatrix1 <- cor(prodsurvey[,-1])\r\ncorrplot(datamatrix1, method=\"number\")\r\n\r\n\r\nAs we can see from the above correlation matrix:\r\nComplaint resolution (CompRes) and delivery speed\r\n(DelSpeed) are highly correlated\r\nOrder billing (OrdBilling) and CompRes are\r\nhighly correlated\r\nWartyClaim and TechSupport are highly\r\ncorrelated\r\nOrdBilling and DelSpeed are highly\r\ncorrelated\r\ne-commerce (Ecom) and sales force image\r\n(SalesFImage) are highly correlated\r\n\r\npcor(prodsurvey[,-1], method=\"pearson\")\r\n$estimate\r\n                 ProdQual          Ecom      TechSup      CompRes\r\nProdQual      1.000000000  0.1549597037  0.002424222 -0.056354142\r\nEcom          0.154959704  1.0000000000  0.082359000 -0.033513777\r\nTechSup       0.002424222  0.0823590001  1.000000000  0.143603415\r\nCompRes      -0.056354142 -0.0335137768  0.143603415  1.000000000\r\nAdvertising   0.112376746 -0.0002972504 -0.059300254 -0.064806093\r\nProdLine      0.281144724  0.1538660545 -0.125349050  0.020305650\r\nSalesFImage  -0.376228551  0.7321011890 -0.093310796 -0.022150933\r\nComPricing   -0.014021386  0.0149131857 -0.132972485 -0.004487151\r\nWartyClaim   -0.042752416 -0.1232553822  0.787729606 -0.109686211\r\nOrdBilling    0.054523215  0.1546990521 -0.165914724  0.288344382\r\nDelSpeed     -0.335084210 -0.0083917930 -0.021914916  0.528932328\r\nSatisfaction  0.607438787 -0.3308440834  0.055111867  0.172416347\r\n               Advertising    ProdLine  SalesFImage   ComPricing\r\nProdQual      0.1123767461  0.28114472 -0.376228551 -0.014021386\r\nEcom         -0.0002972504  0.15386605  0.732101189  0.014913186\r\nTechSup      -0.0593002540 -0.12534905 -0.093310796 -0.132972485\r\nCompRes      -0.0648060931  0.02030565 -0.022150933 -0.004487151\r\nAdvertising   1.0000000000 -0.13192319  0.262879343 -0.063298038\r\nProdLine     -0.1319231866  1.00000000 -0.230170570 -0.361757904\r\nSalesFImage   0.2628793429 -0.23017057  1.000000000  0.126612489\r\nComPricing   -0.0632980381 -0.36175790  0.126612489  1.000000000\r\nWartyClaim    0.0275909587  0.25718360  0.189300271  0.020351554\r\nOrdBilling   -0.0326580430 -0.28098068 -0.182359301 -0.086500869\r\nDelSpeed      0.2046544064  0.50189505  0.005889925  0.190437993\r\nSatisfaction -0.0449735411  0.18325156  0.660251850 -0.087467711\r\n              WartyClaim  OrdBilling     DelSpeed Satisfaction\r\nProdQual     -0.04275242  0.05452322 -0.335084210   0.60743879\r\nEcom         -0.12325538  0.15469905 -0.008391793  -0.33084408\r\nTechSup       0.78772961 -0.16591472 -0.021914916   0.05511187\r\nCompRes      -0.10968621  0.28834438  0.528932328   0.17241635\r\nAdvertising   0.02759096 -0.03265804  0.204654406  -0.04497354\r\nProdLine      0.25718360 -0.28098068  0.501895051   0.18325156\r\nSalesFImage   0.18930027 -0.18235930  0.005889925   0.66025185\r\nComPricing    0.02035155 -0.08650087  0.190437993  -0.08746771\r\nWartyClaim    1.00000000  0.25984451 -0.091649002  -0.08868071\r\nOrdBilling    0.25984451  1.00000000  0.350530212   0.14880055\r\nDelSpeed     -0.09164900  0.35053021  1.000000000   0.08955614\r\nSatisfaction -0.08868071  0.14880055  0.089556143   1.00000000\r\n\r\n$p.value\r\n                 ProdQual         Ecom      TechSup      CompRes\r\nProdQual     0.000000e+00 1.447435e-01 9.819081e-01 5.977987e-01\r\nEcom         1.447435e-01 0.000000e+00 4.402823e-01 7.538375e-01\r\nTechSup      9.819081e-01 4.402823e-01 0.000000e+00 1.769171e-01\r\nCompRes      5.977987e-01 7.538375e-01 1.769171e-01 0.000000e+00\r\nAdvertising  2.916350e-01 9.977814e-01 5.787601e-01 5.439511e-01\r\nProdLine     7.269030e-03 1.476353e-01 2.391187e-01 8.493380e-01\r\nSalesFImage  2.576212e-04 2.442012e-16 3.817042e-01 8.358301e-01\r\nComPricing   8.956443e-01 8.890480e-01 2.115149e-01 9.665194e-01\r\nWartyClaim   6.890839e-01 2.471182e-01 3.262868e-20 3.034147e-01\r\nOrdBilling   6.097697e-01 1.454288e-01 1.180864e-01 5.850710e-03\r\nDelSpeed     1.245192e-03 9.374303e-01 8.375553e-01 8.359069e-08\r\nSatisfaction 2.182238e-10 1.447603e-03 6.059096e-01 1.041593e-01\r\n             Advertising     ProdLine  SalesFImage   ComPricing\r\nProdQual      0.29163501 7.269030e-03 2.576212e-04 0.8956443272\r\nEcom          0.99778145 1.476353e-01 2.442012e-16 0.8890479591\r\nTechSup       0.57876015 2.391187e-01 3.817042e-01 0.2115148546\r\nCompRes       0.54395113 8.493380e-01 8.358301e-01 0.9665194173\r\nAdvertising   0.00000000 2.151737e-01 1.230672e-02 0.5533828069\r\nProdLine      0.21517368 0.000000e+00 2.907563e-02 0.0004593443\r\nSalesFImage   0.01230672 2.907563e-02 0.000000e+00 0.2343791374\r\nComPricing    0.55338281 4.593443e-04 2.343791e-01 0.0000000000\r\nWartyClaim    0.79629803 1.440249e-02 7.394541e-02 0.8490014635\r\nOrdBilling    0.75993047 7.304606e-03 8.538047e-02 0.4175555780\r\nDelSpeed      0.05300070 4.664278e-07 9.560619e-01 0.0721942454\r\nSatisfaction  0.67382405 8.383625e-02 1.449719e-12 0.4123500152\r\n               WartyClaim   OrdBilling     DelSpeed Satisfaction\r\nProdQual     6.890839e-01 0.6097697424 1.245192e-03 2.182238e-10\r\nEcom         2.471182e-01 0.1454287628 9.374303e-01 1.447603e-03\r\nTechSup      3.262868e-20 0.1180864174 8.375553e-01 6.059096e-01\r\nCompRes      3.034147e-01 0.0058507099 8.359069e-08 1.041593e-01\r\nAdvertising  7.962980e-01 0.7599304715 5.300070e-02 6.738241e-01\r\nProdLine     1.440249e-02 0.0073046065 4.664278e-07 8.383625e-02\r\nSalesFImage  7.394541e-02 0.0853804743 9.560619e-01 1.449719e-12\r\nComPricing   8.490015e-01 0.4175555780 7.219425e-02 4.123500e-01\r\nWartyClaim   0.000000e+00 0.0133877361 3.902770e-01 4.058729e-01\r\nOrdBilling   1.338774e-02 0.0000000000 7.064114e-04 1.615975e-01\r\nDelSpeed     3.902770e-01 0.0007064114 0.000000e+00 4.012356e-01\r\nSatisfaction 4.058729e-01 0.1615974575 4.012356e-01 0.000000e+00\r\n\r\n$statistic\r\n                ProdQual         Ecom     TechSup     CompRes\r\nProdQual      0.00000000  1.471424516  0.02274128 -0.52949015\r\nEcom          1.47142452  0.000000000  0.77522957 -0.31456380\r\nTechSup       0.02274128  0.775229571  0.00000000  1.36122814\r\nCompRes      -0.52949015 -0.314563798  1.36122814  0.00000000\r\nAdvertising   1.06090746 -0.002788456 -0.55726637 -0.60921569\r\nProdLine      2.74821968  1.460787002 -1.18522655  0.19052316\r\nSalesFImage  -3.80921125 10.081854496 -0.87916864 -0.20784517\r\nComPricing   -0.13154519  0.139913642 -1.25856891 -0.04209363\r\nWartyClaim   -0.40142023 -1.165122048 11.99562484 -1.03519395\r\nOrdBilling    0.51223505  1.468888754 -1.57829305  2.82489237\r\nDelSpeed     -3.33624278 -0.078724768 -0.20562952  5.84663073\r\nSatisfaction  7.17336519 -3.288799958  0.51778207  1.64199901\r\n              Advertising   ProdLine SalesFImage  ComPricing\r\nProdQual      1.060907458  2.7482197 -3.80921125 -0.13154519\r\nEcom         -0.002788456  1.4607870 10.08185450  0.13991364\r\nTechSup      -0.557266374 -1.1852266 -0.87916864 -1.25856891\r\nCompRes      -0.609215688  0.1905232 -0.20784517 -0.04209363\r\nAdvertising   0.000000000 -1.2484608  2.55592188 -0.59498137\r\nProdLine     -1.248460807  0.0000000 -2.21876450 -3.64012829\r\nSalesFImage   2.555921881 -2.2187645  0.00000000  1.19736653\r\nComPricing   -0.594981366 -3.6401283  1.19736653  0.00000000\r\nWartyClaim    0.258924708  2.4965744  1.80849286  0.19095404\r\nOrdBilling   -0.306523103 -2.7464786 -1.73985585 -0.81450302\r\nDelSpeed      1.961341689  5.4434474  0.05525335  1.81976992\r\nSatisfaction -0.422316520  1.7486638  8.24679932 -0.82367672\r\n             WartyClaim OrdBilling    DelSpeed Satisfaction\r\nProdQual     -0.4014202  0.5122350 -3.33624278    7.1733652\r\nEcom         -1.1651220  1.4688888 -0.07872477   -3.2888000\r\nTechSup      11.9956248 -1.5782930 -0.20562952    0.5177821\r\nCompRes      -1.0351939  2.8248924  5.84663073    1.6419990\r\nAdvertising   0.2589247 -0.3065231  1.96134169   -0.4223165\r\nProdLine      2.4965744 -2.7464786  5.44344736    1.7486638\r\nSalesFImage   1.8084929 -1.7398559  0.05525335    8.2467993\r\nComPricing    0.1909540 -0.8145030  1.81976992   -0.8236767\r\nWartyClaim    0.0000000  2.5242649 -0.86337748   -0.8351894\r\nOrdBilling    2.5242649  0.0000000  3.51103503    1.4115877\r\nDelSpeed     -0.8633775  3.5110350  0.00000000    0.8435005\r\nSatisfaction -0.8351894  1.4115877  0.84350046    0.0000000\r\n\r\n$n\r\n[1] 100\r\n\r\n$gp\r\n[1] 10\r\n\r\n$method\r\n[1] \"pearson\"\r\n\r\nWe can see that there is a high degree of collinearity between the\r\nindependent variables.\r\nMultiple Linear Regression\r\n\r\nmodel1 <- lm(Satisfaction ~ ., prodsurvey[,-1])\r\nsummary(model1)\r\n\r\nCall:\r\nlm(formula = Satisfaction ~ ., data = prodsurvey[, -1])\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.43005 -0.31165  0.07621  0.37190  0.90120 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -0.66961    0.81233  -0.824  0.41199    \r\nProdQual     0.37137    0.05177   7.173 2.18e-10 ***\r\nEcom        -0.44056    0.13396  -3.289  0.00145 ** \r\nTechSup      0.03299    0.06372   0.518  0.60591    \r\nCompRes      0.16703    0.10173   1.642  0.10416    \r\nAdvertising -0.02602    0.06161  -0.422  0.67382    \r\nProdLine     0.14034    0.08025   1.749  0.08384 .  \r\nSalesFImage  0.80611    0.09775   8.247 1.45e-12 ***\r\nComPricing  -0.03853    0.04677  -0.824  0.41235    \r\nWartyClaim  -0.10298    0.12330  -0.835  0.40587    \r\nOrdBilling   0.14635    0.10367   1.412  0.16160    \r\nDelSpeed     0.16570    0.19644   0.844  0.40124    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.5623 on 88 degrees of freedom\r\nMultiple R-squared:  0.8021,    Adjusted R-squared:  0.7774 \r\nF-statistic: 32.43 on 11 and 88 DF,  p-value: < 2.2e-16\r\n\r\nAs in our model the adjusted R-squared: 0.7774, meaning that\r\nindependent variables explain 78% of the variance of the dependent\r\nvariable, only 3 variables are significant out of 11 independent\r\nvariables.\r\nThe p-value of the F-statistic is less than 0.05 (level of\r\nSignificance), which means our model is significant. This means that, at\r\nleast, one of the predictor variables is significantly related to the\r\noutcome variable.\r\nOur model equation can be written as:\r\nSatisfaction = -0.66 + 0.37 x ProdQual -0.44 x\r\nEcom + 0.034 x TechSup + 0.16 x\r\nCompRes -0.02 x Advertising + 0.14 x\r\nProdLine + 0.80 x SalesFImage - 0.038 x\r\nCompPricing - 0.10 x WartyClaim + 0.14 x\r\nOrdBilling + 0.16 x DelSpeed\r\nVariable Inflation Factor\r\n(VIF)\r\n\r\nvif(model1)\r\n   ProdQual        Ecom     TechSup     CompRes Advertising \r\n   1.635797    2.756694    2.976796    4.730448    1.508933 \r\n   ProdLine SalesFImage  ComPricing  WartyClaim  OrdBilling \r\n   3.488185    3.439420    1.635000    3.198337    2.902999 \r\n   DelSpeed \r\n   6.516014 \r\n\r\nHigh Variable Inflation Factor (VIF) is a sign of\r\nmulticollinearity. There is no formal VIF value for determining the\r\npresence of multicollinearity; however, in weaker models, VIF value\r\ngreater than 2.5 may be a cause of concern.\r\nFrom the VIF values, we can infer that variables\r\nCompRes and DelSpeed are a cause of\r\nconcern.\r\nRemedial Measures:\r\nTwo of the most commonly used methods to deal with multicollinearity in\r\nthe model is the following.\r\nRemove some of the highly correlated variables using VIF or stepwise\r\nalgorithms.\r\nPerform an analysis design like principal component analysis (PCA)/\r\nFactor Analysis on the correlated variables.\r\n\r\nFactor Analysis\r\n\r\nX <- prodsurvey[,-c(1,13)]\r\ny <- prodsurvey[,13]\r\n\r\nLet’s check the factorability of the variables in the dataset:\r\nperform the Kaiser-Meyer-Olkin (KMO) Test.\r\n\r\nKaiser-Meyer-Olkin (KMO) Test and/or Bartlett’s Test is a measure of\r\nhow suited your data is for Factor Analysis. The test measures sampling\r\nadequacy for each variable in the model and for the complete model. The\r\nstatistic is a measure of the proportion of variance among variables\r\nthat might be common variance. The lower the proportion, the more suited\r\nyour data is to Factor Analysis.\r\n\r\n\r\ndatamatrix2 <- cor(X)\r\nKMO(r=datamatrix2)\r\nKaiser-Meyer-Olkin factor adequacy\r\nCall: KMO(r = datamatrix2)\r\nOverall MSA =  0.65\r\nMSA for each item = \r\n   ProdQual        Ecom     TechSup     CompRes Advertising \r\n       0.51        0.63        0.52        0.79        0.78 \r\n   ProdLine SalesFImage  ComPricing  WartyClaim  OrdBilling \r\n       0.62        0.62        0.75        0.51        0.76 \r\n   DelSpeed \r\n       0.67 \r\n\r\nMSA (measure of sampling adequacy) > 0.5, we can run Factor\r\nAnalysis on this data.\r\n\r\nprint(cortest.bartlett(datamatrix2, nrow(prodsurvey[,-1])))\r\n$chisq\r\n[1] 619.2726\r\n\r\n$p.value\r\n[1] 1.79337e-96\r\n\r\n$df\r\n[1] 55\r\n\r\nThe approximate of Chi-square is 619.27 with 55 degrees of freedom,\r\nwhich is significant at 0.05 Level of significance.\r\nHence Factor Analysis is considered as an appropriate technique for\r\nfurther analysis of the data.\r\nFind the optimal clusters\r\nPlotting the eigenvalues from our factor analysis (whether it’s based\r\non principal axis or principal components extraction), a parallel\r\nanalysis involves generating random correlation matrices and after\r\nfactor analyzing them, comparing the resulting eigenvalues to the\r\neigenvalues of the observed data. The idea behind this method is\r\nthatobserved eigenvalues that are higher than their corresponding random\r\neigenvalues are more likely to be from “meaningful factors” than\r\nobserved eigenvalues that are below their corresponding random\r\neigenvalue.\r\n\r\nparallel <- fa.parallel(X)\r\n\r\nParallel analysis suggests that the number of factors =  3  and the number of components =  3 \r\n\r\n\r\n# other option of scree plot\r\nscree(X)\r\n\r\n\r\nSelection of factors from the scree plot can be based on:\r\nKaiser-Guttman normalization rule says that we should choose all\r\nfactors with an eigenvalue greater than 1.\r\nBend elbow rule\r\nWhen looking at the parallel analysis scree plots, there are two\r\nplaces to look depending on which type of factor analysis you’re looking\r\nto run. The two blue lines show you the observed eigenvalues - they\r\nshould look identical to the scree plots drawn by the scree function.\r\nThe red dotted lines show the random eigenvalues or the simulated data\r\nline. Each point on the blue line that lies above the corresponding\r\nsimulated data line is a factor or component to extract. In this\r\nanalysis, you can see that 4 factors in the “Factor Analysis” parallel\r\nanalysis lie above the corresponding simulated data line and 3\r\ncomponents in the “Principal Components” parallel analysis lie above the\r\ncorresponding simulated data line.\r\nIn our case, however, the last factor/component lies very close to the\r\nline - for both principal components extraction and principal axis\r\nextraction. Thus, we should probably compare the 3 factor and 4 factor\r\nsolutions, to see which one is most interpretable.\r\nThe scree plot graphs the Eigenvalue against each factor. We can see\r\nfrom the graph that after factor 4 there is a sharp change in the\r\ncurvature of the scree plot. This shows that after factor 4 the total\r\nvariance accounts for smaller amounts.\r\n\r\nev <- eigen(cor(X))\r\nev$values\r\n [1] 3.42697133 2.55089671 1.69097648 1.08655606 0.60942409 0.55188378\r\n [7] 0.40151815 0.24695154 0.20355327 0.13284158 0.09842702\r\n\r\n\r\nFactor <- rep(1:11)\r\nEigen_Values <- ev$values\r\nScree <- data.frame(Factor, Eigen_Values)\r\n\r\nggplot(data = Scree, mapping = aes(x = Factor, y = Eigen_Values)) + \r\n  geom_point() +\r\n  geom_line() +\r\n  scale_y_continuous(name = \"Eigen Values\", limits = c(0,4)) +\r\n  theme_bw() + \r\n  theme(panel.grid.major.y = element_line(color = \"skyblue\")) +\r\n  ggtitle(\"Scree Plot\")\r\n\r\n\r\nSo as per the elbow or Kaiser-Guttman normalization rule, we are good\r\nto go ahead with 4 factors.\r\nConducting the Factor\r\nAnalysis\r\nFactor analysis using fa method:\r\n\r\nfa.none <-  fa(r=X, \r\n              nfactors = 4, \r\n#              covar = FALSE, SMC = TRUE,\r\n              fm=\"pa\", # type of factor analysis we want to use (“pa” is principal axis factoring)\r\n              max.iter=100, # (50 is the default, but we have changed it to 100\r\n              rotate=\"none\") # none rotation\r\nprint(fa.none)\r\nFactor Analysis using method =  pa\r\nCall: fa(r = X, nfactors = 4, rotate = \"none\", max.iter = 100, fm = \"pa\")\r\nStandardized loadings (pattern matrix) based upon correlation matrix\r\n              PA1   PA2   PA3   PA4   h2    u2 com\r\nProdQual     0.20 -0.41 -0.06  0.46 0.42 0.576 2.4\r\nEcom         0.29  0.66  0.27  0.22 0.64 0.362 2.0\r\nTechSup      0.28 -0.38  0.74 -0.17 0.79 0.205 1.9\r\nCompRes      0.86  0.01 -0.26 -0.18 0.84 0.157 1.3\r\nAdvertising  0.29  0.46  0.08  0.13 0.31 0.686 1.9\r\nProdLine     0.69 -0.45 -0.14  0.31 0.80 0.200 2.3\r\nSalesFImage  0.39  0.80  0.35  0.25 0.98 0.021 2.1\r\nComPricing  -0.23  0.55 -0.04 -0.29 0.44 0.557 1.9\r\nWartyClaim   0.38 -0.32  0.74 -0.15 0.81 0.186 2.0\r\nOrdBilling   0.75  0.02 -0.18 -0.18 0.62 0.378 1.2\r\nDelSpeed     0.90  0.10 -0.30 -0.20 0.94 0.058 1.4\r\n\r\n                       PA1  PA2  PA3  PA4\r\nSS loadings           3.21 2.22 1.50 0.68\r\nProportion Var        0.29 0.20 0.14 0.06\r\nCumulative Var        0.29 0.49 0.63 0.69\r\nProportion Explained  0.42 0.29 0.20 0.09\r\nCumulative Proportion 0.42 0.71 0.91 1.00\r\n\r\nMean item complexity =  1.9\r\nTest of the hypothesis that 4 factors are sufficient.\r\n\r\nThe degrees of freedom for the null model are  55  and the objective function was  6.55 with Chi Square of  619.27\r\nThe degrees of freedom for the model are 17  and the objective function was  0.33 \r\n\r\nThe root mean square of the residuals (RMSR) is  0.02 \r\nThe df corrected root mean square of the residuals is  0.03 \r\n\r\nThe harmonic number of observations is  100 with the empirical chi square  3.19  with prob <  1 \r\nThe total number of observations was  100  with Likelihood Chi Square =  30.27  with prob <  0.024 \r\n\r\nTucker Lewis Index of factoring reliability =  0.921\r\nRMSEA index =  0.088  and the 90 % confidence intervals are  0.032 0.139\r\nBIC =  -48.01\r\nFit based upon off diagonal values = 1\r\nMeasures of factor score adequacy             \r\n                                                   PA1  PA2  PA3  PA4\r\nCorrelation of (regression) scores with factors   0.98 0.97 0.95 0.88\r\nMultiple R square of scores with factors          0.96 0.95 0.91 0.78\r\nMinimum correlation of possible factor scores     0.92 0.90 0.82 0.56\r\n\r\nFactor analysis using the factanal method:\r\n\r\nfactanal.none <- factanal(X, factors=4, scores = c(\"regression\"), rotation = \"none\")\r\nprint(factanal.none)\r\n\r\nCall:\r\nfactanal(x = X, factors = 4, scores = c(\"regression\"), rotation = \"none\")\r\n\r\nUniquenesses:\r\n   ProdQual        Ecom     TechSup     CompRes Advertising \r\n      0.682       0.360       0.228       0.178       0.679 \r\n   ProdLine SalesFImage  ComPricing  WartyClaim  OrdBilling \r\n      0.005       0.017       0.636       0.163       0.347 \r\n   DelSpeed \r\n      0.076 \r\n\r\nLoadings:\r\n            Factor1 Factor2 Factor3 Factor4\r\nProdQual     0.467  -0.148  -0.229  -0.159 \r\nEcom                 0.791                 \r\nTechSup      0.198          -0.482   0.707 \r\nCompRes      0.589   0.316   0.535   0.300 \r\nAdvertising          0.556   0.110         \r\nProdLine     0.997                         \r\nSalesFImage          0.987                 \r\nComPricing  -0.491   0.252   0.238         \r\nWartyClaim   0.279   0.124  -0.487   0.712 \r\nOrdBilling   0.452   0.275   0.493   0.360 \r\nDelSpeed     0.629   0.364   0.582   0.241 \r\n\r\n               Factor1 Factor2 Factor3 Factor4\r\nSS loadings      2.522   2.316   1.469   1.322\r\nProportion Var   0.229   0.211   0.134   0.120\r\nCumulative Var   0.229   0.440   0.573   0.694\r\n\r\nTest of the hypothesis that 4 factors are sufficient.\r\nThe chi square statistic is 24.26 on 17 degrees of freedom.\r\nThe p-value is 0.113 \r\nfactanal.var <- factanal(X, factors=4, scores = c(\"regression\"), rotation = \"varimax\")\r\nprint(factanal.var)\r\n\r\nCall:\r\nfactanal(x = X, factors = 4, scores = c(\"regression\"), rotation = \"varimax\")\r\n\r\nUniquenesses:\r\n   ProdQual        Ecom     TechSup     CompRes Advertising \r\n      0.682       0.360       0.228       0.178       0.679 \r\n   ProdLine SalesFImage  ComPricing  WartyClaim  OrdBilling \r\n      0.005       0.017       0.636       0.163       0.347 \r\n   DelSpeed \r\n      0.076 \r\n\r\nLoadings:\r\n            Factor1 Factor2 Factor3 Factor4\r\nProdQual                             0.557 \r\nEcom                 0.793                 \r\nTechSup                      0.872   0.102 \r\nCompRes      0.884   0.142           0.135 \r\nAdvertising  0.190   0.521          -0.110 \r\nProdLine     0.502           0.104   0.856 \r\nSalesFImage  0.119   0.974          -0.130 \r\nComPricing           0.225  -0.216  -0.514 \r\nWartyClaim                   0.894   0.158 \r\nOrdBilling   0.794   0.101   0.105         \r\nDelSpeed     0.928   0.189           0.164 \r\n\r\n               Factor1 Factor2 Factor3 Factor4\r\nSS loadings      2.592   1.977   1.638   1.423\r\nProportion Var   0.236   0.180   0.149   0.129\r\nCumulative Var   0.236   0.415   0.564   0.694\r\n\r\nTest of the hypothesis that 4 factors are sufficient.\r\nThe chi square statistic is 24.26 on 17 degrees of freedom.\r\nThe p-value is 0.113 \r\n\r\nGraph Factor Loading\r\nMatrices\r\nFactor analysis results are typically interpreted in terms of the\r\nmajor loadings on each factor. These structures may be represented an\r\n“interpretable” solution as a table of loadings or graphically, where\r\nall loadings with an absolute value \\(\\rightarrow\\) some cut point are\r\nrepresented as an edge (path).\r\n\r\nfa.diagram(fa.none)\r\n\r\n\r\nThe first 4 factors have an Eigenvalue >1 and which explains\r\nalmost 69% of the variance. We can effectively reduce dimensionality\r\nfrom 11 to 4 while only losing about 31% of the variance.\r\nFactor 1 accounts for 29.20% of the variance; Factor 2 accounts for\r\n20.20% of the variance; Factor 3 accounts for 13.60% of the variance;\r\nFactor 4 accounts for 6% of the variance. All the 4 factors together\r\nexplain for 69% of the variance in performance.\r\n\r\nfa.var <-  fa(r=X, \r\n              nfactors = 4, \r\n#              covar = FALSE, SMC = TRUE,\r\n              fm=\"pa\", # type of factor analysis we want to use (“pa” is principal axis factoring)\r\n              max.iter=100, # (50 is the default, but we have changed it to 100\r\n              rotate=\"varimax\") # none rotation\r\nprint(fa.var)\r\nFactor Analysis using method =  pa\r\nCall: fa(r = X, nfactors = 4, rotate = \"varimax\", max.iter = 100, fm = \"pa\")\r\nStandardized loadings (pattern matrix) based upon correlation matrix\r\n              PA1   PA2   PA3   PA4   h2    u2 com\r\nProdQual     0.02 -0.07  0.02  0.65 0.42 0.576 1.0\r\nEcom         0.07  0.79  0.03 -0.11 0.64 0.362 1.1\r\nTechSup      0.02 -0.03  0.88  0.12 0.79 0.205 1.0\r\nCompRes      0.90  0.13  0.05  0.13 0.84 0.157 1.1\r\nAdvertising  0.17  0.53 -0.04 -0.06 0.31 0.686 1.2\r\nProdLine     0.53 -0.04  0.13  0.71 0.80 0.200 1.9\r\nSalesFImage  0.12  0.97  0.06 -0.13 0.98 0.021 1.1\r\nComPricing  -0.08  0.21 -0.21 -0.59 0.44 0.557 1.6\r\nWartyClaim   0.10  0.06  0.89  0.13 0.81 0.186 1.1\r\nOrdBilling   0.77  0.13  0.09  0.09 0.62 0.378 1.1\r\nDelSpeed     0.95  0.19  0.00  0.09 0.94 0.058 1.1\r\n\r\n                       PA1  PA2  PA3  PA4\r\nSS loadings           2.63 1.97 1.64 1.37\r\nProportion Var        0.24 0.18 0.15 0.12\r\nCumulative Var        0.24 0.42 0.57 0.69\r\nProportion Explained  0.35 0.26 0.22 0.18\r\nCumulative Proportion 0.35 0.60 0.82 1.00\r\n\r\nMean item complexity =  1.2\r\nTest of the hypothesis that 4 factors are sufficient.\r\n\r\nThe degrees of freedom for the null model are  55  and the objective function was  6.55 with Chi Square of  619.27\r\nThe degrees of freedom for the model are 17  and the objective function was  0.33 \r\n\r\nThe root mean square of the residuals (RMSR) is  0.02 \r\nThe df corrected root mean square of the residuals is  0.03 \r\n\r\nThe harmonic number of observations is  100 with the empirical chi square  3.19  with prob <  1 \r\nThe total number of observations was  100  with Likelihood Chi Square =  30.27  with prob <  0.024 \r\n\r\nTucker Lewis Index of factoring reliability =  0.921\r\nRMSEA index =  0.088  and the 90 % confidence intervals are  0.032 0.139\r\nBIC =  -48.01\r\nFit based upon off diagonal values = 1\r\nMeasures of factor score adequacy             \r\n                                                   PA1  PA2  PA3  PA4\r\nCorrelation of (regression) scores with factors   0.98 0.99 0.94 0.88\r\nMultiple R square of scores with factors          0.96 0.97 0.88 0.78\r\nMinimum correlation of possible factor scores     0.93 0.94 0.77 0.55\r\n\r\n\r\nfa.var$loadings\r\n\r\nLoadings:\r\n            PA1    PA2    PA3    PA4   \r\nProdQual                          0.647\r\nEcom                0.787        -0.113\r\nTechSup                    0.883  0.116\r\nCompRes      0.898  0.130         0.132\r\nAdvertising  0.166  0.530              \r\nProdLine     0.525         0.127  0.712\r\nSalesFImage  0.115  0.971        -0.135\r\nComPricing          0.213 -0.209 -0.590\r\nWartyClaim   0.103         0.885  0.128\r\nOrdBilling   0.768  0.127              \r\nDelSpeed     0.949  0.185              \r\n\r\n                 PA1   PA2   PA3   PA4\r\nSS loadings    2.635 1.967 1.641 1.371\r\nProportion Var 0.240 0.179 0.149 0.125\r\nCumulative Var 0.240 0.418 0.568 0.692\r\n\r\n\r\nfa.diagram(fa.var)\r\n\r\n\r\nThe red dotted line means that Competitive Pricing marginally falls\r\nunder the PA4 bucket and the loading are negative.\r\nLabeling and\r\ninterpretation of the factors\r\nFactors\r\nVariables\r\nLabel\r\nShort Interpretaation\r\nPA1\r\nDelSpeed, CompRes, OrdBilling\r\nPurchase\r\nAll the items are related to purchasing the product; from placing\r\norder to billing and getting it delivered.\r\nPA2\r\nSalesFImage, Ecom, Advertising\r\nMarketing\r\nIn this factors the item are related to marketing processes like the\r\nimage of sales force and spending on adverstising\r\nPA3\r\nWartyClaim, TechSup\r\nPost Purchase\r\nPost purchase activities are included in this factor; like warranty\r\nclaims and technical support.\r\nPA4\r\nProdLine, ProdQual, CompPricing\r\nProduct Position\r\nProduct positioning related items are grouped in this factor.\r\nScores for all the rows\r\n\r\nhead(fa.var$scores)\r\n            PA1        PA2          PA3         PA4\r\n[1,] -0.1338871  0.9175166 -1.719604873  0.09135411\r\n[2,]  1.6297604 -2.0090053 -0.596361722  0.65808192\r\n[3,]  0.3637658  0.8361736  0.002979966  1.37548765\r\n[4,] -1.2225230 -0.5491336  1.245473305 -0.64421384\r\n[5,] -0.4854209 -0.4276223 -0.026980304  0.47360747\r\n[6,] -0.5950924 -1.3035333 -1.183019401 -0.95913571\r\n#fa.var$scores # for 100 observations\r\n\r\nRegression\r\nanalysis using the factors scores as the independent variable\r\n\r\nregdata <- cbind(prodsurvey[\"Satisfaction\"], fa.var$scores)\r\n\r\n#Labeling the data\r\nnames(regdata) <- c(\"Satisfaction\", \"Purchase\", \"Marketing\",\r\n                    \"Post_purchase\", \"Prod_positioning\")\r\nhead(regdata)\r\n  Satisfaction   Purchase  Marketing Post_purchase Prod_positioning\r\n1          8.2 -0.1338871  0.9175166  -1.719604873       0.09135411\r\n2          5.7  1.6297604 -2.0090053  -0.596361722       0.65808192\r\n3          8.9  0.3637658  0.8361736   0.002979966       1.37548765\r\n4          4.8 -1.2225230 -0.5491336   1.245473305      -0.64421384\r\n5          7.1 -0.4854209 -0.4276223  -0.026980304       0.47360747\r\n6          4.7 -0.5950924 -1.3035333  -1.183019401      -0.95913571\r\n\r\nSplitting the data to\r\ntrain and test set\r\n\r\n#Splitting the data 70:30\r\n#Random number generator, set seed.\r\nset.seed(100)\r\nindices= sample(1:nrow(regdata), 0.7*nrow(regdata))\r\ntrain=regdata[indices,]\r\ntest = regdata[-indices,]\r\n\r\nRegression Model using train\r\ndata\r\n\r\nmodel.fa.score = lm(Satisfaction~., train)\r\nsummary(model.fa.score)\r\n\r\nCall:\r\nlm(formula = Satisfaction ~ ., data = train)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.76280 -0.48717  0.06799  0.46459  1.24022 \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       6.91852    0.08068  85.757  < 2e-16 ***\r\nPurchase          0.50230    0.07641   6.574 9.74e-09 ***\r\nMarketing         0.75488    0.08390   8.998 5.00e-13 ***\r\nPost_purchase     0.08755    0.08216   1.066    0.291    \r\nProd_positioning  0.58074    0.08781   6.614 8.30e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6629 on 65 degrees of freedom\r\nMultiple R-squared:  0.7261,    Adjusted R-squared:  0.7093 \r\nF-statistic: 43.08 on 4 and 65 DF,  p-value: < 2.2e-16\r\n\r\nCheck vif\r\n\r\nvif(model.fa.score)\r\n        Purchase        Marketing    Post_purchase Prod_positioning \r\n        1.009648         1.008235         1.015126         1.024050 \r\n\r\nAs per the VIF values, we don’t have multicollinearity in the\r\nmodel\r\nCheck\r\nprediction of the model in the test dataset\r\n\r\n#Model Performance metrics:\r\npred_test <- predict(model.fa.score, newdata = test, type = \"response\")\r\npred_test\r\n       6        8       11       13       17       19       26 \r\n4.975008 5.908267 6.951629 8.677431 6.613838 6.963113 6.313513 \r\n      33       34       35       37       40       42       44 \r\n6.141338 6.158993 7.415742 6.589746 6.858206 7.133989 8.533080 \r\n      49       50       53       56       57       60       65 \r\n8.765145 8.078744 7.395438 7.468360 8.744402 6.276660 5.936570 \r\n      67       71       73       75       80       96       97 \r\n6.650322 8.299545 7.685564 7.330191 6.719528 7.540233 6.143172 \r\n      99      100 \r\n8.084583 5.799897 \r\n\r\n\r\n#Find MSE and MAPE scores:\r\n#MSE/ MAPE of Model1\r\ntest$Satisfaction_Predicted <- pred_test\r\nhead(test[c(\"Satisfaction\",\"Satisfaction_Predicted\")], 10)\r\n   Satisfaction Satisfaction_Predicted\r\n6           4.7               4.975008\r\n8           6.3               5.908267\r\n11          7.4               6.951629\r\n13          8.4               8.677431\r\n17          6.4               6.613838\r\n19          6.8               6.963113\r\n26          6.6               6.313513\r\n33          5.4               6.141338\r\n34          7.3               6.158993\r\n35          6.3               7.415742\r\n\r\n\r\ntest_r2 <- cor(test$Satisfaction, test$Satisfaction_Predicted) ^2\r\n\r\nmse_test <- mse(test$Satisfaction, pred_test)\r\nrmse_test <- sqrt(mse(test$Satisfaction, pred_test))\r\nmape_test <- mape(test$Satisfaction, pred_test)\r\nmodel_metrics <- cbind(mse_test,rmse_test,mape_test,test_r2)\r\nprint(model_metrics, 3)\r\n     mse_test rmse_test mape_test test_r2\r\n[1,]    0.551     0.742    0.0875   0.554\r\n\r\nAs the feature Post_purchase is not significant so we\r\nwill drop this feature and then let’s run the regression model\r\nagain.\r\n\r\n##Regression model without post_purchase:\r\nmodel2 <- lm(Satisfaction ~ Purchase+ Marketing+ \r\n                Prod_positioning, data= train)\r\nsummary(model2)\r\n\r\nCall:\r\nlm(formula = Satisfaction ~ Purchase + Marketing + Prod_positioning, \r\n    data = train)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.7268 -0.5036  0.1117  0.4408  1.2962 \r\n\r\nCoefficients:\r\n                 Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       6.92158    0.08071  85.761  < 2e-16 ***\r\nPurchase          0.49779    0.07637   6.518 1.15e-08 ***\r\nMarketing         0.75870    0.08391   9.042 3.66e-13 ***\r\nProd_positioning  0.59084    0.08739   6.761 4.30e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6636 on 66 degrees of freedom\r\nMultiple R-squared:  0.7213,    Adjusted R-squared:  0.7087 \r\nF-statistic: 56.95 on 3 and 66 DF,  p-value: < 2.2e-16\r\n\r\n\r\npred_test2 <- predict(model2, newdata = test, type = \"response\")\r\npred_test2\r\n       6        8       11       13       17       19       26 \r\n5.069667 5.958765 7.001694 8.695630 6.521440 6.994018 6.249667 \r\n      33       34       35       37       40       42       44 \r\n6.123357 6.132165 7.415882 6.566556 6.964570 7.233376 8.424720 \r\n      49       50       53       56       57       60       65 \r\n8.853060 8.177009 7.413693 7.404078 8.746511 6.215816 5.893515 \r\n      67       71       73       75       80       96       97 \r\n6.476465 8.227837 7.839941 7.354907 6.734279 7.559995 6.293774 \r\n      99      100 \r\n8.083909 5.832094 \r\n\r\n\r\ntest$Satisfaction_Predicted2 <- pred_test2\r\nhead(test[c(1,7)], 10)\r\n   Satisfaction Satisfaction_Predicted2\r\n6           4.7                5.069667\r\n8           6.3                5.958765\r\n11          7.4                7.001694\r\n13          8.4                8.695630\r\n17          6.4                6.521440\r\n19          6.8                6.994018\r\n26          6.6                6.249667\r\n33          5.4                6.123357\r\n34          7.3                6.132165\r\n35          6.3                7.415882\r\n\r\n\r\ntest_r22 <- cor(test$Satisfaction, test$Satisfaction_Predicted2) ^2\r\nmse_test2 <- mse(test$Satisfaction, pred_test2)\r\nrmse_test2 <- sqrt(mse(test$Satisfaction, pred_test2))\r\nmape_test2 <- mape(test$Satisfaction, pred_test2)\r\n\r\nmodel2_metrics <- cbind(mse_test2,rmse_test2,mape_test2,test_r22)\r\nmodel2_metrics\r\n     mse_test2 rmse_test2 mape_test2  test_r22\r\n[1,] 0.5446039  0.7379728 0.08490197 0.5595099\r\n\r\n\r\nOverall <- rbind(model_metrics,model2_metrics)\r\nrow.names(Overall) <- c(\"Test1\", \"Test2\")\r\ncolnames(Overall) <- c(\"MSE\", \"RMSE\", \"MAPE\", \"R-squared\")\r\nprint(Overall,3)\r\n        MSE  RMSE   MAPE R-squared\r\nTest1 0.551 0.742 0.0875     0.554\r\nTest2 0.545 0.738 0.0849     0.560\r\n\r\nTry the interaction?!\r\nIncluding Interaction model, we are able to make a better\r\nprediction!??\r\nConclusion\r\nWe saw how Factor Analysis can be used to reduce the dimensionality\r\nof a dataset and then we used multiple linear regression on the\r\ndimensionally reduced columns/Features for further\r\nanalysis/predictions.\r\nFurther Reading\r\nA repost from\r\nHaibiostat’s rpubs\r\nAnalytixLabs\r\nJay Narayan, Multiple\r\nLinear Regression & Factor Analysis in R, May 28, 2019\r\nRachael Smyth and Andrew Johnson, Factor\r\nAnalysis\r\nSreenu Konda, Lecture on Factor Analysis, Bistatistics, SPH, UIC,\r\nSpring 2020\r\n",
    "preview": "posts/2022-09-30-FA in Multivariate Regression/distill-preview.png",
    "last_modified": "2023-02-23T22:07:01-06:00",
    "input_file": "Multiple-Linear-Regression-and-Factor-Analysis-in-R.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-09-15-Factor Analysis/",
    "title": "Exploratory / Confirmatory Factor Analysis?",
    "description": "Correlation matrix;  \nExploratory Factor Analysis vs. Confirmatory Factor Analysis;\nRun An Example with Categorical Data in R (psych package);",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-09-19",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "EFA",
      "Model Fit Statistics"
    ],
    "contents": "\r\n\r\nContents\r\nWhy use factor analysis?\r\nWhat is a factor?\r\nA case study of Depression data of ordinal variables\r\nRead raw data into R\r\nFA using fa() form psych\r\n\r\nWhat are factor loadings?\r\nRotation in EFA\r\nComparison of EFA and CFA\r\n\r\nWhy use factor analysis?\r\nFactor analysis is a useful tool for investigating variable relationships for complex concepts such as socioeconomic status, dietary patterns, or psychological scales.\r\nIt allows researchers to investigate concepts that are not easily measured directly by collapsing a large number of variables into a few interpretable underlying factors.\r\nWhat is a factor?\r\nIn factor analysis, a factor is an latent (unmeasured) variable that expresses itself through its relationship with other measured variables.\r\nTake for example a variable like cognition. We may want to measure a person’s cognition, but this is the kind of construct that would be impossible to measure using a single variable. It’s just too abstract and multifaceted, although it does represent a single concept. So instead, you may have to develop the scale with many items, each of which measures some more measurable part of leadership. The idea would be that there is an underlying unmeasurable factor, cognition, that causes people to respond in certain patterns on the many items on the scale.\r\nThe purpose of factor analysis is to analyze these patterns of response as a way of getting at this underlying factor. Factor analysis also allows you to use the weighted item responses to create what are called factor scores. These represent a single score for each person on the factor. Factor scores are nice because they allow you to use a single variable as a measure of the factor in the other analyses, rather than a set of items.\r\nFactor analysis often applied for continuous variables, but for nominal variables we would have no problem when using fa() in library psych\r\n\\(\\Rightarrow\\) As demonstrated below, using ordinal or nominal/binary data for factor analysis in R is no more difficult than using continuous data for factor analysis in R.\r\nIf variables include mixed variables. If that is the case use cor=\"mixed\" in fa() of psych library\r\nA case study of Depression data of ordinal variables\r\nNow, we jump on an example: Depression data (NHANES, 2014-15) includes 10 ordinal variables. This approach would be better helping us see through the meaningfulness of FA.\r\nDPQ010 Have little interest in doing things\r\nDPQ020* Feeling down, depressed, or hopeless\r\nDPQ030 Trouble sleeping or sleeping too much\r\nDPQ040 Feeling tired or having little energy\r\nDPQ050 Poor appetite or overeating\r\nDPQ060* Feeling bad about yourself\r\nDPQ070 Trouble concentrating on things\r\nDPQ080 Moving or speaking slowly or too fast\r\nDPQ090* Thought you would be better off dead\r\nDPQ100 Difficulty these problems have caused\r\nFew more variables of different ordinal scales, which are not used in this example\r\n* more critical/serious variables\r\nValues of each variable has the same format of “ordinal” numerical values:\r\n0 Not at all\r\n1 Several days\r\n2 More than half the days\r\n3 Nearly every day\r\nHigher is the value \\(\\Rightarrow\\) “more” likely to be depressed ???\r\nRead raw data into R\r\n\r\noptions(width = 300)\r\ndep <- read.csv(\"data/dep.csv\",header = TRUE, \r\n                sep = \",\", na.strings = \"NA\")\r\n# na.strings : a character vector of strings which are to be \r\n#             interpreted as NA values.\r\nhead(dep)\r\n   SEQN DPQ010 DPQ020 DPQ030 DPQ040 DPQ050 DPQ060 DPQ070 DPQ080 DPQ090 DPQ100 SLD010H SLQ050 SLQ060\r\n1 73557      1      0      0      0      0      0      0      0      0      1       7      1      2\r\n2 73558      2      0      0      0      0      0      0      0      0      0       9      2      2\r\n3 73561      2      1      0      3      3      0      0      0      0      1       9      2      2\r\n4 73562      3      3      3      3      3      1      2      1      0      3       5      2      1\r\n5 73564      0      1      0      1      0      0      0      0      0      0       9      2      1\r\n6 73566      0      0      1      0      0      0      0      0      0      0       6      1      2\r\nnrow(dep)\r\n[1] 3651\r\nx <- dep[,2:11]\r\n\r\nWe first check our correlation matrix. Visual analysis of the correlation matrix is done to reveal some similarities or correlations that can be found in the data between groups of questions.\r\n\r\nR <- cor(x)\r\nlibrary(corrplot)\r\ncorrplot(R,type=\"upper\",order=\"hclust\")\r\n\r\n\r\nThe blue structures are positively correlated groups and the red structure represents a negatively correlated group that emerges among the data.\r\nFA using fa() form psych\r\n\r\nlibrary(psych)\r\nlibrary(GPArotation)\r\n\r\n\r\nNote: psych package also has Tetrachoric, polychoric, biserial and polyserial correlations for various types of variables\r\n\r\nhead(x)\r\n  DPQ010 DPQ020 DPQ030 DPQ040 DPQ050 DPQ060 DPQ070 DPQ080 DPQ090\r\n1      1      0      0      0      0      0      0      0      0\r\n2      2      0      0      0      0      0      0      0      0\r\n3      2      1      0      3      3      0      0      0      0\r\n4      3      3      3      3      3      1      2      1      0\r\n5      0      1      0      1      0      0      0      0      0\r\n6      0      0      1      0      0      0      0      0      0\r\n  DPQ100\r\n1      1\r\n2      0\r\n3      1\r\n4      3\r\n5      0\r\n6      0\r\n\r\nImportant note for FA using psych lib of different types of variables\r\nfa(data,n.factors) has an additional argument: cor with several options depending on input variable types\r\ncor=\"cor\" Pearson correlation, for numeric variables with different scales\r\ncor=\"cov\" Covariance, for numeric variables with similar scales\r\ncor=\"tet\" tetrachoric, for dichotomous variables\r\ncor=\"poly\" polychoric, for ordinal variables\r\ncor=\"mixed\" mixed cor for a mixture of tetrachorics, polychorics, Pearsons, biserials, and polyserials, Yuleb is Yulebonett, Yuleq and YuleY are the obvious Yule coefficients as appropriate\r\nRemarks\r\ntetrachoric “correlation” for 2 x 2 table or two dichotomous variables.\r\npolychoric correlation for k1 x k2 ordinal variables.\r\nbiserial correlation for continuous variable and dichotmous x variable.\r\npolyserial correlation for continuous variable and a ordered categorical Y.\r\nMay convert nominal variables into dummy variables but that increases dimensionality very quickly\r\nAssumption for ordinal variable, it has latenent normal variable\r\nCorrelations between non-numeric variables is often related chi-square test stats\r\nSince our variables are all ordinal lets use polychoric correlations\r\n\r\npolychoric(x)\r\nCall: polychoric(x = x)\r\nPolychoric correlations \r\n       DPQ01 DPQ02 DPQ03 DPQ04 DPQ05 DPQ06 DPQ07 DPQ08 DPQ09 DPQ10\r\nDPQ010 1.00                                                       \r\nDPQ020 0.58  1.00                                                 \r\nDPQ030 0.31  0.33  1.00                                           \r\nDPQ040 0.38  0.38  0.37  1.00                                     \r\nDPQ050 0.38  0.43  0.34  0.34  1.00                               \r\nDPQ060 0.47  0.70  0.30  0.36  0.43  1.00                         \r\nDPQ070 0.47  0.51  0.34  0.35  0.41  0.51  1.00                   \r\nDPQ080 0.48  0.51  0.41  0.37  0.41  0.51  0.59  1.00             \r\nDPQ090 0.40  0.65  0.34  0.37  0.37  0.70  0.49  0.49  1.00       \r\nDPQ100 0.57  0.66  0.42  0.47  0.46  0.61  0.56  0.59  0.61  1.00 \r\n\r\n with tau of \r\n            1    2   3\r\nDPQ010  0.296 1.06 1.5\r\nDPQ020  0.368 1.22 1.6\r\nDPQ030 -0.096 0.74 1.1\r\nDPQ040 -0.665 0.68 1.1\r\nDPQ050  0.333 1.07 1.5\r\nDPQ060  0.660 1.36 1.7\r\nDPQ070  0.635 1.26 1.6\r\nDPQ080  0.980 1.52 1.9\r\nDPQ090  1.646 2.12 2.4\r\nDPQ100  0.629 1.60 2.1\r\n\r\nOutput has\r\nrho=“correlations” and\r\ntau=“the normal equivalent of the cutpoints”\r\nfa for ordinal variables of more than two values.\r\nIn this case, assuming two factors.\r\n\r\nfa.out <- fa(dep[,2:11], nfactors=2, cor='poly', rotate = \"oblimin\")\r\n\r\n\r\nNote:\r\n- oblimin is default\r\n- If the variables are all 0/1 binary, use tetrachoric correlations i.e. cor=“tet”\r\n\r\nfa.out\r\nFactor Analysis using method =  minres\r\nCall: fa(r = dep[, 2:11], nfactors = 2, rotate = \"oblimin\", cor = \"poly\")\r\nStandardized loadings (pattern matrix) based upon correlation matrix\r\n         MR2   MR1   h2   u2 com\r\nDPQ010  0.54  0.15 0.44 0.56 1.1\r\nDPQ020  0.19  0.67 0.69 0.31 1.2\r\nDPQ030  0.69 -0.18 0.32 0.68 1.1\r\nDPQ040  0.61 -0.06 0.32 0.68 1.0\r\nDPQ050  0.54  0.05 0.34 0.66 1.0\r\nDPQ060 -0.05  0.90 0.75 0.25 1.0\r\nDPQ070  0.58  0.14 0.49 0.51 1.1\r\nDPQ080  0.70  0.05 0.54 0.46 1.0\r\nDPQ090  0.07  0.74 0.63 0.37 1.0\r\nDPQ100  0.60  0.26 0.68 0.32 1.4\r\n\r\n                       MR2  MR1\r\nSS loadings           2.95 2.24\r\nProportion Var        0.29 0.22\r\nCumulative Var        0.29 0.52\r\nProportion Explained  0.57 0.43\r\nCumulative Proportion 0.57 1.00\r\n\r\n With factor correlations of \r\n     MR2  MR1\r\nMR2 1.00 0.78\r\nMR1 0.78 1.00\r\n\r\nMean item complexity =  1.1\r\nTest of the hypothesis that 2 factors are sufficient.\r\n\r\nThe degrees of freedom for the null model are  45  and the objective function was  4.72 with Chi Square of  17223.44\r\nThe degrees of freedom for the model are 26  and the objective function was  0.14 \r\n\r\nThe root mean square of the residuals (RMSR) is  0.03 \r\nThe df corrected root mean square of the residuals is  0.04 \r\n\r\nThe harmonic number of observations is  3651 with the empirical chi square  246.89  with prob <  7.1e-38 \r\nThe total number of observations was  3651  with Likelihood Chi Square =  524.04  with prob <  3.7e-94 \r\n\r\nTucker Lewis Index of factoring reliability =  0.95\r\nRMSEA index =  0.072  and the 90 % confidence intervals are  0.067 0.078\r\nBIC =  310.76\r\nFit based upon off diagonal values = 1\r\nMeasures of factor score adequacy             \r\n                                                   MR2  MR1\r\nCorrelation of (regression) scores with factors   0.93 0.94\r\nMultiple R square of scores with factors          0.87 0.89\r\nMinimum correlation of possible factor scores     0.74 0.78\r\nnames(fa.out)\r\n [1] \"residual\"      \"dof\"           \"chi\"           \"nh\"           \r\n [5] \"rms\"           \"EPVAL\"         \"crms\"          \"EBIC\"         \r\n [9] \"ESABIC\"        \"fit\"           \"fit.off\"       \"sd\"           \r\n[13] \"factors\"       \"complexity\"    \"n.obs\"         \"objective\"    \r\n[17] \"criteria\"      \"STATISTIC\"     \"PVAL\"          \"Call\"         \r\n[21] \"null.model\"    \"null.dof\"      \"null.chisq\"    \"TLI\"          \r\n[25] \"RMSEA\"         \"BIC\"           \"SABIC\"         \"r.scores\"     \r\n[29] \"R2\"            \"valid\"         \"score.cor\"     \"weights\"      \r\n[33] \"rotation\"      \"communality\"   \"communalities\" \"uniquenesses\" \r\n[37] \"values\"        \"e.values\"      \"loadings\"      \"model\"        \r\n[41] \"fm\"            \"rot.mat\"       \"Phi\"           \"Structure\"    \r\n[45] \"method\"        \"scores\"        \"R2.scores\"     \"r\"            \r\n[49] \"np.obs\"        \"fn\"            \"Vaccounted\"   \r\nplot(fa.out$values,type='b')\r\n\r\n\r\n\r\nbiplot(fa.out)\r\n\r\n\r\n\r\nhead(fa.out$scores)\r\n            MR2        MR1\r\n[1,] -0.3143623 -0.3168916\r\n[2,] -0.5140691 -0.4382790\r\n[3,]  0.7452267  0.2120120\r\n[4,]  3.2561279  2.2674301\r\n[5,] -0.6919326 -0.3763604\r\n[6,] -0.7741730 -0.6857362\r\n\r\nLet’s check factor scores of subject 122, who is really depressed!\r\n\r\nx[122,]\r\n    DPQ010 DPQ020 DPQ030 DPQ040 DPQ050 DPQ060 DPQ070 DPQ080 DPQ090\r\n122      3      3      3      2      2      3      2      0      3\r\n    DPQ100\r\n122      3\r\nfa.out$scores[122,]\r\n     MR2      MR1 \r\n5.193272 6.472891 \r\n\r\nReally large numbers, look at the second score\r\nLet’s get predicted values of x[122,] and they should be high\r\n\r\napply(x,2,mean) + fa.out$scores[122,]%*%t(as.matrix(fa.out$loadings[,1:2]))\r\n       DPQ010   DPQ020   DPQ030   DPQ040   DPQ050   DPQ060   DPQ070\r\n[1,] 4.350776 5.852346 3.323438 3.933419 3.722472 6.000532 4.341527\r\n       DPQ080   DPQ090   DPQ100\r\n[1,] 4.179323 5.194817 5.158928\r\n\r\nAlthough 2nd factor is not significant according to Elbow rule, we still include it in analysis for its meaning!\r\nInterpret the output as of numeric variables\r\nBut, care must be taken while generalizing FA of non-numeric variables\r\nInterpretation:\r\nF1 or MR1: weighted average non-critical/non-clinical depression variables\r\nF2 or MR2: weighted average critical/clinical depression variables\r\nAlternative way of doing same FA using correlation matrix but less output(no scores)\r\n\r\npoly.R <- polychoric(x)$rho\r\nfa.out <- fa(r=poly.R, nfactors=2, n.obs = nrow(dep[,2:11]))\r\n\r\nWhat are factor loadings?\r\n\r\nfa.diagram(fa.out, digits = 2)\r\n\r\n\r\nThe relationship of each variable to the underlying factor is expressed by the so-called factor loading. Let’s go through each part of the printed output.\r\n\r\nround(cbind(fa.out$loadings,\"h2\"=fa.out$communality,\"u2\"=fa.out$uniquenesses,\"com\"=fa.out$complexity),2)\r\n         MR2   MR1   h2   u2  com\r\nDPQ010  0.54  0.15 0.44 0.56 1.15\r\nDPQ020  0.19  0.67 0.69 0.31 1.17\r\nDPQ030  0.69 -0.18 0.32 0.68 1.14\r\nDPQ040  0.61 -0.06 0.32 0.68 1.02\r\nDPQ050  0.54  0.05 0.34 0.66 1.02\r\nDPQ060 -0.05  0.90 0.75 0.25 1.01\r\nDPQ070  0.58  0.14 0.49 0.51 1.11\r\nDPQ080  0.70  0.05 0.54 0.46 1.01\r\nDPQ090  0.07  0.74 0.63 0.37 1.02\r\nDPQ100  0.60  0.26 0.68 0.32 1.37\r\n\r\nWhat’s MR, ML, PC etc.? These are factors, and the name merely reflects the fitting method, e.g. minimum residual, maximum likelihood, principal components. The default is minimum residual, so in this case MR.\r\nWhy are they ‘out of order’? the number assigned is arbitrary, but this has to do with a rotated solution. See the help file for more details, otherwise they are numbered in terms of variance accounted for.\r\nh2: the amount of variance in the item/variable explained by the (retained) factors. It is the sum of the squared loadings, a.k.a. communality.\r\nu2: 1 - h2. residual variance, a.k.a. uniqueness\r\ncom: Item complexity. Specifically it is “Hoffman’s index of complexity for each item. This is just \\((\\sum \\lambda_i^2)^2/\\sum \\lambda_i^4\\), where \\(\\lambda_i\\) is the factor loading on the \\(i^{th}\\) factor. Basically it tells you how much an item reflects a single construct. It will be lower for relatively lower loadings.\r\n\r\nfa.out$Vaccounted\r\n                            MR2       MR1\r\nSS loadings           2.9498769 2.2362545\r\nProportion Var        0.2949877 0.2236254\r\nCumulative Var        0.2949877 0.5186131\r\nProportion Explained  0.5688010 0.4311990\r\nCumulative Proportion 0.5688010 1.0000000\r\n\r\nThe variance accounted for portion of the output can be explained as follows:\r\nSS loadings: These are the eigenvalues, the sum of the squared loadings. In this case where we are using a correlation matrix, summing across all factors would equal the number of variables used in the analysis.\r\nProportion Var: tells us how much of the overall variance the factor accounts for out of all the variables.\r\nCumulative Var: the cumulative sum of Proportion Var.\r\nProportion Explained: The relative amount of variance explained- Proportion Var/sum(Proportion Var).\r\nCumulative Proportion: the cumulative sum of Proportion Explained.\r\nThese are contained in model$Vaccounted.\r\nThe variable with the strongest association to the underlying latent variable. Factor 1, is DPQ060 (Feeling bad about yourself), with a factor loading of 0.90.\r\nSince factor loadings can be interpreted like standardized regression coefficients, one could also say that the variable DPQ060 has a correlation of 0.90 with Factor 1. This would be considered a strong association for a factor analysis in most research fields.\r\nTwo other variables, DPQ090 (Thought you would be better off dead) and DPQ020 (Feeling down, depressed, or hopeless), are also associated with Factor 1. Based on the variables loading highly onto Factor 1, we could call it “Low self-esteem individual awareness.”\r\nDPQ010, DPQ030, DPQ040, DPQ050, DPQ070, DPQ080 and DPQ100, however, have high factor loadings on the Factor 2. They seem to indicate the observed signs of depression, so we may want to call Factor 2 “Physical depression signs.”\r\n\r\nfa.out$Phi\r\n          MR2       MR1\r\nMR2 1.0000000 0.7835837\r\nMR1 0.7835837 1.0000000\r\n\r\nFactor correlations\r\nWhether you get this part of the analysis depends on whether or not these are estimated. You have to have multiple factors and a rotation that allows for the correlations.\r\nfactor correlations: the correlation matrix for the factors. \\(\\phi (phi)\\)\r\nMean item complexity: the mean of com.\r\nThese are contained in model$Phi\r\nModel test results and Fit Indices (already discuss in SEM)\r\nRotation in EFA\r\nAn feature of EFA is that the axes of the factors can be rotated within the multidimensional variable space.\r\nWhat a factor analysis program does while determining the best fit between the variables and the latent factors. We have 10 variables that go into a factor analysis.\r\nThe program looks first for the strongest correlations between variables and the latent factor, and makes that Factor 1. Visually, one can think of it as an axis (Axis 1). The factor analysis program then looks for the second set of correlations and calls it Factor 2, and so on. Sometimes, the initial solution results in strong correlations of a variable with several factors or in a variable that has no strong correlations with any of the factors.\r\n\r\nfa.dem <- factor.rotate(fa.out,0,plot=TRUE,xlim=c(-1,1),ylim=c(-1,1), title=\"oblimin rotation\")\r\n\r\n\r\nRotations that allow for correlation are called oblique rotations; rotations that assume the factors are not correlated are called orthogonal rotations. Our graph shows an orthogonal rotation. We run the example above using the oblimin (oblique). Below, we test with varimax (orthogonal):\r\n\r\nfa.out <- fa(dep[,2:11], nfactors=2, cor='poly', rotate = \"varimax\")\r\nfa.out\r\nFactor Analysis using method =  minres\r\nCall: fa(r = dep[, 2:11], nfactors = 2, rotate = \"varimax\", cor = \"poly\")\r\nStandardized loadings (pattern matrix) based upon correlation matrix\r\n        MR2  MR1   h2   u2 com\r\nDPQ010 0.40 0.53 0.44 0.56 1.9\r\nDPQ020 0.71 0.43 0.69 0.31 1.6\r\nDPQ030 0.17 0.54 0.32 0.68 1.2\r\nDPQ040 0.24 0.52 0.32 0.68 1.4\r\nDPQ050 0.31 0.49 0.34 0.66 1.7\r\nDPQ060 0.81 0.31 0.75 0.25 1.3\r\nDPQ070 0.41 0.56 0.49 0.51 1.8\r\nDPQ080 0.38 0.63 0.54 0.46 1.6\r\nDPQ090 0.71 0.34 0.63 0.37 1.4\r\nDPQ100 0.53 0.63 0.68 0.32 1.9\r\n\r\n                       MR2  MR1\r\nSS loadings           2.60 2.58\r\nProportion Var        0.26 0.26\r\nCumulative Var        0.26 0.52\r\nProportion Explained  0.50 0.50\r\nCumulative Proportion 0.50 1.00\r\n\r\nMean item complexity =  1.6\r\nTest of the hypothesis that 2 factors are sufficient.\r\n\r\nThe degrees of freedom for the null model are  45  and the objective function was  4.72 with Chi Square of  17223.44\r\nThe degrees of freedom for the model are 26  and the objective function was  0.14 \r\n\r\nThe root mean square of the residuals (RMSR) is  0.03 \r\nThe df corrected root mean square of the residuals is  0.04 \r\n\r\nThe harmonic number of observations is  3651 with the empirical chi square  246.89  with prob <  7.1e-38 \r\nThe total number of observations was  3651  with Likelihood Chi Square =  524.04  with prob <  3.7e-94 \r\n\r\nTucker Lewis Index of factoring reliability =  0.95\r\nRMSEA index =  0.072  and the 90 % confidence intervals are  0.067 0.078\r\nBIC =  310.76\r\nFit based upon off diagonal values = 1\r\nMeasures of factor score adequacy             \r\n                                                   MR2  MR1\r\nCorrelation of (regression) scores with factors   0.87 0.82\r\nMultiple R square of scores with factors          0.75 0.68\r\nMinimum correlation of possible factor scores     0.51 0.35\r\n\r\nSometimes, the orthogonal rotation did not work out. i.e. no variable is loading highly onto another factor.\r\n\r\nop <- par(mfrow=c(1,2))\r\ncluster.plot(fa.out,xlim=c(-1,1),ylim=c(-1,1),title=\"Unrotated \")\r\nf2r <- factor.rotate(fa.out,-65,plot=TRUE,xlim=c(-1,1),ylim=c(-1,1),title=\"rotated -65 degrees\")\r\n\r\n\r\nfactor.rotate is useful for those cases that require specific rotations that are not available in more advanced packages such as GPArotation\r\nComparison of EFA and CFA\r\n\r\nEFA\r\nCFA\r\nWhat gets analyzed\r\n\r\n\r\n\r\nCorrelation matrix (of items = indicators)\r\nCovariance matrix (of items = indicators)\r\n\r\n➢ Only correlations among observed item responses are used ➢ Only a standardized solution is provided, so the original means and variances of the observed item responses are irrelevant\r\n➢ Means, variances, and covariances of item responses are analyzed➢ Item response means historically have been ignored➢Output includes unstandardized AND standardized solutions\r\nInterpretation\r\n\r\n\r\n\r\nRotation\r\nDefining interpretation\r\n\r\n➢ All items load on all factors (latent traits)➢ Goal is to pick a rotation that gives closest approximation to “simple structure”➢ No way of distinguishing latent variables due to traits being measured from correlation\r\n➢ CFA is theory-driven: any structure becomes a testable hypothesis➢ You specify number of latent factors and their inter-correlations➢You specify which items load on which latent factors ➢ You specify any additional relationships for method/other covariance➢ You just need a clue; you don’t have to be right (misfit is informative)\r\nModel Fit\r\n\r\n\r\n\r\nEye-balls and Opinion\r\nInferential tests via ML\r\n\r\n➢ #Factors? Scree plots, interpretability➢ Which rotation? ➢ Which items load on each factor? Arbitrary cutoff of .3-.4ish➢ Standard errors infrequently used)\r\n➢ Global model fit test (and local)➢Standard errors (and significance tests) of item loadings, error variances, and error covariances (all parameters)➢ Ability to test appropriateness of model constraints or model additions via tests for change in model fit\r\nFactor scores\r\n\r\n\r\n\r\nDon’t ever use factor scores from an EFA\r\nFactor scores can be used, but only if necessary\r\n\r\n➢ Factor scores are indeterminate (especially due to rotation) ➢ Inconsistency in how factor models are applied to data\r\nBest option: Test relations among latent factors directly through SEM➢ Factor scores are less indeterminate in CFA, and could be used\r\n",
    "preview": "posts/2022-09-15-Factor Analysis/distill-preview.png",
    "last_modified": "2023-02-23T00:16:33-06:00",
    "input_file": "Exploratory-or-Confirmatory-Factor-Analysis.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-06-20-SEM/",
    "title": "Structural Equation Modeling: What Fun if We Cannot Run in Mplus",
    "description": "The introduction composed on Lecture Notes from authors: Michael Zyphur, Lesa Hoffman and Johnny Lin;    \nReview of SEM;    \nRead correlation/covariance matrix as input in Mplus & lavaan (R);    \nRun SEM in Mplus & lavaan (R);    \nModel fit indices",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-06-20",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "SEM",
      "Mplus",
      "lavaan (R)",
      "Model Fit Statistics"
    ],
    "contents": "\r\n\r\nContents\r\nFirst build\r\nDemonstation\r\n1\r\nDemonstation\r\n2\r\n\r\nBe aware of 2 types of path\r\nmodels\r\nConfusing terminolgy\r\nSteps in Model Building\r\nPractical Steps: 2 Step\r\nPractical Steps: 4 Step\r\n\r\nAn example\r\nSet up Mplus in Rmarkdown\r\nRun Mplus in Rmarkdown\r\nRun lavaan (R)\r\nConvert correlation to\r\ncovariance matrix\r\nLoad lavaan\r\nRead covariance as input\r\nModel fit\r\nFit\r\nstatistics\r\n\r\n\r\nModel\r\nFit Statistics\r\nModel\r\nchi-square\r\nCFI, TLI\r\nRMSEA, SRMR\r\n\r\n\r\n\r\nSummary\r\n\r\nSpecification (estimating in our models) \\(\\Rightarrow\\) Measurement model\r\nspecification (relationships among observed and latent variables,\r\nincluding variances and covariance) \\(\\Rightarrow\\) Structural model\r\nspecification (classic regression relationships among (latent) variables\r\nof substantive interest)\r\nSEM = measurement model + structural model\r\nSpecify # of factors and factor loadings with “BY” or\r\n=~\r\nSpecify regression/covariance with “ON” or ~  and\r\n“WITH” or ~~\r\n\r\nSEM solves two regression problems:\r\nSimple structural models (path analysis)\r\nVariables measured with error (CFA)\r\n\r\nFirst build\r\nQuick reference of lavaan syntax (Web Page (2021))\r\nQuick reference of\r\nlavaan syntax\r\nIntroduce some of the most frequently used syntax in\r\nlavaan\r\n~ predict, used for regression of observed outcome\r\nto observed predictors (e.g., y ~ x)\r\n=~ indicator, used for latent variable to observed\r\nindicator in factor analysis measurement models (e.g., f =~ q + r +\r\ns)\r\n~~ covariance (e.g., x ~~ x)\r\n~1 intercept or mean (e.g., x ~ 1 estimates the\r\nmean of variable x)\r\n1* fixes parameter or loading to one (e.g., f =~\r\n1*q)\r\nNA* frees parameter or loading (useful to override\r\ndefault marker method, (e.g., f =~ NA*q)\r\na* labels the parameter ‘a’, used for model\r\nconstraints (e.g., f =~ a*q)\r\nExtract from lecture note Web Page (2019)Demonstation 1\r\nModel in Mplus:\r\nf1 BY y1-y3;\r\nf2 BY y4-y6;\r\nf2 ON f1;\r\nModel in lavaan (R):\r\nf1 =~ y1+y2+y3\r\nf2 =~ y4+y5+y6\r\nf2 ~ f1\r\nDemonstation 2\r\nExtract from lecture note Web Page (2022)Model in Mplus:\r\n! Required code to estimate regression\r\npaths\r\nD ON A B;\r\nE ON D C;\r\nF ON E;! Outcome intercepts and residual variances\r\nestimated by default\r\n[D E F]; D E! To bring A, B, and C predictors into the\r\nlikelihood,! Request their covariances\r\nA B C WITH A B C;! Predictor means and variances then estimated\r\nby default\r\n[A B C]; A B C;\r\nModel in lavaan (R):\r\nD ~ A+B\r\nE ~ D+C\r\nF ~ E\r\nD ~ 1; D ~~ D\r\nA B C ~~ A B C\r\nA ~ 1; A~~A\r\nBe aware of 2 types of path\r\nmodels\r\nUnstandardized \\(\\rightarrow\\)\r\npredicts scale sensitive original variables:\r\nUseful for comparing across groups (whenever absolute values\r\nmatter)\r\nParameters predict the variables’ means, variance s, and\r\ncovariances\r\n\r\nStandardized \\(\\rightarrow\\)\r\npredicts 𝑧 scored versions of variables\r\nUseful when comparing effects within a solution (are then on same\r\nscale)\r\nModel parameters predict the variables’ correlations\r\n\r\nRefer more in Web Page (2022)\r\nConfusing terminolgy\r\nPredictors are known as exogenous\r\nvariables\r\nOutcomes are known as endogenous\r\nvariables\r\nVariables that are both at once are called\r\nendogenous variables\r\n\r\nHow set up in Mplus to run\r\nFIML:\r\n\r\nTrick in Mplus\r\nBy default in Mplus , truly exogenous\r\npredictor variables cannot have missing data, the same as in any\r\ngeneral(ized) linear model\r\nCases with missing predictors are listwise deleted\r\n(incomplete data then are assumed missing completely at random ), no\r\nmatter which\r\nBecause truly exogenous predictors are not part of\r\nlikelihood function + Log likelihood (LL) contains \\(\\hat{y}_i\\) for each person and \\(\\sigma^2_e\\) for each outcome\r\nSo LL can’t be calculated without the predictors that create each\r\n\\(\\hat{y}_i\\)\r\n\r\n\r\nBut truly exogenous predictors also do not have assumed\r\ndistributions\r\nGood when you have non normally distributed predictors (e.g.,\r\nANOVA)!\r\n\r\nWe thought full information ML allows missing data???\r\nNO: only endogenous outcomes can be incomplete (then assumed missing\r\nat random, which means random only after conditioning on model\r\nvariables\r\nBTW, we can add other variables into the likelihood but not the\r\nmodel to help (untestable) missing at random assumption using AUXILIARY\r\noption\r\nIs a “saturated correlates” approach (they just covary with all\r\noutcomes)\r\n\r\n\r\nMplus allows a work around: you can bring exogenous\r\npredictors into the likelihood by listing their means,\r\nvariances, or covariances as parameters \\(\\rightarrow\\) predictors then\r\nbecome “outcomes”\r\nEven if nothing predicts the predictor (i.e., it’s not really an\r\noutcome)\r\nIncomplete “endogenous predictors” can be included assuming missing\r\nat random (MAR), but they also then have distributional assumptions\r\n(MNV)\r\nHistorically Mplus has not let endogenous predictors have other\r\ndistributions, so you may have to make non normal predictors an outcome\r\nof something else\r\nBut there may be ways to trick it in doing this that I haven’t found\r\nyet\r\n\r\n\r\nSteps in Model Building\r\nPractical Steps: 2 Step\r\n2-step (Anderson and\r\nGerbing (1988))\r\nCFA with “fully-saturated” covariance structure\r\nAllow all latent variables to freely covary (i.e., only WITH, no\r\nON)\r\nRemoves structural model from estimation of fit\r\nMisfit can only be due to measurement model problems\r\n\r\nObtain fit to evaluate measurement model\r\nEstimate structural model with regression\r\nObtain fit & contrast with previous model\r\nChanges in fit due to structural model misfit\r\n\r\nPractical Steps: 4 Step\r\n4-step (Mulaik and Millsap\r\n(2000), Hayduk and Glaser (2000), and Bollen (2000))\r\nExploratory search for # of factors\r\nCFA with fully-saturated covariance structure\r\nEstimate structural model with regression\r\nTest nested/alternative models\r\nAn example\r\nMathieu and Farr (1991) article about\r\n“Further evidence for the discriminant validity of measures of\r\norganizational commitment, job involvement, and job\r\nsatisfaction”\r\nMeasured organizational commitment, performance, job involvement,\r\nand job satisfaction and other variables\r\nShow discriminant validity with good-fitting CFA that specifies each\r\nas a distinct latent variable\r\nThen show non-huge inter-correlations among factors\r\nData (Means – Stdeviations – Correlation) from engineers is used, in\r\nAppendix A\r\n\r\nprint(\"Mathieu & Farr 1991.txt\")\r\n[1] \"Mathieu & Farr 1991.txt\"\r\nwriteLines(readLines(\"Mathieu & Farr 1991.txt\"))\r\n5.58    4.89    3.76    4.35    3.82    3.87    5.19    5.07    5.14    3.32    3.44    3.14    6.7 6.29    6.69    6.45    6.25    6.69    1.6 5.95    11.3    37.32\r\n.89 1   1.11    1.1 1.31    1.11    .86 .88 .89 .43 .4  .41 .85 .92 .77 1.11    1.11    1.09    1.05    6.18    8.62    9.92\r\n1\r\n.76 1\r\n.67 .68 1\r\n.26 .2  .22 1\r\n.28 .25 .17 .43 1\r\n.34 .33 .26 .52 .52 1\r\n.57 .57 .55 .09 .2  .21 1\r\n.55 .53 .55 .16 .2  .21 .84 1\r\n.48 .46 .49 .15 .23 .21 .79 .71 1\r\n.36 .31 .31 .14 .13 .16 .46 .42 .48 1\r\n.42 .36 .38 .15 .12 .14 .51 .5  .5  .7  1\r\n.32 .34 .37 .11 .13 .13 .47 .46 .46 .65 .64 1\r\n.21 .19 .13 .14 .18 .15 .31 .29 .31 .32 .5  .1  1\r\n.21 .17 .1  .14 .16 .15 .25 .23 .29 .37 .25 .17 .81 1\r\n.22 .18 .12 .15 .14 .12 .33 .29 .31 .39 .29 .15 .82 .82 1\r\n.15 .13 .02 .23 .13 .18 .09 .07 .04 .19 .16 .08 .23 .15 .14 1\r\n.16 .09 -.01    .17 .07 .05 .09 .09 .07 .25 .24 .16 .22 .28 .19 .59 1\r\n.14 .09 -.01    .13 .04 .06 .07 .05 .07 .2  .17 .09 .22 .19 .23 .59 .62 1\r\n.06 .09 .01 .07 .11 .16 .09 .1  .04 .14 .04 .05 .2  .19 .15 .15 .17 .15 1\r\n.03 .06 .1  .17 .19 .17 .02 .01 .1  .15 .04 .09 .03 .05 .06 .1  .16 .07 .1  1\r\n.1      .09 .1  .1  .16 .15 -.001   .02 .04 .12 .05 -.01    .03 .03 .05 .001    .18 .02 .05 .52 1\r\n.14 .1  .12 .07 .15 .17 -.01    .01 -.001   .12 .03 .06 .1  .01 .12 .06 .18 .04 .07 .46 .8  1\r\n\r\nExtract from lecture note (Web Page (2019))Set up Mplus in Rmarkdown\r\n\r\nknitr::opts_chunk$set(engine.path = list(\r\n  mplus = \"C:/Program Files/Mplus/Mplus\"\r\n))\r\n\r\n\r\nknitr::knit_engines$set(mplus = function(options) {\r\n    code <- paste(options$code, collapse = \"\\n\")\r\n    fileConn<-file(\"formplus.inp\")\r\n    writeLines(code, fileConn)\r\n    close(fileConn)\r\n    out  <- system2(\"C:/Program Files/Mplus/Mplus\", \"formplus.inp\")\r\n    fileConnOutput <- file(\"formplus.out\")\r\n    mplusOutput <- readLines(fileConnOutput)\r\n    knitr::engine_output(options, code, mplusOutput)\r\n})\r\n\r\nRun Mplus in Rmarkdown\r\n\r\nTitle: CFA Example\r\n\r\nData: File is Mathieu & Farr 1991.txt;\r\n\r\nType is Means Stdeviations Correlation;\r\n\r\nNobservations = 483;\r\n\r\nVariable:\r\nNames are \r\nOC1 OC2 OC3 ! Organizational commitment\r\nJI1 JI2 JI3 ! Job involvement\r\nSAT1 SAT2 SAT3 ! Job satisfaction\r\nJS1 JS2 JS3 ! Job scope\r\nSELF1 SELF2 SELF3 ! Self ratings of performance\r\nSUPR1 SUPR2 SUPR3 ! Supervisor ratings of performance\r\nEducation\r\nPostTen ! Position tenure\r\nOrgTen ! Organizational tenure\r\nAge;\r\n\r\nUseVariables are OC1-OC3 SAT1-SAT3 \r\nJS1-JS3 SUPR1-SUPR3 Education;\r\n\r\nAnalysis:\r\n\r\nModel:\r\nOC by OC1 OC2 OC3;\r\nSAT by SAT1 SAT2 SAT3;\r\nJS by JS1 JS2 JS3;\r\nPERF by SUPR1 SUPR2 SUPR3;\r\n\r\nPERF on OC SAT JS Education;\r\nOC on SAT; \r\nSAT on JS;\r\n\r\nOutput: Tech1 Tech8 standardized sampstat;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n02/25/2023  12:18 AM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  Title: CFA Example\r\n\r\n  Data: File is Mathieu & Farr 1991.txt;\r\n\r\n  Type is Means Stdeviations Correlation;\r\n\r\n  Nobservations = 483;\r\n\r\n  Variable:\r\n  Names are\r\n  OC1 OC2 OC3 ! Organizational commitment\r\n  JI1 JI2 JI3 ! Job involvement\r\n  SAT1 SAT2 SAT3 ! Job satisfaction\r\n  JS1 JS2 JS3 ! Job scope\r\n  SELF1 SELF2 SELF3 ! Self ratings of performance\r\n  SUPR1 SUPR2 SUPR3 ! Supervisor ratings of performance\r\n  Education\r\n  PostTen ! Position tenure\r\n  OrgTen ! Organizational tenure\r\n  Age;\r\n\r\n  UseVariables are OC1-OC3 SAT1-SAT3\r\n  JS1-JS3 SUPR1-SUPR3 Education;\r\n\r\n  Analysis:\r\n\r\n  Model:\r\n  OC by OC1 OC2 OC3;\r\n  SAT by SAT1 SAT2 SAT3;\r\n  JS by JS1 JS2 JS3;\r\n  PERF by SUPR1 SUPR2 SUPR3;\r\n\r\n  PERF on OC SAT JS Education;\r\n  OC on SAT;\r\n  SAT on JS;\r\n\r\n  Output: Tech1 Tech8 standardized sampstat;\r\n\r\n\r\n\r\n*** WARNING in VARIABLE command\r\n  Note that only the first 8 characters of variable names are used in the output.\r\n  Shorten variable names to avoid any confusion.\r\n*** WARNING in OUTPUT command\r\n  TECH8 option is available only with analysis types MIXTURE, RANDOM, or\r\n  TWOLEVEL with estimators ML, MLF, or MLR or ALGORITHM=INTEGRATION.\r\n  Request for TECH8 is ignored.\r\n   2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\nCFA Example\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         483\r\n\r\nNumber of dependent variables                                   12\r\nNumber of independent variables                                  1\r\nNumber of continuous latent variables                            4\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   OC1         OC2         OC3         SAT1        SAT2        SAT3\r\n   JS1         JS2         JS3         SUPR1       SUPR2       SUPR3\r\n\r\nObserved independent variables\r\n   EDUCATIO\r\n\r\nContinuous latent variables\r\n   OC          SAT         JS          PERF\r\n\r\n\r\nEstimator                                                       ML\r\nInformation matrix                                        EXPECTED\r\nMaximum number of iterations                                  1000\r\nConvergence criterion                                    0.500D-04\r\nMaximum number of steepest descent iterations                   20\r\n\r\nInput data file(s)\r\n  Mathieu & Farr 1991.txt\r\n\r\nInput data format  FREE\r\n\r\n\r\nSAMPLE STATISTICS\r\n\r\n     SAMPLE STATISTICS\r\n\r\n           Means/Intercepts/Thresholds\r\n              OC1           OC2           OC3           SAT1          SAT2\r\n              ________      ________      ________      ________      ________\r\n                5.580         4.890         3.760         5.190         5.070\r\n\r\n           Means/Intercepts/Thresholds\r\n              SAT3          JS1           JS2           JS3           SUPR1\r\n              ________      ________      ________      ________      ________\r\n                5.140         3.320         3.440         3.140         6.450\r\n\r\n           Means/Intercepts/Thresholds\r\n              SUPR2         SUPR3         EDUCATIO\r\n              ________      ________      ________\r\n                6.250         6.690         1.600\r\n\r\n           Covariances/Correlations/Residual Correlations\r\n              OC1           OC2           OC3           SAT1          SAT2\r\n              ________      ________      ________      ________      ________\r\n OC1            0.792\r\n OC2            0.676         1.000\r\n OC3            0.662         0.755         1.232\r\n SAT1           0.436         0.490         0.525         0.740\r\n SAT2           0.431         0.466         0.537         0.636         0.774\r\n SAT3           0.380         0.409         0.484         0.605         0.556\r\n JS1            0.138         0.133         0.148         0.170         0.159\r\n JS2            0.150         0.144         0.169         0.175         0.176\r\n JS3            0.117         0.139         0.168         0.166         0.166\r\n SUPR1          0.148         0.144         0.025         0.086         0.068\r\n SUPR2          0.158         0.100        -0.012         0.086         0.088\r\n SUPR3          0.136         0.098        -0.012         0.066         0.048\r\n EDUCATIO       0.056         0.095         0.012         0.081         0.092\r\n\r\n           Covariances/Correlations/Residual Correlations\r\n              SAT3          JS1           JS2           JS3           SUPR1\r\n              ________      ________      ________      ________      ________\r\n SAT3           0.792\r\n JS1            0.184         0.185\r\n JS2            0.178         0.120         0.160\r\n JS3            0.168         0.115         0.105         0.168\r\n SUPR1          0.040         0.091         0.071         0.036         1.232\r\n SUPR2          0.069         0.119         0.107         0.073         0.727\r\n SUPR3          0.068         0.094         0.074         0.040         0.714\r\n EDUCATIO       0.037         0.063         0.017         0.022         0.175\r\n\r\n           Covariances/Correlations/Residual Correlations\r\n              SUPR2         SUPR3         EDUCATIO\r\n              ________      ________      ________\r\n SUPR2          1.232\r\n SUPR3          0.750         1.188\r\n EDUCATIO       0.198         0.172         1.103\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       42\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                       -5138.829\r\n          H1 Value                       -5075.557\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                   10361.657\r\n          Bayesian (BIC)                 10537.218\r\n          Sample-Size Adjusted BIC       10403.913\r\n            (n* = (n + 2) / 24)\r\n\r\nChi-Square Test of Model Fit\r\n\r\n          Value                            126.544\r\n          Degrees of Freedom                    60\r\n          P-Value                           0.0000\r\n\r\nRMSEA (Root Mean Square Error Of Approximation)\r\n\r\n          Estimate                           0.048\r\n          90 Percent C.I.                    0.036  0.060\r\n          Probability RMSEA <= .05           0.599\r\n\r\nCFI/TLI\r\n\r\n          CFI                                0.981\r\n          TLI                                0.976\r\n\r\nChi-Square Test of Model Fit for the Baseline Model\r\n\r\n          Value                           3614.872\r\n          Degrees of Freedom                    78\r\n          P-Value                           0.0000\r\n\r\nSRMR (Standardized Root Mean Square Residual)\r\n\r\n          Value                              0.040\r\n\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n OC       BY\r\n    OC1                1.000      0.000    999.000    999.000\r\n    OC2                1.124      0.049     22.934      0.000\r\n    OC3                1.128      0.056     20.156      0.000\r\n\r\n SAT      BY\r\n    SAT1               1.000      0.000    999.000    999.000\r\n    SAT2               0.949      0.031     30.970      0.000\r\n    SAT3               0.900      0.034     26.719      0.000\r\n\r\n JS       BY\r\n    JS1                1.000      0.000    999.000    999.000\r\n    JS2                0.950      0.048     19.671      0.000\r\n    JS3                0.886      0.049     17.969      0.000\r\n\r\n PERF     BY\r\n    SUPR1              1.000      0.000    999.000    999.000\r\n    SUPR2              1.071      0.073     14.751      0.000\r\n    SUPR3              1.028      0.070     14.666      0.000\r\n\r\n PERF     ON\r\n    OC                 0.115      0.084      1.370      0.171\r\n    SAT               -0.207      0.093     -2.214      0.027\r\n    JS                 0.800      0.173      4.620      0.000\r\n\r\n OC       ON\r\n    SAT                0.667      0.041     16.421      0.000\r\n\r\n SAT      ON\r\n    JS                 1.471      0.107     13.733      0.000\r\n\r\n PERF     ON\r\n    EDUCATION          0.145      0.038      3.781      0.000\r\n\r\n Intercepts\r\n    OC1                5.580      0.040    137.933      0.000\r\n    OC2                4.890      0.045    107.581      0.000\r\n    OC3                3.760      0.050     74.523      0.000\r\n    SAT1               5.190      0.039    132.768      0.000\r\n    SAT2               5.070      0.040    126.751      0.000\r\n    SAT3               5.140      0.040    127.057      0.000\r\n    JS1                3.320      0.020    169.861      0.000\r\n    JS2                3.440      0.018    189.200      0.000\r\n    JS3                3.140      0.019    168.488      0.000\r\n    SUPR1              6.219      0.079     78.778      0.000\r\n    SUPR2              6.002      0.082     73.107      0.000\r\n    SUPR3              6.452      0.080     81.098      0.000\r\n\r\n Variances\r\n    JS                 0.127      0.012     10.489      0.000\r\n\r\n Residual Variances\r\n    OC1                0.195      0.021      9.514      0.000\r\n    OC2                0.246      0.026      9.479      0.000\r\n    OC3                0.473      0.038     12.491      0.000\r\n    SAT1               0.072      0.012      6.089      0.000\r\n    SAT2               0.173      0.015     11.389      0.000\r\n    SAT3               0.250      0.019     13.186      0.000\r\n    JS1                0.058      0.006     10.172      0.000\r\n    JS2                0.046      0.005      9.427      0.000\r\n    JS3                0.068      0.006     12.061      0.000\r\n    SUPR1              0.547      0.050     11.028      0.000\r\n    SUPR2              0.446      0.049      9.106      0.000\r\n    SUPR3              0.464      0.047      9.814      0.000\r\n    OC                 0.298      0.029     10.122      0.000\r\n    SAT                0.392      0.033     11.982      0.000\r\n    PERF               0.595      0.071      8.417      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.496E-03\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nSTANDARDIZED MODEL RESULTS\r\n\r\n\r\nSTDYX Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n OC       BY\r\n    OC1                0.868      0.016     53.099      0.000\r\n    OC2                0.868      0.016     53.248      0.000\r\n    OC3                0.784      0.021     37.284      0.000\r\n\r\n SAT      BY\r\n    SAT1               0.950      0.009    106.178      0.000\r\n    SAT2               0.881      0.013     69.396      0.000\r\n    SAT3               0.827      0.016     50.580      0.000\r\n\r\n JS       BY\r\n    JS1                0.828      0.020     41.273      0.000\r\n    JS2                0.845      0.019     43.921      0.000\r\n    JS3                0.770      0.023     33.257      0.000\r\n\r\n PERF     BY\r\n    SUPR1              0.743      0.028     26.563      0.000\r\n    SUPR2              0.797      0.026     30.319      0.000\r\n    SUPR3              0.779      0.027     29.027      0.000\r\n\r\n PERF     ON\r\n    OC                 0.108      0.078      1.377      0.169\r\n    SAT               -0.205      0.092     -2.238      0.025\r\n    JS                 0.346      0.070      4.912      0.000\r\n\r\n OC       ON\r\n    SAT                0.706      0.028     25.614      0.000\r\n\r\n SAT      ON\r\n    JS                 0.641      0.032     19.908      0.000\r\n\r\n PERF     ON\r\n    EDUCATION          0.185      0.047      3.893      0.000\r\n\r\n Intercepts\r\n    OC1                6.276      0.207     30.320      0.000\r\n    OC2                4.895      0.164     29.859      0.000\r\n    OC3                3.391      0.118     28.686      0.000\r\n    SAT1               6.041      0.200     30.262      0.000\r\n    SAT2               5.767      0.191     30.186      0.000\r\n    SAT3               5.781      0.191     30.190      0.000\r\n    JS1                7.729      0.253     30.573      0.000\r\n    JS2                8.609      0.281     30.669      0.000\r\n    JS3                7.666      0.251     30.565      0.000\r\n    SUPR1              5.623      0.206     27.316      0.000\r\n    SUPR2              5.429      0.202     26.917      0.000\r\n    SUPR3              5.943      0.217     27.434      0.000\r\n\r\n Variances\r\n    JS                 1.000      0.000    999.000    999.000\r\n\r\n Residual Variances\r\n    OC1                0.247      0.028      8.723      0.000\r\n    OC2                0.246      0.028      8.694      0.000\r\n    OC3                0.385      0.033     11.653      0.000\r\n    SAT1               0.098      0.017      5.745      0.000\r\n    SAT2               0.224      0.022     10.002      0.000\r\n    SAT3               0.317      0.027     11.730      0.000\r\n    JS1                0.314      0.033      9.447      0.000\r\n    JS2                0.285      0.033      8.760      0.000\r\n    JS3                0.408      0.036     11.441      0.000\r\n    SUPR1              0.447      0.042     10.750      0.000\r\n    SUPR2              0.365      0.042      8.714      0.000\r\n    SUPR3              0.394      0.042      9.426      0.000\r\n    OC                 0.501      0.039     12.872      0.000\r\n    SAT                0.589      0.041     14.243      0.000\r\n    PERF               0.881      0.034     25.794      0.000\r\n\r\n\r\nSTDY Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n OC       BY\r\n    OC1                0.868      0.016     53.099      0.000\r\n    OC2                0.868      0.016     53.248      0.000\r\n    OC3                0.784      0.021     37.284      0.000\r\n\r\n SAT      BY\r\n    SAT1               0.950      0.009    106.178      0.000\r\n    SAT2               0.881      0.013     69.396      0.000\r\n    SAT3               0.827      0.016     50.580      0.000\r\n\r\n JS       BY\r\n    JS1                0.828      0.020     41.273      0.000\r\n    JS2                0.845      0.019     43.921      0.000\r\n    JS3                0.770      0.023     33.257      0.000\r\n\r\n PERF     BY\r\n    SUPR1              0.743      0.028     26.563      0.000\r\n    SUPR2              0.797      0.026     30.319      0.000\r\n    SUPR3              0.779      0.027     29.027      0.000\r\n\r\n PERF     ON\r\n    OC                 0.108      0.078      1.377      0.169\r\n    SAT               -0.205      0.092     -2.238      0.025\r\n    JS                 0.346      0.070      4.912      0.000\r\n\r\n OC       ON\r\n    SAT                0.706      0.028     25.614      0.000\r\n\r\n SAT      ON\r\n    JS                 0.641      0.032     19.908      0.000\r\n\r\n PERF     ON\r\n    EDUCATION          0.176      0.045      3.921      0.000\r\n\r\n Intercepts\r\n    OC1                6.276      0.207     30.320      0.000\r\n    OC2                4.895      0.164     29.859      0.000\r\n    OC3                3.391      0.118     28.686      0.000\r\n    SAT1               6.041      0.200     30.262      0.000\r\n    SAT2               5.767      0.191     30.186      0.000\r\n    SAT3               5.781      0.191     30.190      0.000\r\n    JS1                7.729      0.253     30.573      0.000\r\n    JS2                8.609      0.281     30.669      0.000\r\n    JS3                7.666      0.251     30.565      0.000\r\n    SUPR1              5.623      0.206     27.316      0.000\r\n    SUPR2              5.429      0.202     26.917      0.000\r\n    SUPR3              5.943      0.217     27.434      0.000\r\n\r\n Variances\r\n    JS                 1.000      0.000    999.000    999.000\r\n\r\n Residual Variances\r\n    OC1                0.247      0.028      8.723      0.000\r\n    OC2                0.246      0.028      8.694      0.000\r\n    OC3                0.385      0.033     11.653      0.000\r\n    SAT1               0.098      0.017      5.745      0.000\r\n    SAT2               0.224      0.022     10.002      0.000\r\n    SAT3               0.317      0.027     11.730      0.000\r\n    JS1                0.314      0.033      9.447      0.000\r\n    JS2                0.285      0.033      8.760      0.000\r\n    JS3                0.408      0.036     11.441      0.000\r\n    SUPR1              0.447      0.042     10.750      0.000\r\n    SUPR2              0.365      0.042      8.714      0.000\r\n    SUPR3              0.394      0.042      9.426      0.000\r\n    OC                 0.501      0.039     12.872      0.000\r\n    SAT                0.589      0.041     14.243      0.000\r\n    PERF               0.881      0.034     25.794      0.000\r\n\r\n\r\nSTD Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n OC       BY\r\n    OC1                0.771      0.034     22.930      0.000\r\n    OC2                0.867      0.038     22.957      0.000\r\n    OC3                0.870      0.044     19.799      0.000\r\n\r\n SAT      BY\r\n    SAT1               0.816      0.030     27.461      0.000\r\n    SAT2               0.775      0.032     24.210      0.000\r\n    SAT3               0.735      0.034     21.915      0.000\r\n\r\n JS       BY\r\n    JS1                0.356      0.017     20.978      0.000\r\n    JS2                0.338      0.016     21.597      0.000\r\n    JS3                0.315      0.017     18.955      0.000\r\n\r\n PERF     BY\r\n    SUPR1              0.822      0.048     17.215      0.000\r\n    SUPR2              0.881      0.047     18.653      0.000\r\n    SUPR3              0.845      0.047     18.158      0.000\r\n\r\n PERF     ON\r\n    OC                 0.108      0.078      1.377      0.169\r\n    SAT               -0.205      0.092     -2.238      0.025\r\n    JS                 0.346      0.070      4.912      0.000\r\n\r\n OC       ON\r\n    SAT                0.706      0.028     25.614      0.000\r\n\r\n SAT      ON\r\n    JS                 0.641      0.032     19.908      0.000\r\n\r\n PERF     ON\r\n    EDUCATION          0.176      0.045      3.921      0.000\r\n\r\n Intercepts\r\n    OC1                5.580      0.040    137.933      0.000\r\n    OC2                4.890      0.045    107.581      0.000\r\n    OC3                3.760      0.050     74.523      0.000\r\n    SAT1               5.190      0.039    132.768      0.000\r\n    SAT2               5.070      0.040    126.751      0.000\r\n    SAT3               5.140      0.040    127.057      0.000\r\n    JS1                3.320      0.020    169.861      0.000\r\n    JS2                3.440      0.018    189.200      0.000\r\n    JS3                3.140      0.019    168.488      0.000\r\n    SUPR1              6.219      0.079     78.778      0.000\r\n    SUPR2              6.002      0.082     73.107      0.000\r\n    SUPR3              6.452      0.080     81.098      0.000\r\n\r\n Variances\r\n    JS                 1.000      0.000    999.000    999.000\r\n\r\n Residual Variances\r\n    OC1                0.195      0.021      9.514      0.000\r\n    OC2                0.246      0.026      9.479      0.000\r\n    OC3                0.473      0.038     12.491      0.000\r\n    SAT1               0.072      0.012      6.089      0.000\r\n    SAT2               0.173      0.015     11.389      0.000\r\n    SAT3               0.250      0.019     13.186      0.000\r\n    JS1                0.058      0.006     10.172      0.000\r\n    JS2                0.046      0.005      9.427      0.000\r\n    JS3                0.068      0.006     12.061      0.000\r\n    SUPR1              0.547      0.050     11.028      0.000\r\n    SUPR2              0.446      0.049      9.106      0.000\r\n    SUPR3              0.464      0.047      9.814      0.000\r\n    OC                 0.501      0.039     12.872      0.000\r\n    SAT                0.589      0.041     14.243      0.000\r\n    PERF               0.881      0.034     25.794      0.000\r\n\r\n\r\nR-SQUARE\r\n\r\n    Observed                                        Two-Tailed\r\n    Variable        Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n    OC1                0.753      0.028     26.549      0.000\r\n    OC2                0.754      0.028     26.624      0.000\r\n    OC3                0.615      0.033     18.642      0.000\r\n    SAT1               0.902      0.017     53.089      0.000\r\n    SAT2               0.776      0.022     34.698      0.000\r\n    SAT3               0.683      0.027     25.290      0.000\r\n    JS1                0.686      0.033     20.636      0.000\r\n    JS2                0.715      0.033     21.961      0.000\r\n    JS3                0.592      0.036     16.629      0.000\r\n    SUPR1              0.553      0.042     13.282      0.000\r\n    SUPR2              0.635      0.042     15.159      0.000\r\n    SUPR3              0.606      0.042     14.514      0.000\r\n\r\n     Latent                                         Two-Tailed\r\n    Variable        Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n    OC                 0.499      0.039     12.807      0.000\r\n    SAT                0.411      0.041      9.954      0.000\r\n    PERF               0.119      0.034      3.484      0.000\r\n\r\n\r\nTECHNICAL 1 OUTPUT\r\n\r\n     PARAMETER SPECIFICATION\r\n\r\n           NU\r\n              OC1           OC2           OC3           SAT1          SAT2\r\n              ________      ________      ________      ________      ________\r\n                  1             2             3             4             5\r\n\r\n           NU\r\n              SAT3          JS1           JS2           JS3           SUPR1\r\n              ________      ________      ________      ________      ________\r\n                  6             7             8             9            10\r\n\r\n           NU\r\n              SUPR2         SUPR3         EDUCATIO\r\n              ________      ________      ________\r\n                 11            12             0\r\n\r\n           LAMBDA\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n OC1                0             0             0             0             0\r\n OC2               13             0             0             0             0\r\n OC3               14             0             0             0             0\r\n SAT1               0             0             0             0             0\r\n SAT2               0            15             0             0             0\r\n SAT3               0            16             0             0             0\r\n JS1                0             0             0             0             0\r\n JS2                0             0            17             0             0\r\n JS3                0             0            18             0             0\r\n SUPR1              0             0             0             0             0\r\n SUPR2              0             0             0            19             0\r\n SUPR3              0             0             0            20             0\r\n EDUCATIO           0             0             0             0             0\r\n\r\n           THETA\r\n              OC1           OC2           OC3           SAT1          SAT2\r\n              ________      ________      ________      ________      ________\r\n OC1               21\r\n OC2                0            22\r\n OC3                0             0            23\r\n SAT1               0             0             0            24\r\n SAT2               0             0             0             0            25\r\n SAT3               0             0             0             0             0\r\n JS1                0             0             0             0             0\r\n JS2                0             0             0             0             0\r\n JS3                0             0             0             0             0\r\n SUPR1              0             0             0             0             0\r\n SUPR2              0             0             0             0             0\r\n SUPR3              0             0             0             0             0\r\n EDUCATIO           0             0             0             0             0\r\n\r\n           THETA\r\n              SAT3          JS1           JS2           JS3           SUPR1\r\n              ________      ________      ________      ________      ________\r\n SAT3              26\r\n JS1                0            27\r\n JS2                0             0            28\r\n JS3                0             0             0            29\r\n SUPR1              0             0             0             0            30\r\n SUPR2              0             0             0             0             0\r\n SUPR3              0             0             0             0             0\r\n EDUCATIO           0             0             0             0             0\r\n\r\n           THETA\r\n              SUPR2         SUPR3         EDUCATIO\r\n              ________      ________      ________\r\n SUPR2             31\r\n SUPR3              0            32\r\n EDUCATIO           0             0             0\r\n\r\n           ALPHA\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n                  0             0             0             0             0\r\n\r\n           BETA\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n OC                 0            33             0             0             0\r\n SAT                0             0            34             0             0\r\n JS                 0             0             0             0             0\r\n PERF              35            36            37             0            38\r\n EDUCATIO           0             0             0             0             0\r\n\r\n           PSI\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n OC                39\r\n SAT                0            40\r\n JS                 0             0            41\r\n PERF               0             0             0            42\r\n EDUCATIO           0             0             0             0             0\r\n\r\n     STARTING VALUES\r\n\r\n           NU\r\n              OC1           OC2           OC3           SAT1          SAT2\r\n              ________      ________      ________      ________      ________\r\n                5.580         4.890         3.760         5.190         5.070\r\n\r\n           NU\r\n              SAT3          JS1           JS2           JS3           SUPR1\r\n              ________      ________      ________      ________      ________\r\n                5.140         3.320         3.440         3.140         6.450\r\n\r\n           NU\r\n              SUPR2         SUPR3         EDUCATIO\r\n              ________      ________      ________\r\n                6.250         6.690         0.000\r\n\r\n           LAMBDA\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n OC1            1.000         0.000         0.000         0.000         0.000\r\n OC2            1.000         0.000         0.000         0.000         0.000\r\n OC3            1.000         0.000         0.000         0.000         0.000\r\n SAT1           0.000         1.000         0.000         0.000         0.000\r\n SAT2           0.000         1.000         0.000         0.000         0.000\r\n SAT3           0.000         1.000         0.000         0.000         0.000\r\n JS1            0.000         0.000         1.000         0.000         0.000\r\n JS2            0.000         0.000         1.000         0.000         0.000\r\n JS3            0.000         0.000         1.000         0.000         0.000\r\n SUPR1          0.000         0.000         0.000         1.000         0.000\r\n SUPR2          0.000         0.000         0.000         1.000         0.000\r\n SUPR3          0.000         0.000         0.000         1.000         0.000\r\n EDUCATIO       0.000         0.000         0.000         0.000         1.000\r\n\r\n           THETA\r\n              OC1           OC2           OC3           SAT1          SAT2\r\n              ________      ________      ________      ________      ________\r\n OC1            0.396\r\n OC2            0.000         0.500\r\n OC3            0.000         0.000         0.616\r\n SAT1           0.000         0.000         0.000         0.370\r\n SAT2           0.000         0.000         0.000         0.000         0.387\r\n SAT3           0.000         0.000         0.000         0.000         0.000\r\n JS1            0.000         0.000         0.000         0.000         0.000\r\n JS2            0.000         0.000         0.000         0.000         0.000\r\n JS3            0.000         0.000         0.000         0.000         0.000\r\n SUPR1          0.000         0.000         0.000         0.000         0.000\r\n SUPR2          0.000         0.000         0.000         0.000         0.000\r\n SUPR3          0.000         0.000         0.000         0.000         0.000\r\n EDUCATIO       0.000         0.000         0.000         0.000         0.000\r\n\r\n           THETA\r\n              SAT3          JS1           JS2           JS3           SUPR1\r\n              ________      ________      ________      ________      ________\r\n SAT3           0.396\r\n JS1            0.000         0.092\r\n JS2            0.000         0.000         0.080\r\n JS3            0.000         0.000         0.000         0.084\r\n SUPR1          0.000         0.000         0.000         0.000         0.616\r\n SUPR2          0.000         0.000         0.000         0.000         0.000\r\n SUPR3          0.000         0.000         0.000         0.000         0.000\r\n EDUCATIO       0.000         0.000         0.000         0.000         0.000\r\n\r\n           THETA\r\n              SUPR2         SUPR3         EDUCATIO\r\n              ________      ________      ________\r\n SUPR2          0.616\r\n SUPR3          0.000         0.594\r\n EDUCATIO       0.000         0.000         0.000\r\n\r\n           ALPHA\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n                0.000         0.000         0.000         0.000         1.600\r\n\r\n           BETA\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n OC             0.000         0.000         0.000         0.000         0.000\r\n SAT            0.000         0.000         0.000         0.000         0.000\r\n JS             0.000         0.000         0.000         0.000         0.000\r\n PERF           0.000         0.000         0.000         0.000         0.000\r\n EDUCATIO       0.000         0.000         0.000         0.000         0.000\r\n\r\n           PSI\r\n              OC            SAT           JS            PERF          EDUCATIO\r\n              ________      ________      ________      ________      ________\r\n OC             0.050\r\n SAT            0.000         0.050\r\n JS             0.000         0.000         0.050\r\n PERF           0.000         0.000         0.000         0.050\r\n EDUCATIO       0.000         0.000         0.000         0.000         1.100\r\n\r\n     Beginning Time:  00:18:17\r\n        Ending Time:  00:18:17\r\n       Elapsed Time:  00:00:00\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nRun lavaan (R)\r\nWe have no full dataset, but do have a sample mean, standard\r\ndeviation and correlation matrix. Due to lavaan\r\nsupports covariance matrix as input but not correlation matrix, we\r\nneed to convert correlation matrix to covariance matrix to fit your\r\nmodel.\r\nConvert correlation to\r\ncovariance matrix\r\nFirst, I run SAS\r\n(proc iml) for easier manipulate the correlation matrix transfer to\r\ncovariance matrix. However, we could use\r\nrcompanion::fullPTable() function in R to do the job.\r\n\r\n# in SAS\r\nPROC IML;\r\n/** convert correlation matrix to covariance matrix **/\r\nlR={1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .76 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .67 .68 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .26 .2      .22 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .28 .25 .17 .43 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .34 .33 .26 .52 .52 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .57 .57 .55 .09 .2      .21 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .55 .53 .55 .16 .2      .21 .84 1       .       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .48 .46 .49 .15 .23 .21 .79 .71 1       .       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .36 .31 .31 .14 .13 .16 .46 .42 .48 1       .       .       .       .       .       .       .       .       .       .       .       .,\r\n    .42 .36 .38 .15 .12 .14 .51 .5      .5      .7      1       .       .       .       .       .       .       .       .       .       .       .,\r\n    .32 .34 .37 .11 .13 .13 .47 .46 .46 .65 .64 1       .       .       .       .       .       .       .       .       .       .,\r\n    .21 .19 .13 .14 .18 .15 .31 .29 .31 .32 .5      .1      1       .       .       .       .       .       .       .       .       .,\r\n    .21 .17 .1      .14 .16 .15 .25 .23 .29 .37 .25 .17 .81 1       .       .       .       .       .       .       .       .,\r\n    .22 .18 .12 .15 .14 .12 .33 .29 .31 .39 .29 .15 .82 .82 1       .       .       .       .       .       .       .,\r\n    .15 .13 .02 .23 .13 .18 .09 .07 .04 .19 .16 .08 .23 .15 .14 1       .       .       .       .       .       .,\r\n    .16 .09 -.01    .17 .07 .05 .09 .09 .07 .25 .24 .16 .22 .28 .19 .59 1       .       .       .       .       .,\r\n    .14 .09 -.01    .13 .04 .06 .07 .05 .07 .2      .17 .09 .22 .19 .23 .59 .62 1       .       .       .       .,\r\n    .06 .09 .01 .07 .11 .16 .09 .1      .04 .14 .04 .05 .2      .19 .15 .15 .17 .15 1       .       .       .,\r\n    .03 .06 .1      .17 .19 .17 .02 .01 .1      .15 .04 .09 .03 .05 .06 .1      .16 .07 .1      1       .       .,\r\n    .1      .09 .1      .1      .16 .15 -.001   .02 .04 .12 .05 -.01    .03 .03 .05 .001    .18 .02 .05 .52 1       .,\r\n    .14 .1      .12 .07 .15 .17 -.01    .01 -.001   .12 .03 .06 .1      .01 .12 .06 .18 .04 .07 .46 .8      1 };\r\n\r\np=ncol(lR);  * Number of columns in the lower triangular correlation matrix;\r\nR=J(p,p,0);    * Initialize the correlation matrix;\r\nuR=(lR`); * Transpose the lower triangular matrix to upper;\r\nR=lR<>uR; * Select the max of the lower and upper matrices;\r\nprint R;       * Complete correlation matrix;\r\n\r\n/** standard deviations of each variable **/\r\nc = {.89    1   1.11    1.1 1.31    1.11    .86 .88 .89 .43 .4  .41 .85 .92 .77 1.11    1.11    1.09    1.05    6.18    8.62    9.92};\r\nD = diag(c);\r\n \r\nS = D*R*D; /** covariance matrix **/\r\nprint S;\r\nquit;\r\n\r\nLoad lavaan\r\nSecond, load lavaan\r\n\r\nlibrary(lavaan)\r\n\r\nRead covariance as input\r\nThird, we read in the lower half of the covariance matrix (including\r\nthe diagonal)\r\n\r\noptions(width = 300)\r\nlower <- '\r\n0.7921\r\n0.6764    1\r\n0.661893  0.7548  1.2321\r\n0.25454   0.22    0.26862   1.21 \r\n0.326452  0.3275  0.247197  0.61963 1.7161 \r\n0.335886  0.3663  0.320346  0.63492 0.756132  1.2321\r\n0.436278  0.4902  0.52503   0.08514 0.22532   0.200466 0.7396\r\n0.43076   0.4664  0.53724   0.15488 0.23056   0.205128 0.635712 0.7744 \r\n0.380208  0.4094  0.484071  0.14685 0.268157  0.207459 0.604666 0.556072 0.7921\r\n0.137772  0.1333  0.147963  0.06622 0.073229  0.076368 0.170108 0.158928 0.183696 0.1849 \r\n0.14952   0.144   0.16872   0.066   0.06288   0.06216  0.17544  0.176    0.178    0.1204    0.16\r\n0.116768  0.1394  0.168387  0.04961 0.069823  0.059163 0.165722 0.165968 0.167854 0.114595  0.10496 0.1681\r\n0.158865  0.1615  0.122655  0.1309  0.20043   0.141525 0.22661  0.21692  0.234515 0.11696   0.17    0.03485   0.7225\r\n0.171948  0.1564  0.10212   0.14168 0.192832  0.15318  0.1978   0.186208 0.237452 0.146372  0.092   0.064124  0.63342  0.8464\r\n0.150766  0.1386  0.102564  0.12705 0.141218  0.102564 0.218526 0.196504 0.212443 0.129129  0.08932 0.047355  0.53669  0.580888 0.5929\r\n0.148185  0.1443  0.024642  0.28083 0.189033  0.221778 0.085914 0.068376 0.039516 0.090687  0.07104 0.036408  0.217005 0.15318  0.119658 1.2321\r\n0.158064  0.0999 -0.012321  0.20757 0.101787  0.061605 0.085914 0.087912 0.069153 0.119325  0.10656 0.072816  0.20757  0.285936 0.162393 0.726939 1.2321\r\n0.135814  0.0981 -0.012099  0.15587 0.057116  0.072594 0.065618 0.04796  0.067907 0.09374   0.07412 0.040221  0.20383  0.190532 0.193039 0.713841 0.750138 1.1881\r\n0.05607   0.0945  0.011655  0.08085 0.151305  0.18648  0.08127  0.0924   0.03738  0.06321   0.0168  0.021525  0.1785   0.18354  0.121275 0.174825 0.198135 0.171675 1.1025\r\n0.165006  0.3708  0.68598   1.15566 1.538202  1.166166 0.106296 0.054384 0.55002  0.39861   0.09888 0.228042  0.15759  0.28428  0.285516 0.68598  1.097568 0.471534 0.6489 38.1924\r\n0.76718   0.7758  0.95682   0.9482  1.806752  1.43523 -0.007413 0.151712 0.306872 0.444792  0.1724 -0.035342  0.21981  0.237912 0.33187  0.0095682 1.722276 0.187916 0.45255 27.701232 74.3044\r\n1.236032  0.992   1.321344  0.76384 1.94928   1.871904 -0.085312 0.087296 -0.008829 0.511872 0.11904 0.244032 0.8432   0.091264 0.916608 0.660672 1.982016 0.432512 0.72912 28.200576 68.40832 98.4064  '\r\n\r\nmathieu.cov <- \r\n    getCov(lower, names = c(\r\n      \"OC1\", \"OC2\", \"OC3\", # Organizational commitment\r\n      \"JI1\", \"JI2\", \"JI3\", # Job involvement\r\n      \"SAT1\", \"SAT2\", \"SAT3\", # Job satisfaction\r\n      \"JS1\", \"JS2\", \"JS3\", # Job scope\r\n      \"SELF1\", \"SELF2\", \"SELF3\", # Self ratings of performance\r\n      \"SUPR1\", \"SUPR2\", \"SUPR3\", # Supervisor ratings of performance\r\n      \"Education\",\r\n      \"PostTen\", # Position tenure\r\n      \"OrgTen\", # Organizational tenure\r\n      \"Age\"\r\n    ))\r\n\r\nModel fit\r\nFourth, we fit model\r\n\r\noptions(width = 300)\r\nmathieu.model <- '\r\n  # latent variables\r\n    OC  =~ OC1  + OC2   + OC3\r\n    SAT =~ SAT1 + SAT2  + SAT3\r\n    JS  =~ JS1  + JS2   + JS3\r\n    PERF=~ SUPR1+ SUPR2 + SUPR3\r\n  # regressions\r\n    PERF ~ OC + SAT + JS + Education\r\n    OC   ~ SAT\r\n    SAT  ~ JS\r\n'\r\nfit <- sem(mathieu.model, \r\n           sample.cov = mathieu.cov, \r\n           sample.nobs = 483)\r\nsummary(fit, standardized = TRUE)\r\nlavaan 0.6-10 ended normally after 47 iterations\r\n\r\n  Estimator                                         ML\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        30\r\n                                                      \r\n  Number of observations                           483\r\n                                                      \r\nModel Test User Model:\r\n                                                      \r\n  Test statistic                               126.544\r\n  Degrees of freedom                                60\r\n  P-value (Chi-square)                           0.000\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                             Standard\r\n  Information                                 Expected\r\n  Information saturated (h1) model          Structured\r\n\r\nLatent Variables:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\r\n  OC =~                                                                 \r\n    OC1               1.000                               0.771    0.868\r\n    OC2               1.124    0.049   22.934    0.000    0.867    0.868\r\n    OC3               1.128    0.056   20.156    0.000    0.870    0.784\r\n  SAT =~                                                                \r\n    SAT1              1.000                               0.816    0.950\r\n    SAT2              0.949    0.031   30.970    0.000    0.775    0.881\r\n    SAT3              0.900    0.034   26.719    0.000    0.735    0.827\r\n  JS =~                                                                 \r\n    JS1               1.000                               0.356    0.828\r\n    JS2               0.950    0.048   19.671    0.000    0.338    0.845\r\n    JS3               0.886    0.049   17.969    0.000    0.315    0.770\r\n  PERF =~                                                               \r\n    SUPR1             1.000                               0.822    0.743\r\n    SUPR2             1.071    0.073   14.751    0.000    0.881    0.797\r\n    SUPR3             1.028    0.070   14.666    0.000    0.845    0.779\r\n\r\nRegressions:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\r\n  PERF ~                                                                \r\n    OC                0.115    0.084    1.370    0.171    0.108    0.108\r\n    SAT              -0.207    0.093   -2.214    0.027   -0.205   -0.205\r\n    JS                0.800    0.173    4.619    0.000    0.346    0.346\r\n    Education         0.145    0.038    3.781    0.000    0.176    0.185\r\n  OC ~                                                                  \r\n    SAT               0.667    0.041   16.421    0.000    0.706    0.706\r\n  SAT ~                                                                 \r\n    JS                1.471    0.107   13.733    0.000    0.641    0.641\r\n\r\nVariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\r\n   .OC1               0.195    0.021    9.514    0.000    0.195    0.247\r\n   .OC2               0.246    0.026    9.479    0.000    0.246    0.246\r\n   .OC3               0.473    0.038   12.491    0.000    0.473    0.385\r\n   .SAT1              0.072    0.012    6.089    0.000    0.072    0.098\r\n   .SAT2              0.173    0.015   11.389    0.000    0.173    0.224\r\n   .SAT3              0.250    0.019   13.186    0.000    0.250    0.317\r\n   .JS1               0.058    0.006   10.172    0.000    0.058    0.314\r\n   .JS2               0.046    0.005    9.427    0.000    0.046    0.285\r\n   .JS3               0.068    0.006   12.061    0.000    0.068    0.408\r\n   .SUPR1             0.547    0.050   11.028    0.000    0.547    0.447\r\n   .SUPR2             0.446    0.049    9.106    0.000    0.446    0.365\r\n   .SUPR3             0.464    0.047    9.814    0.000    0.464    0.394\r\n   .OC                0.298    0.029   10.122    0.000    0.501    0.501\r\n   .SAT               0.392    0.033   11.982    0.000    0.589    0.589\r\n    JS                0.127    0.012   10.489    0.000    1.000    1.000\r\n   .PERF              0.595    0.071    8.417    0.000    0.881    0.881\r\n\r\nFit statistics\r\nFifth, fit indices\r\n\r\noptions(width = 300)\r\n#fit statistics\r\nsummary(fit, fit.measures=TRUE)\r\nlavaan 0.6-10 ended normally after 47 iterations\r\n\r\n  Estimator                                         ML\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        30\r\n                                                      \r\n  Number of observations                           483\r\n                                                      \r\nModel Test User Model:\r\n                                                      \r\n  Test statistic                               126.544\r\n  Degrees of freedom                                60\r\n  P-value (Chi-square)                           0.000\r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                              3614.872\r\n  Degrees of freedom                                78\r\n  P-value                                        0.000\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    0.981\r\n  Tucker-Lewis Index (TLI)                       0.976\r\n\r\nLoglikelihood and Information Criteria:\r\n\r\n  Loglikelihood user model (H0)              -5138.829\r\n  Loglikelihood unrestricted model (H1)      -5075.557\r\n                                                      \r\n  Akaike (AIC)                               10337.657\r\n  Bayesian (BIC)                             10463.058\r\n  Sample-size adjusted Bayesian (BIC)        10367.840\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.048\r\n  90 Percent confidence interval - lower         0.036\r\n  90 Percent confidence interval - upper         0.060\r\n  P-value RMSEA <= 0.05                          0.599\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  SRMR                                           0.042\r\n\r\nParameter Estimates:\r\n\r\n  Standard errors                             Standard\r\n  Information                                 Expected\r\n  Information saturated (h1) model          Structured\r\n\r\nLatent Variables:\r\n                   Estimate  Std.Err  z-value  P(>|z|)\r\n  OC =~                                               \r\n    OC1               1.000                           \r\n    OC2               1.124    0.049   22.934    0.000\r\n    OC3               1.128    0.056   20.156    0.000\r\n  SAT =~                                              \r\n    SAT1              1.000                           \r\n    SAT2              0.949    0.031   30.970    0.000\r\n    SAT3              0.900    0.034   26.719    0.000\r\n  JS =~                                               \r\n    JS1               1.000                           \r\n    JS2               0.950    0.048   19.671    0.000\r\n    JS3               0.886    0.049   17.969    0.000\r\n  PERF =~                                             \r\n    SUPR1             1.000                           \r\n    SUPR2             1.071    0.073   14.751    0.000\r\n    SUPR3             1.028    0.070   14.666    0.000\r\n\r\nRegressions:\r\n                   Estimate  Std.Err  z-value  P(>|z|)\r\n  PERF ~                                              \r\n    OC                0.115    0.084    1.370    0.171\r\n    SAT              -0.207    0.093   -2.214    0.027\r\n    JS                0.800    0.173    4.619    0.000\r\n    Education         0.145    0.038    3.781    0.000\r\n  OC ~                                                \r\n    SAT               0.667    0.041   16.421    0.000\r\n  SAT ~                                               \r\n    JS                1.471    0.107   13.733    0.000\r\n\r\nVariances:\r\n                   Estimate  Std.Err  z-value  P(>|z|)\r\n   .OC1               0.195    0.021    9.514    0.000\r\n   .OC2               0.246    0.026    9.479    0.000\r\n   .OC3               0.473    0.038   12.491    0.000\r\n   .SAT1              0.072    0.012    6.089    0.000\r\n   .SAT2              0.173    0.015   11.389    0.000\r\n   .SAT3              0.250    0.019   13.186    0.000\r\n   .JS1               0.058    0.006   10.172    0.000\r\n   .JS2               0.046    0.005    9.427    0.000\r\n   .JS3               0.068    0.006   12.061    0.000\r\n   .SUPR1             0.547    0.050   11.028    0.000\r\n   .SUPR2             0.446    0.049    9.106    0.000\r\n   .SUPR3             0.464    0.047    9.814    0.000\r\n   .OC                0.298    0.029   10.122    0.000\r\n   .SAT               0.392    0.033   11.982    0.000\r\n    JS                0.127    0.012   10.489    0.000\r\n   .PERF              0.595    0.071    8.417    0.000\r\n\r\nModel Fit Statistics\r\nWe focus on the 5 commonly used:\r\n1- Model chi-square is the chi-square statistic we\r\nobtain from the maximum likelihood statistic (in lavaan,\r\nthis is known as the Test Statistic for the Model Test User Model)\r\n2- CFI is the Comparative Fit Index – values can range\r\nbetween 0 and 1 (values greater than 0.90, conservatively 0.95 indicate\r\ngood fit)\r\n3- TLI Tucker Lewis Index which also ranges between 0\r\nand 1 (if it’s greater than 1 it should be rounded to 1) with values\r\ngreater than 0.90 indicating good fit. If the CFI and TLI are less than\r\none, the CFI is always greater than the TLI.\r\n4- RMSEA is the root mean square error of approximation\r\nIn lavaan, you also obtain a p-value of close fit, that the RMSEA <\r\n0.05. If you reject the model, it means your model is not a close\r\nfitting model.\r\n5- SRMR is standardized root mean squared residual to\r\ntest close fit. Also we also obtain a p-value of close fit, that the\r\nRMSEA < 0.05. If you reject the model, it means your model is not a\r\nclose fitting model.\r\nIn general, researchers should avoid sample sizes less than 100 when\r\ntesting small degrees of freedom models. In fact, science and math\r\neducation researchers should avoid reporting the RMSEA when sample sizes\r\nare smaller than 200, particularly when combined with small degrees of\r\nfreedom. Small degrees of freedom do not tend to result in rejection of\r\ncorrectly specified models for the TLI, CFI, and SRMR, particularly if\r\nthey tested using larger sample sizes (Taasoobshirazi and Wang (2016)).\r\n\r\n(be continued)\r\nModel chi-square\r\nCFI, TLI\r\nRMSEA, SRMR\r\n\r\n\r\nSummary\r\nSEM was running both in Mplus and lavaan produced the identical\r\nresults. Fantastic!\r\nSEM is simply path analysis + CFA\r\nModel-based procedure: a means to make almost any idea an empirical\r\nquestion\r\nModel comparisons allow testing theory\r\nMeasurement models create latent constructs (= random effects) that\r\nbetter represent trait individual differences than any one outcome\r\nStructural models test relations involving those latent\r\nconstructs\r\n\r\nIt’s easy to setup, but with complicated models\r\nML MAY BREAK when your models get too complicated (or\r\nrealistic)\r\nYou have named your factors, but it doesn’t mean you are right!\r\n(Validity)\r\nDistributional assumptions matter, but so do linear model\r\nassumptions (nonlinear measurement and structural models may be\r\nFactor scores are not perfectly determined (and neither are sum\r\nscores), so make sure to represent their uncertainty in any SEM\r\nalternative\r\n\r\n\r\n\r\n\r\nAnderson, James C., and David W. Gerbing. 1988. “Structural\r\nEquation Modeling in Practice: A Review and Recommended Two-Step\r\nApproach.” Journal Article. Psychological Bulletin 103\r\n(3): 411–23. https://doi.org/10.1037/0033-2909.103.3.411.\r\n\r\n\r\nBollen, Kenneth A. 2000. “Modeling Strategies: In Search of the\r\nHoly Grail.” Journal Article. Structural Equation Modeling: A\r\nMultidisciplinary Journal 7 (1): 74–81. https://doi.org/10.1207/S15328007SEM0701_03.\r\n\r\n\r\nHayduk, Leslie A., and Dale N. Glaser. 2000. “Doing the Four-Step,\r\nRight–2–3, Wrong–2–3: A Brief Reply to Mulaik and Millsap; Bollen;\r\nBentler; and Herting and Costner.” Journal Article.\r\nStructural Equation Modeling 7: 111–23. https://doi.org/10.1207/S15328007SEM0701_06.\r\n\r\n\r\nMathieu, John E., and James L. Farr. 1991. “Further Evidence for\r\nthe Discriminant Validity of Measures of Organizational Commitment, Job\r\nInvolvement, and Job Satisfaction.” Journal Article. Journal\r\nof Applied Psychology 76 (1): 127–33. https://doi.org/10.1037/0021-9010.76.1.127.\r\n\r\n\r\nMulaik, Stanley A., and Roger E. Millsap. 2000. “Doing the\r\nFour-Step Right.” Journal Article. Structural Equation\r\nModeling: A Multidisciplinary Journal 7 (1): 36–73. https://doi.org/10.1207/S15328007SEM0701_02.\r\n\r\n\r\nTaasoobshirazi, Gita, and Shanshan Wang. 2016. “The Performance of\r\nthe SRMR, RMSEA, CFI, and TLI: An Examination of Sample Size, Path Size,\r\nand Degrees of Freedom.” Journal Article. Journal of Applied\r\nQuantitative Methods 11 (3): 31.\r\n\r\n\r\nWeb Page. 2019. https://melbourne.figshare.com/articles/media/Mplus_Workshop_at_The_University_of_Melbourne_February_4-8_2019_5_Days_/7797620.\r\n\r\n\r\n———. 2021. https://stats.oarc.ucla.edu/r/seminars/rsem/.\r\n\r\n\r\n———. 2022. https://www.lesahoffman.com/PSQF6249/index.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-25T00:18:22-06:00",
    "input_file": "SEM.knit.md"
  },
  {
    "path": "posts/2022-04-19-Basic Tools of Multivariate Matching/",
    "title": "Basic Tools of Multivariate Matching",
    "description": "Observational study;    \nCausal Inference;    \nMatching;    \nR",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-04-19",
    "categories": [
      "Biostatistics",
      "Causal Inference",
      "Tutorial",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nA Small\r\nExample\r\nLoad dataset\r\nDescriptive Statistics\r\nNotation\r\n\r\nPropensity\r\nScore\r\nConduct\r\nPS\r\n\r\nDistance\r\nMatrices\r\nSquared\r\ndifference in the \\(\\hat{e}(\\textbf{x})\\) distance\r\nJustifying by adding\r\ncaliper on the PS\r\nMahalanobis distance\r\nmatrix\r\nMahalanobis dmat within\r\nPS calipers.\r\nCons of Mahalanobis\r\ndistance\r\nSolution\r\nrank-based Mahalanobis distance\r\n\r\nOptimal Pair Matching\r\nMatch pair on\r\nPS\r\nMatch\r\npair on Mahalanobis distance within PS calipers\r\nMatch\r\npair on\r\nrank-based Mahalanobis distance within PS calipers\r\n\r\nOptimal Matching with\r\nMultiple Controls\r\nOptimal Full Matching\r\nEfficiency\r\nSummary\r\nFurther\r\nReading\r\n\r\nMotivation\r\nThe post here is the re-post in 2020, when I start to research in\r\nCausal Inference.\r\nThe context leaded me to the topic were:\r\nAfter the discussion with my advisor, Dr. Dulal Bhaumik, regarding\r\nthe direction of my Ph.D. dissertation, we decided to work toward the\r\nCAUSAL INFERENCE.\r\nMatching is the very first aim in Causal Inference: “Obtaining\r\ninferring causal effects from observational data using the Matching\r\nApproach”. Thus, I have all prepared to learn and write the Matching\r\nissues.\r\nParallel, I have been also analyzing the HR Exercise in Congenital\r\nHeart Disease patient vs. Normal person applying the matching problem.\r\nMaybe, revisit the matching problem in Breast Cancer nested case-control\r\nstudy last summer (using SAS) if I still have time during this Summer\r\n2020.\r\n\r\nA Small Example\r\nWelders are exposed to chromium and nickel, substances that can cause\r\ninappropriate links between DNA and proteins, which in turn may disrupt\r\ngene expression or interfere with replication of DNA. Costa, Zhitkovich,\r\nand Toniolo [10] measured DNA-protein cross-links in samples of white\r\nblood cells from 47 subjects (all male). 01\r\n\r\nUnmatched data for 21 railroad arc welders and 26 potential controls.\r\nCovariates are age, race (C=Caucasian, AA=African American), current\r\nsmoker (Y=yes, N=no). Response is DPC=DNA-protein cross-links in percent\r\nin white blood cells. All 47 subjects are male.\r\n\r\nWelders — Controls\r\nID\r\nAge\r\nRace\r\nSmoker\r\nDPC\r\n\r\nID\r\nAge\r\nRace\r\nSmoker\r\nDPC\r\n1\r\n38\r\nC\r\nN\r\n1.77\r\n\r\n1\r\n48\r\nAA\r\nN\r\n1.08\r\n2\r\n44\r\nC\r\nN\r\n1.02\r\n\r\n2\r\n63\r\nC\r\nN\r\n1.09\r\n3\r\n39\r\nC\r\nY\r\n1.44\r\n\r\n3\r\n44\r\nC\r\nY\r\n1.1\r\n4\r\n33\r\nAA\r\nY\r\n0.65\r\n\r\n4\r\n40\r\nC\r\nN\r\n1.1\r\n5\r\n35\r\nC\r\nY\r\n2.08\r\n\r\n5\r\n50\r\nC\r\nN\r\n0.93\r\n6\r\n39\r\nC\r\nY\r\n0.61\r\n\r\n6\r\n52\r\nC\r\nN\r\n1.11\r\n7\r\n27\r\nC\r\nN\r\n2.86\r\n\r\n7\r\n56\r\nC\r\nN\r\n0.98\r\n8\r\n43\r\nC\r\nY\r\n4.19\r\n\r\n8\r\n47\r\nC\r\nN\r\n2.2\r\n9\r\n39\r\nC\r\nY\r\n4.88\r\n\r\n9\r\n38\r\nC\r\nN\r\n0.88\r\n10\r\n43\r\nAA\r\nN\r\n1.08\r\n\r\n10\r\n34\r\nC\r\nN\r\n1.55\r\n11\r\n41\r\nC\r\nY\r\n2.03\r\n\r\n11\r\n42\r\nC\r\nN\r\n0.55\r\n12\r\n36\r\nC\r\nN\r\n2.81\r\n\r\n12\r\n36\r\nC\r\nY\r\n1.04\r\n13\r\n35\r\nC\r\nN\r\n0.94\r\n\r\n13\r\n41\r\nC\r\nN\r\n1.66\r\n14\r\n37\r\nC\r\nN\r\n1.43\r\n\r\n14\r\n41\r\nAA\r\nY\r\n1.49\r\n15\r\n39\r\nC\r\nY\r\n1.25\r\n\r\n15\r\n31\r\nAA\r\nY\r\n1.36\r\n16\r\n34\r\nC\r\nN\r\n2.97\r\n\r\n16\r\n56\r\nAA\r\nY\r\n1.02\r\n17\r\n35\r\nC\r\nY\r\n1.01\r\n\r\n17\r\n51\r\nAA\r\nN\r\n0.99\r\n18\r\n53\r\nC\r\nN\r\n2.07\r\n\r\n18\r\n36\r\nC\r\nY\r\n0.65\r\n19\r\n38\r\nC\r\nY\r\n1.15\r\n\r\n19\r\n44\r\nC\r\nN\r\n0.42\r\n20\r\n37\r\nC\r\nN\r\n1.07\r\n\r\n20\r\n35\r\nC\r\nN\r\n2.33\r\n21\r\n38\r\nC\r\nY\r\n1.63\r\n\r\n21\r\n34\r\nC\r\nY\r\n0.97\r\n\r\n\r\n\r\n\r\n\r\n\r\n22\r\n39\r\nC\r\nY\r\n0.62\r\n\r\n\r\n\r\n\r\n\r\n\r\n23\r\n45\r\nC\r\nN\r\n1.02\r\n\r\n\r\n\r\n\r\n\r\n\r\n24\r\n42\r\nC\r\nN\r\n1.78\r\n\r\n\r\n\r\n\r\n\r\n\r\n25\r\n30\r\nC\r\nN\r\n0.95\r\n\r\n\r\n\r\n\r\n\r\n\r\n26\r\n35\r\nC\r\nY\r\n1.59\r\nLoad dataset\r\n\r\n# loadind data\r\ngen.tox <- read_excel(\"data/table81.xlsx\", sheet = \"data\")\r\n\r\ndata.tab <- gen.tox[c(1:3, 22:24),]\r\nkable(data.tab, caption = \"Generic Toxicology Dataset (6-observation example)\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:load data)Generic Toxicology Dataset (6-observation example)\r\n\r\n\r\nID\r\n\r\n\r\nGroup\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nDPC\r\n\r\n\r\n1\r\n\r\n\r\nWelders\r\n\r\n\r\n38\r\n\r\n\r\nC\r\n\r\n\r\nN\r\n\r\n\r\n1.77\r\n\r\n\r\n2\r\n\r\n\r\nWelders\r\n\r\n\r\n44\r\n\r\n\r\nC\r\n\r\n\r\nN\r\n\r\n\r\n1.02\r\n\r\n\r\n3\r\n\r\n\r\nWelders\r\n\r\n\r\n39\r\n\r\n\r\nC\r\n\r\n\r\nY\r\n\r\n\r\n1.44\r\n\r\n\r\n22\r\n\r\n\r\nControls\r\n\r\n\r\n48\r\n\r\n\r\nAA\r\n\r\n\r\nN\r\n\r\n\r\n1.08\r\n\r\n\r\n23\r\n\r\n\r\nControls\r\n\r\n\r\n63\r\n\r\n\r\nC\r\n\r\n\r\nN\r\n\r\n\r\n1.09\r\n\r\n\r\n24\r\n\r\n\r\nControls\r\n\r\n\r\n44\r\n\r\n\r\nC\r\n\r\n\r\nY\r\n\r\n\r\n1.10\r\n\r\n\r\nDescriptive Statistics\r\n\r\nm.age.w <- round(mean(gen.tox$Age[gen.tox$Group==\"Welders\"]),0)\r\nm.age.c <- round(mean(gen.tox$Age[gen.tox$Group==\"Controls\"]),0)\r\naa.p.w <- round(table(gen.tox$Race[gen.tox$Group==\"Welders\"])[1]/sum(table(gen.tox$Race[gen.tox$Group==\"Welders\"]))*100,0)\r\naa.p.c <- round(table(gen.tox$Race[gen.tox$Group==\"Controls\"])[1]/sum(table(gen.tox$Race[gen.tox$Group==\"Controls\"]))*100,0)\r\ns.p.w <- round(table(gen.tox$Smoker[gen.tox$Group==\"Welders\"])[2]/sum(table(gen.tox$Smoker[gen.tox$Group==\"Welders\"]))*100,0)\r\ns.p.c <- round(table(gen.tox$Smoker[gen.tox$Group==\"Controls\"])[2]/sum(table(gen.tox$Smoker[gen.tox$Group==\"Controls\"]))*100,0)\r\n\r\nWelders — Controls\r\nMean Age\r\nAA\r\nSmoker\r\n—\r\nMean Age\r\nAA\r\nSmoker\r\n38\r\n10\r\n52\r\n—\r\n43\r\n19\r\n35\r\n\r\nt.test(Age ~ Group, data=gen.tox)\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  Age by Group\r\nt = 2.2537, df = 42.167, p-value = 0.02947\r\nalternative hypothesis: true difference in means between group Controls and group Welders is not equal to 0\r\n95 percent confidence interval:\r\n 0.4662145 8.4422104\r\nsample estimates:\r\nmean in group Controls  mean in group Welders \r\n              42.69231               38.23810 \r\ntbl.age.group <- table(gen.tox$Group, gen.tox$Race)\r\ntbl.age.group\r\n          \r\n           AA  C\r\n  Controls  5 21\r\n  Welders   2 19\r\nchisq.test(tbl.age.group)\r\n\r\n    Pearson's Chi-squared test with Yates' continuity correction\r\n\r\ndata:  tbl.age.group\r\nX-squared = 0.26754, df = 1, p-value = 0.605\r\nfisher.test(tbl.age.group, conf.int = T, conf.level = 0.95)\r\n\r\n    Fisher's Exact Test for Count Data\r\n\r\ndata:  tbl.age.group\r\np-value = 0.4364\r\nalternative hypothesis: true odds ratio is not equal to 1\r\n95 percent confidence interval:\r\n  0.3170452 25.9844336\r\nsample estimates:\r\nodds ratio \r\n   2.22477 \r\n\r\nThe welders are about 5 years younger than the controls on\r\naverage, have relatively fewer African Americans, and more\r\nsmokers.\r\nGiven the t-test (t = 2.25) and a two-sided significance level of\r\n0.03 showed the difference in age between welders and controls\r\nis\r\nThe 2 binary variables race & smoking are neither significant\r\nby that standard nor by Fisher’s exact test for a 2×2 table.\r\nNotation\r\nData genetic toxicity had \\(L\\) = 47\r\nsubjects, \\(l = 1,2, ..., L\\).\r\nFor subject \\(l\\), the observed\r\ncovariate, \\(\\textbf{x}_l\\), is\r\n3-dimensional, \\(\\textbf{x}_l = (x_{l1},\r\nx_{l2}, x_{l3})^T\\) , where\r\n\\(x_{l1}\\) is the age\r\n\\(x_{l2}\\) encodes race, \\(x_{l2}\\) = 1 if \\(l\\) is African American, \\(x_{l2}\\) =0 if \\(l\\) is Caucasian,\r\n\\(x_{l3}\\) encodes smoking,\r\n\\(x_{l3}\\) = 1 if \\(l\\) is is a current smoker, \\(x_{l3}\\) = 0 otherwise.\r\n\\(\\rightarrow\\) For instance, \\(x_1\\) = \\((38,0,0)^T\\) .\r\nThe variable \\(Z_l\\) distinguishes\r\ntreated subjects from potential controls: \\(Z_l\\) = 1 for a treated subject, here a\r\nwelder; \\(Z_l\\) = 0 for a potential\r\ncontrol.\r\n\r\ngen.tox$Race <- ifelse(gen.tox$Race==\"C\", 0, 1)\r\ngen.tox$Smoker <- ifelse(gen.tox$Smoker==\"N\", 0, 1)\r\n\r\nPropensity Score\r\nThe PS is the conditional probability of exposure to\r\ntreatment given the observed covariates, \\(e\r\n(\\textbf{X}) = Pr(\\textbf{Z} = 1| \\textbf{x})\\).\r\nThe PS is defined in terms of the observed covariates,\r\nx, even though there is invariably concern about other covariates that\r\nwere not measured.\r\nProperties of the PS:\r\nbalance property\r\nthe ‘ideal match’ could be produced simply by matching for the\r\nobserved covariate x. Recall also that it may be difficult to match\r\nclosely for every one of the many covariates in \\(x\\), but it is easy to match on one\r\nvariable, the propensity score, \\(e(\\mathbf{x})\\), and doing that balances\r\nall of \\(x\\).\r\n\r\nBrief examination of Genetic Toxicity data suggested:\r\nat least age, \\(x_1\\), and\r\npossibly race and smoking, \\(x_2\\) and\r\n\\(x_3\\), could be used to predict\r\ntreatment assignment \\(Z\\), so that the\r\nPS is not constant.\r\nSaying that welders tend to be somewhat younger than controls is\r\nmuch the same as saying that the chance of being a welder is lower for\r\nsomeone who is older, i.e. \\(e(\\textbf{X})\\) is lower when \\(x_1\\) is higher.\r\nIf 2 subjects have the same PS \\(e(\\textbf{X})\\), they may have different\r\nvalues of \\(X\\) \\(\\rightarrow\\) subjects were matched for\r\n\\(e(\\textbf{X})\\), they may be\r\nmismatched for x \\(\\rightarrow\\) the\r\nmismatches in x will be due to chance and will tend to balance,\r\nparticularly in large samples. ( e.g. if young\r\nnonsmokers and old smokers have the same PS, then a match\r\non the PS may pair a young nonsmoking welder to an old\r\nsmoking control, but it will do this about as often as it pairs an old\r\nsmoking welder to a young nonsmoking control) \\(\\rightarrow\\) matching on \\(e(\\textbf(X))\\) tends to balance \\(\\textbf{X}\\) \\(\\rightarrow\\) \\(Z\\)| \\(e(\\textbf{X})\\) indep. \\(X\\)\r\nIn short,\r\nmatching on \\(e(\\textbf{X})\\) is\r\noften practical even when there are many covariates in x because \\(e(\\textbf{X})\\) is a single\r\nvariable,\r\nfailure to balance \\(e(\\textbf{X})\\) implies that \\(X\\) is not balanced\r\nmatching on \\(e(\\textbf{X})\\)\r\ntends to balance all of \\(X\\). On the\r\nnegative side, success in balancing the observed covariates, \\(X\\), provides no assurance that unmeasured\r\ncovariates are balanced\r\n\\(\\rightarrow\\) For example, the men\r\nwhose his father worked as a welder is associated with a much higher\r\nchance that the son also will work as a welder, so father’s occupation\r\nis an unmeasured covariate that could be used to predict treatment \\(Z\\); then success in matching on the\r\npropensity score \\(e(\\textbf{X})\\)\r\nwould tend to balance age, race and smoking, but there is no reason to\r\nexpect it to balance father’s occupation.\r\nConduct PS\r\nThe PS is unknown, but it can be estimated from the data\r\nat hand. PS is estimated by a linear logit model \\[ log {\\frac{e(\\textbf{X}_l)}{1 -\r\ne(\\textbf{X}_l)}} = \\zeta_0 + \\zeta_1 x_{l1} + \\zeta_2 x_{l2} + \\zeta_3\r\nx_{l3}\\]\r\nand the fitted values \\(\\hat{e}(\\textbf{X}_l)\\) from this model are\r\nthe estimates of the PS.\r\n\r\n#fit a propensity score model. logistic regression\r\ngen.tox$Group <- relevel(as.factor(gen.tox$Group), ref=\"Controls\")\r\npsmodel <- glm(Group ~ Age + Race + Smoker, family=binomial(), data=gen.tox)\r\n\r\n#show coefficients etc\r\nsummary(psmodel)\r\n\r\nCall:\r\nglm(formula = Group ~ Age + Race + Smoker, family = binomial(), \r\n    data = gen.tox)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-1.4968  -1.0216  -0.5245   1.0786   1.8186  \r\n\r\nCoefficients:\r\n            Estimate Std. Error z value Pr(>|z|)\r\n(Intercept)  3.06543    2.14200   1.431    0.152\r\nAge         -0.08503    0.05178  -1.642    0.101\r\nRace        -0.76900    0.98252  -0.783    0.434\r\nSmoker       0.55095    0.65212   0.845    0.398\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 64.623  on 46  degrees of freedom\r\nResidual deviance: 58.706  on 43  degrees of freedom\r\nAIC: 66.706\r\n\r\nNumber of Fisher Scoring iterations: 3\r\n#create propensity score\r\npscore<-psmodel$fitted.values\r\ngen.tox$ps <- round(pscore,2)\r\n\r\n# mean of PS in Welders group\r\nmean.ps.welder <- round(mean(gen.tox$ps[gen.tox$Group==\"Welders\"]),2)\r\n\r\n# mean of PS in 21 largest Controls group\r\nmean.ps.21largestcontrols <- gen.tox %>%\r\n  filter(Group==\"Controls\") %>%\r\n  top_n(21, ps) %>% # wrapper that uses filter() and min_rank() to select the top or bottom entries in each group, ordered by ps.\r\n  arrange(desc(ps)) %>% # order data follow ps decreasing\r\n  summarize(round(mean(ps),2)) %>%\r\n  as.numeric()\r\n\r\nEstimated PS for 21 railroad arc welders and 26\r\npotential controls. Covariates are age, race (C=Caucasian, AA=African\r\nAmerican), current smoker (Y=yes, N=no).\r\nID\r\nAge\r\nRace\r\nSmoker\r\nDPC\r\n\\(\\hat{e}(\\textbf{X})\\)\r\nID\r\nAge\r\nRace\r\nSmoker\r\nDPC\r\n\\(\\hat{e}(\\textbf{X})\\)\r\n1\r\n38\r\nC\r\nN\r\n1.77\r\n0.46\r\n1\r\n48\r\nAA\r\nN\r\n1.08\r\n0.14\r\n2\r\n44\r\nC\r\nN\r\n1.02\r\n0.34\r\n2\r\n63\r\nC\r\nN\r\n1.09\r\n0.09\r\n3\r\n39\r\nC\r\nY\r\n1.44\r\n0.57\r\n3\r\n44\r\nC\r\nY\r\n1.1\r\n0.47\r\n4\r\n33\r\nAA\r\nY\r\n0.65\r\n0.51\r\n4\r\n40\r\nC\r\nN\r\n1.1\r\n0.42\r\n5\r\n35\r\nC\r\nY\r\n2.08\r\n0.65\r\n5\r\n50\r\nC\r\nN\r\n0.93\r\n0.23\r\n6\r\n39\r\nC\r\nY\r\n0.61\r\n0.57\r\n6\r\n52\r\nC\r\nN\r\n1.11\r\n0.2\r\n7\r\n27\r\nC\r\nN\r\n2.86\r\n0.68\r\n7\r\n56\r\nC\r\nN\r\n0.98\r\n0.15\r\n8\r\n43\r\nC\r\nY\r\n4.19\r\n0.49\r\n8\r\n47\r\nC\r\nN\r\n2.2\r\n0.28\r\n9\r\n39\r\nC\r\nY\r\n4.88\r\n0.57\r\n9\r\n38\r\nC\r\nN\r\n0.88\r\n0.46\r\n10\r\n43\r\nAA\r\nN\r\n1.08\r\n0.2\r\n10\r\n34\r\nC\r\nN\r\n1.55\r\n0.46\r\n11\r\n41\r\nC\r\nY\r\n2.03\r\n0.53\r\n11\r\n42\r\nC\r\nN\r\n0.55\r\n0.54\r\n12\r\n36\r\nC\r\nN\r\n2.81\r\n0.5\r\n12\r\n36\r\nC\r\nY\r\n1.04\r\n0.38\r\n13\r\n35\r\nC\r\nN\r\n0.94\r\n0.52\r\n13\r\n41\r\nC\r\nN\r\n1.66\r\n0.64\r\n14\r\n37\r\nC\r\nN\r\n1.43\r\n0.48\r\n14\r\n41\r\nAA\r\nY\r\n1.49\r\n0.4\r\n15\r\n39\r\nC\r\nY\r\n1.25\r\n0.57\r\n15\r\n31\r\nAA\r\nY\r\n1.36\r\n0.35\r\n16\r\n34\r\nC\r\nN\r\n2.97\r\n0.54\r\n16\r\n56\r\nAA\r\nY\r\n1.02\r\n0.55\r\n17\r\n35\r\nC\r\nY\r\n1.01\r\n0.65\r\n17\r\n51\r\nAA\r\nN\r\n0.99\r\n0.13\r\n18\r\n53\r\nC\r\nN\r\n2.07\r\n0.19\r\n18\r\n36\r\nC\r\nY\r\n0.65\r\n0.12\r\n19\r\n38\r\nC\r\nY\r\n1.15\r\n0.6\r\n19\r\n44\r\nC\r\nN\r\n0.42\r\n0.64\r\n20\r\n37\r\nC\r\nN\r\n1.07\r\n0.48\r\n20\r\n35\r\nC\r\nN\r\n2.33\r\n0.34\r\n21\r\n38\r\nC\r\nY\r\n1.63\r\n0.6\r\n21\r\n34\r\nC\r\nY\r\n0.97\r\n0.52\r\n\r\n\r\n\r\n\r\n\r\n\r\n22\r\n39\r\nC\r\nY\r\n0.62\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n23\r\n45\r\nC\r\nN\r\n1.02\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n24\r\n42\r\nC\r\nN\r\n1.78\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n25\r\n30\r\nC\r\nN\r\n0.95\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n26\r\n35\r\nC\r\nY\r\n1.59\r\n\r\n\r\nps.w <- round(mean(gen.tox$ps[gen.tox$Group==\"Welders\"]),2)\r\nps.c <- round(mean(gen.tox$ps[gen.tox$Group==\"Controls\"]),2)\r\n\r\nWelders — Controls\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n—\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n38\r\n10\r\n52\r\n0.51\r\n—\r\n43\r\n19\r\n35\r\n0.4\r\nWelders #10 and #18 have similar \\(\\hat{e}(\\textbf{x})\\) but different\r\npatterns of covariates \\(X\\).\r\nThe limitations of pair matching, apply with any variable,\r\nincluding the PS. The mean of the 21 largest \\(\\hat{e}(\\textbf{x})\\)’s in the control\r\ngroup is 0.46, somewhat less than the mean of \\(\\hat{e}(\\textbf{x})\\)) in the treated\r\ngroup, namely 0.51, so no pair matching can completely close that\r\ngap.\r\nDistance Matrices\r\nSquared\r\ndifference in the \\(\\hat{e}(\\textbf{x})\\) distance\r\nA distance matrix is a table with one row for each\r\ntreated subject and one column for each potential control.\r\nThe value in row \\(i\\) and column \\(j\\) of the table is the\r\ndistance between the \\(i^{th}\\) treated subject and the \\(j^{th}\\) potential control. 2 individuals\r\nwith the same value \\(\\mathbf{x}\\)\r\nwould have distance zero.\r\nThe welder data: 21 rows x 26 columns (21x26) distance. The\r\ndistance is the squared difference in the \\(\\hat{e}(\\textbf{x})\\)\r\n\r\nps.vec.c <- gen.tox$ps[gen.tox$Group==\"Controls\"]\r\nps.vec.w <- gen.tox$ps[gen.tox$Group==\"Welders\"]\r\nd.m <- matrix(NA, nrow = 21, ncol = 26)\r\nfor (i in 1:21){\r\n  for (j in 1:26){\r\n    d.m[i,j] <- round((ps.vec.w[i] - ps.vec.c[j])^2, 2)\r\n  }\r\n}\r\n\r\nTable. Squared differences in PS\r\nbetween welders and controls: \\((\\hat{e}(\\textbf{X})_{Welder} -\r\n\\hat{e}(\\textbf{X})_{Control})^2\\)\r\nRows are the 21 welders and columns are for the first 6 of 26\r\npotential controls.\r\n\r\nWelder <- c(1:21)\r\nd.m.d <- as.data.frame(d.m)\r\nnames(d.m.d) <- paste0(\"Control \", 1:26)\r\nd.m.d.n <- as.data.frame(cbind(Welder, d.m.d))\r\nkable(d.m.d.n[,1:7], caption = \"Squared differences in `PS` between welders and controls\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:table of sq difference PS)Squared differences in PS\r\nbetween welders and controls\r\n\r\n\r\nWelder\r\n\r\n\r\nControl 1\r\n\r\n\r\nControl 2\r\n\r\n\r\nControl 3\r\n\r\n\r\nControl 4\r\n\r\n\r\nControl 5\r\n\r\n\r\nControl 6\r\n\r\n\r\n1\r\n\r\n\r\n0.10\r\n\r\n\r\n0.14\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.05\r\n\r\n\r\n0.07\r\n\r\n\r\n2\r\n\r\n\r\n0.04\r\n\r\n\r\n0.06\r\n\r\n\r\n0.02\r\n\r\n\r\n0.01\r\n\r\n\r\n0.01\r\n\r\n\r\n0.02\r\n\r\n\r\n3\r\n\r\n\r\n0.18\r\n\r\n\r\n0.23\r\n\r\n\r\n0.01\r\n\r\n\r\n0.02\r\n\r\n\r\n0.12\r\n\r\n\r\n0.14\r\n\r\n\r\n4\r\n\r\n\r\n0.14\r\n\r\n\r\n0.18\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.08\r\n\r\n\r\n0.10\r\n\r\n\r\n5\r\n\r\n\r\n0.26\r\n\r\n\r\n0.31\r\n\r\n\r\n0.03\r\n\r\n\r\n0.05\r\n\r\n\r\n0.18\r\n\r\n\r\n0.20\r\n\r\n\r\n6\r\n\r\n\r\n0.18\r\n\r\n\r\n0.23\r\n\r\n\r\n0.01\r\n\r\n\r\n0.02\r\n\r\n\r\n0.12\r\n\r\n\r\n0.14\r\n\r\n\r\n7\r\n\r\n\r\n0.29\r\n\r\n\r\n0.35\r\n\r\n\r\n0.04\r\n\r\n\r\n0.07\r\n\r\n\r\n0.20\r\n\r\n\r\n0.23\r\n\r\n\r\n8\r\n\r\n\r\n0.12\r\n\r\n\r\n0.16\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.07\r\n\r\n\r\n0.08\r\n\r\n\r\n9\r\n\r\n\r\n0.18\r\n\r\n\r\n0.23\r\n\r\n\r\n0.01\r\n\r\n\r\n0.02\r\n\r\n\r\n0.12\r\n\r\n\r\n0.14\r\n\r\n\r\n10\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.07\r\n\r\n\r\n0.05\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n11\r\n\r\n\r\n0.15\r\n\r\n\r\n0.19\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.09\r\n\r\n\r\n0.11\r\n\r\n\r\n12\r\n\r\n\r\n0.13\r\n\r\n\r\n0.17\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.07\r\n\r\n\r\n0.09\r\n\r\n\r\n13\r\n\r\n\r\n0.14\r\n\r\n\r\n0.18\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.08\r\n\r\n\r\n0.10\r\n\r\n\r\n14\r\n\r\n\r\n0.12\r\n\r\n\r\n0.15\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.06\r\n\r\n\r\n0.08\r\n\r\n\r\n15\r\n\r\n\r\n0.18\r\n\r\n\r\n0.23\r\n\r\n\r\n0.01\r\n\r\n\r\n0.02\r\n\r\n\r\n0.12\r\n\r\n\r\n0.14\r\n\r\n\r\n16\r\n\r\n\r\n0.16\r\n\r\n\r\n0.20\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.10\r\n\r\n\r\n0.12\r\n\r\n\r\n17\r\n\r\n\r\n0.26\r\n\r\n\r\n0.31\r\n\r\n\r\n0.03\r\n\r\n\r\n0.05\r\n\r\n\r\n0.18\r\n\r\n\r\n0.20\r\n\r\n\r\n18\r\n\r\n\r\n0.00\r\n\r\n\r\n0.01\r\n\r\n\r\n0.08\r\n\r\n\r\n0.05\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n19\r\n\r\n\r\n0.21\r\n\r\n\r\n0.26\r\n\r\n\r\n0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n0.14\r\n\r\n\r\n0.16\r\n\r\n\r\n20\r\n\r\n\r\n0.12\r\n\r\n\r\n0.15\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.06\r\n\r\n\r\n0.08\r\n\r\n\r\n21\r\n\r\n\r\n0.21\r\n\r\n\r\n0.26\r\n\r\n\r\n0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n0.14\r\n\r\n\r\n0.16\r\n\r\n\r\nLook at the disadvantage:\r\n- that 2 controls with the same \\(\\hat{e}(\\mathbf{x})\\) may have different\r\npatterns of covariates, \\(\\mathbf{x}\\),\r\nand this is ignored\\(\\Rightarrow\\) In the \\(1^{st}\\) row and \\(3^{rd}\\) and \\(4^{th}\\) columns of Table above, the\r\ndistance is 0 to two decimal places. Actually, the distances between\r\nwelder #1 and potential controls #3 and #4 are, respectively, \\((0.46−0.47)^2 = 0.0001\\) and \\((0.46−0.42)^2 = 0.0016,\\) so control #3 is\r\never so slightly closer to welder #1.\r\n- However, in terms of the details of \\(\\textbf{x}\\), control #4\r\nlooks to be the better match, a nonsmoker with a two-year\r\ndifference in age, as opposed to control #3, a\r\nsmoker with a six-year difference in age.\\(\\rightarrow\\) Because younger smokers\r\nare more common in the welder group, the PS is indifferent\r\nbetween a younger nonsmoker and an older smoker, but the details of\r\n\\(\\textbf{x}\\) suggest that\r\ncontrol #4 is a better match for welder #1 than is\r\ncontrol #3.\r\nJustifying by adding\r\ncaliper on the PS\r\nAn alternative distance (Paul R. Rosenbaum and Rubin\r\n1985):\r\nindividuals be close on the PS, \\(\\hat{e}(\\textbf{x})\\), but once this is\r\nachieved, the details of \\(\\textbf{x}\\)\r\naffect the distance.\r\nwith a caliper of width \\(w\\), if two individuals, say \\(k\\) and \\(l\\),\r\nhave PS that differ by more than \\(w\\) i.e. \\(|\r\n\\hat{e}(\\textbf{x}_k) - \\hat{e}(\\textbf{x}_l) | > w\\) \\(\\rightarrow\\) set \\(w\\) = \\(\\infty\\)\r\nif \\(| \\hat{e}(\\textbf{x}_k) -\r\n\\hat{e}(\\textbf{x}_l) | \\le w\\) \\(\\rightarrow\\) the distance is a measure of\r\nproximity of \\(\\textbf{x}_k\\) and \\(\\textbf{x}_l\\).\r\n\r\n\r\nsd.ps <- round(sd(pscore),3) # [1] 0.172\r\nw <- round(sd.ps/2,3)\r\npaste0(\"The standard deviation of ps: \", sd.ps)\r\n[1] \"The standard deviation of ps: 0.172\"\r\npaste0(\"The caliper w (half of sd of ps): \", sd.ps, \", used to demonstate in this post.\")\r\n[1] \"The caliper w (half of sd of ps): 0.172, used to demonstate in this post.\"\r\n\r\nThe caliper width, \\(w\\), is often\r\ntaken as a multiple of the standard deviation of the PS, so\r\nthat by varying the multiplier, one can vary the relative importance\r\ngiven to \\(\\hat{e}(\\textbf{x})\\) and\r\n\\(\\textbf{x}\\).\r\nThe standard deviation of \\(\\hat{e}(\\textbf{X})\\) is 0.172.\r\nWe illustrate 2 distance matrices using a caliper on the\r\nPS, in which the caliper is half of the standard deviation\r\nof the PS, or 0.172/2 = 0.086.\r\n\r\n\r\nIn problems of practical size, a\r\ncaliper of 20% of the sd of ps is more common,\r\nand even that may be too large. A reasonable strategy: to start with a\r\ncaliper of 20% of the sd of ps \\(\\rightarrow\\) adjusting the caliper if\r\nneeded to obtain balance on the propensity score.\r\n\r\nMahalanobis distance matrix\r\nIf \\(\\hat{\\Sigma}\\) is the sample\r\ncovariance matrix of \\(\\textbf{x}\\),\r\nthen the estimated Mahalanobis distance between \\(\\textbf{x}_k\\) and \\(\\textbf{x}_l\\) is \\[ (\\textbf{x}_k - \\textbf{x}_l)^T\r\n\\hat{\\Sigma}^{-1} (\\textbf{x}_k - \\textbf{x}_l) \\]\r\n\r\nvar.m <-gen.tox %>%\r\n  dplyr::select(Age, Race, Smoker) %>%\r\n  unname() %>%\r\n  as.matrix()\r\n\r\nSample var-cov matrix\r\n\r\n# sample variance covariance matrix\r\ncov.m <- cov(var.m)\r\nround(cov.m,2)\r\n      [,1] [,2]  [,3]\r\n[1,] 54.04 0.39 -0.94\r\n[2,]  0.39 0.13  0.02\r\n[3,] -0.94 0.02  0.25\r\n\r\nInverse of sample var-cov\r\nmatrix\r\n\r\n# inverse of sample variance covariance matrix\r\ncov.m.inv <- inv(cov.m)\r\nround(cov.m.inv,3)\r\n       [,1]   [,2]   [,3]\r\n[1,]  0.021 -0.077  0.084\r\n[2,] -0.077  8.127 -1.009\r\n[3,]  0.084 -1.009  4.407\r\n\r\nMahalanobis dmat\r\nRubin (1980)\r\nDistance matrix would have 21 treated subject (Welders) and 26\r\ncontrols; the value in row \\(i\\) and\r\ncolumn \\(j\\) of the table is the\r\ndistance between the \\(i^{th}\\) treated subject and the \\(j^{th}\\) potential control.\r\n\r\nvar.m.w <-gen.tox %>%\r\n  filter(Group==\"Welders\") %>%\r\n  dplyr::select(Age, Race, Smoker) %>%\r\n  as.matrix()\r\n\r\nvar.m.c <-gen.tox %>%\r\n  filter(Group==\"Controls\") %>%\r\n  dplyr::select(Age, Race, Smoker) %>%\r\n  as.matrix()\r\n\r\nm.d.m <- matrix(NA, nrow = 21, ncol = 26)\r\nfor (i in 1:21){\r\n  for (j in 1:26){\r\n    m.d.m[i,j] <- t(var.m.w[i,] - var.m.c[j,]) %*% cov.m.inv %*% (var.m.w[i,] - var.m.c[j,])\r\n  }\r\n}\r\n\r\n# Mahalanobis distance\r\nWelder <- c(1:21)\r\nd.m.d.m <- as.data.frame(m.d.m)\r\nnames(d.m.d.m) <- paste0(\"Control \", 1:26)\r\nd.d.m.d.n <- cbind(Welder, d.m.d.m)\r\nkable(round(d.d.m.d.n[,1:7],2), caption = \"Mahalanobis distance between 21 Welders vs 26 Controls dealed with Age, Race and Smoker\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 1: Mahalanobis distance between 21 Welders vs 26 Controls\r\ndealed with Age, Race and Smoker\r\n\r\n\r\nWelder\r\n\r\n\r\nControl 1\r\n\r\n\r\nControl 2\r\n\r\n\r\nControl 3\r\n\r\n\r\nControl 4\r\n\r\n\r\nControl 5\r\n\r\n\r\nControl 6\r\n\r\n\r\n1\r\n\r\n\r\n8.65\r\n\r\n\r\n12.82\r\n\r\n\r\n6.15\r\n\r\n\r\n0.08\r\n\r\n\r\n2.95\r\n\r\n\r\n4.02\r\n\r\n\r\n2\r\n\r\n\r\n7.84\r\n\r\n\r\n7.40\r\n\r\n\r\n4.41\r\n\r\n\r\n0.33\r\n\r\n\r\n0.74\r\n\r\n\r\n1.31\r\n\r\n\r\n3\r\n\r\n\r\n13.33\r\n\r\n\r\n12.21\r\n\r\n\r\n0.51\r\n\r\n\r\n4.26\r\n\r\n\r\n5.05\r\n\r\n\r\n5.70\r\n\r\n\r\n4\r\n\r\n\r\n6.51\r\n\r\n\r\n28.55\r\n\r\n\r\n12.29\r\n\r\n\r\n11.42\r\n\r\n\r\n16.20\r\n\r\n\r\n17.65\r\n\r\n\r\n5\r\n\r\n\r\n13.85\r\n\r\n\r\n15.80\r\n\r\n\r\n1.66\r\n\r\n\r\n4.08\r\n\r\n\r\n6.51\r\n\r\n\r\n7.49\r\n\r\n\r\n6\r\n\r\n\r\n13.33\r\n\r\n\r\n12.21\r\n\r\n\r\n0.51\r\n\r\n\r\n4.26\r\n\r\n\r\n5.05\r\n\r\n\r\n5.70\r\n\r\n\r\n7\r\n\r\n\r\n13.95\r\n\r\n\r\n26.58\r\n\r\n\r\n13.18\r\n\r\n\r\n3.47\r\n\r\n\r\n10.85\r\n\r\n\r\n12.82\r\n\r\n\r\n8\r\n\r\n\r\n13.46\r\n\r\n\r\n9.27\r\n\r\n\r\n0.02\r\n\r\n\r\n5.09\r\n\r\n\r\n4.24\r\n\r\n\r\n4.56\r\n\r\n\r\n9\r\n\r\n\r\n13.33\r\n\r\n\r\n12.21\r\n\r\n\r\n0.51\r\n\r\n\r\n4.26\r\n\r\n\r\n5.05\r\n\r\n\r\n5.70\r\n\r\n\r\n10\r\n\r\n\r\n0.51\r\n\r\n\r\n19.40\r\n\r\n\r\n14.89\r\n\r\n\r\n7.85\r\n\r\n\r\n10.20\r\n\r\n\r\n11.17\r\n\r\n\r\n11\r\n\r\n\r\n13.31\r\n\r\n\r\n10.65\r\n\r\n\r\n0.18\r\n\r\n\r\n4.59\r\n\r\n\r\n4.56\r\n\r\n\r\n5.05\r\n\r\n\r\n12\r\n\r\n\r\n9.24\r\n\r\n\r\n14.95\r\n\r\n\r\n7.06\r\n\r\n\r\n0.33\r\n\r\n\r\n4.02\r\n\r\n\r\n5.25\r\n\r\n\r\n13\r\n\r\n\r\n9.60\r\n\r\n\r\n16.08\r\n\r\n\r\n7.57\r\n\r\n\r\n0.51\r\n\r\n\r\n4.61\r\n\r\n\r\n5.93\r\n\r\n\r\n14\r\n\r\n\r\n8.92\r\n\r\n\r\n13.87\r\n\r\n\r\n6.58\r\n\r\n\r\n0.18\r\n\r\n\r\n3.47\r\n\r\n\r\n4.61\r\n\r\n\r\n15\r\n\r\n\r\n13.33\r\n\r\n\r\n12.21\r\n\r\n\r\n0.51\r\n\r\n\r\n4.26\r\n\r\n\r\n5.05\r\n\r\n\r\n5.70\r\n\r\n\r\n16\r\n\r\n\r\n10.00\r\n\r\n\r\n17.25\r\n\r\n\r\n8.13\r\n\r\n\r\n0.74\r\n\r\n\r\n5.25\r\n\r\n\r\n6.65\r\n\r\n\r\n17\r\n\r\n\r\n13.85\r\n\r\n\r\n15.80\r\n\r\n\r\n1.66\r\n\r\n\r\n4.08\r\n\r\n\r\n6.51\r\n\r\n\r\n7.49\r\n\r\n\r\n18\r\n\r\n\r\n9.41\r\n\r\n\r\n2.05\r\n\r\n\r\n4.56\r\n\r\n\r\n3.47\r\n\r\n\r\n0.18\r\n\r\n\r\n0.02\r\n\r\n\r\n19\r\n\r\n\r\n13.40\r\n\r\n\r\n13.04\r\n\r\n\r\n0.74\r\n\r\n\r\n4.15\r\n\r\n\r\n5.35\r\n\r\n\r\n6.08\r\n\r\n\r\n20\r\n\r\n\r\n8.92\r\n\r\n\r\n13.87\r\n\r\n\r\n6.58\r\n\r\n\r\n0.18\r\n\r\n\r\n3.47\r\n\r\n\r\n4.61\r\n\r\n\r\n21\r\n\r\n\r\n13.40\r\n\r\n\r\n13.04\r\n\r\n\r\n0.74\r\n\r\n\r\n4.15\r\n\r\n\r\n5.35\r\n\r\n\r\n6.08\r\n\r\n\r\nAnother way to produce the Mahalanobis distance matrix: look at the\r\nsource code of mahal function.\r\n\r\nsource(\"mahal.R\")\r\nprint(mahal)\r\nfunction (z, X) \r\n{\r\n    X <- as.matrix(X)\r\n    n <- dim(X)[1]\r\n    rownames(X) <- 1:n\r\n    k <- dim(X)[2]\r\n    m <- sum(z)\r\n    cv <- cov(X)\r\n    out <- matrix(NA, m, n - m)\r\n    Xt <- X[z == 1, ]\r\n    Xc <- X[z == 0, ]\r\n    rownames(out) <- rownames(X)[z == 1]\r\n    colnames(out) <- rownames(X)[z == 0]\r\n    require(MASS)\r\n    icov <- ginv(cv)\r\n    for (i in 1:m) {\r\n        out[i, ] <- mahalanobis(Xc, Xt[i, ], icov, inverted = T)\r\n    }\r\n    out\r\n}\r\n\r\nMahalanobis dmat within\r\nPS calipers.\r\nRows are the 21 welders and columns are for the first 6 of 26\r\npotential controls. An \\(\\infty\\)\r\nsignifies that the caliper is violated.\r\n\r\nm.m <- matrix(NA, nrow = 21, ncol = 26)\r\nfor (i in 1:21){\r\n  for (j in 1:26){\r\n    m.m[i,j] <- ifelse(abs(ps.vec.w[i] - ps.vec.c[j]) > w, Inf, m.d.m[i,j])\r\n  }\r\n}\r\nWelder <- c(1:21)\r\nd.m.m <- as.data.frame(m.m)\r\nnames(d.m.m) <- paste0(\"Control \", 1:26)\r\nd.d.m.m <- cbind(Welder, d.m.m)\r\nkable(round(d.d.m.m[,1:7],2), caption = \"Mahalanobis distances within PS calipers. An `Inf` signifies that the caliper is violated.\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:mahal w PS caliper)Mahalanobis distances within PS calipers. An\r\nInf signifies that the caliper is violated.\r\n\r\n\r\nWelder\r\n\r\n\r\nControl 1\r\n\r\n\r\nControl 2\r\n\r\n\r\nControl 3\r\n\r\n\r\nControl 4\r\n\r\n\r\nControl 5\r\n\r\n\r\nControl 6\r\n\r\n\r\n1\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6.15\r\n\r\n\r\n0.08\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n2\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.33\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n3\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n4\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n12.29\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n5\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n7\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n8\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.02\r\n\r\n\r\n5.09\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n9\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n10\r\n\r\n\r\n0.51\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n10.20\r\n\r\n\r\n11.17\r\n\r\n\r\n11\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.18\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n12\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n7.06\r\n\r\n\r\n0.33\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n13\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n7.57\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n14\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6.58\r\n\r\n\r\n0.18\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n15\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n16\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n8.13\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n17\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n18\r\n\r\n\r\n9.41\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.18\r\n\r\n\r\n0.02\r\n\r\n\r\n19\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n20\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6.58\r\n\r\n\r\n0.18\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n21\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nCons of Mahalanobis distance\r\nMahalanobis distance was originally developed for\r\nuse with multivariate Normal data\\(\\rightarrow\\) With data that are not\r\nNormal, Mahalanobis distance can exhibit some rather odd\r\nbehavior.\r\nIf one covariate\r\ncontains extreme outliers or\r\nhas a long-tailed distribution,\r\n\r\n\\(\\rightarrow\\) its standard\r\ndeviation will be inflated, and \\(\\rightarrow\\) Mahalanobis distance will\r\ntend to ignore that covariate in matching.\r\nWith binary indicators,\r\nthe variance is largest for events that occur about half the time,\r\nand\r\nit is smallest for events with probabilities near 0 and 1\r\n\r\n\\(\\rightarrow\\)\r\nMahalanobis distance gives greater weight to binary\r\nvariables with probabilities near 0 or 1 than to binary variables with\r\nprobabilities closer to 1/2.\r\nIn welder data,\r\n2 individuals with the same Age and Race\r\nbut different smoking behavior have a Mahalanobis distance\r\nof 4.41,\r\nwhereas 2 individuals with the same Age and\r\nSmoking behavior but different Race have a\r\nMahalanobis distance of 8.06,\\(\\rightarrow\\) so a mismatch on\r\nRace is counted as almost twice as bad as a mismatch on\r\nSmoking.\r\n2 people who differed by 20 years in Age with the same\r\nRace and Smoking would have a\r\nMahalanobis distance of 0.021 × \\(20^2\\) = 8.204232,\\(\\rightarrow\\) a difference in\r\nRace counts about as much as a 20-year difference in\r\nAge.\r\n\r\n\r\n\\(\\Longrightarrow\\) In many\r\ncontexts, rare binary covariates are not of overriding importance, and\r\noutliers do not make a covariate unimportant \\(\\rightarrow\\)\r\nMahalanobis distance may not be appropriate with covariates\r\nof this kind.\r\n\r\nSolution\r\nrank-based Mahalanobis distance\r\nreplaces each of the covariates, one at a time, by its ranks, with\r\naverage ranks for ties,\\(\\Longrightarrow\\) limits the\r\ninfluence of outliers.\r\npremultiplies and postmultiplies the covariance matrix of the ranks\r\nby a diagonal matrix whose diagonal elements are the ratios of the\r\nstandard deviation of untied ranks, 1, …, L, to the standard deviations\r\nof the tied ranks of the covariates,\\(\\rightarrow\\) the adjusted covariance\r\nmatrix has a constant diagonal\\(\\Longrightarrow\\) prevents heavily\r\ntied covariates, such as rare binary variables, from having increased\r\ninfluence due to reduced variance.\r\ncomputes the Mahalanobis distance using the ranks and this adjusted\r\ncovariance matrix.\r\n\r\n### Note that the covariance matrix now is on the rank-based matrix\r\ndata.r <- gen.tox %>%\r\n  dplyr::select(Age, Race, Smoker) %>%\r\n  as.matrix()\r\nfor (j in 1:dim(data.r)[2]){data.r[,j] <- rank(data.r[,j])}\r\ncov.m.r <- cov(data.r)\r\n\r\n## Step 1\r\nvar.m.w.r <- data.r[gen.tox$Group==\"Welders\",]\r\nvar.m.c.r <- data.r[gen.tox$Group==\"Controls\",]\r\n\r\n## Step 2:\r\nvar.untied <- var(1:dim(gen.tox)[1])\r\nrat <- sqrt(var.untied/diag(cov.m.r))\r\n\r\ncov.m.r.adjusted <- diag(rat) %*% cov.m.r %*% diag(rat)\r\ncov.m.r.adjusted.inv <- inv(cov.m.r.adjusted)\r\n\r\nm.d.m.r <- matrix(NA, nrow = 21, ncol = 26)\r\n\r\nfor (i in 1:21){\r\n  for (j in 1:26){\r\n    m.d.m.r[i,j] <- t(var.m.w.r[i,] - var.m.c.r[j,]) %*% cov.m.r.adjusted.inv %*% (var.m.w.r[i,] - var.m.c.r[j,])\r\n  }\r\n}\r\n\r\n# rank-based Mahalanobis distance\r\nWelder <- c(1:21)\r\nd.m.d.m.r <- as.data.frame(m.d.m.r)\r\nnames(d.m.d.m.r) <- paste0(\"Control \", 1:26)\r\nd.d.m.d.m.r <- cbind(Welder, d.m.d.m.r)\r\nkable(round(d.d.m.d.m.r[,1:7],2), caption = \"Rank-based Mahalanobis distance between 21 Welders vs 26 Controls dealed with Age, Race and Smoker\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:rank-based Mahalanobis distance)Rank-based Mahalanobis distance\r\nbetween 21 Welders vs 26 Controls dealed with Age, Race and Smoker\r\n\r\n\r\nWelder\r\n\r\n\r\nControl 1\r\n\r\n\r\nControl 2\r\n\r\n\r\nControl 3\r\n\r\n\r\nControl 4\r\n\r\n\r\nControl 5\r\n\r\n\r\nControl 6\r\n\r\n\r\n1\r\n\r\n\r\n4.61\r\n\r\n\r\n4.40\r\n\r\n\r\n5.98\r\n\r\n\r\n0.33\r\n\r\n\r\n2.69\r\n\r\n\r\n3.21\r\n\r\n\r\n2\r\n\r\n\r\n2.98\r\n\r\n\r\n0.70\r\n\r\n\r\n3.21\r\n\r\n\r\n0.47\r\n\r\n\r\n0.15\r\n\r\n\r\n0.28\r\n\r\n\r\n3\r\n\r\n\r\n6.91\r\n\r\n\r\n4.62\r\n\r\n\r\n0.84\r\n\r\n\r\n3.04\r\n\r\n\r\n3.66\r\n\r\n\r\n3.93\r\n\r\n\r\n4\r\n\r\n\r\n8.14\r\n\r\n\r\n14.80\r\n\r\n\r\n10.43\r\n\r\n\r\n7.69\r\n\r\n\r\n12.17\r\n\r\n\r\n13.00\r\n\r\n\r\n5\r\n\r\n\r\n9.03\r\n\r\n\r\n8.49\r\n\r\n\r\n3.93\r\n\r\n\r\n3.66\r\n\r\n\r\n6.55\r\n\r\n\r\n7.15\r\n\r\n\r\n6\r\n\r\n\r\n6.91\r\n\r\n\r\n4.62\r\n\r\n\r\n0.84\r\n\r\n\r\n3.04\r\n\r\n\r\n3.66\r\n\r\n\r\n3.93\r\n\r\n\r\n7\r\n\r\n\r\n10.20\r\n\r\n\r\n12.31\r\n\r\n\r\n12.86\r\n\r\n\r\n3.93\r\n\r\n\r\n9.30\r\n\r\n\r\n10.26\r\n\r\n\r\n8\r\n\r\n\r\n6.77\r\n\r\n\r\n3.29\r\n\r\n\r\n0.04\r\n\r\n\r\n3.92\r\n\r\n\r\n2.99\r\n\r\n\r\n3.05\r\n\r\n\r\n9\r\n\r\n\r\n6.91\r\n\r\n\r\n4.62\r\n\r\n\r\n0.84\r\n\r\n\r\n3.04\r\n\r\n\r\n3.66\r\n\r\n\r\n3.93\r\n\r\n\r\n10\r\n\r\n\r\n0.25\r\n\r\n\r\n4.72\r\n\r\n\r\n7.61\r\n\r\n\r\n3.03\r\n\r\n\r\n3.72\r\n\r\n\r\n4.01\r\n\r\n\r\n11\r\n\r\n\r\n6.71\r\n\r\n\r\n3.79\r\n\r\n\r\n0.28\r\n\r\n\r\n3.38\r\n\r\n\r\n3.18\r\n\r\n\r\n3.34\r\n\r\n\r\n12\r\n\r\n\r\n5.86\r\n\r\n\r\n6.33\r\n\r\n\r\n7.61\r\n\r\n\r\n0.98\r\n\r\n\r\n4.24\r\n\r\n\r\n4.89\r\n\r\n\r\n13\r\n\r\n\r\n6.98\r\n\r\n\r\n7.96\r\n\r\n\r\n9.02\r\n\r\n\r\n1.68\r\n\r\n\r\n5.59\r\n\r\n\r\n6.33\r\n\r\n\r\n14\r\n\r\n\r\n5.25\r\n\r\n\r\n5.41\r\n\r\n\r\n6.83\r\n\r\n\r\n0.64\r\n\r\n\r\n3.49\r\n\r\n\r\n4.08\r\n\r\n\r\n15\r\n\r\n\r\n6.91\r\n\r\n\r\n4.62\r\n\r\n\r\n0.84\r\n\r\n\r\n3.04\r\n\r\n\r\n3.66\r\n\r\n\r\n3.93\r\n\r\n\r\n16\r\n\r\n\r\n8.30\r\n\r\n\r\n9.78\r\n\r\n\r\n10.61\r\n\r\n\r\n2.56\r\n\r\n\r\n7.12\r\n\r\n\r\n7.96\r\n\r\n\r\n17\r\n\r\n\r\n9.03\r\n\r\n\r\n8.49\r\n\r\n\r\n3.93\r\n\r\n\r\n3.66\r\n\r\n\r\n6.55\r\n\r\n\r\n7.15\r\n\r\n\r\n18\r\n\r\n\r\n3.33\r\n\r\n\r\n0.05\r\n\r\n\r\n3.00\r\n\r\n\r\n1.68\r\n\r\n\r\n0.05\r\n\r\n\r\n0.01\r\n\r\n\r\n19\r\n\r\n\r\n7.34\r\n\r\n\r\n5.62\r\n\r\n\r\n1.58\r\n\r\n\r\n2.99\r\n\r\n\r\n4.34\r\n\r\n\r\n4.72\r\n\r\n\r\n20\r\n\r\n\r\n5.25\r\n\r\n\r\n5.41\r\n\r\n\r\n6.83\r\n\r\n\r\n0.64\r\n\r\n\r\n3.49\r\n\r\n\r\n4.08\r\n\r\n\r\n21\r\n\r\n\r\n7.34\r\n\r\n\r\n5.62\r\n\r\n\r\n1.58\r\n\r\n\r\n2.99\r\n\r\n\r\n4.34\r\n\r\n\r\n4.72\r\n\r\n\r\nAnother way to produce the rank-based Mahalanobis distance matrix:\r\nlook at the source code of smahal function.\r\n\r\nsource(\"smahal.R\")\r\nprint(smahal)\r\nfunction (z, X) \r\n{\r\n    X <- as.matrix(X)\r\n    n <- dim(X)[1]\r\n    rownames(X) <- 1:n\r\n    k <- dim(X)[2]\r\n    m <- sum(z)\r\n    for (j in 1:k) {\r\n        X[, j] <- rank(X[, j])\r\n    }\r\n    cv <- cov(X)\r\n    vuntied <- var(1:n)\r\n    rat <- sqrt(vuntied/diag(cv))\r\n    cv <- diag(rat) %*% cv %*% diag(rat)\r\n    out <- matrix(NA, m, n - m)\r\n    Xc <- X[z == 0, ]\r\n    Xt <- X[z == 1, ]\r\n    rownames(out) <- rownames(X)[z == 1]\r\n    colnames(out) <- rownames(X)[z == 0]\r\n    library(MASS)\r\n    icov <- ginv(cv)\r\n    for (i in 1:m) {\r\n        out[i, ] <- mahalanobis(Xc, Xt[i, ], icov, inverted = T)\r\n    }\r\n    out\r\n}\r\n\r\nIn welder data,\r\n2 individuals (Welder 2 vs Control 3) with the same Age\r\nand Race but different smoking behavior have a\r\nrank-based Mahalanobis distance of 3.21 (4.41 for a Mahalanobis\r\ndistance),\r\nwhereas 2 individuals (Welder 11 vs Control 14) with the same\r\nAge and Smoking behavior but different\r\nRace have a rank-based Mahalanobis distance of 3.07 (8.06\r\nfor a Mahalanobis distance),\r\n\r\n\r\n\\(\\Longrightarrow\\) a mismatch on\r\nrace is counted as about equal to a mismatch on\r\nsmoking.\r\n\r\nRank-based Mahalanobis distances within Propensity Score\r\ncalipers. An Inf signifies that the caliper is violated\r\n\r\nm.m.r <- matrix(NA, nrow = 21, ncol = 26)\r\nfor (i in 1:21){\r\n  for (j in 1:26){\r\n    m.m.r[i,j] <- ifelse(abs(ps.vec.w[i] - ps.vec.c[j]) > w, Inf, m.d.m.r[i,j])\r\n  }\r\n}\r\nWelder <- c(1:21)\r\nd.m.m.r <- as.data.frame(m.m.r)\r\nnames(d.m.m.r) <- paste0(\"Control \", 1:26)\r\nd.d.m.m.r <- cbind(Welder, d.m.m.r)\r\nkable(round(d.d.m.m.r[,1:7],2), caption = \"rank-based Mahalanobis distances within PS calipers. An `Inf` signifies that the caliper is violated.\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:r-mahal w PS caliper)rank-based Mahalanobis distances within PS\r\ncalipers. An Inf signifies that the caliper is violated.\r\n\r\n\r\nWelder\r\n\r\n\r\nControl 1\r\n\r\n\r\nControl 2\r\n\r\n\r\nControl 3\r\n\r\n\r\nControl 4\r\n\r\n\r\nControl 5\r\n\r\n\r\nControl 6\r\n\r\n\r\n1\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n5.98\r\n\r\n\r\n0.33\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n2\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.47\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n3\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n4\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n10.43\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n5\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n7\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n8\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.04\r\n\r\n\r\n3.92\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n9\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n10\r\n\r\n\r\n0.25\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n3.72\r\n\r\n\r\n4.01\r\n\r\n\r\n11\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.28\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n12\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n7.61\r\n\r\n\r\n0.98\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n13\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n9.02\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n14\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6.83\r\n\r\n\r\n0.64\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n15\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n16\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n10.61\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n17\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n18\r\n\r\n\r\n3.33\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n0.05\r\n\r\n\r\n0.01\r\n\r\n\r\n19\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n20\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n6.83\r\n\r\n\r\n0.64\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\n21\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nInf\r\n\r\n\r\nOptimal Pair Matching\r\nAn optimal pair matching pairs each treated subject\r\nwith a different control to minimize the total distance within matched\r\npairs.\r\nIn the welder data: forming 21 pairs using 21 different controls\r\nfrom the 26 potential controls so that the sum of the 21 distances\r\nwithin pairs is minimized.\r\nThe problem is serious: the closest control to one treated subject\r\nmay also be the closest control to another treated subject.\r\n\r\nHansen’s pairmatch function in his optmatch\r\npackage makes Bertsekas’ Fortran code (aka ‘assignment\r\nproblem’ solved by Kuhn in 1955) available from inside the\r\nstatistical package R;\r\nThe SAS program proc assign also solves the assignment\r\nproblem.\r\nMatch pair on PS\r\nI generated code in which the treated’s ps match with the closest\r\ncontrol’s ps. It seemed the marginal means were well balanced.\r\n\r\nctrl <- NULL\r\nk <- NULL\r\n\r\nd <- d.m.d.n[-1]\r\n\r\nfor (i in 1:21){\r\n  k <- which.min(d[i,])\r\n  ctrl[i] <- names(k)\r\n  d <- d[-k]\r\n}\r\n\r\nindex <- base::order(ctrl)\r\n\r\nWelders <- gen.tox %>%\r\n  filter(Group==\"Welders\") %>%\r\n  dplyr::select(Age, Race, Smoker, ps)\r\nWelders$Pair <- paste0(\"Pair \",1:21)\r\nWelders <- Welders[,c(\"Pair\",\"Age\",\"Race\",\"Smoker\",\"ps\")]\r\n\r\nControls <- gen.tox %>%\r\n  filter(Group==\"Controls\") %>%\r\n  dplyr::select(Age, Race, Smoker, ps)\r\n\r\nControls$id <- paste0(\"Control \",1:26)\r\n\r\nmatched.controls <- Controls[which(Controls$id %in% ctrl),]\r\n\r\n# Reorder follow the ctrl match order\r\nmatched.controls$id <- reorder.factor(matched.controls$id, new.order=ctrl)\r\n#matched.controls <- matched.controls[-5]\r\n\r\nmatched.controls <- matched.controls %>% dplyr::arrange(id)\r\n\r\nkable(cbind(Welders,matched.controls), caption=\"Pair match using the closed measure in the PS\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:pairmatch using closest ps)Pair match using the closed measure in\r\nthe PS\r\n\r\n\r\nPair\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nid\r\n\r\n\r\nPair 1\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.47\r\n\r\n\r\nControl 3\r\n\r\n\r\nPair 2\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n47\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.28\r\n\r\n\r\nControl 8\r\n\r\n\r\nPair 3\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\nControl 10\r\n\r\n\r\nPair 4\r\n\r\n\r\n33\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.51\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\nControl 9\r\n\r\n\r\nPair 5\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\nControl 12\r\n\r\n\r\nPair 6\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n31\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.55\r\n\r\n\r\nControl 15\r\n\r\n\r\nPair 7\r\n\r\n\r\n27\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.68\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\nControl 18\r\n\r\n\r\nPair 8\r\n\r\n\r\n43\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.49\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.42\r\n\r\n\r\nControl 4\r\n\r\n\r\nPair 9\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\nControl 20\r\n\r\n\r\nPair 10\r\n\r\n\r\n43\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n48\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.14\r\n\r\n\r\nControl 1\r\n\r\n\r\nPair 11\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.53\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\nControl 22\r\n\r\n\r\nPair 12\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.50\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\nControl 11\r\n\r\n\r\nPair 13\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.40\r\n\r\n\r\nControl 13\r\n\r\n\r\nPair 14\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\nControl 24\r\n\r\n\r\nPair 15\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n30\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.63\r\n\r\n\r\nControl 25\r\n\r\n\r\nPair 16\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\nControl 26\r\n\r\n\r\nPair 17\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.67\r\n\r\n\r\nControl 21\r\n\r\n\r\nPair 18\r\n\r\n\r\n53\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.19\r\n\r\n\r\n50\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.23\r\n\r\n\r\nControl 5\r\n\r\n\r\nPair 19\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n41\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.35\r\n\r\n\r\nControl 14\r\n\r\n\r\nPair 20\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\nControl 19\r\n\r\n\r\nPair 21\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n45\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.32\r\n\r\n\r\nControl 23\r\n\r\n\r\n\r\nm.age.mps <- round(mean(matched.controls$Age),0)\r\n\r\naa.p.mps <- round(table(matched.controls$Race)[2]/21*100,0)\r\n\r\ns.p.mps <- round(table(matched.controls$Smoker)[2]/21*100,0)\r\n\r\nps.mps <- round(mean(matched.controls$ps),2)\r\n\r\nWelders — Matched Controls\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n—\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n38\r\n10\r\n52\r\n0.51\r\n—\r\n40\r\n14\r\n38\r\n0.46\r\n\\(\\Longrightarrow\\) the marginal\r\nmeans the matching above are well balanced as expected\r\nNext, I use the pairmatch function in\r\noptmatch package to compare.\r\n\r\ngen.tox.op <- gen.tox\r\ngen.tox.op$Group1 <- as.numeric(ifelse(gen.tox.op$Group==\"Welders\", 1,0))\r\n#pairmatch(match_on( Group~ps, data=gen.tox ) )# did not run\r\npm <- pairmatch(Group1 ~ ps, data = gen.tox.op)\r\nprint(pairmatch(Group1 ~ ps, data = gen.tox.op), grouped = TRUE) # knowing the ordered id of treated and controls\r\n Group Members\r\n   1.1   1, 35\r\n  1.10  18, 27\r\n  1.11  19, 46\r\n  1.12   2, 29\r\n  1.13  20, 34\r\n  1.14  21, 39\r\n  1.15   3, 41\r\n  1.16   4, 30\r\n  1.17   5, 47\r\n  1.18   6, 43\r\n  1.19   7, 42\r\n   1.2  10, 26\r\n  1.20   8, 45\r\n  1.21   9, 31\r\n   1.3  11, 25\r\n   1.4  12, 40\r\n   1.5  13, 44\r\n   1.6  14, 24\r\n   1.7  15, 36\r\n   1.8  16, 32\r\n   1.9  17, 33\r\nsummary(pairmatch(Group1 ~ ps, data = gen.tox.op))\r\nStructure of matched sets:\r\n1:1 0:1 \r\n 21   5 \r\nEffective Sample Size:  21 \r\n(equivalent number of matched pairs).\r\n#all.equal(names(pm), row.names(gen.tox))\r\n\r\n## Housekeeping\r\nipm <- as.integer(pm)\r\ngen.tox.op <- cbind(gen.tox.op, matches=pm, ipm)\r\ndata.pairmatch<-gen.tox.op[matched(pm),] # only select matched cases\r\n\r\nWelders <- data.pairmatch %>%\r\n  filter(Group==\"Welders\") %>%\r\n  arrange(ipm) %>%\r\n  dplyr::select(Age, Race, Smoker, ps, ipm)\r\nWelders$Pair <- paste0(\"Pair \",1:21)\r\nWelders <- Welders[,c(\"Pair\",\"Age\",\"Race\",\"Smoker\",\"ps\",\"ipm\")]\r\n\r\nmatched.controls <- data.pairmatch %>%\r\n  filter(Group==\"Controls\") %>%\r\n  arrange(ipm) %>%\r\n  dplyr::select(Age, Race, Smoker, ps, ipm)\r\n\r\nkable(cbind(Welders,matched.controls), caption=\"Optimal pair match using the squared difference in the PS\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 2: Optimal pair match using the squared difference in\r\nthe PS\r\n\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nipm\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nipm\r\n\r\n\r\n1\r\n\r\n\r\nPair 1\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n1\r\n\r\n\r\n41\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.35\r\n\r\n\r\n1\r\n\r\n\r\n18\r\n\r\n\r\nPair 2\r\n\r\n\r\n53\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.19\r\n\r\n\r\n2\r\n\r\n\r\n52\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n2\r\n\r\n\r\n19\r\n\r\n\r\nPair 3\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n3\r\n\r\n\r\n30\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.63\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nPair 4\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n4\r\n\r\n\r\n47\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.28\r\n\r\n\r\n4\r\n\r\n\r\n20\r\n\r\n\r\nPair 5\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n5\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.40\r\n\r\n\r\n5\r\n\r\n\r\n21\r\n\r\n\r\nPair 6\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n6\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n6\r\n\r\n\r\n3\r\n\r\n\r\nPair 7\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n7\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n7\r\n\r\n\r\n4\r\n\r\n\r\nPair 8\r\n\r\n\r\n33\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.51\r\n\r\n\r\n8\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n8\r\n\r\n\r\n5\r\n\r\n\r\nPair 9\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n9\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n9\r\n\r\n\r\n6\r\n\r\n\r\nPair 10\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n10\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n10\r\n\r\n\r\n7\r\n\r\n\r\nPair 11\r\n\r\n\r\n27\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.68\r\n\r\n\r\n11\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.67\r\n\r\n\r\n11\r\n\r\n\r\n10\r\n\r\n\r\nPair 12\r\n\r\n\r\n43\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n12\r\n\r\n\r\n50\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.23\r\n\r\n\r\n12\r\n\r\n\r\n8\r\n\r\n\r\nPair 13\r\n\r\n\r\n43\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.49\r\n\r\n\r\n13\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n13\r\n\r\n\r\n9\r\n\r\n\r\nPair 14\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n14\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n14\r\n\r\n\r\n11\r\n\r\n\r\nPair 15\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.53\r\n\r\n\r\n15\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.42\r\n\r\n\r\n15\r\n\r\n\r\n12\r\n\r\n\r\nPair 16\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.50\r\n\r\n\r\n16\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n16\r\n\r\n\r\n13\r\n\r\n\r\nPair 17\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n17\r\n\r\n\r\n45\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.32\r\n\r\n\r\n17\r\n\r\n\r\n14\r\n\r\n\r\nPair 18\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n18\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.47\r\n\r\n\r\n18\r\n\r\n\r\n15\r\n\r\n\r\nPair 19\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n19\r\n\r\n\r\n31\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.55\r\n\r\n\r\n19\r\n\r\n\r\n16\r\n\r\n\r\nPair 20\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n20\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n20\r\n\r\n\r\n17\r\n\r\n\r\nPair 21\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n21\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n21\r\n\r\n\r\n\r\nm.age.mps <- round(mean(matched.controls$Age),0)\r\n\r\naa.p.mps <- round(table(matched.controls$Race)[2]/21*100,0)\r\n\r\ns.p.mps <- round(table(matched.controls$Smoker)[2]/21*100,0)\r\n\r\nps.mps <- round(mean(matched.controls$ps),2)\r\n\r\nWelders — Matched Controls\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n—\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n38\r\n10\r\n52\r\n0.51\r\n—\r\n40\r\n10\r\n38\r\n0.46\r\n\\(\\Longrightarrow\\) the marginal\r\nmeans the matching above are fairly well balanced, but the individual\r\npairs are often not matched for race or smoking.\r\nMatch\r\npair on Mahalanobis distance within PS calipers\r\nThe caliper is half the standard deviation of the PS, or\r\n0.172/2 = 0.086\r\n\r\nz<-1*(gen.tox$Group==\"Welders\")\r\naa<-gen.tox$Race\r\nsmoker=gen.tox$Smoker\r\nage<-gen.tox$Age\r\nx<-cbind(age,aa,smoker)\r\n# Mahalanobis distances\r\ndmat<-mahal(z,x)\r\n\r\n# Impose propensity score calipers\r\nprop<-gen.tox$ps # propensity score\r\n# Mahalanobis distanced penalized for violations of a propensity score caliper.\r\n# This version is used for numerical work.\r\ndmat<-addcaliper(dmat,z,prop,caliper=.5)\r\n\r\n# Find the minimum distance match within propensity score calipers.\r\npm.cal<-optmatch::pairmatch(dmat,data=gen.tox)#, remove.unmatchables = TRUE)\r\n\r\nipm.cal <- as.integer(pm.cal)\r\ngen.tox.op.caliper <- cbind(gen.tox, matches=pm.cal,ipm.cal)\r\ndata.pairmatch.cal<-gen.tox.op.caliper[matched(pm.cal),] # only select matched cases\r\n\r\nWelders <- data.pairmatch.cal %>%\r\n  filter(Group==\"Welders\") %>%\r\n  arrange(ipm.cal) %>%\r\n  dplyr::select(Age, Race, Smoker, ps,ipm.cal)\r\nWelders$Pair <- paste0(\"Pair \",1:21)\r\nWelders <- Welders[,c(\"Pair\",\"Age\",\"Race\",\"Smoker\",\"ps\",\"ipm.cal\")]\r\n\r\nmatched.controls <- data.pairmatch.cal %>%\r\n  filter(Group==\"Controls\") %>%\r\n  arrange(ipm.cal) %>%\r\n  dplyr::select(Age, Race, Smoker, ps, ipm.cal)\r\n\r\nkable(cbind(Welders,matched.controls), caption=\"Optimal pair match using the Mahalanobis distance within PS calipers\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 3: Optimal pair match using the Mahalanobis distance\r\nwithin PS calipers\r\n\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nipm.cal\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nipm.cal\r\n\r\n\r\n1\r\n\r\n\r\nPair 1\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n1\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n1\r\n\r\n\r\n18\r\n\r\n\r\nPair 2\r\n\r\n\r\n53\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.19\r\n\r\n\r\n2\r\n\r\n\r\n52\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n2\r\n\r\n\r\n19\r\n\r\n\r\nPair 3\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n3\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nPair 4\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n4\r\n\r\n\r\n47\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.28\r\n\r\n\r\n4\r\n\r\n\r\n20\r\n\r\n\r\nPair 5\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n5\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n5\r\n\r\n\r\n21\r\n\r\n\r\nPair 6\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n6\r\n\r\n\r\n31\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.55\r\n\r\n\r\n6\r\n\r\n\r\n3\r\n\r\n\r\nPair 7\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n7\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n7\r\n\r\n\r\n4\r\n\r\n\r\nPair 8\r\n\r\n\r\n33\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.51\r\n\r\n\r\n8\r\n\r\n\r\n41\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.35\r\n\r\n\r\n8\r\n\r\n\r\n5\r\n\r\n\r\nPair 9\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n9\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n9\r\n\r\n\r\n6\r\n\r\n\r\nPair 10\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n10\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n10\r\n\r\n\r\n7\r\n\r\n\r\nPair 11\r\n\r\n\r\n27\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.68\r\n\r\n\r\n11\r\n\r\n\r\n30\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.63\r\n\r\n\r\n11\r\n\r\n\r\n10\r\n\r\n\r\nPair 12\r\n\r\n\r\n43\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n12\r\n\r\n\r\n48\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.14\r\n\r\n\r\n12\r\n\r\n\r\n8\r\n\r\n\r\nPair 13\r\n\r\n\r\n43\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.49\r\n\r\n\r\n13\r\n\r\n\r\n45\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.32\r\n\r\n\r\n13\r\n\r\n\r\n9\r\n\r\n\r\nPair 14\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n14\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n14\r\n\r\n\r\n11\r\n\r\n\r\nPair 15\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.53\r\n\r\n\r\n15\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.47\r\n\r\n\r\n15\r\n\r\n\r\n12\r\n\r\n\r\nPair 16\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.50\r\n\r\n\r\n16\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.40\r\n\r\n\r\n16\r\n\r\n\r\n13\r\n\r\n\r\nPair 17\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n17\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.42\r\n\r\n\r\n17\r\n\r\n\r\n14\r\n\r\n\r\nPair 18\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n18\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n18\r\n\r\n\r\n15\r\n\r\n\r\nPair 19\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n19\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n19\r\n\r\n\r\n16\r\n\r\n\r\nPair 20\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n20\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n20\r\n\r\n\r\n17\r\n\r\n\r\nPair 21\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n21\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.67\r\n\r\n\r\n21\r\n\r\n\r\n\r\nm.age.mps <- round(mean(matched.controls$Age),0)\r\n\r\naa.p.mps <- round(table(matched.controls$Race)[2]/21*100,0)\r\n\r\ns.p.mps <- round(table(matched.controls$Smoker)[2]/21*100,0)\r\n\r\nps.mps <- round(mean(matched.controls$ps),2)\r\n\r\nWelders — Matched Controls\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n—\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n38\r\n10\r\n52\r\n0.51\r\n—\r\n40\r\n14\r\n38\r\n0.45\r\nMatch\r\npair on\r\nrank-based Mahalanobis distance within PS calipers\r\nThe difference here is just substitute the mahal by\r\nsmahal\r\n\r\nz<-1*(gen.tox$Group==\"Welders\")\r\naa<-gen.tox$Race\r\nsmoker=gen.tox$Smoker\r\nage<-gen.tox$Age\r\nx<-cbind(age,aa,smoker)\r\n# rank-based Mahalanobis distances\r\ndmat<-smahal(z,x)\r\n\r\n# Impose propensity score calipers\r\nprop<-gen.tox$ps # propensity score\r\n# Mahalanobis distanced penalized for violations of a propensity score caliper.\r\n# This version is used for numerical work.\r\ndmat<-addcaliper(dmat,z,prop,caliper=.5)\r\n\r\n# Find the minimum distance match within propensity score calipers.\r\npm.cal<-optmatch::pairmatch(dmat,data=gen.tox)#, remove.unmatchables = TRUE)\r\n\r\nipm.cal <- as.integer(pm.cal)\r\ngen.tox.op.caliper <- cbind(gen.tox, matches=pm.cal,ipm.cal)\r\ndata.pairmatch.cal<-gen.tox.op.caliper[matched(pm.cal),] # only select matched cases\r\n\r\nWelders <- data.pairmatch.cal %>%\r\n  filter(Group==\"Welders\") %>%\r\n  arrange(ipm.cal) %>%\r\n  dplyr::select(Age, Race, Smoker, ps,ipm.cal)\r\nWelders$Pair <- paste0(\"Pair \",1:21)\r\nWelders <- Welders[,c(\"Pair\",\"Age\",\"Race\",\"Smoker\",\"ps\",\"ipm.cal\")]\r\n\r\nmatched.controls <- data.pairmatch.cal %>%\r\n  filter(Group==\"Controls\") %>%\r\n  arrange(ipm.cal) %>%\r\n  dplyr::select(Age, Race, Smoker, ps, ipm.cal)\r\n\r\nkable(cbind(Welders,matched.controls), caption=\"Optimal pair match using the Mahalanobis distance within PS calipers\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 4: Optimal pair match using the Mahalanobis distance\r\nwithin PS calipers\r\n\r\n\r\n\r\n\r\nPair\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nipm.cal\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nipm.cal\r\n\r\n\r\n1\r\n\r\n\r\nPair 1\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n1\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n1\r\n\r\n\r\n18\r\n\r\n\r\nPair 2\r\n\r\n\r\n53\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.19\r\n\r\n\r\n2\r\n\r\n\r\n52\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n2\r\n\r\n\r\n19\r\n\r\n\r\nPair 3\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n3\r\n\r\n\r\n31\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.55\r\n\r\n\r\n3\r\n\r\n\r\n2\r\n\r\n\r\nPair 4\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n4\r\n\r\n\r\n47\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.28\r\n\r\n\r\n4\r\n\r\n\r\n20\r\n\r\n\r\nPair 5\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n5\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n5\r\n\r\n\r\n21\r\n\r\n\r\nPair 6\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n6\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n6\r\n\r\n\r\n3\r\n\r\n\r\nPair 7\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n7\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n7\r\n\r\n\r\n4\r\n\r\n\r\nPair 8\r\n\r\n\r\n33\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.51\r\n\r\n\r\n8\r\n\r\n\r\n41\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.35\r\n\r\n\r\n8\r\n\r\n\r\n5\r\n\r\n\r\nPair 9\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n9\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n9\r\n\r\n\r\n6\r\n\r\n\r\nPair 10\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n10\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n10\r\n\r\n\r\n7\r\n\r\n\r\nPair 11\r\n\r\n\r\n27\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.68\r\n\r\n\r\n11\r\n\r\n\r\n30\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.63\r\n\r\n\r\n11\r\n\r\n\r\n10\r\n\r\n\r\nPair 12\r\n\r\n\r\n43\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n12\r\n\r\n\r\n48\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.14\r\n\r\n\r\n12\r\n\r\n\r\n8\r\n\r\n\r\nPair 13\r\n\r\n\r\n43\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.49\r\n\r\n\r\n13\r\n\r\n\r\n45\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.32\r\n\r\n\r\n13\r\n\r\n\r\n9\r\n\r\n\r\nPair 14\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n14\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n14\r\n\r\n\r\n11\r\n\r\n\r\nPair 15\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.53\r\n\r\n\r\n15\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.47\r\n\r\n\r\n15\r\n\r\n\r\n12\r\n\r\n\r\nPair 16\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.50\r\n\r\n\r\n16\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.40\r\n\r\n\r\n16\r\n\r\n\r\n13\r\n\r\n\r\nPair 17\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n17\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.42\r\n\r\n\r\n17\r\n\r\n\r\n14\r\n\r\n\r\nPair 18\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n18\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n18\r\n\r\n\r\n15\r\n\r\n\r\nPair 19\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n19\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n19\r\n\r\n\r\n16\r\n\r\n\r\nPair 20\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n20\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n20\r\n\r\n\r\n17\r\n\r\n\r\nPair 21\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n21\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.67\r\n\r\n\r\n21\r\n\r\n\r\n\r\nm.age.mps <- round(mean(matched.controls$Age),0)\r\n\r\naa.p.mps <- round(table(matched.controls$Race)[2]/21*100,0)\r\n\r\ns.p.mps <- round(table(matched.controls$Smoker)[2]/21*100,0)\r\n\r\nps.mps <- round(mean(matched.controls$ps),2)\r\n\r\nWelders — Matched Controls\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n—\r\nMean Age\r\nAA\r\nSmoker\r\n\\(\\hat{e}(\\mathbf{x})\\)\r\n38\r\n10\r\n52\r\n0.51\r\n—\r\n40\r\n14\r\n38\r\n0.45\r\nOptimal Matching with\r\nMultiple Controls\r\nIn matching with multiple controls, each treated subject is matched\r\nto at least one, and possibly more than one, control:\r\nTo match in a fixed ratio is to match each treated\r\nsubject to the same number of controls; for instance, pair matching in a\r\nratio of 1-to-1, or matching in a ratio of 1-to-2.\r\nTo match in a variable ratio: to allow the number of\r\ncontrols to vary from one treated subject to another. In the welder data\r\nabove, allows the possibility of matching with multiple controls in a\r\nvariable ratio, and in particular to use all of the controls.\r\nThe decision to match in fixed or\r\nvariable ratio that affects both:\r\nthe quality of the matching,\r\nthe analysis, and\r\npresentation of results.\r\nMatching with a variable number of controls is slightly more\r\ncomplex:\r\nThe optimization algorithm decides:\r\nwho is matched to whom,\r\nhow many controls to assign to each treated subject.\r\n\r\nThe choices of number of controls must be constrained in some\r\nreasonable way to avoid trivial results.\r\nwithout constraints imposed, if all the distances were positive, the\r\nminimum distance matching with variable numbers of controls would always\r\nbe a pair matching.\r\nwith a simple constraint is to insist that a certain number of\r\ncontrols be used.\r\nfor example of the welder data, one might insist that all 26\r\ncontrols be used. Alternatively, one might insist that 23 controls be\r\nused, which would permit, but not require, the algorithm to discard the\r\n3 potential controls who are older than all of the welders.\r\n\r\n\r\nAn attractive feature of fixing the total number of controls\r\nthe total distance = a sum of a fixed number of distances.\r\nto constraining the total number of controls: a different type of\r\nconstraint permits a treated subject to have at least one but at most\r\nthree controls.\r\n\r\ngiven some set of constraints on the matching, an optimal matching\r\nwith variable controls minimizes the total distance between\r\ntreated subjects and controls inside matched sets.\r\n\r\n\r\n\r\n\r\nThe principal advantage of matching in a fixed ratio:\r\nsummary statistics, including those that might be displayed in\r\ngraphs, may be computed from treated and control groups in the usual\r\nway, without using direct adjustment to give unequal\r\nweights to observations. This advantage will weigh heavily when\r\npotential controls are abundant and the biases that must be removed are\r\nnot large (!?!).\r\nA smaller issue is statistical efficiency, which in nominal terms,\r\nthough often not in practical terms, slightly favors matching in fixed\r\nratio over matching with a variable ratio (Ming and\r\nRosenbaum 2000).\r\nSeveral advantages to matching with a variable ratio Paul R. Rosenbaum (1989):\r\nFirst, the matched sets will be more closely matched, in the\r\nfollowing precise sense. If one finds the minimum distance match with,\r\nsay, an average of three controls per treated subject and two to four\r\ncontrols for each treated subject, the total distance within matched\r\nsets will never be larger, and will typically be quite a bit smaller,\r\nthan in fixed ratio matching with three controls.4 This is visible in\r\nTable: the two welders with low propensity scores were matched to the\r\nseven controls with low propensity scores, and trying to allocate these\r\nseven more evenly among welders would have produced a larger mismatch on\r\nthese propensity scores.\r\nSecond, matching in fixed ratio requires the number of controls to\r\nbe an integer multiple of the number of treated subjects, and this\r\nrestriction may be inconvenient or undesirable for any of a variety of\r\nreasons; for instance, for the welder data, the only possible matching\r\nin fixed ratio is pair matching.\r\nThird, just as §8.1 discussed definite limits to what can be\r\naccomplished with pair matching, there are also definite limits to what\r\ncan be accomplished by matching with multiple controls, but these limits\r\nare better when matching with a variable ratio.\r\nFinding an optimal match with variable controls is equivalent to\r\nsolving a particular minimum-cost flow problem in a network (Paul R. Rosenbaum 1989).\r\nOptimal Full Matching\r\nIn full matching, besides matching with a variable number of\r\ncontrols, i.e. a treated subject could be matched to one or more\r\ncontrols, the reverse situation is permitted: one control may be matched\r\nto several treated subjects (P. R.\r\nRosenbaum 1991).\r\nAs in matching with variable controls, summary statistics for the\r\ncontrol group in full matching must be directly adjusted.\r\nFull matching can be vastly better than pair matching or matching\r\nwith a variable number of controls. The matched sets would be quite\r\nhomogeneous, the quite unequal set sizes can lead to some\r\ninefficiency.\r\nIn one specific sense, an optimal full matching is an optimal\r\ndesign for an observational study (P. R.\r\nRosenbaum 1991). Specifically, define a stratification to be\r\na partitioning of the subjects into groups or strata based on the\r\ncovariates with the one requirement that each stratum must contain at\r\nleast one treated subject and at least one control. The quality of a\r\nstratification might reasonably be judged by a weighted average of all\r\nthe within strata distances between treated subjects and\r\ncontrols.\r\nNo matter which of these weightings is used, no matter what\r\ndistance is used, there is always a full matching that minimizes this\r\nweighted average distance (P. R. Rosenbaum\r\n1991).\r\n\r\nm<-fullmatch(dmat, data=gen.tox, min.controls = 1/7,max.controls = 7)\r\nlength(m)\r\n[1] 47\r\nsum(matched(m))\r\n[1] 47\r\n# Housekeeping\r\nim<-as.integer(m)\r\ngen.tox.fm<-cbind(gen.tox,im)\r\ng.t.m<-gen.tox[matched(m),]\r\n\r\nWelders <- gen.tox.fm %>%\r\n  filter(Group==\"Welders\") %>%\r\n  arrange(im) %>%\r\n  dplyr::select(Age, Race, Smoker, ps, im)\r\n#Welders$Pair <- paste0(\"Pair \",1:21)\r\nWelders <- Welders[,c(\"Age\",\"Race\",\"Smoker\",\"ps\",\"im\")]\r\n\r\nfmatched.controls <- gen.tox.fm %>%\r\n  filter(Group==\"Controls\") %>%\r\n  arrange(im) %>%\r\n  dplyr::select(Age, Race, Smoker, ps, im)\r\n\r\nkable(list(Welders,fmatched.controls), caption=\"Optimal full match using the Mahalanobis distance within PS calipers\")  %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 5: Optimal full match using the Mahalanobis distance\r\nwithin PS calipers\r\n\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nim\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n1\r\n\r\n\r\n53\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.19\r\n\r\n\r\n2\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n3\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n4\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.60\r\n\r\n\r\n5\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n6\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n6\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n6\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.53\r\n\r\n\r\n6\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n6\r\n\r\n\r\n33\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.51\r\n\r\n\r\n7\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n8\r\n\r\n\r\n27\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.68\r\n\r\n\r\n9\r\n\r\n\r\n43\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n10\r\n\r\n\r\n43\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.49\r\n\r\n\r\n11\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.50\r\n\r\n\r\n12\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n12\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n13\r\n\r\n\r\n37\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.48\r\n\r\n\r\n13\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n14\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n15\r\n\r\n\r\n\r\n\r\nAge\r\n\r\n\r\nRace\r\n\r\n\r\nSmoker\r\n\r\n\r\nps\r\n\r\n\r\nim\r\n\r\n\r\n40\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.42\r\n\r\n\r\n1\r\n\r\n\r\n63\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.09\r\n\r\n\r\n2\r\n\r\n\r\n50\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.23\r\n\r\n\r\n2\r\n\r\n\r\n52\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.20\r\n\r\n\r\n2\r\n\r\n\r\n56\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.15\r\n\r\n\r\n2\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n3\r\n\r\n\r\n47\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.28\r\n\r\n\r\n4\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n4\r\n\r\n\r\n41\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.40\r\n\r\n\r\n4\r\n\r\n\r\n41\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.35\r\n\r\n\r\n4\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.34\r\n\r\n\r\n4\r\n\r\n\r\n45\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.32\r\n\r\n\r\n4\r\n\r\n\r\n42\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.38\r\n\r\n\r\n4\r\n\r\n\r\n36\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.64\r\n\r\n\r\n5\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.57\r\n\r\n\r\n6\r\n\r\n\r\n31\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.55\r\n\r\n\r\n7\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.65\r\n\r\n\r\n8\r\n\r\n\r\n30\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.63\r\n\r\n\r\n9\r\n\r\n\r\n48\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.14\r\n\r\n\r\n10\r\n\r\n\r\n56\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0.13\r\n\r\n\r\n10\r\n\r\n\r\n51\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0.12\r\n\r\n\r\n10\r\n\r\n\r\n44\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.47\r\n\r\n\r\n11\r\n\r\n\r\n35\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.52\r\n\r\n\r\n12\r\n\r\n\r\n38\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.46\r\n\r\n\r\n13\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0.54\r\n\r\n\r\n14\r\n\r\n\r\n34\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0.67\r\n\r\n\r\n15\r\n\r\n\r\n\r\nEfficiency\r\n(be continued)\r\nSummary\r\nMatching on one variable, the propensity score, tends to\r\nproduce treated and control groups that, in aggregate, are balanced with\r\nrespect to observed covariates;\r\n\\(\\rightarrow\\) Cons: individual\r\npairs that are close on the propensity score may differ widely on\r\nspecific covariates.\r\n\\(\\rightarrow\\) Solution: to form\r\ncloser pairs, a distance is used that penalizes\r\nlarge differences on the propensity score, and then finds individual\r\npairs that are as close as possible.\r\nPairs or matched sets are constructed using an optimization\r\nalgorithm. Matching with variable controls and\r\nfull matching combine elements of matching with elements of\r\ndirect adjustment.\r\nFull matching can often produce closer matches than\r\npair matching.\r\nFurther Reading\r\nChapter 9, Basic Tools of Multivariate Matching from\r\nRosenbaum, Paul R. Design of Observational Studies. 2nd ed. 2020.,\r\nSpringer International Publishing, 2020, https://doi.org/10.1007/978-3-030-46405-9.\r\n\r\n\r\nMahalanobis, P. C. 1936. “On the Generalized Distance in\r\nStatistics.” Journal Article. Proceedings of Indian National\r\nScience Academy 2 (1): 49–55.\r\n\r\n\r\nMing, K., and P. R. Rosenbaum. 2000. “Substantial Gains in Bias\r\nReduction from Matching with a Variable Number of Controls.”\r\nJournal Article. Biometrics 56 (1): 118–24. https://doi.org/10.1111/j.0006-341X.2000.00118.x.\r\n\r\n\r\nRosenbaum, P. R. 1991. “A Characterization of Optimal Designs for\r\nObservational Studies.” Journal Article. Journal of the Royal\r\nStatistical Society. Series B (Methodological) 53 (3): 597–610. www.jstor.org/stable/2345589.\r\n\r\n\r\nRosenbaum, Paul R. 1989. “Optimal Matching for Observational\r\nStudies.” Journal Article. Journal of the American\r\nStatistical Association 84 (408): 1024–32. https://doi.org/10.2307/2290079.\r\n\r\n\r\nRosenbaum, Paul R., and Donald B. Rubin. 1985. “Constructing a\r\nControl Group Using Multivariate Matched Sampling Methods That\r\nIncorporate the Propensity Score.” Journal Article. The\r\nAmerican Statistician 39 (1): 33–38. https://doi.org/10.2307/2683903.\r\n\r\n\r\nRubin, Donald B. 1980. “Bias Reduction Using Mahalanobis-Metric\r\nMatching.” Journal Article. Biometrics 36 (2): 293–98.\r\nhttps://doi.org/10.2307/2529981.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-14T16:39:30-06:00",
    "input_file": "Basic-Tools-of-Multivariate-Matching.knit.md"
  },
  {
    "path": "posts/2022-04-04-Bayesian methods - Series 8 of 10/",
    "title": "Series 8 - Ordinal Response & Probit Model",
    "description": "How to fit an Ordinal Regression (unconditional/conditional) in Bayes   \nCompare 2 groups",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-04-04",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nOrdinal\r\nResponse\r\nSingle Group\r\nTwo Groups\r\n\r\nProbit Model with Metric\r\nPredictors\r\nSingle predictor:\r\nHappiness and Money\r\n\r\n\r\nOrdinal Response\r\nOrdinal responses are ordered; include Likert scales for agreement\r\nwith attitude statements (e.g., disagree, neither agree nor disagree,\r\nand agree) and reported frequencies of doing something such as helping\r\nchildren with homework (e.g., daily, several times per week,\r\noccasionally, and never).\r\nOrdinal responses are often modeled with probit regression.\r\nAccording to probit approach there is a Gaussian latent variable \\(z\\sim N(\\mu,\\sigma)\\) that may depend on\r\nsome predictors.\r\nLatent variable \\(z\\) defines the\r\nordinal output \\(y = k, \\ k:\\{1,\r\n...,K\\}\\) through formulas\r\n\\[\\begin{align*}\r\nP(y = k \\mid \\mu, \\sigma, \\theta_j) &= \\phi\\Big(\\frac{\\theta_k -\r\n\\mu}{\\sigma}\\Big) - \\phi\\Big(\\frac{\\theta_{k-1} - \\mu}{\\sigma}\\Big), \\\r\n&k = 2, ..., K-1 \\\\\r\nP(y = k \\mid \\mu, \\sigma, \\theta_j) &= \\phi\\Big(\\frac{\\theta_1 -\r\n\\mu}{\\sigma}\\Big), \\ &k = 1 \\\\\r\nP(y = k \\mid \\mu, \\sigma, \\theta_j) &= 1 -\r\n\\phi\\Big(\\frac{\\theta_{k-1} - \\mu}{\\sigma}\\Big), \\ &k = K\r\n\\end{align*}\\]\r\nSingle Group\r\nConsider the simplest case when latent variable is standard normal\r\ndoes not have predictors, threshold parameters have uniform prior.\r\nThere is only one ordinal variable.\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=2> K;  // num of y classes\r\n    int<lower=0> N;  // num of observations\r\n    int<lower=1,upper=K> y[N];\r\n}\r\nparameters {\r\n    ordered[K-1] c;   // thersholds\r\n}\r\nmodel {\r\n    vector[K] theta;\r\n    for (n in 1:N) {\r\n        theta[1] = Phi(c[1]);\r\n        for (k in 2:(K-1))\r\n            theta[k] = Phi(c[k]) - Phi(c[k-1]);\r\n        theta[K] = 1 - Phi(c[K-1]);\r\n        y[n] ~ categorical(theta);\r\n    }\r\n}\r\n\"\r\n\r\nPhi(T x)\r\nstandard normal cumulative distribution function of x\r\nNote that in the description above prior is not defined.\r\nNot defined prior is equivalent to uniform prior. Omitting definition of\r\nprior when variables are bound results in uniform prior on [0,1]. If the\r\nvariable is unbound the prior is improper uniform.\r\nRead the data.\r\nThe sample is just one column of ratings between 1 and 7.\r\nCalculate frequencies of different levels of the rating.\r\n\r\nmdta = read.csv(file = \"data/OrdinalProbitData-1grp.csv\")\r\nhead(mdta)\r\n  Y\r\n1 1\r\n2 1\r\n3 1\r\n4 1\r\n5 1\r\n6 1\r\ntable(mdta$Y)\r\n\r\n 1  2  3  4  5  6  7 \r\n58 15 12  8  4  2  1 \r\n(frequencies <- prop.table(table(mdta$Y)))\r\n\r\n   1    2    3    4    5    6    7 \r\n0.58 0.15 0.12 0.08 0.04 0.02 0.01 \r\n\r\nCompile the model DSO and run Markov Chains.\r\n\r\nmodel <- stan_model(model_code=modelString)\r\nfit <- sampling(model,\r\n                data=list(N=nrow(mdta),  # num of observations\r\n                          K=max(mdta$Y), # num of outcome classes\r\n                          y=mdta$Y),\r\n                pars=c('c'),\r\n                iter=5000, chains = 2, cores = 2)\r\n\r\n\r\n# analyze fitted model using shinystan\r\nlaunch_shinystan(fit)\r\n\r\nCheck the chains.\r\n\r\npairs(fit)\r\n\r\n(fitResults<-summary(fit)$summary[,c(1,4,6,8,10)])\r\n             mean          2.5%          50%        97.5%      Rhat\r\nc[1]    0.1811849   -0.06299102    0.1822527    0.4300299 0.9997891\r\nc[2]    0.6070660    0.35191827    0.6040370    0.8689709 0.9998762\r\nc[3]    1.0480714    0.74641862    1.0463543    1.3666270 0.9998301\r\nc[4]    1.5156082    1.16003395    1.5070613    1.9045675 0.9999335\r\nc[5]    1.9875471    1.53136095    1.9734539    2.5301832 1.0000932\r\nc[6]    2.6172553    1.94435433    2.5745726    3.5445753 0.9998225\r\nlp__ -138.0311040 -142.37436655 -137.6879538 -135.5741869 0.9998792\r\n\r\nReturned parameters are 6 thresholds separating 7 ordinal\r\ncategories.\r\n\r\nstan_ac(fit, separate_chains = T)\r\n\r\nstan_trace(fit)\r\n\r\nstan_dens(fit)\r\n\r\n\r\nCompare estimated means with the frequencies of the sample\r\n\r\ncbind(Cumulativefrequencies=head(cumsum(frequencies)),\r\n      EstimatedProbabilities=pnorm(head(fitResults[,1])))\r\n  Cumulativefrequencies EstimatedProbabilities\r\n1                  0.58              0.5718888\r\n2                  0.73              0.7280965\r\n3                  0.85              0.8526971\r\n4                  0.93              0.9351908\r\n5                  0.97              0.9765691\r\n6                  0.99              0.9955680\r\n\r\nPlot HDI of the parameters.\r\n\r\nplot(fit)\r\n\r\n\r\nHDI intervals overlap which may seem counter-intuitive: thresholds\r\nhave to be ordered.\r\nExtract the chains.\r\n\r\nfit_ext <- rstan::extract(fit)\r\nfit_ext<-fit_ext$c\r\nhead(fit_ext)\r\n          \r\niterations       [,1]      [,2]      [,3]     [,4]     [,5]     [,6]\r\n      [1,] 0.35116666 0.7537651 0.9995782 1.288744 1.567238 2.221483\r\n      [2,] 0.09469918 0.5023792 0.9997730 1.284909 1.948384 2.698532\r\n      [3,] 0.06120643 0.7224987 1.0377697 1.395147 1.999715 2.478845\r\n      [4,] 0.03202448 0.4761984 0.9188529 1.516598 1.913221 2.842915\r\n      [5,] 0.21959980 0.5235180 1.2094977 1.838333 2.382429 2.755269\r\n      [6,] 0.06633391 0.6574331 1.1090569 1.551895 1.889166 2.678495\r\n\r\nPlot first 6 rows of the chains to see that at each step of the chain\r\nthresholds are ordered.\r\n\r\nplot(fit_ext[1,],rep(1,6),ylim=c(1,6),xlim=c(0,3),col=1,pch=16)\r\npoints(fit_ext[2,],rep(2,6),col=2,pch=16)\r\npoints(fit_ext[3,],rep(3,6),col=3,pch=16)\r\npoints(fit_ext[4,],rep(4,6),col=4,pch=16)\r\npoints(fit_ext[5,],rep(5,6),col=5,pch=16)\r\npoints(fit_ext[6,],rep(6,6),col=6,pch=16)\r\n\r\n\r\nTwo Groups\r\nTwo ordinal variables may need to be compared, for example, in the\r\nfollowing situations:\r\n1- Two groups of participants asked to answer a questionnaire with\r\nthe same scale of ordinal categories, for example, “Strongly\r\nDisagree”, “Disagree”, “Undecided”,\r\n“Agree”, “Strongly Agree”. But two groups are asked\r\nabout their agreement/disagreement with two different statements about\r\nsocial issues: “Left-handed people should have equal rights under the\r\nlaw” – for group 1, and “Disabled people should be given equal rights\r\nunder the law”.\r\nThe assumption is that people in both groups have the same latent\r\nvariable, call it “sense of fairness”. But two\r\nsocial issues have different responses formalized in mean value and\r\nstandard deviation of the latent variable with respect to the\r\nissues.\r\nThe goal of the experiment may be analysis of contrast between the two\r\nstatements, i.e. how significant the difference is between the two\r\nstatements.\r\n2- Two groups of participants are asked to rate two different\r\nproducts and select level of satisfaction from 1 (lowest) to\r\n10 (highest). Again the scale of ordinal responses is the same.\r\nAssumption is that the latent variable of evaluation of each product,\r\n“satisfaction”, is shared by both groups of participants. But different\r\nproducts result in different mean values and variances of the latent\r\nvariable. The goal of the study is understanding the difference between\r\nthe products.\r\nWrite description for 2 nominal groups.\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=2> K;  // num of y classes\r\n    int<lower=0> N;  // num of observations\r\n    int<lower=1> D;  // num of groups\r\n    int<lower=1,upper=K> y[N]; // response\r\n    int<lower=1,upper=D> x[N]; // group index: each obeservation has its own group\r\n}\r\nparameters {\r\n    ordered[K-3] c_raw;\r\n    vector[D] sigma; // group sigma\r\n    vector[D] beta;  // group mean\r\n}\r\ntransformed parameters{ // renormalize vector c\r\n    vector[K-1] c;\r\n    c[1] = 1.5;\r\n    for (k in 2:(K-2))\r\n        c[k] = 1.5 + (K - 2.0)* inv_logit(c_raw[k-1]); // sigmoid\r\n    c[K-1] = K - 0.5;\r\n}\r\nmodel {\r\n    vector[K] theta;\r\n    real mu;\r\n    real sgm;\r\n    beta ~ normal((1+K)/2.0, K);\r\n    sigma ~ uniform(K/1000.0, K * 10.0);\r\n    for (n in 1:N) {\r\n        mu = beta[x[n]];\r\n        sgm = sigma[x[n]];\r\n        theta[1] = Phi((c[1] - mu)/sgm);\r\n        for (k in 2:(K-1))\r\n            theta[k] = Phi((c[k] - mu)/sgm) - Phi((c[k-1] - mu)/sgm);\r\n        theta[K] = 1 - Phi((c[K-1] - mu)/sgm);\r\n        y[n] ~ categorical(theta);\r\n    }\r\n}\"\r\n\r\nRe-scaling of thresholds (adding a constant and dividing by a\r\nconstant) is not going to change probabilities if the same\r\ntransformation is done to the distribution.\r\nIn order to remove such degrees of freedom “pin down” the extreme values\r\nof thresholds to 1.5 and \\(K - 0.5\\)\r\ncorrespondingly and re-scale the rest of the thresholds between them.\r\nThis is what is done in the block “transformed parameters” above.\r\nCreate DSO.\r\n\r\nmodel <- stan_model(model_code=modelString)\r\n\r\nRead the data.\r\n\r\nmdt = read.csv(file = \"data/OrdinalProbitData1.csv\", stringsAsFactors=TRUE)\r\nhead(mdt)\r\n  X Y\r\n1 A 1\r\n2 A 1\r\n3 A 1\r\n4 A 1\r\n5 A 1\r\n6 A 1\r\ntable(mdt)\r\n   Y\r\nX    1  2  3  4  5\r\n  A 31  8  4  1  0\r\n  B 22 11  7  3  1\r\n\r\nThe dataset now has 2 groups A and B, showing different distributions\r\non the 5 ordinal categories.\r\nRun the chains.\r\n\r\nfit <- sampling(model,\r\n                data=list(N=nrow(mdt),  # num of observations\r\n                          K=max(mdt$Y), # num of outcome classes\r\n                          D=nlevels(mdt$X),\r\n                          y=mdt$Y,\r\n                          x=as.integer(mdt$X)),\r\n                pars=c('c', 'beta', 'sigma'),\r\n                iter=5000, chains = 2, cores = 2\r\n)\r\n\r\nAnalyze fitted model\r\n\r\npairs(fit)\r\n\r\nstan_ac(fit, separate_chains = T)\r\n\r\nstan_trace(fit)\r\n\r\nstan_dens(fit)\r\n\r\nsummary(fit)\r\n$summary\r\n                mean     se_mean        sd         2.5%         25%\r\nc[1]       1.5000000         NaN 0.0000000    1.5000000   1.5000000\r\nc[2]       2.5687264 0.004564235 0.2495291    2.1297057   2.3935228\r\nc[3]       3.6650824 0.005892113 0.3162772    3.0108633   3.4515403\r\nc[4]       4.5000000         NaN 0.0000000    4.5000000   4.5000000\r\nbeta[1]    0.4810617 0.012087812 0.5324673   -0.8054914   0.2065422\r\nbeta[2]    1.4104089 0.006548890 0.3521547    0.6278093   1.1962206\r\nsigma[1]   1.7714314 0.013797419 0.5486677    0.9494024   1.3924604\r\nsigma[2]   1.7536222 0.007729298 0.3849482    1.1429208   1.4888102\r\nlp__     -96.5054790 0.051380648 2.0078407 -101.2465659 -97.5325740\r\n                 50%         75%      97.5%    n_eff      Rhat\r\nc[1]       1.5000000   1.5000000   1.500000      NaN       NaN\r\nc[2]       2.5505579   2.7310103   3.088581 2988.867 0.9997306\r\nc[3]       3.6867641   3.9003436   4.209861 2881.335 0.9996982\r\nc[4]       4.5000000   4.5000000   4.500000      NaN       NaN\r\nbeta[1]    0.5700466   0.8559553   1.262025 1940.397 0.9999410\r\nbeta[2]    1.4373681   1.6476200   2.032807 2891.555 1.0006034\r\nsigma[1]   1.6861781   2.0671413   3.075729 1581.332 0.9999935\r\nsigma[2]   1.7001947   1.9638541   2.649887 2480.416 1.0005770\r\nlp__     -96.1183970 -95.0223348 -93.752522 1527.071 0.9999967\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n          stats\r\nparameter         mean        sd         2.5%         25%         50%\r\n  c[1]       1.5000000 0.0000000    1.5000000   1.5000000   1.5000000\r\n  c[2]       2.5656778 0.2504301    2.1163268   2.3908231   2.5493339\r\n  c[3]       3.6688164 0.3169793    3.0266797   3.4572647   3.6870800\r\n  c[4]       4.5000000 0.0000000    4.5000000   4.5000000   4.5000000\r\n  beta[1]    0.4831698 0.5340802   -0.7738274   0.1930877   0.5734193\r\n  beta[2]    1.4199808 0.3544642    0.6193359   1.2114230   1.4483583\r\n  sigma[1]   1.7703654 0.5460767    0.9637670   1.3980432   1.6967757\r\n  sigma[2]   1.7439336 0.3835729    1.1482349   1.4754805   1.6862058\r\n  lp__     -96.5209197 2.0575388 -101.4332967 -97.5533587 -96.1147975\r\n          stats\r\nparameter         75%      97.5%\r\n  c[1]       1.500000   1.500000\r\n  c[2]       2.725942   3.076784\r\n  c[3]       3.911798   4.210600\r\n  c[4]       4.500000   4.500000\r\n  beta[1]    0.859498   1.268055\r\n  beta[2]    1.652155   2.050165\r\n  sigma[1]   2.061358   3.023260\r\n  sigma[2]   1.950170   2.643014\r\n  lp__     -95.038429 -93.752162\r\n\r\n, , chains = chain:2\r\n\r\n          stats\r\nparameter         mean        sd         2.5%         25%         50%\r\n  c[1]       1.5000000 0.0000000    1.5000000   1.5000000   1.5000000\r\n  c[2]       2.5717750 0.2486376    2.1467098   2.3946781   2.5513959\r\n  c[3]       3.6613483 0.3155928    3.0004842   3.4476953   3.6846535\r\n  c[4]       4.5000000 0.0000000    4.5000000   4.5000000   4.5000000\r\n  beta[1]    0.4789535 0.5309480   -0.8431789   0.2261896   0.5658407\r\n  beta[2]    1.4008371 0.3496387    0.6568121   1.1796308   1.4286961\r\n  sigma[1]   1.7724974 0.5513536    0.9431984   1.3861733   1.6731259\r\n  sigma[2]   1.7633108 0.3861523    1.1305726   1.4973936   1.7159548\r\n  lp__     -96.4900383 1.9571712 -100.9751571 -97.5118612 -96.1227563\r\n          stats\r\nparameter          75%      97.5%\r\n  c[1]       1.5000000   1.500000\r\n  c[2]       2.7343485   3.099431\r\n  c[3]       3.8882334   4.209746\r\n  c[4]       4.5000000   4.500000\r\n  beta[1]    0.8554803   1.243746\r\n  beta[2]    1.6452455   2.006866\r\n  sigma[1]   2.0728024   3.100717\r\n  sigma[2]   1.9807715   2.657443\r\n  lp__     -95.0109336 -93.759982\r\nplot(fit,pars=c(\"beta\"))\r\n\r\n\r\nHDIs show that even at 80% the difference between the two means is\r\ninsignificant.\r\n\r\nplot(fit,pars=c(\"sigma\"))\r\n\r\n\r\nStandard deviations are identical.\r\nWhat would the FNP approach conclude about this data?\r\nFirst, assume that the data are of metric type.\r\nCheck whether t-test is able to distinguish the means.\r\n\r\nsubsetA <- subset(mdt,X==\"A\")\r\nsubsetB <- subset(mdt,X==\"B\")\r\nt.test(subsetA$Y,subsetB$Y)\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  subsetA$Y and subsetB$Y\r\nt = -2.1838, df = 77.571, p-value = 0.032\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.82551722 -0.03811914\r\nsample estimates:\r\nmean of x mean of y \r\n 1.431818  1.863636 \r\n\r\nWith any level greater than 0.032 the equality hypothesis is\r\nrejected.\r\nIn the book this is explained by non-normality of the data.\r\nConfirm this with qq-plot.\r\n\r\nqqnorm(subsetA$Y)\r\nqqline(subsetA$Y)\r\n\r\nqqnorm(subsetB$Y)\r\nqqline(subsetB$Y)\r\n\r\n\r\nDistributions are indeed not normal. But, probably, the main reason\r\nfor rejecting the null hypothesis is discrete format of the data.\r\nCompare the samples assuming that they have multinomial distribution.\r\nThen apply \\(\\chi-square\\) test.\r\n\r\nchisq.test(c(table(subsetA$Y)/length(subsetA$Y),0),table(subsetB$Y)/length(subsetB$Y))\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  c(table(subsetA$Y)/length(subsetA$Y), 0) and table(subsetB$Y)/length(subsetB$Y)\r\nX-squared = 20, df = 16, p-value = 0.2202\r\n\r\nWith proper method selected the hypothesis of equality of means is\r\nnot rejected.\r\nPlot row means against simulated thresholds.\r\n\r\ndraws <- extract(fit)\r\nc1 <- draws$c[,1]\r\nc2 <- draws$c[,2]\r\nc3 <- draws$c[,3]\r\nc4 <- draws$c[,4]\r\nmc <- rowMeans(draws$c)\r\nhead(cbind(c1,c4))\r\n      c1  c4\r\n[1,] 1.5 4.5\r\n[2,] 1.5 4.5\r\n[3,] 1.5 4.5\r\n[4,] 1.5 4.5\r\n[5,] 1.5 4.5\r\n[6,] 1.5 4.5\r\nhead(mc)\r\n[1] 3.096186 3.176928 2.952425 3.217452 3.061712 2.988291\r\nplot(c1, mc, xlim = c(1,5),xlab=\"c\")\r\npoints(c2,mc)\r\npoints(c3,mc)\r\npoints(c4,mc)\r\nabline(v=mean(c2))\r\nabline(v=mean(c3))\r\n\r\n\r\nProbit Model with Metric\r\nPredictors\r\n(be continued)\r\nSingle predictor:\r\nHappiness and Money\r\n\r\n\r\n",
    "preview": "posts/2022-04-04-Bayesian methods - Series 8 of 10/distill-preview.png",
    "last_modified": "2023-02-23T22:28:52-06:00",
    "input_file": "Serries-8.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-20-Bayesian methods - Series 7 of 10/",
    "title": "Series 7 -- Fitting a Logistic Regression in Bayes",
    "description": "How to fit a Logistic Regression using Bayesian Methods      \nThe advantage of using Bayes to overcome the Separation in Logistic Regression",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-03-20",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nLogistic\r\nregression\r\nPrediction\r\nof gender by hight and weight. Probability close to 0.5 (balanced\r\ndata)\r\nPrediction\r\nof gender by hight and weight. Probability close to extreme (imbalanced\r\nsample)\r\n\r\nRobust logistic regression\r\nPrediction\r\nof gender by height and weight. Robust model\r\nPrediction\r\nof gender by height and weight with sparse data. Robust model\r\nAnesthesia\r\nexample\r\nData\r\nLogistic model by\r\nglm()\r\nRunning\r\nchains\r\nAnalysis of logistic model\r\nAnalysis of robust logistic\r\nmodel\r\nComparison of\r\nlogistic and robust logistic models\r\n\r\n\r\nSoftmax\r\nregression\r\nSimulated data from the\r\nbook\r\nSoftmax\r\nmodel\r\nAnalysis\r\nClassification\r\n\r\nFurther\r\nreading\r\n\r\nLogistic regression\r\nDichotomous \\(y\\) with multiple\r\nmetric predictors.\r\nLogistic regression model would be constructed based on\r\ndiagram.\r\nWe use Height-Weight Data\r\nPrediction\r\nof gender by hight and weight. Probability close to 0.5 (balanced\r\ndata)\r\n\r\nmydf <- read.csv(file = 'data/HtWtData300.csv')\r\nhead(mydf)\r\n  male height weight\r\n1    0   64.0  136.4\r\n2    0   62.3  215.1\r\n3    1   67.9  173.6\r\n4    0   64.2  117.3\r\n5    0   64.8  123.3\r\n6    0   57.5   96.5\r\n\r\nProbability close to 0.5\r\n\r\ntable(mydf$male)\r\n\r\n  0   1 \r\n152 148 \r\nprop.table(table(mydf$male))\r\n\r\n        0         1 \r\n0.5066667 0.4933333 \r\n\r\nFirstly, we try to fit with glm\r\n\r\nfmod1 <- glm(male ~ height + weight, \r\n             family = \"binomial\", data = mydf)\r\nsummary(fmod1)\r\n\r\nCall:\r\nglm(formula = male ~ height + weight, family = \"binomial\", data = mydf)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.7751  -0.4674  -0.0165   0.3655   3.1906  \r\n\r\nCoefficients:\r\n              Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept) -59.994880   7.136907  -8.406  < 2e-16 ***\r\nheight        0.890858   0.108873   8.183 2.78e-16 ***\r\nweight        0.005320   0.005225   1.018    0.309    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 415.83  on 299  degrees of freedom\r\nResidual deviance: 189.49  on 297  degrees of freedom\r\nAIC: 195.49\r\n\r\nNumber of Fisher Scoring iterations: 6\r\n\r\n\\(\\Rightarrow\\) We will see the\r\nestimates as same as the Bayesian method.\r\n\r\nplot(mydf$height, \r\n     mydf$weight,pch=16, col=\"blue\")\r\npoints(mydf$height[mydf$male==1], \r\n       mydf$weight[mydf$male==1], \r\n       col=\"orange\", pch=16)\r\nlegend(\"topleft\", legend=c(\"Female\",\"Male\"), \r\n       col=c(\"blue\",\"orange\"), pch=16)\r\n\r\n\r\nModel for Dichotomous y with multiple metric predictors set up in\r\nStan\r\nBut firstly, we look at the simplified modelString of\r\nStan:\r\n\r\n# Not eval\r\nmodelString <- \"\r\n  data{\r\n    int<lower=0> N;\r\n    real xc[N];\r\n    int xb[N];\r\n    int<lower=0,upper=1> y[N]; //Binary outcome\r\n  }\r\n  parameters{\r\n    real b0;\r\n    real b1;\r\n    real b2;\r\n  }\r\n  model{\r\n    b0 ~ normal(0,1);\r\n    b1 ~ normal(0,1);\r\n    b2 ~ normal(0,1);\r\n    y  ~ bernoulli_logit(b0 + b1*xc + b2*xb);\r\n  }\r\n\"\r\n\r\nThen move on the destination one:\r\n\r\nmodelString <- \"\r\n  data {\r\n    int<lower=1> Ntotal; // num of observations\r\n    int<lower=1> Nx;     // num of predictors\r\n    int<lower=0, upper=1> y[Ntotal];\r\n    matrix[Ntotal, Nx] x;\r\n  }\r\n  \r\n  transformed data {\r\n    vector[Nx] meanX;\r\n    vector[Nx] sdX;\r\n    matrix[Ntotal, Nx] zx; // normalized\r\n\r\n    for ( j in 1:Nx ) {\r\n      meanX[j] = mean(x[,j]);\r\n      sdX[j] = sd(x[,j]);\r\n        for (i in 1:Ntotal) {\r\n          zx[i,j] = (x[i,j] - meanX[j]) / sdX[j];\r\n        }\r\n      }\r\n  }\r\n    \r\n  parameters {\r\n    real zbeta0;\r\n    vector[Nx] zbeta;\r\n  }\r\n\r\n  transformed parameters{\r\n    vector[Ntotal] mu;\r\n    mu = zbeta0 + zx * zbeta;  // matrix product\r\n  }\r\n\r\n  model {\r\n    zbeta0 ~ normal(0, 2);\r\n    zbeta  ~ normal(0, 2);\r\n    y ~ bernoulli_logit(mu);\r\n  }\r\n\r\n  generated quantities { \r\n    // Transform to original scale:\r\n    real beta0; \r\n    vector[Nx] beta;\r\n    // .* and ./ are element-wise product and division\r\n    beta0 = zbeta0 - sum(zbeta .* meanX ./ sdX);\r\n    beta = zbeta ./ sdX;\r\n  } \r\n\"\r\n\r\n\r\nstanDsoLogistic <- stan_model(model_code=modelString)\r\n\r\nFit model\r\n\r\nheightWeightDataList <- list(Ntotal = nrow(mydf),\r\n                             y = mydf$male,\r\n                             x = cbind(mydf$height, mydf$weight),\r\n                             Nx = 2)\r\n\r\n\r\nfit <- sampling(stanDsoLogistic,\r\n                data = heightWeightDataList, \r\n                pars = c('beta0', 'beta'),\r\n                iter = 5000, chains = 2, cores = 2\r\n)\r\n\r\nAnalyze fitted model using shinystan\r\n\r\nlaunch_shinystan(fit)\r\n\r\nAnalyze parameters.\r\n\r\nstan_ac(fit, separate_chains = T)\r\n\r\npairs(fit)\r\n\r\nplot(fit)\r\n\r\nplot(fit,pars=c(\"beta\"))\r\n\r\nplot(fit,pars=c(\"beta[2]\"))\r\n\r\nsummary(fit)$summary[,c(1,4,8)]\r\n                 mean          2.5%        97.5%\r\nbeta0   -58.992924983 -7.323391e+01 -46.12530030\r\nbeta[1]   0.875179982  6.768256e-01   1.09355795\r\nbeta[2]   0.005549854 -4.832449e-03   0.01596438\r\nlp__    -97.715598858 -1.009842e+02 -96.26790960\r\n\r\nParameter \\(\\beta_2\\) is not\r\nsignificant with 95% HDI.\r\n\r\nstan_dens(fit)\r\n\r\n\r\n\r\nestimBetas<-summary(fit)$summary[1:3,1]\r\n\r\nPlot the data with separating hyperplane.\r\n\r\nplot(mydf$height, mydf$weight, pch=16, col=\"blue\")\r\npoints(mydf$height[mydf$male==1], mydf$weight[mydf$male==1], col = \"orange\", pch = 16)\r\nlines(mydf$height, -(estimBetas[1]+estimBetas[2]*mydf$height)/estimBetas[3])\r\nlegend(\"topleft\", legend = c(\"Female\",\"Male\"), col = c(\"blue\",\"orange\"), pch=16)\r\n\r\n\r\nPrediction\r\nof gender by hight and weight. Probability close to extreme (imbalanced\r\nsample)\r\nNow try to remove almost all males from the sample to see what may\r\nhappen when there are few 1’s observed.\r\nIn the original sample the proportion of males is:\r\n\r\nmean(mydf$male)\r\n[1] 0.4933333\r\n\r\nSample with females is\r\n\r\nfemales <- mydf[mydf$male == 0,]\r\n\r\nSelect first 15 males.\r\n\r\nmales <- mydf[mydf$male == 1,][1:15,] # just 15 males (originally was ~150)\r\nmydf_sparse <- rbind(males,females)\r\nrownames(mydf_sparse) <- NULL\r\nhead(mydf_sparse, 20)\r\n   male height weight\r\n1     1   67.9  173.6\r\n2     1   70.2  191.1\r\n3     1   71.1  193.9\r\n4     1   66.5  127.1\r\n5     1   75.1  204.4\r\n6     1   64.6  143.4\r\n7     1   69.2  124.4\r\n8     1   68.1  140.9\r\n9     1   72.6  164.7\r\n10    1   71.5  193.6\r\n11    1   76.0  180.0\r\n12    1   69.7  155.0\r\n13    1   73.3  188.2\r\n14    1   68.3  178.6\r\n15    1   70.8  207.3\r\n16    0   64.0  136.4\r\n17    0   62.3  215.1\r\n18    0   64.2  117.3\r\n19    0   64.8  123.3\r\n20    0   57.5   96.5\r\n\r\nFit sparse model\r\n\r\nfmod2 <- glm(male ~ height + weight, family = \"binomial\", data = mydf_sparse)\r\nsummary(fmod2)\r\n\r\nCall:\r\nglm(formula = male ~ height + weight, family = \"binomial\", data = mydf_sparse)\r\n\r\nDeviance Residuals: \r\n     Min        1Q    Median        3Q       Max  \r\n-1.28353  -0.17117  -0.06829  -0.01696   3.15051  \r\n\r\nCoefficients:\r\n             Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept) -84.24302   20.03395  -4.205 2.61e-05 ***\r\nheight        1.25377    0.30728   4.080 4.50e-05 ***\r\nweight       -0.01190    0.01248  -0.953     0.34    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 100.909  on 166  degrees of freedom\r\nResidual deviance:  35.582  on 164  degrees of freedom\r\nAIC: 41.582\r\n\r\nNumber of Fisher Scoring iterations: 8\r\n\r\n\r\nheightWeightSparseDataList<-list(Ntotal=nrow(mydf_sparse),\r\n                                 y=mydf_sparse$male,\r\n                                 x=cbind(mydf_sparse$height, mydf_sparse$weight),\r\n                                 Nx=2)\r\n\r\n\r\nfit_sparse <- sampling(stanDsoLogistic,\r\n                       data=heightWeightSparseDataList, \r\n                       pars=c('beta0', 'beta'),\r\n                       iter=5000, chains = 2, cores = 2\r\n)\r\n\r\n\r\nstan_ac(fit_sparse, separate_chains = T)\r\n\r\npairs(fit_sparse)\r\n\r\nplot(fit_sparse)\r\n\r\nplot(fit_sparse,pars=c(\"beta\"))\r\n\r\nplot(fit_sparse,pars=c(\"beta[2]\"))\r\n\r\n\r\nCompare summary of the two studies\r\n\r\nrbind(beta0reg=summary(fit)$summary[1,c(1,3)],\r\n      beta0glm=c(summary(fmod1)$coefficients[1,1],summary(fmod1)$coefficients[1,2]*sqrt(dim(mydf)[1])),\r\n      beta0sparce=summary(fit_sparse)$summary[1,c(1,3)],\r\n      beta0sparceglm=c(summary(fmod2)$coefficients[1,1],summary(fmod2)$coefficients[1,2]*sqrt(dim(mydf_sparse)[1])))\r\n                    mean         sd\r\nbeta0reg       -58.99292   6.931705\r\nbeta0glm       -59.99488 123.614858\r\nbeta0sparce    -65.45501  12.684797\r\nbeta0sparceglm -84.24302 258.895713\r\nrbind(beta1reg=summary(fit)$summary[2,c(1,3)],\r\n      beta1glm=c(summary(fmod1)$coefficients[2,1],summary(fmod1)$coefficients[2,2]*sqrt(dim(mydf)[1])),\r\n      beta1sparce=summary(fit_sparse)$summary[2,c(1,3)],\r\n      beta1sparceglm=c(summary(fmod2)$coefficients[2,1],summary(fmod2)$coefficients[2,2]*sqrt(dim(mydf_sparse)[1])))\r\n                    mean        sd\r\nbeta1reg       0.8751800 0.1059241\r\nbeta1glm       0.8908578 1.8857272\r\nbeta1sparce    0.9665638 0.1937020\r\nbeta1sparceglm 1.2537728 3.9708903\r\nrbind(beta2reg=summary(fit)$summary[3,c(1,3)],\r\n      beta2glm=c(summary(fmod1)$coefficients[3,1],summary(fmod1)$coefficients[3,2]*sqrt(dim(mydf)[1])),\r\n      beta2sparce=summary(fit_sparse)$summary[3,c(1,3)],\r\n      beta2sparceglm=c(summary(fmod2)$coefficients[3,1],summary(fmod2)$coefficients[3,2]*sqrt(dim(mydf_sparse)[1])))\r\n                       mean          sd\r\nbeta2reg        0.005549854 0.005325699\r\nbeta2glm        0.005319759 0.090499569\r\nbeta2sparce    -0.007222053 0.010259493\r\nbeta2sparceglm -0.011900490 0.161304086\r\n\r\n\r\nrbind(beta0reg=summary(fit)$summary[1,c(4,8)],\r\n      beta0sparce=summary(fit_sparse)$summary[1,c(4,8)])\r\n                 2.5%     97.5%\r\nbeta0reg    -73.23391 -46.12530\r\nbeta0sparce -92.80089 -42.89777\r\nrbind(beta1reg=summary(fit)$summary[2,c(4,8)],\r\n      beta1sparce=summary(fit_sparse)$summary[2,c(4,8)])\r\n                 2.5%    97.5%\r\nbeta1reg    0.6768256 1.093558\r\nbeta1sparce 0.6177309 1.379462\r\nrbind(beta2reg=summary(fit)$summary[3,c(4,8)],\r\n      beta2sparce=summary(fit_sparse)$summary[3,c(4,8)])\r\n                    2.5%      97.5%\r\nbeta2reg    -0.004832449 0.01596438\r\nbeta2sparce -0.028192807 0.01230010\r\n\r\nHDI of both slopes widened significantly in the sample with\r\nmore extreme disproportion.Standard deviations of betas also increase dramatically,\r\nespecially fit with glm.\r\nRobust logistic regression\r\nPrediction\r\nof gender by height and weight. Robust model\r\nObserve the data of the previous section.\r\nPlot male (1) and female (0) groups with respect to weight.\r\n\r\nplot(mydf$weight,mydf$male)\r\n\r\n\r\nIn the lower right corner there are some outliers representing heavy\r\nfemales.\r\nSuch observations cause bias of model parameters.\r\n\r\nplot(mydf$height,mydf$male)\r\n\r\n\r\nOn the plot relative to height outliers are short males.\r\nWhat does a model robust to such outliers look like?\r\nRobust logistic regression is a mix of “guessing model” and logistic\r\nmodel\r\n\\[\\mu = \\alpha \\frac{1}{2} + (1 - \\alpha)\r\n\\ \\text{logistic}\\Big(\\beta_0 + \\sum_j \\beta_j x_j\\Big)\\]\r\nWhen \\(\\alpha = 0\\) all\r\nobservations fit within logistic regression pattern, i.e. the classes\r\nare separated well enough to be explained by logistic sigmoid function.\r\nIn this case the model becomes pure logistic regression.\r\nWhen \\(\\alpha = 1\\) the classes\r\noverlap completely and the model can only guess with probability 0.5 to\r\nwhich class the observation belongs.\r\nTypically \\(\\alpha\\) is small (few\r\noutliers showing that predictor points to wrong class).\r\nSelect beta distribution as a prior to \\(\\alpha\\) with high concentration near zero:\r\ndbeta(1,9).\r\n\r\nArgument <- seq(from=0,to=1,by=.01)\r\nplot(Argument, dbeta(Argument, 1, 9), type=\"l\")\r\n\r\n\r\nThe modified model is on the .\r\n\r\nmodelString= \r\n\"data {                   // ROBUST LOGISTIC REGRESSION\r\n    int<lower=1> Ntotal;  // num of observations\r\n    int<lower=1> Nx;      // num of predictors\r\n    int<lower=0, upper=1> y[Ntotal];\r\n    matrix[Ntotal, Nx] x;\r\n}\r\ntransformed data {\r\n    vector[Nx] meanX;\r\n    vector[Nx] sdX;\r\n    matrix[Ntotal, Nx] zx;  // normalized\r\n    \r\n    for ( j in 1:Nx ) {\r\n        meanX[j] = mean(x[,j]);\r\n        sdX[j] = sd(x[,j]);\r\n        for ( i in 1:Ntotal ) {\r\n            zx[i,j] = ( x[i,j] - meanX[j] ) / sdX[j];\r\n        }\r\n    }\r\n}\r\nparameters {\r\n    real zbeta0;\r\n    vector[Nx] zbeta;\r\n    real<lower=0,upper=1> guess;  // mixture param\r\n}\r\ntransformed parameters{\r\n    vector[Ntotal] mu;\r\n    for ( i in 1:Ntotal ) {\r\n        mu[i] = guess * (1/2.0) + (1-guess) * inv_logit(zbeta0 + zx[i,] * zbeta);\r\n    }\r\n}\r\nmodel {\r\n    zbeta0 ~ normal(0, 2);\r\n    zbeta  ~ normal(0, 2);\r\n    guess ~ beta(1, 9);\r\n    y ~ bernoulli(mu);\r\n}\r\ngenerated quantities { \r\n    // Transform to original scale:\r\n    real beta0; \r\n    vector[Nx] beta;\r\n    // .* and ./ are element-wise product and division\r\n    beta0 =  zbeta0 - sum( zbeta .* meanX ./ sdX );\r\n    beta =  zbeta ./ sdX;\r\n}\r\n\"\r\n\r\n\r\nstanDsoRobustLogistic <- stan_model(model_code=modelString)\r\n\r\nRun robust MCMC with the hight/weight data.\r\n\r\nfitRobust <- sampling(stanDsoRobustLogistic,\r\n                data=heightWeightDataList, \r\n                pars=c('beta0', 'beta', 'guess'),\r\n                iter=5000, chains = 2, cores = 2)\r\n\r\nAnalyze results.\r\n\r\nstan_ac(fitRobust, separate_chains = T)\r\n\r\npairs(fitRobust)\r\n\r\nplot(fitRobust)\r\n\r\nplot(fitRobust,pars=c(\"beta[1]\"))\r\n\r\nplot(fitRobust,pars=c(\"beta[2]\"))\r\n\r\nplot(fitRobust,pars=c(\"guess\"))\r\n\r\nrbind(summary(fitRobust)$summary[,c(1,4,7)],\r\n      summary(fit)$summary[,c(1,4,8)]\r\n)\r\n                 mean          2.5%           75%\r\nbeta0   -6.794151e+01 -9.040257e+01  -60.86879681\r\nbeta[1]  1.004011e+00  7.406505e-01    1.09734128\r\nbeta[2]  7.843206e-03 -4.273673e-03    0.01185354\r\nguess    3.618337e-02  2.272841e-03    0.05068176\r\nlp__    -1.019951e+02 -1.056117e+02 -100.89209843\r\nbeta0   -5.899292e+01 -7.323391e+01  -46.12530030\r\nbeta[1]  8.751800e-01  6.768256e-01    1.09355795\r\nbeta[2]  5.549854e-03 -4.832449e-03    0.01596438\r\nlp__    -9.771560e+01 -1.009842e+02  -96.26790960\r\n\r\nNote positive correlation between guess and the slope \\(\\beta_1\\). The more outliers the higher is\r\nthe slope.\r\nSince \\(\\beta_2\\) does not seem\r\nsignificant. Now, we fit both models with height as only predictor.\r\n\r\nheightWeightDataList<-list(Ntotal=nrow(mydf),\r\n                          y=mydf$male,\r\n                          x=cbind(mydf$height),\r\n                          Nx=1)\r\n\r\n\r\nfit <- sampling(stanDsoLogistic,\r\n                data=heightWeightDataList, \r\n                pars=c('beta0', 'beta'),\r\n                iter=5000, chains = 2, cores = 2\r\n)\r\nfitRobust <- sampling(stanDsoRobustLogistic,\r\n                data=heightWeightDataList, \r\n                pars=c('beta0', 'beta', 'guess'),\r\n                iter=5000, chains = 2, cores = 2\r\n)\r\npairs(fit)\r\n\r\npairs(fitRobust)\r\n\r\nplot(fit)\r\n\r\nplot(fit,pars=c(\"beta\"))\r\n\r\nplot(fitRobust)\r\n\r\nplot(fitRobust,pars=c(\"beta[1]\",\"guess\"))\r\n\r\nplot(fitRobust,pars=c(\"guess\"))\r\n\r\nrbind(summary(fitRobust)$summary[,c(1,4,7)],\r\n      summary(fit)$summary[,c(1,4,8)]\r\n)\r\n                 mean          2.5%           75%\r\nbeta0    -66.88526189 -8.866638e+01  -60.00415120\r\nbeta[1]    1.00688726  7.605573e-01    1.09231329\r\nguess      0.03243383  1.645023e-03    0.04555081\r\nlp__    -102.24849261 -1.055917e+02 -101.29745249\r\nbeta0    -59.40455205 -7.350401e+01  -46.79642504\r\nbeta[1]    0.89464752  7.033695e-01    1.10703605\r\nlp__     -97.75056275 -1.006445e+02  -96.76446560\r\n\r\nCompare probabilities predicted by logistic regression and robust\r\nlogistic regression.\r\n\r\n# Coefficients\r\nmeanBeta0Robust<-summary(fitRobust)$summary[1,1]\r\nmeanBeta1Robust<-summary(fitRobust)$summary[2,1]\r\nguess<-summary(fitRobust)$summary[3,1]\r\nmeanBeta0<-summary(fit)$summary[1,1]\r\nmeanBeta1<-summary(fit)$summary[2,1]\r\n\r\n#Linear predictors and probabilities\r\nlinPredRobust_Male.Height<-meanBeta0Robust+meanBeta1Robust*mydf$height\r\npRobustMail_height<-guess/2+(1-guess)*exp(linPredRobust_Male.Height)/(1+exp(linPredRobust_Male.Height))\r\nlinPred_Male.Height<-meanBeta0+meanBeta1*mydf$height\r\npMail_height<-exp(linPred_Male.Height)/(1+exp(linPred_Male.Height))\r\n\r\n# Plot\r\nplot(mydf$height,mydf$male,pch=16)\r\npoints(mydf$height,pRobustMail_height,col=\"orange\",pch=16)\r\npoints(mydf$height,pMail_height,col=\"cyan\",pch=16)\r\nlegend(\"topleft\",\r\n       legend=c(\"Actual\",\"Prob Logistic\",\"Prob. Robust\"),\r\n       col=c(\"black\",\"cyan\",\"orange\"), pch=16)\r\n\r\n\r\nLogistic probability is more extreme: higher in the area of\r\nlow-height male observations because it is biased by an outlier of short\r\nmale below the average female height. But the far tails are closer to\r\n0.\r\nRobust logistic regression did not react to this observation as\r\nmuch!!\r\nRobust model does not produce probabilities too close to 0 and 1.\r\nPrediction\r\nof gender by height and weight with sparse data. Robust model\r\nRepeat the same comparison with the sparse data.\r\nCreate data list with one predictor.\r\n\r\nheightWeightSparseDataList <- list(Ntotal = nrow(mydf_sparse),\r\n                                   y = mydf_sparse$male,\r\n                                   x = cbind(mydf_sparse$height),\r\n                                   Nx = 1)\r\n\r\nFit both models with only one predictor height\r\n\r\nfitSparse <- sampling(stanDsoLogistic,\r\n                       data=heightWeightSparseDataList, \r\n                       pars=c('beta0', 'beta'),\r\n                       iter=5000, chains = 2, cores = 2)\r\n\r\nfitSparseRobust <- sampling(stanDsoRobustLogistic,\r\n                data=heightWeightSparseDataList, \r\n                pars=c('beta0', 'beta', 'guess'),\r\n                iter=5000, chains = 2, cores = 2)\r\n\r\nAnalyze the models.\r\n\r\npairs(fitSparse)\r\n\r\npairs(fitSparseRobust)\r\n\r\nplot(fitSparse)\r\n\r\nplot(fitSparse,pars=c(\"beta\"))\r\n\r\nplot(fitSparseRobust)\r\n\r\nplot(fitSparseRobust,pars=c(\"beta[1]\",\"guess\"))\r\n\r\nplot(fitSparseRobust,pars=c(\"guess\"))\r\n\r\nrbind(summary(fitSparseRobust)$summary[,c(1,4,7)],\r\n      summary(fitSparse)$summary[,c(1,4,8)]\r\n)\r\n                mean          2.5%          75%\r\nbeta0   -65.21124168 -92.353657075 -56.21273405\r\nbeta[1]   0.94557811   0.613894602   1.06360012\r\nguess     0.01666415   0.000493968   0.02404738\r\nlp__    -28.82697378 -32.226208453 -27.88666403\r\nbeta0   -62.74745106 -90.025601491 -40.85149755\r\nbeta[1]   0.90990203   0.584875829   1.31576470\r\nlp__    -23.23831969 -26.128614012 -22.20502250\r\n\r\nMake plot of probabilities.\r\n\r\n# Coefficients\r\nmeanBeta0Robust<-summary(fitSparseRobust)$summary[1,1]\r\nmeanBeta1Robust<-summary(fitSparseRobust)$summary[2,1]\r\nguess<-summary(fitSparseRobust)$summary[3,1]\r\nmeanBeta0<-summary(fitSparse)$summary[1,1]\r\nmeanBeta1<-summary(fitSparse)$summary[2,1]\r\n\r\n#Linear predictors and probabilities\r\nlinPredRobust_Male.Height<-meanBeta0Robust+meanBeta1Robust*mydf_sparse$height\r\npRobustMail_height<-guess/2+(1-guess)*exp(linPredRobust_Male.Height)/(1+exp(linPredRobust_Male.Height))\r\nlinPred_Male.Height<-meanBeta0+meanBeta1*mydf_sparse$height\r\npMail_height<-exp(linPred_Male.Height)/(1+exp(linPred_Male.Height))\r\n\r\n# Plot\r\nplot(mydf_sparse$height,mydf_sparse$male,pch=16)\r\npoints(mydf_sparse$height,pRobustMail_height,col=\"orange\",pch=16)\r\npoints(mydf_sparse$height,pMail_height,col=\"cyan\",pch=16)\r\nlegend(\"topleft\",\r\n       legend=c(\"Actual\",\"Prob Logistic\",\"Prob. Robust\"),\r\n       col=c(\"black\",\"cyan\",\"orange\"),pch=16)\r\n\r\n\r\nAnesthesia example\r\nThis data example is from library DAAG.\r\nThirty patients were given an anesthetic agent maintained at a\r\npredetermined concentration level (conc) for 15 minutes\r\nbefore making an incision. It was then noted whether the patient moved\r\n(1), i.e. jerked or twisted.\r\nData\r\n\r\nlibrary(DAAG)\r\nhead(anesthetic)\r\n  move conc   logconc nomove\r\n1    0  1.0 0.0000000      1\r\n2    1  1.2 0.1823216      0\r\n3    0  1.4 0.3364722      1\r\n4    1  1.4 0.3364722      0\r\n5    1  1.2 0.1823216      0\r\n6    0  2.5 0.9162907      1\r\n\r\nUse column move as response and column\r\nlogconc as predictor.\r\nPrepare the data.\r\n\r\ndataListAnesthetic <- list(Ntotal=nrow(anesthetic),\r\n                           y=anesthetic$move,\r\n                           x=cbind(anesthetic$logconc),\r\n                           Nx=1)\r\n\r\nLogistic model by glm()\r\n\r\nlogRegr <- glm(move ~ logconc, \r\n               data=anesthetic,\r\n               family=\"binomial\")\r\nsummary(logRegr)\r\n\r\nCall:\r\nglm(formula = move ~ logconc, family = \"binomial\", data = anesthetic)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.1477  -0.6962  -0.1209   0.7586   1.7528  \r\n\r\nCoefficients:\r\n            Estimate Std. Error z value Pr(>|z|)   \r\n(Intercept)   0.8077     0.5709   1.415  0.15715   \r\nlogconc      -6.2457     2.2618  -2.761  0.00575 **\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 41.455  on 29  degrees of freedom\r\nResidual deviance: 28.007  on 28  degrees of freedom\r\nAIC: 32.007\r\n\r\nNumber of Fisher Scoring iterations: 5\r\npredLogRegr <- predict(logRegr, type=\"response\")\r\n\r\nRunning chains\r\nRun MCMC using logistic and robust logistic models.\r\n\r\nfitAnesth <- sampling(stanDsoLogistic,\r\n                data=dataListAnesthetic, \r\n                pars=c('beta0', 'beta'),\r\n                iter=5000, chains = 2, cores = 2\r\n)\r\n\r\nfitRobustAnesth <- sampling(stanDsoRobustLogistic,\r\n                             data=dataListAnesthetic, \r\n                              pars=c('beta0', 'beta', 'guess'),\r\n                              iter=5000, chains = 2, cores = 2)\r\n\r\nAnalysis of logistic model\r\nExtract diagnostics\r\n\r\nsummary(fitAnesth)$summary[,c(1,4,8:10)]\r\n               mean        2.5%      97.5%    n_eff      Rhat\r\nbeta0     0.8195786  -0.1744736   1.944504 3161.224 1.0004414\r\nbeta[1]  -6.1980944 -10.7384617  -2.545248 2779.034 0.9997342\r\nlp__    -15.4770746 -18.1920124 -14.472321 2004.009 1.0012005\r\nstan_ac(fitAnesth, separate_chains = T)\r\n\r\nstan_trace(fitAnesth)\r\n\r\npairs(fitAnesth,pars=c(\"beta0\",\"beta\"))\r\n\r\nplot(fitAnesth)\r\n\r\n\r\nAnalysis of robust logistic\r\nmodel\r\n\r\nsummary(fitRobustAnesth)$summary[,c(1,4,8:10)]\r\n                mean          2.5%       97.5%    n_eff      Rhat\r\nbeta0     1.03390884  -0.204622023   2.7745487 2884.167 1.0002948\r\nbeta[1]  -7.38947230 -14.369144436  -2.7431485 2649.559 1.0022518\r\nguess     0.09805441   0.003237798   0.2942556 3631.455 0.9997073\r\nlp__    -19.53242382 -22.933897477 -18.0116805 1675.079 1.0019349\r\nstan_ac(fitRobustAnesth, separate_chains = T)\r\n\r\nstan_trace(fitRobustAnesth)\r\n\r\npairs(fitRobustAnesth,pars=c(\"beta0\",\"beta\",\"guess\"))\r\n\r\nplot(fitRobustAnesth)\r\n\r\nplot(fitRobustAnesth,pars=c(\"guess\"))\r\n\r\n\r\nParameter guess is almost \\(10%\\).\r\nThis means that there should be a significant difference between the two\r\nmodels.\r\nHowever, 95% HDI almost covers \\(0\\).\r\nComparison of\r\nlogistic and robust logistic models\r\nCompare intercepts.\r\n\r\nrbind(Logistic=summary(fitAnesth)$summary[1,c(1,4,8)],\r\n      Robust=summary(fitRobustAnesth)$summary[1,c(1,4,8)])\r\n              mean       2.5%    97.5%\r\nLogistic 0.8195786 -0.1744736 1.944504\r\nRobust   1.0339088 -0.2046220 2.774549\r\n\r\nCompare slopes.\r\n\r\nrbind(Logistic=summary(fitAnesth)$summary[2,c(1,4,8)],\r\n      Robust=summary(fitRobustAnesth)$summary[2,c(1,4,8)])\r\n              mean      2.5%     97.5%\r\nLogistic -6.198094 -10.73846 -2.545248\r\nRobust   -7.389472 -14.36914 -2.743148\r\n\r\nCompare probabilities.\r\n\r\n# Coefficients\r\nmeanBeta0Robust<-summary(fitRobustAnesth)$summary[1,1]\r\nmeanBeta1Robust<-summary(fitRobustAnesth)$summary[2,1]\r\nguess<-summary(fitRobustAnesth)$summary[3,1]\r\nmeanBeta0<-summary(fitAnesth)$summary[1,1]\r\nmeanBeta1<-summary(fitAnesth)$summary[2,1]\r\n\r\n#Linear predictors and probabilities\r\nlinPredRobust_Move<-meanBeta0Robust+meanBeta1Robust*anesthetic$logconc\r\npRobustMove<-guess/2+(1-guess)*exp(linPredRobust_Move)/(1+exp(linPredRobust_Move))\r\nlinPred_Move<-meanBeta0+meanBeta1*anesthetic$logconc\r\npMove<-exp(linPred_Move)/(1+exp(linPred_Move))\r\n\r\n# Plot\r\nplot(anesthetic$logconc,anesthetic$move,pch=16)\r\npoints(anesthetic$logconc,pRobustMove,col=\"orange\",pch=15)\r\npoints(anesthetic$logconc,pMove,col=\"cyan\",pch=17)\r\npoints(anesthetic$logconc,predLogRegr,col=\"purple\",pch=25)\r\nlegend(\"topright\",\r\n       legend=c(\"Actual\",\"Prob Logistic\",\"Prob. Robust\",\"Glm\"),\r\n       col=c(\"black\",\"cyan\",\"orange\",\"purple\"),pch=c(16,17,15,25))\r\n\r\n\r\nAgain, robust method does not return extreme probabilities.\r\nSoftmax regression\r\nSoftmax (or multinomial logistic regression) is a generalization of\r\nlogistic regression to the case where we want to handle multiple\r\nclasses. In logistic regression, the response was binary: \\(y^{(i)} \\in \\{0,1\\}\\), meanwhile softmax\r\nhandles \\(y^{(i)} \\in \\{1, ...,\r\nK\\}\\)\r\nIt is based on modification of odds ratio from\\[\\frac{p}{1-p}\\] to \\[\\frac{p_s}{p_r},\\]\r\nwhere \\(p_r\\) is probability of one of\r\nseveral classes selected as reference (for example, control group).\r\nThe model changes very little from logistic regression and is shown\r\non this diagram.\r\nBecause adding constants to coefficients does not change softmax the\r\nreference class coefficients are assumed equal to zero.\r\nSimulated data from the book\r\nThe data SoftmaxRegData1.csv are from Kruschke, 2015\r\nchapter 22.\r\n\r\nmyData <- read.csv(file=\"data/SoftmaxRegData2.csv\" )\r\nhead(myData)\r\n           X1         X2 Y\r\n1 -0.08714736 -1.0813422 2\r\n2 -0.72256565 -1.5838631 2\r\n3  0.17918961  0.9717904 3\r\n4 -1.15975176  0.5026244 3\r\n5 -0.72711762  1.3757045 3\r\n6  0.53341559  1.7746506 3\r\ntable(myData$Y, useNA = \"always\")\r\n\r\n   1    2    3    4 <NA> \r\n  58  145  139  133    0 \r\nidx2<-myData$Y==2\r\nidx3<-myData$Y==3\r\nidx4<-myData$Y==4\r\n\r\nplot(myData$X1,myData$X2,pch=16)\r\npoints(myData$X1[idx2],myData$X2[idx2],pch=16,col=\"orange\")\r\npoints(myData$X1[idx3],myData$X2[idx3],pch=16,col=\"cyan\")\r\npoints(myData$X1[idx4],myData$X2[idx4],pch=16,col=\"magenta\")\r\n\r\n\r\nPrepare data list\r\n\r\ndataListSoftmax <- list(N=nrow(myData),  # num of observations\r\n                          K=max(myData$Y), # num of groups\r\n                          y=myData$Y,\r\n                          x=cbind(x1 = myData$X1, x2 = myData$X2),\r\n                          D=2)  # num of predictiors\r\n\r\nSoftmax model\r\nDescribe the model.\r\n\r\nmodelString=\"\r\ndata {\r\n    int<lower=2> K;  // num of groups\r\n    int<lower=0> N;  // num of observations\r\n    int<lower=1> D;  // num of predictors \r\n    int<lower=1,upper=K> y[N];\r\n    matrix[N, D] x;\r\n}\r\ntransformed data {\r\n    row_vector[D] zeros;\r\n    row_vector[D] x_m;  // x means\r\n    row_vector[D] x_sd; // x standard deviations\r\n    matrix[N, D] zx;    // normalized x\r\n    zeros = rep_row_vector(0, D); // coefficients are zeros for the baseline class\r\n    for (j in 1:D) {\r\n        x_m[j] = mean(x[,j]);\r\n        x_sd[j] = sd(x[,j]);\r\n        zx[,j] = (x[,j] - x_m[j]) / x_sd[j];\r\n    }\r\n}\r\nparameters {\r\n    matrix[K-1,D] zbeta_raw;  // K-1 makes model identifiable\r\n    vector[K-1] zbeta0_raw;\r\n}\r\ntransformed parameters {\r\n    vector[K] zbeta0;   // intersection coeffs\r\n    matrix[K, D] zbeta; // predictor coeffs\r\n    zbeta0 = append_row(0, zbeta0_raw);\r\n    zbeta = append_row(zeros, zbeta_raw); // add zeros for coefficients of the baseclass\r\n}\r\nmodel {\r\n    zbeta0_raw ~ normal(0, 5);\r\n    for (k in 1:(K-1))\r\n        zbeta_raw[k,] ~ normal(0, 5);\r\n    for (n in 1:N)\r\n        y[n] ~ categorical(softmax(zbeta0 + zbeta * to_vector(zx[n,]) ));\r\n}\r\ngenerated quantities {\r\n    vector[K] beta0;\r\n    matrix[K, D] beta;\r\n    // transform zbetas to original betas:\r\n    for (k in 1:K) {\r\n        beta0[k] = zbeta0[k];\r\n        for (j in 1:D) {\r\n            beta0[k] = beta0[k] - zbeta[k,j] * x_m[j] / x_sd[j];\r\n            beta[k,j] = zbeta[k,j] / x_sd[j];\r\n        }\r\n     }\r\n}\r\n\"\r\n\r\nCreate DSO.\r\n\r\nmodelSoftmax <- stan_model(model_code=modelString)\r\n\r\n\r\nfit <- sampling(modelSoftmax,\r\n                data=dataListSoftmax,\r\n                pars=c('beta0', 'beta'),\r\n                iter=5000, chains = 2, cores = 2)\r\n\r\nAnalysis\r\nAnalyze fitted model using shinystan, but check directly\r\nfor purposely demonstration.\r\n\r\nsummary(fit)$summary[,c(1,4,8:10)]\r\n                  mean        2.5%        97.5%    n_eff      Rhat\r\nbeta0[1]     0.0000000    0.000000    0.0000000      NaN       NaN\r\nbeta0[2]    -3.8862330   -5.342788   -2.6322714 2248.385 1.0004666\r\nbeta0[3]    -3.4331047   -4.773270   -2.2796907 3296.605 0.9997387\r\nbeta0[4]    -3.8265114   -5.189532   -2.6072841 3164.843 0.9996566\r\nbeta[1,1]    0.0000000    0.000000    0.0000000      NaN       NaN\r\nbeta[1,2]    0.0000000    0.000000    0.0000000      NaN       NaN\r\nbeta[2,1]   -5.5316729   -7.270050   -4.0375865 2447.257 1.0004653\r\nbeta[2,2]   -5.3960430   -7.160777   -3.8669145 2404.709 1.0003155\r\nbeta[3,1]   -1.4401162   -2.480155   -0.4521930 3834.628 0.9997128\r\nbeta[3,2]    8.7333497    6.723432   11.0850555 2888.872 0.9999173\r\nbeta[4,1]    8.1774008    6.235139   10.3564345 3186.626 0.9996173\r\nbeta[4,2]   -0.7379993   -1.678778    0.2006212 3773.177 0.9999730\r\nlp__      -123.4277181 -128.596525 -120.1976451 1642.521 1.0026270\r\n\r\nThe model has 4 classes and 2 predictors. The returned coefficients\r\nform a matrix \\[\\lambda_{i,1} = \\beta_{0,1} +\r\n\\beta{1,1} x_{i,1} + \\beta_{2,1} x_{i,2} \\\\\r\n\\lambda_{i,2} = \\beta_{0,2} + \\beta{1,2} x_{i,1} + \\beta_{2,2} x_{i,2}\r\n\\\\\r\n\\lambda_{i,3} = \\beta_{0,3} + \\beta{1,3} x_{i,1} + \\beta_{2,31} x_{i,2}\r\n\\\\\r\n\\lambda_{i,4} = \\beta_{0,4} + \\beta{1,4} x_{i,1} + \\beta_{2,4}\r\nx_{i,2}\\]\r\nIn this system first class is selected as reference, so \\(\\beta_{0,1} = \\beta_{1,1} = \\beta_{2,1} =\r\n0\\).\r\n\r\nstan_ac(fit, separate_chains = T)\r\n\r\nstan_trace(fit)\r\n\r\npairs(fit,pars=c(\"beta0\"))\r\n\r\npairs(fit,pars=c(\"beta\"))\r\n\r\nplot(fit)\r\n\r\n\r\nClassification\r\nTo predict classes use formula for probability of class \\(k\\) \\[p_k =\r\n\\frac{e^{\\lambda_k}}{\\sum_s e^{\\lambda_s}}.\\]\r\nCreate matrix of coefficients.\r\n\r\nSoftmaxCoeff<-summary(fit)$summary[1:12,c(1)]\r\nSoftmaxCoeff<-cbind(SoftmaxCoeff[1:4],matrix(SoftmaxCoeff[-(1:4)],ncol=2,byrow=T))\r\nrownames(SoftmaxCoeff)<-paste0(\"Class\",1:4)\r\nSoftmaxCoeff\r\n            [,1]      [,2]       [,3]\r\nClass1  0.000000  0.000000  0.0000000\r\nClass2 -3.886233 -5.531673 -5.3960430\r\nClass3 -3.433105 -1.440116  8.7333497\r\nClass4 -3.826511  8.177401 -0.7379993\r\n\r\nCreate linear predictors.\r\n\r\nhead(myData)\r\n           X1         X2 Y\r\n1 -0.08714736 -1.0813422 2\r\n2 -0.72256565 -1.5838631 2\r\n3  0.17918961  0.9717904 3\r\n4 -1.15975176  0.5026244 3\r\n5 -0.72711762  1.3757045 3\r\n6  0.53341559  1.7746506 3\r\nlinPredictors<-apply(SoftmaxCoeff[,-1],1,function(z) z%*%t(myData[,1:2]))\r\ndim(linPredictors)\r\n[1] 475   4\r\nhead(linPredictors)\r\n     Class1     Class2     Class3      Class4\r\n[1,]      0   6.317040  -9.318237  0.08539082\r\n[2,]      0  12.543590 -12.791852 -4.73981916\r\n[3,]      0  -6.235041   8.228932  0.74812465\r\n[4,]      0   3.703185   6.059772 -9.85469145\r\n[5,]      0  -3.401184  13.061642 -6.96120110\r\n[6,]      0 -12.526772  14.730464  3.05226225\r\nlinPredictors<-t(apply(linPredictors,1,function(z) z+SoftmaxCoeff[,1]))\r\ndim(linPredictors)\r\n[1] 475   4\r\nhead(linPredictors)\r\n     Class1      Class2     Class3      Class4\r\n[1,]      0   2.4308066 -12.751342  -3.7411206\r\n[2,]      0   8.6573571 -16.224956  -8.5663305\r\n[3,]      0 -10.1212743   4.795827  -3.0783867\r\n[4,]      0  -0.1830483   2.626667 -13.6812028\r\n[5,]      0  -7.2874165   9.628537 -10.7877125\r\n[6,]      0 -16.4130045  11.297359  -0.7742491\r\n\r\nCheck calculation for the first row of the data and second class.\r\n\r\nrow1<-myData[1,]\r\nClass2<-SoftmaxCoeff[2,1]+SoftmaxCoeff[2,2]*row1[1]+SoftmaxCoeff[2,3]*row1[2]\r\nc(Class2,linPredictors[1,2])\r\n$X1\r\n[1] 2.430807\r\n\r\n$Class2\r\n[1] 2.430807\r\n\r\nCreate probabilities\r\n\r\nsoftmaxProb<-exp(linPredictors)/apply(exp(linPredictors),1,sum)\r\napply(head(softmaxProb),1,sum)\r\n[1] 1 1 1 1 1 1\r\n\r\nPredict classes.\r\n\r\npredClass<-apply(softmaxProb,1,which.max)\r\nhead(predClass)\r\n[1] 2 2 3 3 3 3\r\n\r\nPlot predicted classes and compare them with the data.\r\n\r\nidx2Pred<-predClass==2\r\nidx3Pred<-predClass==3\r\nidx4Pred<-predClass==4\r\n\r\npar(mfrow=c(1,2))\r\nplot(myData$X1,myData$X2,pch=16)\r\npoints(myData$X1[idx2],myData$X2[idx2],pch=16,col=\"orange\")\r\npoints(myData$X1[idx3],myData$X2[idx3],pch=16,col=\"cyan\")\r\npoints(myData$X1[idx4],myData$X2[idx4],pch=16,col=\"magenta\")\r\n\r\nplot(myData$X1,myData$X2,pch=16)\r\npoints(myData$X1[idx2Pred],myData$X2[idx2Pred],pch=16,col=\"orange\")\r\npoints(myData$X1[idx3Pred],myData$X2[idx3Pred],pch=16,col=\"cyan\")\r\npoints(myData$X1[idx4Pred],myData$X2[idx4Pred],pch=16,col=\"magenta\")\r\n\r\npar(mfrow=c(1,1))\r\n\r\nSee how different classes are separated by hyperplanes.\r\nAdd hyperplane between class 1 and class 2:\r\n\r\nplot(myData$X1,myData$X2,pch=16)\r\npoints(myData$X1[idx2Pred],myData$X2[idx2Pred],pch=16,col=\"orange\")\r\npoints(myData$X1[idx3Pred],myData$X2[idx3Pred],pch=16,col=\"cyan\")\r\npoints(myData$X1[idx4Pred],myData$X2[idx4Pred],pch=16,col=\"magenta\")\r\nlines(myData$X1,-(SoftmaxCoeff[2,1]+SoftmaxCoeff[2,2]*myData$X1)/SoftmaxCoeff[2,3],col=\"grey\")\r\n\r\n\r\nAdd hyperplane between class 1 and class 3.\r\n\r\nplot(myData$X1,myData$X2,pch=16)\r\npoints(myData$X1[idx2Pred],myData$X2[idx2Pred],pch=16,col=\"orange\")\r\npoints(myData$X1[idx3Pred],myData$X2[idx3Pred],pch=16,col=\"cyan\")\r\npoints(myData$X1[idx4Pred],myData$X2[idx4Pred],pch=16,col=\"magenta\")\r\nlines(myData$X1,-(SoftmaxCoeff[2,1]+SoftmaxCoeff[2,2]*myData$X1)/SoftmaxCoeff[2,3],col=\"grey\")\r\nlines(myData$X1,-(SoftmaxCoeff[3,1]+SoftmaxCoeff[3,2]*myData$X1)/SoftmaxCoeff[3,3],col=\"grey\")\r\n\r\n\r\nAdd hyperplane between class 1 and class 4.\r\n\r\nplot(myData$X1,myData$X2,pch=16)\r\npoints(myData$X1[idx2Pred],myData$X2[idx2Pred],pch=16,col=\"orange\")\r\npoints(myData$X1[idx3Pred],myData$X2[idx3Pred],pch=16,col=\"cyan\")\r\npoints(myData$X1[idx4Pred],myData$X2[idx4Pred],pch=16,col=\"magenta\")\r\nlines(myData$X1,-(SoftmaxCoeff[2,1]+SoftmaxCoeff[2,2]*myData$X1)/SoftmaxCoeff[2,3],col=\"grey\")\r\nlines(myData$X1,-(SoftmaxCoeff[3,1]+SoftmaxCoeff[3,2]*myData$X1)/SoftmaxCoeff[3,3],col=\"grey\")\r\nlines(myData$X1,-(SoftmaxCoeff[4,1]+SoftmaxCoeff[4,2]*myData$X1)/SoftmaxCoeff[4,3],col=\"grey\")\r\n\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": "posts/2022-03-20-Bayesian methods - Series 7 of 10/distill-preview.png",
    "last_modified": "2023-02-23T22:41:40-06:00",
    "input_file": "Series-7----Fitting-a-Logistic-Regression-in-Bayes.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-28-Bayesian methods - Series 6 of 10/",
    "title": "Series 6 -- ANOVA & ANCOVA",
    "description": "A bit review ANOVA & ANCOVA in the Frequentist's view   \n\"ANOVA & ANCOVA\" in Bayesian Context   \nContrast Comparison",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-02-28",
    "categories": [
      "Biostatistics",
      "Bayesian Methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nOne-way ANOVA\r\nSex and Death\r\nexample of section 19.3 in (Kruschke 2015)\r\nTraditional analysis\r\nBayesian\r\napproach\r\n\r\nTwo-way ANOVA\r\nModel\r\ndescription\r\nData of section 20.1 in (Kruschke 2015)\r\nMCMC\r\nWorking with chains and\r\ncontrasts\r\nContrast for comparison\r\nof departments\r\nContrast for comparison of\r\npositions\r\nContrast for comparison of\r\nspreads\r\n\r\nUnderstanding\r\nthe effect of scaling and transformations on interactions\r\n\r\nANCOVA\r\nTraditional analysis\r\nBayesian\r\napproach\r\n\r\nHeterogeneous variances\r\nApply traditional ANOVA\r\nmethod.\r\nBayes’\r\n\r\nFurther\r\nReadings\r\n\r\nIf we pay a bit attention to the ANOVA and ANCOVA, we can see the\r\ninteresting point of views in these cases:\r\nthey greatly depend on assumptions (homoscedastic, normally\r\ndistributed)\r\n“Bayes” perhaps not call the terms ANOVA or ANCOVA. The analysis is\r\nbased on linear regression’s hierarchical modeling with the restriction\r\non some beta coefficient distributions. Thus, “Bayes” would call the\r\nmetric response with nominal predictor with/without metric\r\npredictor.\r\nOne-way ANOVA\r\nThis type of analysis actually is a model with metric output and one\r\nnominal predictor.\r\n\r\n\r\nA diagram for metric output and one nominal predictor shows parameters\r\ndenoted as \\(\\alpha\\) instead of\r\ntraditional \\(\\beta\\).\r\n\r\n\r\nWe represent the nominal predictor by a vector \\(\\vec{x}\\) = {\\(x_{1}, ..., x_{J}\\)}, where \\(J\\) is the number of categories that the\r\npredictor has. When an individual falls in group \\(j\\) of the nominal predictor, this is\r\nrepresented by setting \\(x_{j} = 1\\)\r\nand \\(x_{i\\neq j} = 0\\). The predicted\r\nvalue, denoted \\(\\mu\\), is the overall\r\nbaseline plus the deflection of a group:\r\n\\[\\begin{align*}  \r\n\\mu &= \\beta_0 + \\sum_j \\beta_{j} x_{j} \\\\  \r\n    &= \\beta_0 + \\vec{\\beta} \\cdot \\vec{x}  \r\n\\end{align*}\\]\r\nThe overall baseline is constrained so that the deflections sum to\r\n\\(0\\) across the categories:\\[ \\sum_j \\beta_{[j]} = 0 \\]\r\nIn order to satisfy this constraint, first, find unconstrained slopes\r\n\\(\\alpha_1, ..., \\alpha_J\\) and then\r\nadjust them using formulas:\r\n\\[\\begin{align*}  \r\n\\mu &= \\alpha_0 + \\sum_j \\alpha_{j} x_{j} \\\\  \r\n    &= (\\alpha_0 + \\bar{\\alpha}) + \\sum_j (\\alpha_{j} -\r\n\\bar{\\alpha}) \\cdot x_{j}\\\\  \r\n    &= \\beta_0 + \\sum_j \\beta_{j} x_{j}\r\n\\end{align*}\\]\r\nwhere \\(\\bar{\\alpha} = \\frac{1}{n} \\sum_j\r\n\\alpha_j\\)\r\nOn the diagram the within-group standard deviation \\(\\sigma\\) is given a broad uniform prior,\r\nthe intercept is given a broad normal prior centered at \\(0\\) and the group coefficients come from\r\nzero-centered normal distribution.\r\nInterpretation of intercept is the grand mean value.\r\nThe standard deviation of slopes \\(\\sigma_{\\beta}\\) may be either a constant\r\nor may have a prior of its own.\r\nIn case when \\(\\sigma_{\\beta}\\) =\r\nconst means of all groups are estimated separately, they do not inform\r\neach other through estimation of \\(\\sigma_{\\beta}\\). A large number for \\(\\sigma_{\\beta}\\) will make the results\r\nclose or equivalent to ANOVA.\r\nGiving \\(\\sigma_{\\beta}\\) a prior\r\naffects shrinkage rate. If all group means are close to the grand mean\r\nthen \\(\\sigma_{\\beta}\\) is estimated as\r\nsmall number. This makes shrinkage stronger.\r\nA key assumption behind estimating \\(\\sigma_{\\beta}\\) from the data is that all\r\ngroups can be meaningfully described as representatives of shared\r\nhigher-level distribution.\r\nExample. Giving \\(\\sigma_{\\beta}\\) a prior distribution is\r\nnot justified if, for example, in medical studies there are several\r\ncontrol groups and one treated group. In this case small difference\r\nbetween placebo groups may result in significant shrinkage (bias) of the\r\ntreated group.\r\nIn this example the treated group is an outlier. Heavy-tailed prior\r\ndistribution for \\(\\beta_j\\) may help\r\nto overcome excessive shrinkage.\r\nPossible priors for \\(\\sigma_{\\alpha}\\):\r\nNo prior, i.e. parameter is not estimated from the data. Estimated\r\ngroup means do not inform each other through \\(\\sigma_{\\alpha}\\)\r\nGamma distribution or any other distribution concentrated on\r\npositive part of real line with shape allowing large deviations from\r\nzero\r\nDistributions on non-negative part of real line allowing large\r\nvalues, but also giving positive probability to zero value: half-Cauchy,\r\nfolded t-distribution. Should be careful with this option since with\r\nsmall data set it may cause an implosive shrinkage.\r\nAnother interpretation of imploding shrinkage is: the model has to\r\nmake choice between assigning more variability to between the groups\r\n(\\(\\sigma_{\\alpha}\\)) or shrinking the\r\ngroups and assigning more variability to within the groups noise (\\(\\sigma_{y}\\)).\r\nWhenever it is possible the model will prefer the latter.\r\nShrinkage will not be very efficient in case of binary predictor.\r\nSex and Death\r\nexample of section 19.3 in (Kruschke 2015)\r\nThe fruit fly, Drosophila melanogaster, is known for this species\r\nthat newly inseminated females will not remate (for at least two days),\r\nand males will not actively court pregnant females. There were 25 male\r\nfruit flies in each of the five groups.\r\nCompanionNumber: group\r\nNone0: males with zero companions\r\nPregnant1: males with one pregnant female\r\nPregnant8: males accompanied by eight pregnant\r\nfemales\r\nVirgin1: males accompanied by one virgin female\r\nVirgin8: males accompanied by eight virgin females\r\nResearch Question: estimate the life spans and the\r\nmagnitude of differences between groups.\r\n\r\n#read the data, the file is available at [K].\r\ndta <- read.csv(\"data/Fruitfly.csv\")\r\nnames(dta)\r\n[1] \"Longevity\"       \"CompanionNumber\" \"Thorax\"         \r\ndatatable(dta)\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\"],[35,37,49,46,63,39,46,56,63,65,56,65,70,63,65,70,77,81,86,70,70,77,77,81,77,40,37,44,47,47,47,68,47,54,61,71,75,89,58,59,62,79,96,58,62,70,72,75,96,75,46,42,65,46,58,42,48,58,50,80,63,65,70,70,72,97,46,56,70,70,72,76,90,76,92,21,40,44,54,36,40,56,60,48,53,60,60,65,68,60,81,81,48,48,56,68,75,81,48,68,16,19,19,32,33,33,30,42,42,33,26,30,40,54,34,34,47,47,42,47,54,54,56,60,44],[\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"Pregnant8\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"None0\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Pregnant1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin1\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\",\"Virgin8\"],[0.64,0.68,0.68,0.72,0.72,0.76,0.76,0.76,0.76,0.76,0.8,0.8,0.8,0.84,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.92,0.92,0.92,0.94,0.64,0.7,0.72,0.72,0.72,0.76,0.78,0.8,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.88,0.88,0.92,0.92,0.92,0.92,0.92,0.92,0.94,0.64,0.68,0.72,0.76,0.76,0.8,0.8,0.8,0.82,0.82,0.84,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.88,0.88,0.88,0.88,0.92,0.92,0.68,0.68,0.72,0.76,0.78,0.8,0.8,0.8,0.84,0.84,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.9,0.9,0.9,0.9,0.9,0.9,0.92,0.92,0.64,0.64,0.68,0.72,0.72,0.74,0.76,0.76,0.76,0.78,0.8,0.8,0.82,0.82,0.84,0.84,0.84,0.84,0.88,0.88,0.88,0.88,0.88,0.88,0.92]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Longevity<\\/th>\\n      <th>CompanionNumber<\\/th>\\n      <th>Thorax<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,3]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nTraditional analysis\r\n\r\nlibrary(tidyverse)\r\ngrMeans<-group_by(dta, CompanionNumber) %>%\r\n  summarise(\r\n    count = n(),\r\n    mean = mean(Longevity, na.rm = TRUE),\r\n    sd = sd(Longevity, na.rm = TRUE)\r\n  )\r\nlevels(as.factor(dta$CompanionNumber))\r\n[1] \"None0\"     \"Pregnant1\" \"Pregnant8\" \"Virgin1\"   \"Virgin8\"  \r\ngrMeans\r\n# A tibble: 5 × 4\r\n  CompanionNumber count  mean    sd\r\n  <chr>           <int> <dbl> <dbl>\r\n1 None0              25  63.6  16.5\r\n2 Pregnant1          25  64.8  15.7\r\n3 Pregnant8          25  63.4  14.5\r\n4 Virgin1            25  56.8  14.9\r\n5 Virgin8            25  38.7  12.1\r\n\r\n\r\n# Box plots\r\n# ++++++++++++++++++++\r\n# Plot weight by group and color by group\r\nlibrary(\"ggpubr\")\r\nggboxplot(dta, x = \"CompanionNumber\", y = \"Longevity\", \r\n          color = \"CompanionNumber\", \r\n          order = c(\"None0\", \"Pregnant1\", \"Pregnant8\", \"Virgin1\", \"Virgin8\"),\r\n          ylab = \"Longevity\", xlab = \"Companion Number\")\r\n\r\nggline(dta, x = \"CompanionNumber\", y = \"Longevity\", \r\n       add = c(\"mean_se\", \"jitter\"), \r\n       order = c(\"None0\", \"Pregnant1\", \"Pregnant8\", \"Virgin1\", \"Virgin8\"),\r\n       ylab = \"Longevity\", xlab = \"Companion Number\")\r\n\r\n\r\n\r\n# Compute the analysis of variance\r\nlongevity.aov <- aov(Longevity ~ CompanionNumber, data = dta)\r\n# Summary of the analysis\r\nsummary(longevity.aov)\r\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \r\nCompanionNumber   4  11939  2984.8   13.61 3.52e-09 ***\r\nResiduals       120  26314   219.3                     \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nInterpret the result of one-way ANOVA tests:\r\nAs the p-value is less than the significance level \\(0.05\\), we can conclude that there are\r\nsignificant differences between the groups highlighted with “***” in the\r\nmodel summary.\r\nTukey multiple pairwise-comparisons\r\nAs the ANOVA test is significant, we can compute Tukey HSD (Tukey\r\nHonest Significant Differences, R function: TukeyHSD()) for performing\r\nmultiple pairwise-comparison between the means of groups.\r\nThe function TukeyHD() takes the fitted ANOVA as an argument.\r\n\r\nTukeyHSD(longevity.aov)\r\n  Tukey multiple comparisons of means\r\n    95% family-wise confidence level\r\n\r\nFit: aov(formula = Longevity ~ CompanionNumber, data = dta)\r\n\r\n$CompanionNumber\r\n                      diff       lwr        upr     p adj\r\nPregnant1-None0       1.24 -10.36047  12.840468 0.9983034\r\nPregnant8-None0      -0.20 -11.80047  11.400468 0.9999988\r\nVirgin1-None0        -6.80 -18.40047   4.800468 0.4854206\r\nVirgin8-None0       -24.84 -36.44047 -13.239532 0.0000003\r\nPregnant8-Pregnant1  -1.44 -13.04047  10.160468 0.9969591\r\nVirgin1-Pregnant1    -8.04 -19.64047   3.560468 0.3126549\r\nVirgin8-Pregnant1   -26.08 -37.68047 -14.479532 0.0000001\r\nVirgin1-Pregnant8    -6.60 -18.20047   5.000468 0.5157692\r\nVirgin8-Pregnant8   -24.64 -36.24047 -13.039532 0.0000004\r\nVirgin8-Virgin1     -18.04 -29.64047  -6.439532 0.0003240\r\n\r\nCheck ANOVA assumptions: test validity?\r\n\r\n# 1. Homogeneity of variances\r\nplot(longevity.aov, 1)\r\n\r\n# 2. Normality\r\nplot(longevity.aov, 2)\r\n\r\n# Extract the residuals\r\naov_residuals <- residuals(object = longevity.aov )\r\n# Run Shapiro-Wilk test\r\nshapiro.test(x = aov_residuals ) #Shapiro-Wilk test on the ANOVA residuals (W = 0.99, p = 0.4) which finds no indication that normality is violated.\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  aov_residuals\r\nW = 0.98887, p-value = 0.4088\r\n\r\nA Shapiro-Wilk test on the ANOVA residuals (W = 0.99, p = 0.4) which\r\nfinds no indication that normality is violated.\r\nBayesian approach\r\n\r\ndataList <- list(Ntotal = nrow(dta),\r\n                 y = dta$Longevity,\r\n                 x = as.integer(as.factor(dta$CompanionNumber)),\r\n                 NxLvl = nlevels(as.factor(dta$CompanionNumber)),\r\n                 agammaShRa=unlist(gammaShRaFromModeSD(mode = sd(dta$Longevity)/2,\r\n                                                       sd = 2*sd(dta$Longevity))))\r\n\r\nFunction gammaShRaFromModeSD() from the book calculates\r\nshape and rate parameters of gamma distribution from mode and standard\r\ndeviation.\r\n\r\n#view source code of function gammaShRaFromModeSD()\r\ngammaShRaFromModeSD\r\nfunction (mode, sd) \r\n{\r\n    if (mode <= 0) \r\n        stop(\"mode must be > 0\")\r\n    if (sd <= 0) \r\n        stop(\"sd must be > 0\")\r\n    rate = (mode + sqrt(mode^2 + 4 * sd^2))/(2 * sd^2)\r\n    shape = 1 + mode * rate\r\n    return(list(shape = shape, rate = rate))\r\n}\r\n(ShRa<-gammaShRaFromModeSD(mode=sd(dta$Longevity)/2,sd=2*sd(dta$Longevity)))\r\n$shape\r\n[1] 1.283196\r\n\r\n$rate\r\n[1] 0.03224747\r\n\r\nWith these parameters standard deviation of slopes is concentrated\r\naround a relatively small value 8.7819463, but has fat enough right\r\ntail.\r\n\r\nxAxis <- seq(from=0.001, to=100, by=1 )\r\nplot(xAxis, dgamma(xAxis, shape = ShRa$shape, rate = ShRa$rate), type='l',\r\n     ylab=\"Gamma Density\",xlab=\"Sigma_Alpha\", main=\"Prior for Sigma_Alpha: mode=8.78\")\r\nabline(v=sd(dta$Longevity)/2)\r\n\r\n\r\nPrepare the model description.\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    int<lower=2> NxLvl;\r\n    int<lower=1, upper=NxLvl> x[Ntotal];\r\n    real<lower=0> agammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n}\r\nparameters {\r\n    real a0;\r\n    real<lower=0> aSigma;\r\n    vector[NxLvl] a;\r\n    real<lower=0> ySigma;\r\n}\r\nmodel {\r\n    a0 ~ normal(meanY, 5*sdY);\r\n    aSigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a ~ normal(0, aSigma);\r\n    ySigma ~ uniform(sdY/100, sdY*10);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ normal(a0 + a[x[i]], ySigma);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a0,a[] to sum-to-zero b0,b[] :\r\n        real b0;\r\n    vector[NxLvl] b;\r\n    b0 = a0 + mean(a);\r\n    b = a - mean(a);\r\n}\"\r\n\r\nRun MCMC.\r\n\r\nstanDso <- stan_model(model_code = modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(stanDso,file=\"data/stanDSO.Rds\")\r\nstanDso <- readRDS(\"data/stanDSO.Rds\")\r\n\r\n\r\nfit <- sampling(stanDso, \r\n                data = dataList, \r\n                pars = c('b0', 'b', 'aSigma', 'ySigma'),\r\n                iter = 5000, chains = 2, cores = 4)\r\n\r\nAnalyze the chains using shinystan()\r\n\r\nlaunch_shinystan(fit)\r\n\r\n\r\nsummary(fit)$summary[,c(1,4,6,8,10)]\r\n               mean         2.5%          50%      97.5%      Rhat\r\nb0       57.4393525   54.8635804   57.4343609   60.01657 1.0001866\r\nb[1]      5.7588760    0.5134606    5.7673484   10.97372 0.9999060\r\nb[2]      6.9174882    1.8407142    6.8867440   12.16411 0.9999481\r\nb[3]      5.5581312    0.4278215    5.5290942   10.80727 0.9997391\r\nb[4]     -0.5954606   -5.7719881   -0.5929975    4.69568 1.0000531\r\nb[5]    -17.6390348  -22.7535663  -17.6253508  -12.37578 1.0002923\r\naSigma   15.4223486    6.3137021   13.3207333   37.62849 1.0021829\r\nySigma   14.9461756   13.2475061   14.9014775   16.95289 1.0005380\r\nlp__   -409.5860642 -414.8603961 -409.2042069 -406.39237 1.0017972\r\ncbind(GroupMeans=grMeans,\r\n      EstimatedMeans=summary(fit)$summary[2:6,1]+summary(fit)$summary[1,1])\r\n     GroupMeans.CompanionNumber GroupMeans.count GroupMeans.mean\r\nb[1]                      None0               25           63.56\r\nb[2]                  Pregnant1               25           64.80\r\nb[3]                  Pregnant8               25           63.36\r\nb[4]                    Virgin1               25           56.76\r\nb[5]                    Virgin8               25           38.72\r\n     GroupMeans.sd EstimatedMeans\r\nb[1]      16.45215       63.19823\r\nb[2]      15.65248       64.35684\r\nb[3]      14.53983       62.99748\r\nb[4]      14.92838       56.84389\r\nb[5]      12.10207       39.80032\r\n\r\nNote differences between estimated group means and also\r\nshrinkage.\r\n\r\nstan_ac(fit, separate_chains = T)\r\n\r\npairs(fit)\r\n\r\nplot(fit)\r\n\r\nplot(fit,pars=c(\"b\"))\r\n\r\nstan_dens(fit)\r\n\r\n\r\nFrom MCMC we see immediately that not all parameters \\(\\beta_j\\) are zeros (omnibus test).\r\nAs usual, the following question in ANOVA is: which group means are not\r\nequal.\r\nTo answer this question we use contrasts.\r\nIn FNP approach there is a constraint on how many pairs or subgroups\r\ncan be checked with contrasts with desired accuracy of inference.\r\nIn Bayesian approach testing of contrasts is straight forward and\r\ndoes not have restrictions: each step of chain returns a combination of\r\ngroup means. Thus we only need to compare frequency of these\r\ncombinations in the chain.\r\nCalculate contrasts for betas.\r\nExtract chains for parameters b.\r\n\r\nfit_ext <- rstan::extract(fit)\r\nhead(fit_ext$b)\r\n          \r\niterations     [,1]       [,2]     [,3]       [,4]      [,5]\r\n      [1,] 7.477280  3.4507237 6.191515 -4.0575910 -13.06193\r\n      [2,] 3.379211 12.8289133 5.852268  0.1562871 -22.21668\r\n      [3,] 7.777005  0.5933093 6.875631  0.7496060 -15.99555\r\n      [4,] 6.199593  4.9381578 5.802773  2.4810871 -19.42161\r\n      [5,] 8.107023  4.9390397 6.255589 -1.3545609 -17.94709\r\n      [6,] 5.496331  7.7539725 9.307447 -5.7502948 -16.80746\r\ndim(fit_ext$b)\r\n[1] 5000    5\r\n\r\nContrast between group 1 (‘None0’) and group 5 (‘Virgin8’)\r\n\r\ncontrast_1_5 <- fit_ext$b[,1] - fit_ext$b[,5]\r\nplot(contrast_1_5)\r\n\r\nhist(contrast_1_5)\r\n\r\n(hdiContrast_1_5 <- hdi(contrast_1_5))\r\n   lower    upper \r\n15.21982 31.95605 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast_1_5 <- sd(contrast_1_5))\r\n[1] 4.256989\r\nplot(rank(fit_ext$b[,1]), rank(fit_ext$b[,5]))\r\n\r\n\r\nContrast between average of groups 1-3 (None0, Pregnant1, Pregnant8)\r\nand average of groups 4-5 (Virgin1, Virgin8)\r\n\r\nComb1<-(fit_ext$b[,1] + fit_ext$b[,2] + fit_ext$b[,3])/3\r\nComb2<-(fit_ext$b[,4] + fit_ext$b[,5])/2\r\ncontrast_123_45 <- Comb1 - Comb2\r\nplot(contrast_123_45)\r\n\r\nhist(contrast_123_45)\r\n\r\n(hdiContrast_123_45<-hdi(contrast_123_45))\r\n    lower     upper \r\n 9.713508 20.416612 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast_123_45<-sd(contrast_123_45))\r\n[1] 2.734301\r\nhead(cbind(Comb1,Comb2))\r\n        Comb1      Comb2\r\n[1,] 5.706506  -8.559759\r\n[2,] 7.353464 -11.030196\r\n[3,] 5.081982  -7.622973\r\n[4,] 5.646841  -8.470262\r\n[5,] 6.433884  -9.650826\r\n[6,] 7.519250 -11.278875\r\nplot(Comb1[1:100],Comb2[1:100])\r\n\r\nplot(rank((fit_ext$b[,1] + fit_ext$b[,2] + fit_ext$b[,3])/3),rank((fit_ext$b[,4] + fit_ext$b[,5])/2))\r\n\r\n\r\nTwo-way ANOVA\r\nModel description\r\nDefine the model\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    vector[Ntotal] y;\r\n    int<lower=2> Nx1Lvl;\r\n    int<lower=2> Nx2Lvl;\r\n    int<lower=1, upper=Nx1Lvl> x1[Ntotal];\r\n    int<lower=1, upper=Nx2Lvl> x2[Ntotal];\r\n    real<lower=0> agammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    vector[Ntotal] zy;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    zy = (y - mean(y)) / sdY;  // center & normalize\r\n}\r\nparameters {\r\n    real a0;\r\n    real<lower=0> a1Sigma;\r\n    real<lower=0> a2Sigma;\r\n    real<lower=0> a1a2Sigma;\r\n    vector[Nx1Lvl] a1;\r\n    vector[Nx2Lvl] a2;\r\n    matrix[Nx1Lvl,Nx2Lvl] a1a2;\r\n    real<lower=0> zySigma;\r\n}\r\nmodel {\r\n    a0 ~ normal(0, 1);\r\n    a1Sigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a1 ~ normal(0, a1Sigma);\r\n    a2Sigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a2 ~ normal(0, a2Sigma);\r\n    a1a2Sigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    for (j1 in 1:Nx1Lvl) {\r\n        a1a2[j1,] ~ normal(0, a1a2Sigma);\r\n    }\r\n    zySigma ~ uniform(1.0/10, 10);\r\n    for ( i in 1:Ntotal ) {\r\n        zy[i] ~ normal(a0 + a1[x1[i]] + a2[x2[i]]+ a1a2[x1[i],x2[i]], zySigma);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a to sum-to-zero b :\r\n    real b0;\r\n    vector[Nx1Lvl] b1;\r\n    vector[Nx2Lvl] b2;\r\n    matrix[Nx1Lvl,Nx2Lvl] b1b2;\r\n    matrix[Nx1Lvl,Nx2Lvl] m;\r\n    real<lower=0> b1Sigma;\r\n    real<lower=0> b2Sigma;\r\n    real<lower=0> b1b2Sigma;\r\n    real<lower=0> ySigma;\r\n    for ( j1 in 1:Nx1Lvl ) { for ( j2 in 1:Nx2Lvl ) {\r\n        m[j1,j2] = a0 + a1[j1] + a2[j2] + a1a2[j1,j2]; // cell means \r\n    } }\r\n    b0 = mean(m);\r\n    for ( j1 in 1:Nx1Lvl ) { b1[j1] = mean( m[j1,] ) - b0; }\r\n    for ( j2 in 1:Nx2Lvl ) { b2[j2] = mean( m[,j2] ) - b0; }\r\n    for ( j1 in 1:Nx1Lvl ) { for ( j2 in 1:Nx2Lvl ) {\r\n        b1b2[j1,j2] = m[j1,j2] - ( b0 + b1[j1] + b2[j2] );  \r\n    } }\r\n    // transform to original scale:\r\n    b0 = meanY + sdY * b0;\r\n    b1 = sdY * b1;\r\n    b2 = sdY * b2;\r\n    b1b2 = sdY * b1b2;\r\n    b1Sigma = sdY * a1Sigma;\r\n    b2Sigma = sdY * a2Sigma;\r\n    b1b2Sigma = sdY * a1a2Sigma;\r\n    ySigma = sdY * zySigma;\r\n}\"\r\n\r\n\r\n#Create DSO.\r\nstanDsoANOVA2Way <- stan_model(model_code=modelString)\r\n\r\n\r\n#If saved DSO is used load it, then run the chains.\r\n# saveRDS(stanDsoANOVA2Way,file=\"data/stanDso2WayANOVA.Rds\")\r\nstanDsoANOVA2Way <- readRDS(file=\"data/stanDso2WayANOVA.Rds\")\r\n\r\nData of section 20.1 in (Kruschke 2015)\r\nRead the data.\r\nThe data show faculty salaries of a university.\r\nThe columns selected as predictors are:\r\nPosition (Assistant Professor, Associate Professor, Full Professor,\r\nFull Professor with endowment salary and Distinguished Professor)\r\nDepartment (total 60 departments)\r\n\r\n# 20_1: Metric Predicted Variable with Two Nominal Predictors\r\n\r\n# load data from 'Salary.csv' (see Kruschke)\r\ndf <- read.csv(\"data/Salary.csv\")\r\nmean(df$Salary)\r\n[1] 110452\r\ntail(df)\r\n      Org                                  OrgName Cla Pos ClaPos\r\n1075 OADT Business Operations & Decision Technolog  PT FT3 PT.FT3\r\n1076 CSCI                         Computer Science  PT FT3 PT.FT3\r\n1077 PHYS                                  Physics  PT FT3 PT.FT3\r\n1078 GEOL                      Geological Sciences  PR FT1 PR.FT1\r\n1079   BI                                  Biology  PR FT1 PR.FT1\r\n1080 MUST           Music School-String Technology  PR FT1 PR.FT1\r\n     Salary\r\n1075 147400\r\n1076  92500\r\n1077  76748\r\n1078 105786\r\n1079 116592\r\n1080 138022\r\ndim(df)\r\n[1] 1080    6\r\nnames(df)\r\n[1] \"Org\"     \"OrgName\" \"Cla\"     \"Pos\"     \"ClaPos\"  \"Salary\" \r\nwith(df, table(Pos))\r\nPos\r\nDST FT1 FT2 FT3 NDW \r\n 32 360 364 240  84 \r\nwith(df, table(Org))\r\nOrg\r\nACTG AFRO AMST ANTH APHS  AST BEPP BFIN   BI BLAN CEDP CEUS CHEM CMCL \r\n  19    7    7   23   18    7    9   21   51    7   25   11   35   17 \r\nCMLT CRIN CSCI EALC ECON ELPS  ENG FINH FINS FOLK FRIT GEOG GEOL GERM \r\n   8   23   30   14   20   16   39    9   14   11   14    8   19    9 \r\nHIST INFO JOUR KINE LAWS LGED LING MATH MGMT MKTG MUHI MUIN MUST MUTH \r\n  35   20   14   15   37   11    8   40   12   13    9   38   14    8 \r\nMUVO OADT  OPT PHYS   PL POLS  PSY  REL RPAD SLIS  SLS  SOC SPAN SPEA \r\n  10   14   11   32   14   22   41   13   10   13    7   20   21   40 \r\nSPHS STAT TELC THTR \r\n  15    7   12   13 \r\nlength(table(df$Org))\r\n[1] 60\r\ny <- df$Salary\r\nx1 <- as.factor(df$Pos)\r\nx2 <- as.factor(df$Org)\r\nstr(df)\r\n'data.frame':   1080 obs. of  6 variables:\r\n $ Org    : chr  \"PL\" \"MUTH\" \"ENG\" \"CMLT\" ...\r\n $ OrgName: chr  \"Philosophy\" \"Music Theory\" \"English\" \"Comparative Literature\" ...\r\n $ Cla    : chr  \"PC\" \"PC\" \"PC\" \"PC\" ...\r\n $ Pos    : chr  \"FT2\" \"FT2\" \"FT2\" \"FT2\" ...\r\n $ ClaPos : chr  \"PC.FT2\" \"PC.FT2\" \"PC.FT2\" \"PC.FT2\" ...\r\n $ Salary : int  72395 61017 82370 68805 63796 219600 98814 107745 114275 173302 ...\r\ndataListSalary <- list(Ntotal=length(y),\r\n                       y=y,\r\n                       x1=as.integer(x1),\r\n                       x2=as.integer(x2),\r\n                       Nx1Lvl=nlevels(x1),\r\n                       Nx2Lvl=nlevels(x2),\r\n                       agammaShRa=unlist(gammaShRaFromModeSD(mode=1/2, sd=2)))\r\n\r\nCreate names of variables and their interactions\r\n\r\n(namesPos<-names(table(df$Pos)))\r\n[1] \"DST\" \"FT1\" \"FT2\" \"FT3\" \"NDW\"\r\n(namesOrg<-names(table(df$Org)))\r\n [1] \"ACTG\" \"AFRO\" \"AMST\" \"ANTH\" \"APHS\" \"AST\"  \"BEPP\" \"BFIN\" \"BI\"  \r\n[10] \"BLAN\" \"CEDP\" \"CEUS\" \"CHEM\" \"CMCL\" \"CMLT\" \"CRIN\" \"CSCI\" \"EALC\"\r\n[19] \"ECON\" \"ELPS\" \"ENG\"  \"FINH\" \"FINS\" \"FOLK\" \"FRIT\" \"GEOG\" \"GEOL\"\r\n[28] \"GERM\" \"HIST\" \"INFO\" \"JOUR\" \"KINE\" \"LAWS\" \"LGED\" \"LING\" \"MATH\"\r\n[37] \"MGMT\" \"MKTG\" \"MUHI\" \"MUIN\" \"MUST\" \"MUTH\" \"MUVO\" \"OADT\" \"OPT\" \r\n[46] \"PHYS\" \"PL\"   \"POLS\" \"PSY\"  \"REL\"  \"RPAD\" \"SLIS\" \"SLS\"  \"SOC\" \r\n[55] \"SPAN\" \"SPEA\" \"SPHS\" \"STAT\" \"TELC\" \"THTR\"\r\n\r\nInteractions names:\r\n\r\n#use outer() to create names for interactions.\r\nas.vector(outer(1:4,1:2,paste,sep=\"-\"))\r\n[1] \"1-1\" \"2-1\" \"3-1\" \"4-1\" \"1-2\" \"2-2\" \"3-2\" \"4-2\"\r\n#apply to our case\r\n(namesInter<-as.vector(outer(namesOrg,namesPos,paste,sep=\"-\")))\r\n  [1] \"ACTG-DST\" \"AFRO-DST\" \"AMST-DST\" \"ANTH-DST\" \"APHS-DST\"\r\n  [6] \"AST-DST\"  \"BEPP-DST\" \"BFIN-DST\" \"BI-DST\"   \"BLAN-DST\"\r\n [11] \"CEDP-DST\" \"CEUS-DST\" \"CHEM-DST\" \"CMCL-DST\" \"CMLT-DST\"\r\n [16] \"CRIN-DST\" \"CSCI-DST\" \"EALC-DST\" \"ECON-DST\" \"ELPS-DST\"\r\n [21] \"ENG-DST\"  \"FINH-DST\" \"FINS-DST\" \"FOLK-DST\" \"FRIT-DST\"\r\n [26] \"GEOG-DST\" \"GEOL-DST\" \"GERM-DST\" \"HIST-DST\" \"INFO-DST\"\r\n [31] \"JOUR-DST\" \"KINE-DST\" \"LAWS-DST\" \"LGED-DST\" \"LING-DST\"\r\n [36] \"MATH-DST\" \"MGMT-DST\" \"MKTG-DST\" \"MUHI-DST\" \"MUIN-DST\"\r\n [41] \"MUST-DST\" \"MUTH-DST\" \"MUVO-DST\" \"OADT-DST\" \"OPT-DST\" \r\n [46] \"PHYS-DST\" \"PL-DST\"   \"POLS-DST\" \"PSY-DST\"  \"REL-DST\" \r\n [51] \"RPAD-DST\" \"SLIS-DST\" \"SLS-DST\"  \"SOC-DST\"  \"SPAN-DST\"\r\n [56] \"SPEA-DST\" \"SPHS-DST\" \"STAT-DST\" \"TELC-DST\" \"THTR-DST\"\r\n [61] \"ACTG-FT1\" \"AFRO-FT1\" \"AMST-FT1\" \"ANTH-FT1\" \"APHS-FT1\"\r\n [66] \"AST-FT1\"  \"BEPP-FT1\" \"BFIN-FT1\" \"BI-FT1\"   \"BLAN-FT1\"\r\n [71] \"CEDP-FT1\" \"CEUS-FT1\" \"CHEM-FT1\" \"CMCL-FT1\" \"CMLT-FT1\"\r\n [76] \"CRIN-FT1\" \"CSCI-FT1\" \"EALC-FT1\" \"ECON-FT1\" \"ELPS-FT1\"\r\n [81] \"ENG-FT1\"  \"FINH-FT1\" \"FINS-FT1\" \"FOLK-FT1\" \"FRIT-FT1\"\r\n [86] \"GEOG-FT1\" \"GEOL-FT1\" \"GERM-FT1\" \"HIST-FT1\" \"INFO-FT1\"\r\n [91] \"JOUR-FT1\" \"KINE-FT1\" \"LAWS-FT1\" \"LGED-FT1\" \"LING-FT1\"\r\n [96] \"MATH-FT1\" \"MGMT-FT1\" \"MKTG-FT1\" \"MUHI-FT1\" \"MUIN-FT1\"\r\n[101] \"MUST-FT1\" \"MUTH-FT1\" \"MUVO-FT1\" \"OADT-FT1\" \"OPT-FT1\" \r\n[106] \"PHYS-FT1\" \"PL-FT1\"   \"POLS-FT1\" \"PSY-FT1\"  \"REL-FT1\" \r\n[111] \"RPAD-FT1\" \"SLIS-FT1\" \"SLS-FT1\"  \"SOC-FT1\"  \"SPAN-FT1\"\r\n[116] \"SPEA-FT1\" \"SPHS-FT1\" \"STAT-FT1\" \"TELC-FT1\" \"THTR-FT1\"\r\n[121] \"ACTG-FT2\" \"AFRO-FT2\" \"AMST-FT2\" \"ANTH-FT2\" \"APHS-FT2\"\r\n[126] \"AST-FT2\"  \"BEPP-FT2\" \"BFIN-FT2\" \"BI-FT2\"   \"BLAN-FT2\"\r\n[131] \"CEDP-FT2\" \"CEUS-FT2\" \"CHEM-FT2\" \"CMCL-FT2\" \"CMLT-FT2\"\r\n[136] \"CRIN-FT2\" \"CSCI-FT2\" \"EALC-FT2\" \"ECON-FT2\" \"ELPS-FT2\"\r\n[141] \"ENG-FT2\"  \"FINH-FT2\" \"FINS-FT2\" \"FOLK-FT2\" \"FRIT-FT2\"\r\n[146] \"GEOG-FT2\" \"GEOL-FT2\" \"GERM-FT2\" \"HIST-FT2\" \"INFO-FT2\"\r\n[151] \"JOUR-FT2\" \"KINE-FT2\" \"LAWS-FT2\" \"LGED-FT2\" \"LING-FT2\"\r\n[156] \"MATH-FT2\" \"MGMT-FT2\" \"MKTG-FT2\" \"MUHI-FT2\" \"MUIN-FT2\"\r\n[161] \"MUST-FT2\" \"MUTH-FT2\" \"MUVO-FT2\" \"OADT-FT2\" \"OPT-FT2\" \r\n[166] \"PHYS-FT2\" \"PL-FT2\"   \"POLS-FT2\" \"PSY-FT2\"  \"REL-FT2\" \r\n[171] \"RPAD-FT2\" \"SLIS-FT2\" \"SLS-FT2\"  \"SOC-FT2\"  \"SPAN-FT2\"\r\n[176] \"SPEA-FT2\" \"SPHS-FT2\" \"STAT-FT2\" \"TELC-FT2\" \"THTR-FT2\"\r\n[181] \"ACTG-FT3\" \"AFRO-FT3\" \"AMST-FT3\" \"ANTH-FT3\" \"APHS-FT3\"\r\n[186] \"AST-FT3\"  \"BEPP-FT3\" \"BFIN-FT3\" \"BI-FT3\"   \"BLAN-FT3\"\r\n[191] \"CEDP-FT3\" \"CEUS-FT3\" \"CHEM-FT3\" \"CMCL-FT3\" \"CMLT-FT3\"\r\n[196] \"CRIN-FT3\" \"CSCI-FT3\" \"EALC-FT3\" \"ECON-FT3\" \"ELPS-FT3\"\r\n[201] \"ENG-FT3\"  \"FINH-FT3\" \"FINS-FT3\" \"FOLK-FT3\" \"FRIT-FT3\"\r\n[206] \"GEOG-FT3\" \"GEOL-FT3\" \"GERM-FT3\" \"HIST-FT3\" \"INFO-FT3\"\r\n[211] \"JOUR-FT3\" \"KINE-FT3\" \"LAWS-FT3\" \"LGED-FT3\" \"LING-FT3\"\r\n[216] \"MATH-FT3\" \"MGMT-FT3\" \"MKTG-FT3\" \"MUHI-FT3\" \"MUIN-FT3\"\r\n[221] \"MUST-FT3\" \"MUTH-FT3\" \"MUVO-FT3\" \"OADT-FT3\" \"OPT-FT3\" \r\n[226] \"PHYS-FT3\" \"PL-FT3\"   \"POLS-FT3\" \"PSY-FT3\"  \"REL-FT3\" \r\n[231] \"RPAD-FT3\" \"SLIS-FT3\" \"SLS-FT3\"  \"SOC-FT3\"  \"SPAN-FT3\"\r\n[236] \"SPEA-FT3\" \"SPHS-FT3\" \"STAT-FT3\" \"TELC-FT3\" \"THTR-FT3\"\r\n[241] \"ACTG-NDW\" \"AFRO-NDW\" \"AMST-NDW\" \"ANTH-NDW\" \"APHS-NDW\"\r\n[246] \"AST-NDW\"  \"BEPP-NDW\" \"BFIN-NDW\" \"BI-NDW\"   \"BLAN-NDW\"\r\n[251] \"CEDP-NDW\" \"CEUS-NDW\" \"CHEM-NDW\" \"CMCL-NDW\" \"CMLT-NDW\"\r\n[256] \"CRIN-NDW\" \"CSCI-NDW\" \"EALC-NDW\" \"ECON-NDW\" \"ELPS-NDW\"\r\n[261] \"ENG-NDW\"  \"FINH-NDW\" \"FINS-NDW\" \"FOLK-NDW\" \"FRIT-NDW\"\r\n[266] \"GEOG-NDW\" \"GEOL-NDW\" \"GERM-NDW\" \"HIST-NDW\" \"INFO-NDW\"\r\n[271] \"JOUR-NDW\" \"KINE-NDW\" \"LAWS-NDW\" \"LGED-NDW\" \"LING-NDW\"\r\n[276] \"MATH-NDW\" \"MGMT-NDW\" \"MKTG-NDW\" \"MUHI-NDW\" \"MUIN-NDW\"\r\n[281] \"MUST-NDW\" \"MUTH-NDW\" \"MUVO-NDW\" \"OADT-NDW\" \"OPT-NDW\" \r\n[286] \"PHYS-NDW\" \"PL-NDW\"   \"POLS-NDW\" \"PSY-NDW\"  \"REL-NDW\" \r\n[291] \"RPAD-NDW\" \"SLIS-NDW\" \"SLS-NDW\"  \"SOC-NDW\"  \"SPAN-NDW\"\r\n[296] \"SPEA-NDW\" \"SPHS-NDW\" \"STAT-NDW\" \"TELC-NDW\" \"THTR-NDW\"\r\n#all names:\r\nvarNames<-c(\"Intercept\", namesPos, namesOrg, namesInter, rep(\"Var\",5)) #why need to rep Var 5 times\r\n\r\nMCMC\r\n\r\n# fit model\r\nfit <- sampling(stanDsoANOVA2Way, \r\n                data=dataListSalary, \r\n                pars=c('b0',\r\n                       'b1', \r\n                       'b2', \r\n                       'b1b2',\r\n                       'b1Sigma', \r\n                       'b2Sigma',\r\n                       'b1b2Sigma',\r\n                       'ySigma'),\r\n                iter=5000, chains = 2, cores = 4)\r\n\r\n\r\n# save(fit, file = \"data/fitinstan1.Rdata\")\r\nload(\"data/fitinstan1.Rdata\")\r\n\r\n\r\nlaunch_shinystan(fit)\r\n\r\nCreate results including mean value, 2.5%, 50% and 97.5% quantiles.\r\nAdd variable names as row names.\r\n\r\nSalaryResults <- summary(fit)$summary[,c(1,4,6,8)]\r\nvarNames[nrow(SalaryResults)-(4:0)] <- rownames(SalaryResults)[nrow(SalaryResults)-(4:0)]\r\nrownames(SalaryResults) <- varNames\r\nhead(SalaryResults)\r\n                mean       2.5%        50%       97.5%\r\nIntercept 127090.073 124721.828 127081.992 129373.0577\r\nDST        55548.126  48387.941  55571.959  62569.1969\r\nFT1        -3101.583  -5949.631  -3126.253   -148.3999\r\nFT2       -33047.754 -35838.531 -33046.367 -30203.8486\r\nFT3       -46367.618 -49353.221 -46385.131 -43252.7522\r\nNDW        26968.829  22202.907  26981.535  31482.3894\r\n\r\nMake plots of mean values and HDIs.\r\n\r\nplot(fit,pars=c(\"b1\"))\r\n\r\nplot(fit,pars=c('b2'))\r\n\r\nplot(fit,pars=c(\"b1b2\"))\r\n\r\n\r\nPlots show that not all coefficients of the model equal zero. This\r\nanswers the question of utility test.\r\nWorking with chains and\r\ncontrasts\r\nExtract chains for the position variables.\r\n\r\nfit_ext <- rstan::extract(fit)\r\nnames(fit_ext)\r\n[1] \"b0\"        \"b1\"        \"b2\"        \"b1b2\"      \"b1Sigma\"  \r\n[6] \"b2Sigma\"   \"b1b2Sigma\" \"ySigma\"    \"lp__\"     \r\nfit_ext.b0<-fit_ext$b0\r\nfit_ext.b1<-fit_ext$b1\r\ncolnames(fit_ext.b1) <- namesPos\r\nhead(fit_ext.b1)\r\n          \r\niterations      DST       FT1       FT2       FT3      NDW\r\n      [1,] 53643.51 -2464.063 -32308.67 -48308.55 29437.76\r\n      [2,] 58738.28 -1706.306 -33827.16 -46158.53 22953.73\r\n      [3,] 55505.48 -3764.629 -31670.52 -44799.95 24729.62\r\n      [4,] 53497.58 -4276.004 -31554.91 -48626.05 30959.38\r\n      [5,] 57104.01 -2315.971 -34761.95 -48452.81 28426.72\r\n      [6,] 57498.84 -2508.874 -33852.21 -47274.01 26136.25\r\n\r\nExtract chains for the department variables.\r\n\r\nfit_ext.b2 <- fit_ext$b2\r\ncolnames(fit_ext.b2) <- namesOrg\r\nhead(fit_ext.b2)\r\n          \r\niterations     ACTG       AFRO        AMST       ANTH        APHS\r\n      [1,] 84518.72 -10683.803 -31817.1077  -4720.261   3396.7104\r\n      [2,] 88156.40 -12387.445    836.3933 -20495.377 -11482.3619\r\n      [3,] 81611.69 -13618.307 -28257.8104 -18097.996  11482.2086\r\n      [4,] 79194.53 -21153.965 -15077.6255 -21260.863   -125.4459\r\n      [5,] 85622.17 -25296.108 -18530.4846 -20764.400    564.3309\r\n      [6,] 82473.44  -3693.756 -14717.9823 -22886.365   6663.3884\r\n          \r\niterations        AST     BEPP      BFIN        BI     BLAN      CEDP\r\n      [1,]  -272.2711 56966.20 113408.94 -6397.807 24797.14 -23240.80\r\n      [2,]  8032.0282 50815.51 108789.31 -2003.753 23891.55 -22317.59\r\n      [3,] -4209.3496 54267.00 105146.37 -9549.098 14526.17 -11514.01\r\n      [4,]  8876.8401 30764.92  99360.17  2224.069 15125.27 -20714.19\r\n      [5,]  8044.0916 46878.48 118933.83 -2619.729 10774.34 -13026.26\r\n      [6,]  7926.3391 46454.59 109490.48 -2284.006 24635.29 -21256.39\r\n          \r\niterations      CEUS     CHEM       CMCL      CMLT      CRIN\r\n      [1,] -21339.84 21278.46 -16396.336 -13661.83 -28726.73\r\n      [2,] -18465.13 14471.89   2961.902 -37086.56 -29609.02\r\n      [3,] -19574.64 23867.96 -14992.093 -22798.07 -20194.33\r\n      [4,] -25334.82 21066.61 -12840.202 -21092.47 -17214.27\r\n      [5,] -20353.06 20205.76 -18418.019 -29499.71 -23319.70\r\n      [6,] -19667.90 14114.09  -6386.052 -16853.90 -14670.81\r\n          \r\niterations      CSCI       EALC     ECON        ELPS       ENG\r\n      [1,] 19070.861 -22627.064 53690.70  -2256.9313 -23618.76\r\n      [2,]  6341.859 -16326.435 63001.24  -6692.3318 -24588.41\r\n      [3,]  5777.751 -24292.943 60270.28 -11446.9859 -22431.50\r\n      [4,] 19402.512 -25074.484 56716.46   -865.0668 -21256.75\r\n      [5,] 13926.101 -11248.245 45142.49  -5962.1880 -19477.31\r\n      [6,] 14513.160  -3620.939 57299.77  -7540.6420 -12163.67\r\n          \r\niterations      FINH      FINS      FOLK       FRIT       GEOG\r\n      [1,] -14346.30 -18209.02 -29875.32 -21655.118  -5639.289\r\n      [2,] -15715.47 -20946.13 -23147.34 -23395.993 -16202.811\r\n      [3,] -26414.60 -22185.23 -17332.30 -14807.023 -18898.645\r\n      [4,] -13383.51 -21330.12 -15269.66 -16003.793  -9272.097\r\n      [5,] -23487.76 -14402.51 -20000.94 -15901.330  -5976.184\r\n      [6,] -16291.29 -24708.77 -21488.15  -2759.774 -15596.673\r\n          \r\niterations       GEOL      GERM       HIST      INFO       JOUR\r\n      [1,] -22550.037 -23929.79 -11922.908 14948.452 -24063.132\r\n      [2,]  -7012.096 -51561.03  -9361.597  6416.838 -26205.125\r\n      [3,] -10887.379 -32162.35  -9143.212  4342.127  -9072.310\r\n      [4,] -16233.336 -23952.16  -9301.820 15654.625 -23874.915\r\n      [5,] -21942.573 -35140.01 -15107.426 15316.580 -15393.027\r\n      [6,]  -4562.365 -30880.93 -16131.613  7988.985  -9296.838\r\n          \r\niterations       KINE     LAWS      LGED       LING        MATH\r\n      [1,]  -9193.225 38785.02 -17528.29 -17190.155 -8225.30392\r\n      [2,]  -6282.942 47666.25 -17899.19 -13565.903 -7706.18824\r\n      [3,]  -9910.339 45848.64 -27964.54  -6617.547 -1768.38643\r\n      [4,] -11922.797 33108.61 -18500.86   2572.191 -3658.10736\r\n      [5,]  -2202.068 39704.44 -14898.99 -15872.538 -8693.52608\r\n      [6,]  -5544.008 38161.16 -33744.68  -9544.743   -85.51853\r\n          \r\niterations     MGMT     MKTG      MUHI       MUIN       MUST\r\n      [1,] 62113.34 64268.19 -32968.88 -20481.139  -844.2042\r\n      [2,] 62136.44 67642.69 -27573.69 -18587.600 -6443.1771\r\n      [3,] 74127.78 71343.24 -34818.65  -5149.272  -998.3260\r\n      [4,] 59756.08 62986.43 -24386.81 -27655.967  7488.0542\r\n      [5,] 63347.38 58431.11 -27734.24 -23368.999  7810.1252\r\n      [6,] 47017.26 69201.49 -35955.53 -19428.976 -6371.6317\r\n          \r\niterations      MUTH      MUVO     OADT       OPT       PHYS\r\n      [1,] -27687.26 -29106.00 53817.90 -13708.67   938.1202\r\n      [2,] -37874.35 -31662.78 57211.89 -17478.04 -5738.0122\r\n      [3,] -28434.64 -33955.67 60768.24  -4218.98  3772.0028\r\n      [4,] -41578.53 -26331.99 52976.79 -21015.56  6144.4542\r\n      [5,] -21352.16 -24057.30 55091.18 -14208.71 -3552.6289\r\n      [6,] -35308.73 -39279.62 54524.81 -13330.94 -2781.7758\r\n          \r\niterations         PL      POLS       PSY        REL       RPAD\r\n      [1,] -11753.736  3513.843  7017.447  -6453.305  -3187.879\r\n      [2,]  -8796.469 15546.737  8450.865 -14424.987 -12159.452\r\n      [3,]  -9792.748  5648.549  9457.655 -12918.854  -4778.571\r\n      [4,]   3873.790  7563.929  3148.918 -14619.568  -6131.743\r\n      [5,] -19536.730 10201.619 10906.627  -1860.096  -2935.141\r\n      [6,]  -2805.521  4428.878  9198.403 -12073.818  -1760.722\r\n          \r\niterations       SLIS        SLS        SOC      SPAN     SPEA\r\n      [1,] 12208.8805 -17676.380 -14610.381 -26932.70 12609.29\r\n      [2,]  3209.0633   -152.842 -10660.845 -11541.33 21244.00\r\n      [3,]  6337.0940 -22995.180  -3894.261 -28734.78 13156.72\r\n      [4,]  6231.9672 -17295.152  -5021.769 -16128.08 15097.28\r\n      [5,]   849.9127 -21943.215  -5528.208 -10620.69 12476.60\r\n      [6,]  8901.4092 -26578.505  -5382.470 -21548.77 16643.46\r\n          \r\niterations      SPHS        STAT       TELC      THTR\r\n      [1,] 12925.959 10388.95421  -2909.144 -32256.04\r\n      [2,]  8928.522  6829.27323 -17467.685 -11563.16\r\n      [3,]  2515.913   963.79821  -8621.868 -27778.40\r\n      [4,]  2017.494  -229.95943  -7111.211 -19132.31\r\n      [5,]  2956.340    79.86284 -12122.435 -20908.74\r\n      [6,]  8378.987 -8767.91555 -27142.360 -33120.35\r\n\r\nExtract chains for interaction variables.\r\n\r\nfit_ext.b1.b2<-fit_ext$b1b2\r\ndim(fit_ext.b1.b2)\r\n[1] 5000    5   60\r\n\r\nInteraction chains make a cube.\r\n\r\ndimnames(fit_ext.b1.b2)[[2]]<-namesPos\r\ndimnames(fit_ext.b1.b2)[[3]]<-namesOrg\r\ndimnames(fit_ext.b1.b2)\r\n$iterations\r\nNULL\r\n\r\n[[2]]\r\n[1] \"DST\" \"FT1\" \"FT2\" \"FT3\" \"NDW\"\r\n\r\n[[3]]\r\n [1] \"ACTG\" \"AFRO\" \"AMST\" \"ANTH\" \"APHS\" \"AST\"  \"BEPP\" \"BFIN\" \"BI\"  \r\n[10] \"BLAN\" \"CEDP\" \"CEUS\" \"CHEM\" \"CMCL\" \"CMLT\" \"CRIN\" \"CSCI\" \"EALC\"\r\n[19] \"ECON\" \"ELPS\" \"ENG\"  \"FINH\" \"FINS\" \"FOLK\" \"FRIT\" \"GEOG\" \"GEOL\"\r\n[28] \"GERM\" \"HIST\" \"INFO\" \"JOUR\" \"KINE\" \"LAWS\" \"LGED\" \"LING\" \"MATH\"\r\n[37] \"MGMT\" \"MKTG\" \"MUHI\" \"MUIN\" \"MUST\" \"MUTH\" \"MUVO\" \"OADT\" \"OPT\" \r\n[46] \"PHYS\" \"PL\"   \"POLS\" \"PSY\"  \"REL\"  \"RPAD\" \"SLIS\" \"SLS\"  \"SOC\" \r\n[55] \"SPAN\" \"SPEA\" \"SPHS\" \"STAT\" \"TELC\" \"THTR\"\r\nfit_ext.b1.b2[1,,]\r\n     \r\n            ACTG      AFRO      AMST      ANTH        APHS        AST\r\n  DST   3539.479  6393.899  1304.058 15763.611 -10799.0416   7927.158\r\n  FT1 -16326.820 -7421.503 -6828.542 -9787.681  11662.4623  -2611.558\r\n  FT2   9626.651 -4519.507  3486.851 -9088.313  -2036.0353   2425.408\r\n  FT3   8239.038 -2385.126 10078.649  3483.160   -975.3569 -13603.240\r\n  NDW  -5078.347  7932.237 -8041.017  -370.778   2147.9715   5862.232\r\n     \r\n           BEPP        BFIN        BI       BLAN       CEDP      CEUS\r\n  DST  8728.384  -2330.6655 -2834.672 -5466.1952 -15580.544 -2080.820\r\n  FT1 -3538.370    976.2805  4358.263 -2541.3345  -1900.758  1838.315\r\n  FT2  4630.186  12267.9786 -5681.101  -673.3443   4958.960  4207.809\r\n  FT3 -2312.500   1773.3809 12940.011  1518.5936   9637.629  5874.503\r\n  NDW -7507.699 -12686.9744 -8782.501  7162.2803   2884.713 -9839.807\r\n     \r\n            CHEM      CMCL         CMLT      CRIN       CSCI\r\n  DST   3002.086 -9440.742    -86.93006 -9140.478  3129.8086\r\n  FT1  11524.040 14411.093  -2557.61581 -3161.343 -2309.5690\r\n  FT2  -5293.531 -2668.087 -16853.21654  6077.927 -7840.4627\r\n  FT3 -22330.841 -7414.901   7430.17731 10001.465   422.3283\r\n  NDW  13098.245  5112.637  12067.58510 -3777.572  6597.8948\r\n     \r\n           EALC        ECON       ELPS         ENG       FINH\r\n  DST -4167.890   -258.8566  -721.5942   -692.6053 -4837.1791\r\n  FT1  7427.198   5254.3375  3062.8163   2486.2253 -9159.9057\r\n  FT2  6582.357  12117.9070 -4679.8036   5590.4990 17244.9910\r\n  FT3 -3181.338 -22080.7613 -2932.5083   4007.1760 -2496.7910\r\n  NDW -6660.328   4967.3734  5271.0899 -11391.2951  -751.1152\r\n     \r\n            FINS      FOLK       FRIT       GEOG        GEOL\r\n  DST   325.8752 -3893.956  -6294.419  6675.0396   5774.6693\r\n  FT1   535.5009 -7072.318  17389.175  7479.6932  12352.3715\r\n  FT2  -328.2188  4064.460 -15657.924 -6928.5509   2648.6372\r\n  FT3 -7741.0741  9117.089   6343.980  -701.5859    466.6109\r\n  NDW  7207.9169 -2215.275  -1780.812 -6524.5961 -21242.2889\r\n     \r\n           GERM      HIST        INFO      JOUR       KINE\r\n  DST  4706.445 -8527.565   3237.7335 -1757.081 -10582.306\r\n  FT1 -2672.307 -2134.152   4940.7638 -3147.530  14339.188\r\n  FT2 -5330.064 -3001.392 -12774.4001  4272.784  -3333.583\r\n  FT3  9323.907 -5018.605   5237.0730  9447.929   7535.690\r\n  NDW -6027.980 18681.715   -641.1703 -8816.101  -7958.989\r\n     \r\n             LAWS        LGED       LING       MATH      MGMT\r\n  DST -10015.5598  4591.43556  -7204.124 -11581.540 -3716.644\r\n  FT1   3412.6992  -587.59280   5405.700  -9752.037 -1804.589\r\n  FT2   6352.9234  -399.63708 -14028.469   4526.424  7716.997\r\n  FT3   -899.5615   -10.67023  -4023.939  12426.506 -5186.786\r\n  NDW   1149.4988 -3593.53544  19850.832   4380.648  2991.022\r\n     \r\n             MKTG      MUHI       MUIN        MUST      MUTH\r\n  DST  12502.3899 -7724.675   3129.524   7886.1204 -9039.427\r\n  FT1 -16194.6842  3408.667  -4813.088   5061.4779 12022.885\r\n  FT2  -2920.6533 -3107.146   3336.682  -2460.8449 -1193.335\r\n  FT3    768.3334  6316.702   9569.930 -10862.8928 -6529.798\r\n  NDW   5844.6142  1106.452 -11223.048    376.1394  4739.675\r\n     \r\n           MUVO        OADT        OPT       PHYS        PL      POLS\r\n  DST -9946.788   -888.6629   1872.653  -3744.574  3435.746 -6711.089\r\n  FT1  8902.435 -10644.0479 -12866.850   5992.883  1352.037  1818.654\r\n  FT2 -7975.997  17648.2569   5249.675  14873.035 -9887.859 -4523.008\r\n  FT3 12665.367   3657.3418  -3987.251 -13554.341 12865.421 -2919.634\r\n  NDW -3645.017  -9772.8880   9731.773  -3567.002 -7765.346 12335.077\r\n     \r\n              PSY        REL       RPAD       SLIS       SLS\r\n  DST  11700.3240 -6837.7256   9313.103  15054.896  8808.297\r\n  FT1 -15963.5566  -688.5178 -10034.632   9249.393 -3786.320\r\n  FT2 -12322.9888 -4672.9437   7567.516 -11362.755 -6045.782\r\n  FT3    525.9758 -7990.0302 -13929.224  -9658.127  1906.952\r\n  NDW  16060.2456 20189.2173   7083.237  -3283.406  -883.147\r\n     \r\n            SOC          SPAN       SPEA       SPHS       STAT\r\n  DST -6806.699   1782.323470  12803.000  13314.603  2412.6612\r\n  FT1 -5621.186   1570.915735   4660.468  -6725.916  1597.2451\r\n  FT2  2436.551   9372.847478   2481.490  -6071.677   755.1363\r\n  FT3  1224.220      4.001735   1103.587 -10906.537 -4197.6978\r\n  NDW  8767.113 -12730.088418 -21048.545  10389.527  -567.3448\r\n     \r\n           TELC       THTR\r\n  DST -5399.985   9995.711\r\n  FT1 -7969.306   6130.450\r\n  FT2  3291.536   7848.155\r\n  FT3 -2627.908   4546.298\r\n  NDW 12705.663 -28520.613\r\n\r\nRows correspond to iterations of MCMC.\r\nFor each iteration there is interactions table between schools and\r\npositions.\r\nSalary of a school across positions is base level beta, plus school\r\nbeta: \\(\\beta_0+\\beta_{school}\\).\r\nSalary of a position across schools is base level beta, plus position\r\nbeta: \\(\\beta_0+\\beta_{position}\\).\r\nSalary of a certain position of a certain school is predicted as base\r\nlevel beta, plus school beta, plus position beta, plus the interaction\r\nbeta: \\(\\beta_0+\\beta_{school}+\\beta_{position}+\\beta_{school\r\n\\ \\times \\ position}\\).\r\nTo answer each of the following 4 questions do the\r\nfollowing:\r\nCreate contrast for comparison\r\nMake histogram of the contrast\r\nCalculate mean of the contrast\r\nCalculate 95%-HDI for the contrast. To find HDI use\r\nhdi() from library HDInterval\r\nContrast for comparison\r\nof departments\r\nUse contrasts to compare salaries at Business and Finance (“BFIN”)\r\nwith Physics (“PHYS”) and with Chemistry (“CHEM”) departments.\r\nTo do that select columns of MCMC for departments to “BFIN” and\r\n“PHYS” and “BFIN” and “CHEM”, take their differences and look at the\r\nposterior distribution of the differences\r\n\r\ncontrast.BFIN.PHYS <- fit_ext.b2[,\"BFIN\"]-fit_ext.b2[,\"PHYS\"]\r\nhist(contrast.BFIN.PHYS)\r\n\r\nmean(contrast.BFIN.PHYS)\r\n[1] 111653.5\r\nhdi(contrast.BFIN.PHYS)\r\n    lower     upper \r\n 99528.41 124555.65 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nWhat do we conclude about the differences of\r\nsalaries?\r\nPlot histograms of salaries for both departments. Salaries\r\nBFINComp of Business and Finance and PHYSComp\r\nof Physics are calculate as described above.\r\n\r\nBFINComp <- fit_ext.b2[,\"BFIN\"]\r\nPHYSComp <- fit_ext.b2[,\"PHYS\"]\r\n\r\n\r\ndistrBFIN <- density(BFINComp)\r\ndistrPHYS <- density(PHYSComp)\r\n\r\nplot(distrPHYS,xlim=c(-20000,200000), lwd=2, main=\"Salaries Distributions\", col=\"blue\")\r\nlines(distrBFIN$x, distrPHYS$y, lwd=2, col=\"orange\")\r\nlegend(\"top\", legend = c(\"PHYS\",\"BFIN\"), col = c(\"blue\",\"orange\"), lty=1, lwd=2)\r\n\r\n\r\nDo the same comparison for finance professors and chemistry\r\nprofessors.\r\n\r\ncontrast.BFIN.CHEM<-fit_ext.b2[,\"BFIN\"]-fit_ext.b2[,\"CHEM\"]\r\nhist(contrast.BFIN.CHEM)\r\n\r\nmean(contrast.BFIN.CHEM)\r\n[1] 89933.55\r\nhdi(contrast.BFIN.CHEM)\r\n    lower     upper \r\n 78655.11 101160.71 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n\r\nBFINComp <- fit_ext.b2[,\"BFIN\"]\r\nCHEMComp <- fit_ext.b2[,\"CHEM\"]\r\n\r\n\r\ndistrBFIN <- density(BFINComp)\r\ndistrPHYS <- density(PHYSComp)\r\n\r\nplot(distrPHYS,xlim=c(-20000,200000), lwd=2, main=\"Salaries Distributions\", col=\"blue\")\r\nlines(distrBFIN$x, distrPHYS$y, lwd=2, col=\"orange\")\r\nlegend(\"top\", legend = c(\"PHYS\",\"BFIN\"), col = c(\"blue\",\"orange\"), lty=1, lwd=2)\r\n\r\n\r\nContrast for comparison of\r\npositions\r\nUse contrasts to compare salaries of Endowment Full Professor (“NDW”)\r\nand Distinguished Full Professor (“DST”). Compare salaries of Full\r\nProfessor (“FT1”) and Endowment Full Professor\r\n\r\ncontrast.NDW.DST <- fit_ext.b1[,\"NDW\"] - fit_ext.b1[,\"DST\"]\r\nhist(contrast.NDW.DST)\r\n\r\nmean(contrast.NDW.DST)\r\n[1] -28579.3\r\nhdi(contrast.NDW.DST)\r\n    lower     upper \r\n-38342.94 -17938.62 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\nDSTComp <- fit_ext.b1[,\"NDW\"]\r\nNDWComp <- fit_ext.b1[,\"DST\"]\r\n\r\ndistrDST<-density(DSTComp)\r\ndistrNDW<-density(NDWComp)\r\nplot(distrDST,xlim=c(14000,80000),lwd=2,main=\"Salaries Distributions\",col=\"blue\")\r\nlines(distrNDW$x,distrPHYS$y,lwd=2,col=\"orange\")\r\nlegend(\"top\",legend=c(\"DST\",\"NDW\"),col=c(\"blue\",\"orange\"),lty=1,lwd=2)\r\n\r\n\r\nSalary of a Distinguished Full Professor is significantly higher than\r\nsalary of Endowment Full Professor.\r\nAnalyze difference between salaries of Full Professor (“FT1”) and\r\nEndowment Full Professor\r\n\r\ncontrast.NDW.FT1<-fit_ext.b1[,\"NDW\"]-fit_ext.b1[,\"FT1\"]\r\nhist(contrast.NDW.FT1)\r\n\r\nmean(contrast.NDW.FT1)\r\n[1] 30070.41\r\nhdi(contrast.NDW.FT1)\r\n   lower    upper \r\n23860.41 35549.30 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nContrast for comparison of\r\nspreads\r\nUse contrasts to compare salaries spreads between Full Professor and\r\nAssistant Professor at Physics Department and at Chemistry\r\nDepartment.\r\n\r\nspreadPhys<-fit_ext.b1[,\"FT1\"]+fit_ext.b1.b2[,\"FT1\",\"PHYS\"]-\r\n  (fit_ext.b1[,\"FT3\"]+fit_ext.b1.b2[,\"FT3\",\"PHYS\"])\r\nspreadChem<-fit_ext.b1[,\"FT1\"]+fit_ext.b1.b2[,\"FT1\",\"CHEM\"]-\r\n  (fit_ext.b1[,\"FT3\"]+fit_ext.b1.b2[,\"FT3\",\"CHEM\"])\r\nspread<-spreadPhys-spreadChem\r\nhist(spread)\r\n\r\nmean(spread)\r\n[1] -15530.47\r\nhdi(spread)\r\n     lower      upper \r\n-35565.326   3157.709 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nFind the highest HDI level for which the spread of differences\r\nbetween “FT1” and “FT3” is significant.\r\n\r\nhdi(spread,.88)\r\n      lower       upper \r\n-30176.5124   -661.9843 \r\nattr(,\"credMass\")\r\n[1] 0.88\r\ntryLevels<-seq(from=.8,to=.95,by=.01)\r\nhdis<-sapply(tryLevels,function(z) hdi(spread,z))\r\nmaxLevel<-tail(tryLevels[hdis[2,]<0],1)\r\nhdi(spread,maxLevel)\r\n      lower       upper \r\n-30176.5124   -661.9843 \r\nattr(,\"credMass\")\r\n[1] 0.88\r\n\r\nUnderstanding\r\nthe effect of scaling and transformations on interactions\r\nNonlinear transformations may affect interactions very\r\nsignificantly.\r\nIllustrate it on a simple simulated example.\r\n\r\nmean00<-1\r\nmean10<-3\r\nmean01<-4\r\nmean11<-6\r\ny00<-rnorm(5,mean00,.1)\r\ny10<-rnorm(5,mean10,.1)\r\ny01<-rnorm(5,mean01,.1)\r\ny11<-rnorm(5,mean11,.1)\r\n\r\nPlot the effects. If the lines are parallel the effects are\r\nadditive.\r\n\r\nplot(c(0,1),c(mean(y00),mean(y10)),type=\"b\",ylim=c(1,8),col=\"darkgreen\",lwd=3,ylab=\"Response\",xlab=\"Predictor 1\")\r\nlines(c(0,1),c(mean(y01),mean(y11)),type=\"b\",col=\"lightblue\",lwd=3)\r\nlegend(\"topleft\",legend=c(\"Predictor2 at 0\",\"Predictor2 at 1\"),lty=1,lwd=3,col=c(\"darkgreen\",\"lightblue\"))\r\n\r\n\r\nTaking exponent of the same data introduces significant\r\ninteraction.\r\n\r\nplot(c(0,1),c(mean(exp(y00)),mean(exp(y10))),type=\"b\",ylim=c(1,400),col=\"darkgreen\",lwd=3,ylab=\"Response\",xlab=\"Predictor 1\")\r\nlines(c(0,1),c(mean(exp(y01)),mean(exp(y11))),type=\"b\",col=\"lightblue\",lwd=3)\r\nlegend(\"topleft\",legend=c(\"Predictor2 at 0\",\"Predictor2 at 1\"),lty=1,lwd=3,col=c(\"darkgreen\",\"lightblue\"))\r\n\r\n\r\nANCOVA\r\nTraditional analysis\r\nWe cannot use aov in ANCOVA because it uses Type I SS,\r\ninstead we should use Type II SS to get the correct results.\r\n\r\nlongevity.aov2 <- aov(Longevity ~ CompanionNumber + Thorax, data = dta)\r\n# car Anova command on our longevity.aov2 object,\r\n#summary(longevity.aov2) #produces incorrect results\r\nAnova(longevity.aov2, type=\"II\")\r\nAnova Table (Type II tests)\r\n\r\nResponse: Longevity\r\n                 Sum Sq  Df F value    Pr(>F)    \r\nCompanionNumber  9611.5   4  21.753 1.719e-13 ***\r\nThorax          13168.9   1 119.219 < 2.2e-16 ***\r\nResiduals       13144.7 119                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThe covariate, Thorax, was significantly related to the\r\nfly’s longevity, F(1,119)=119.22, p<.001. There was also a\r\nsignificant effect of the Companion Number on the Longevity after\r\ncontrolling for the effect of the Thorax, F(4,119)=21.753,\r\np<.001.\r\nBayesian approach\r\nData show significant variance within groups.\r\n\r\n\r\nAn additional metric predictor that can explain part of that variance.\r\n\r\n\r\nIn this example the subjects of the experiment were in different\r\nphysical conditions which contributed to large variation of life\r\nexpectancy.\r\nA metric predictor that is available is the size of the species.\r\nCreate data with both predictors.\r\n\r\ndataList2<-list(Ntotal=nrow(dta),\r\n                y=dta$Longevity,\r\n                xMet=dta$Thorax,\r\n                xNom=as.integer(as.factor(dta$CompanionNumber)),\r\n                NxLvl=nlevels(as.factor(dta$CompanionNumber)),\r\n                agammaShRa=unlist(gammaShRaFromModeSD(mode=sd(dta$Longevity)/2, \r\n                                                      sd=2*sd(dta$Longevity))))\r\n\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    int<lower=2> NxLvl;\r\n    int<lower=1, upper=NxLvl> xNom[Ntotal];\r\n    real xMet[Ntotal];\r\n    real<lower=0> agammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    real xMetMean;\r\n    real xMetSD;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    xMetMean = mean(xMet);\r\n    xMetSD = sd(xMet);\r\n}\r\nparameters {\r\n    real a0;\r\n    real<lower=0> aSigma;\r\n    vector[NxLvl] a;\r\n    real aMet;\r\n    real<lower=0> ySigma;\r\n}\r\nmodel {\r\n    a0 ~ normal(meanY, 5*sdY);\r\n    aSigma ~ gamma(agammaShRa[1], agammaShRa[2]);\r\n    a ~ normal(0, aSigma);\r\n    aMet ~ normal(0, 2*sdY/xMetSD);\r\n    ySigma ~ uniform(sdY/100, sdY*10);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ normal(a0 + a[xNom[i]] + aMet*(xMet[i] - xMetMean), ySigma);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a0,a[] to sum-to-zero b0,b[] :\r\n        real b0;\r\n    vector[NxLvl] b;\r\n    b0 = a0 + mean(a) - aMet * xMetMean;\r\n    b = a - mean(a);\r\n}\r\n\"\r\n\r\n\r\nmodel2 <- stan_model(model_code=modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(model2, file=\"data/stanDSO2.Rds\")\r\nmodel2 <- readRDS(\"data/stanDSO2.Rds\")\r\n\r\nRun MCMC.\r\n\r\nfit2 <- sampling(model2, \r\n                data=dataList2, \r\n                pars=c('b0', 'b', 'aMet', 'aSigma', 'ySigma'),\r\n                iter=5000, chains = 2, cores = 4)\r\n\r\n\r\nlaunch_shinystan(fit2)\r\n\r\nCalculate same contrasts for betas:\r\nContrasts between group 1 (‘None0’) and 5 (‘Virgin8’).\r\n\r\nfit_ext2 <- rstan::extract(fit2)\r\nhead(fit_ext2$b)\r\n          \r\niterations     [,1]     [,2]     [,3]       [,4]      [,5]\r\n      [1,] 4.466494 6.207247 3.940179 -0.8029348 -13.81098\r\n      [2,] 7.582592 4.825399 6.512522 -5.6358837 -13.28463\r\n      [3,] 6.948511 7.865498 6.695544 -6.7136045 -14.79595\r\n      [4,] 4.922978 7.802200 6.146581 -5.6397349 -13.23202\r\n      [5,] 7.228799 5.271264 6.506355 -5.4617319 -13.54469\r\n      [6,] 3.844551 8.005606 6.024163 -5.0992662 -12.77505\r\ncontrast2_1_5 <- fit_ext2$b[,1] - fit_ext2$b[,5]\r\nplot(contrast2_1_5)\r\n\r\nhist(contrast2_1_5)\r\n\r\n(hdiContrast2_1_5<-hdi(contrast2_1_5))\r\n   lower    upper \r\n13.40625 25.09579 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast2_1_5<-sd(contrast2_1_5))\r\n[1] 2.995819\r\nplot(rank(fit_ext2$b[,1]),rank(fit_ext2$b[,5]))\r\n\r\n\r\nContrast between average of groups 1-3 and average of groups\r\n4-5\r\n\r\nComb1<-(fit_ext2$b[,1] + fit_ext2$b[,2] + fit_ext2$b[,3])/3\r\nComb2<-(fit_ext2$b[,4] + fit_ext2$b[,5])/2\r\ncontrast2_123_45 <-Comb1-Comb2\r\nplot(contrast2_123_45)\r\n\r\nhist(contrast2_123_45)\r\n\r\n(hdiContrast2_123_45<-hdi(contrast2_123_45))\r\n   lower    upper \r\n11.28173 18.74789 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast2_123_45<-sd(contrast2_123_45))\r\n[1] 1.910106\r\nplot(rank(Comb1),rank(Comb2))\r\n\r\n\r\nCompare the models.\r\nUsing standard deviation:\r\n\r\nrbind(WithoutMetricPred=c(sd.contrast_1_5,sd.contrast_123_45),WithMetricPred=c(sd.contrast2_1_5,sd.contrast2_123_45))\r\n                      [,1]     [,2]\r\nWithoutMetricPred 4.256989 2.734301\r\nWithMetricPred    2.995819 1.910106\r\n\r\nUsing HDI:\r\n\r\nrbind(WithoutMetricPred=c(hdiContrast_1_5,hdiContrast_123_45),WithMetricPred=c(hdiContrast2_1_5,hdiContrast2_123_45))\r\n                     lower    upper     lower    upper\r\nWithoutMetricPred 15.21982 31.95605  9.713508 20.41661\r\nWithMetricPred    13.40625 25.09579 11.281726 18.74789\r\n\r\nUsing HDI width:\r\n\r\nrbind(WithoutMetricPred=c(Contrast_1_5=unname(diff(hdiContrast_1_5)),\r\n                          Contrast_123_45=unname(diff(hdiContrast_123_45))),\r\n      WithMetricPred=c(Contrast_1_5=diff(hdiContrast2_1_5),\r\n                       Contrast_123_45=diff(hdiContrast2_123_45)))\r\n                  Contrast_1_5 Contrast_123_45\r\nWithoutMetricPred     16.73622       10.703103\r\nWithMetricPred        11.68954        7.466168\r\n\r\nObviously, the width was shrinkage so with the metric predictor in\r\nthe model, we can explain more the variability of Longevity due to the\r\ndifference among groups and Thorax.\r\nHeterogeneous variances\r\nThe most restrictive assumption of ANOVA is equivalence of variances\r\nin all groups.\r\nLook how Bayesian approach can remove this assumption.\r\nOn the following model diagram there is an implementation of model\r\nwith heterogeneous variances.\r\nThe model uses t-distributed noise and allows each group to have its\r\nown standard deviation.\r\nData set InsectSprays from datasets shows\r\ntest of 6 different insect sprays. Column count contains numbers of\r\ninsects found in the field after each spraying. Column spray identifies\r\nthe type of spray.\r\nPrepare the data.\r\n\r\ndf <- InsectSprays\r\nhead(df)\r\n  count spray\r\n1    10     A\r\n2     7     A\r\n3    20     A\r\n4    14     A\r\n5    14     A\r\n6    12     A\r\nlevels(df$spray)\r\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\r\ndataListSprays <- list(Ntotal = nrow(df),\r\n                       y = df$count,\r\n                       x = as.integer(df$spray),\r\n                       NxLvl = nlevels(df$spray),\r\n                       aGammaShRa = unlist(gammaShRaFromModeSD(mode=sd(df$count)/2, \r\n                                                               sd=2*sd(df$count))))\r\n\r\nPlot the data.\r\n\r\nplot(df$count ~ df$spray)\r\n\r\n\r\nNote that variances within the groups are very different. If\r\nestimated overall the variance for groups “C”, “D” and “E” is\r\noverestimated and for “A” “B” and “F” - underestimated.\r\nApply traditional ANOVA\r\nmethod.\r\n\r\nm1 <- lm(count~spray, df)\r\nsummary(m1)\r\n\r\nCall:\r\nlm(formula = count ~ spray, data = df)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-8.333 -1.958 -0.500  1.667  9.333 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  14.5000     1.1322  12.807  < 2e-16 ***\r\nsprayB        0.8333     1.6011   0.520    0.604    \r\nsprayC      -12.4167     1.6011  -7.755 7.27e-11 ***\r\nsprayD       -9.5833     1.6011  -5.985 9.82e-08 ***\r\nsprayE      -11.0000     1.6011  -6.870 2.75e-09 ***\r\nsprayF        2.1667     1.6011   1.353    0.181    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.922 on 66 degrees of freedom\r\nMultiple R-squared:  0.7244,    Adjusted R-squared:  0.7036 \r\nF-statistic:  34.7 on 5 and 66 DF,  p-value: < 2.2e-16\r\nanova(m1)\r\nAnalysis of Variance Table\r\n\r\nResponse: count\r\n          Df Sum Sq Mean Sq F value    Pr(>F)    \r\nspray      5 2668.8  533.77  34.702 < 2.2e-16 ***\r\nResiduals 66 1015.2   15.38                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n#m.aov <- aov(count~spray, df)\r\n#summary(m.aov)\r\n\r\nThe categorical factor is significant, i.e. utility test fails.\r\nCreate contrasts to identify differences between combinations of\r\nsprays.\r\n\r\nA <- factor(df$spray)\r\ncontrasts(A)\r\n  B C D E F\r\nA 0 0 0 0 0\r\nB 1 0 0 0 0\r\nC 0 1 0 0 0\r\nD 0 0 1 0 0\r\nE 0 0 0 1 0\r\nF 0 0 0 0 1\r\ncontrasts(A)<-cbind(\"GA vs GB\"=c(1,-1,0,0,0,0),\r\n                    \"GAB vs GF\"=c(1,1,0,0,0,-2),\r\n                    \"GC vs GD\"=c(0,0,1,-1,0,0),\r\n                    \"GC vs GE\"=c(0,0,1,0,-1,0),\r\n                    \"GABF vs GCDE\"=c(1,1,-1,-1,-1,1))\r\nA\r\n [1] A A A A A A A A A A A A B B B B B B B B B B B B C C C C C C C C C\r\n[34] C C C D D D D D D D D D D D D E E E E E E E E E E E E F F F F F F\r\n[67] F F F F F F\r\nattr(,\"contrasts\")\r\n  GA vs GB GAB vs GF GC vs GD GC vs GE GABF vs GCDE\r\nA        1         1        0        0            1\r\nB       -1         1        0        0            1\r\nC        0         0        1        1           -1\r\nD        0         0       -1        0           -1\r\nE        0         0        0       -1           -1\r\nF        0        -2        0        0            1\r\nLevels: A B C D E F\r\naov(df$count ~ A)\r\nCall:\r\n   aov(formula = df$count ~ A)\r\n\r\nTerms:\r\n                       A Residuals\r\nSum of Squares  2668.833  1015.167\r\nDeg. of Freedom        5        66\r\n\r\nResidual standard error: 3.921902\r\nEstimated effects may be unbalanced\r\nsummary.lm(aov(df$count ~ A))\r\n\r\nCall:\r\naov(formula = df$count ~ A)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-8.333 -1.958 -0.500  1.667  9.333 \r\n\r\nCoefficients:\r\n                Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)    9.500e+00  4.622e-01  20.554   <2e-16 ***\r\nAGA vs GB     -4.167e-01  8.006e-01  -0.520    0.604    \r\nAGAB vs GF    -5.833e-01  4.622e-01  -1.262    0.211    \r\nAGC vs GD     -1.417e+00  9.244e-01  -1.533    0.130    \r\nAGC vs GE      8.374e-16  9.244e-01   0.000    1.000    \r\nAGABF vs GCDE  6.000e+00  4.622e-01  12.981   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.922 on 66 degrees of freedom\r\nMultiple R-squared:  0.7244,    Adjusted R-squared:  0.7036 \r\nF-statistic:  34.7 on 5 and 66 DF,  p-value: < 2.2e-16\r\n\r\nSummary shows that sprays A and B, C and D, C and E are\r\nindistinguishable as well as combined observations of sprays A and B\r\nvs. F, but combined observations for sprays A, B, F are significantly\r\ndifferent from C, D and E.\r\nHowever, separate t-test of C vs. D shows significant difference and\r\nequivalence of C and E is almost rejected by t-test.\r\n\r\nt.test(df$count[df$spray==\"C\"], df$count[df$spray==\"D\"])\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  df$count[df$spray == \"C\"] and df$count[df$spray == \"D\"]\r\nt = -3.0782, df = 20.872, p-value = 0.00573\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -4.7482230 -0.9184437\r\nsample estimates:\r\nmean of x mean of y \r\n 2.083333  4.916667 \r\nt.test(df$count[df$spray==\"C\"], df$count[df$spray==\"E\"])\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  df$count[df$spray == \"C\"] and df$count[df$spray == \"E\"]\r\nt = -1.868, df = 21.631, p-value = 0.07536\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -2.9909888  0.1576555\r\nsample estimates:\r\nmean of x mean of y \r\n 2.083333  3.500000 \r\n\r\nThese contradicting results are caused by the fact that for\r\ncomparison of C with D and E the assumption of homoscedasticity leads to\r\noverestimation of noise in general framework of ANOVA, but pairwise\r\nt-test is only accounting for variances of the compared pairs.\r\nBayes’\r\nRun MCMC.\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    int<lower=2> NxLvl;\r\n    int<lower=1, upper=NxLvl> x[Ntotal];\r\n    real<lower=0> aGammaShRa[2];\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n}\r\nparameters {\r\n    real<lower=0> nu;\r\n    real a0;\r\n    real<lower=0> aSigma;\r\n    vector[NxLvl] a;\r\n    real<lower=0> ySigma[NxLvl];\r\n    real<lower=0> ySigmaMode;\r\n    real<lower=0> ySigmaSD;\r\n}\r\ntransformed parameters{\r\n    real<lower=0> ySigmaSh;\r\n    real<lower=0> ySigmaRa;\r\n    ySigmaRa = ( ( ySigmaMode + sqrt( ySigmaMode^2 + 4*ySigmaSD^2 ) ) / ( 2*ySigmaSD^2 ) );\r\n    ySigmaSh = 1 + ySigmaMode * ySigmaRa;\r\n}\r\nmodel {\r\n    nu ~ exponential(1/30.0);\r\n    a0 ~ normal(meanY, 10*sdY);\r\n    aSigma ~ gamma(aGammaShRa[1], aGammaShRa[2]);\r\n    a ~ normal(0, aSigma);\r\n    ySigma ~ gamma(ySigmaSh, ySigmaRa);\r\n    ySigmaMode ~ gamma(aGammaShRa[1], aGammaShRa[2]);\r\n    ySigmaSD ~ gamma(aGammaShRa[1], aGammaShRa[2]);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ student_t(nu, a0 + a[x[i]], ySigma[x[i]]);\r\n    }\r\n}\r\ngenerated quantities {\r\n    // Convert a0,a[] to sum-to-zero b0,b[] :\r\n        real b0;\r\n    vector[NxLvl] b;\r\n    b0 = a0 + mean(a);\r\n    b = a - mean(a);\r\n}\r\n\"\r\n\r\n\r\nmodel3 <- stan_model(model_code = modelString)\r\n\r\n\r\n# saveRDS(model3,file = \"data/stanDSO3.Rds\")\r\nmodel3 <- readRDS(file = \"data/stanDSO3.Rds\")\r\n\r\nRun chains.\r\n\r\nfit.sprays <- sampling(model3, \r\n                data=dataListSprays, \r\n                pars=c('b0', 'b', 'aSigma', 'ySigma', 'nu', 'ySigmaMode', 'ySigmaSD'),\r\n                iter=5000, chains = 2, cores = 4)\r\n\r\n\r\nlaunch_shinystan(fit)\r\n\r\nAnalyze estimates.\r\n\r\nplot(fit.sprays,pars=c(\"b0\",\"aSigma\"))\r\n\r\nplot(fit.sprays,pars=c(\"b\"))\r\n\r\nplot(fit.sprays,pars=c(\"ySigma\"))\r\n\r\n\r\nExtract chains.\r\n\r\nchains_sprays <- rstan::extract(fit.sprays)\r\nhead(chains_sprays$b)\r\n          \r\niterations     [,1]     [,2]      [,3]      [,4]      [,5]     [,6]\r\n      [1,] 5.321336 5.903195 -7.231029 -5.126114 -5.243187 6.375798\r\n      [2,] 6.009771 4.809611 -7.284751 -4.387986 -6.658077 7.511432\r\n      [3,] 3.685636 5.496733 -6.662345 -3.278268 -5.640332 6.398575\r\n      [4,] 3.609592 4.813631 -6.971538 -2.848964 -6.461671 7.858950\r\n      [5,] 7.089465 4.230392 -7.824936 -5.013679 -6.613809 8.132567\r\n      [6,] 4.836717 6.068238 -6.645967 -4.104199 -5.881397 5.726608\r\n\r\nLook at the same contrasts.\r\nA vs. B\r\n\r\ncontrast1_2 <- chains_sprays$b[,1] - chains_sprays$b[,2]\r\nplot(contrast1_2)\r\n\r\nhist(contrast1_2)\r\n\r\n(hdiContrast1_2<-hdi(contrast1_2))\r\n    lower     upper \r\n-4.566850  3.072822 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast1_2<-sd(contrast1_2))\r\n[1] 1.92444\r\nplot(rank(chains_sprays$b[,1]),rank(chains_sprays$b[,5]))\r\n\r\n\r\nThe contrast is not different from zero.\r\nA, B vs. F\r\n\r\nComb1<-chains_sprays$b[,1] + chains_sprays$b[,2]\r\nComb2<-2*chains_sprays$b[,6]\r\ncontrast12_6 <-Comb1  - Comb2\r\nplot(contrast12_6)\r\n\r\nhist(contrast12_6)\r\n\r\n(hdiContrast12_6<-hdi(contrast12_6))\r\n    lower     upper \r\n-11.15426   5.21297 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast12_6<-sd(contrast12_6))\r\n[1] 4.148463\r\nplot(rank(Comb1),rank(Comb2))\r\n\r\n\r\nThe contrast is not different from zero.\r\nC vs. D\r\n\r\ncontrast3_4 <- chains_sprays$b[,3] - chains_sprays$b[,4]\r\nplot(contrast3_4)\r\n\r\nhist(contrast3_4)\r\n\r\n(hdiContrast3_4<-hdi(contrast3_4))\r\n     lower      upper \r\n-4.8309758 -0.7441633 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast3_4<-sd(contrast3_4))\r\n[1] 1.01399\r\nplot(rank(chains_sprays$b[,3]),rank(chains_sprays$b[,4]))\r\n\r\n\r\nThis contrast is different from zero. ANOVA could not detect\r\nthat.\r\nC vs. E\r\n\r\ncontrast3_5 <- chains_sprays$b[,3] - chains_sprays$b[,5]\r\nplot(contrast3_5)\r\n\r\nhist(contrast3_5)\r\n\r\n(hdiContrast3_5<-hdi(contrast3_5))\r\n    lower     upper \r\n-3.206783  0.386665 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast3_5<-sd(contrast3_5))\r\n[1] 0.9066876\r\nplot(rank(chains_sprays$b[,3]),rank(chains_sprays$b[,5]))\r\n\r\n\r\nThe contrast is not different from zero.\r\nCombined observations A, B and F vs. C, D, E\r\n\r\nComb1<-chains_sprays$b[,1] + chains_sprays$b[,2] + chains_sprays$b[,6]\r\nComb2<-chains_sprays$b[,3] + chains_sprays$b[,4] + chains_sprays$b[,5]\r\ncontrast126_345 <-Comb1  - Comb2\r\nplot(contrast126_345)\r\n\r\nhist(contrast126_345)\r\n\r\n(hdiContrast126_345<-hdi(contrast126_345))\r\n   lower    upper \r\n29.35368 40.63068 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n(sd.contrast126_345<-sd(contrast126_345))\r\n[1] 2.911281\r\nplot(rank(Comb1),rank(Comb2))\r\n\r\n\r\nThe contrast is significantly different from zero.\r\nFurther Readings\r\nAdapted from UC’s coursework\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial\r\nwith r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic\r\nPress is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-28-Bayesian methods - Series 6 of 10/distill-preview.png",
    "last_modified": "2023-02-25T00:34:30-06:00",
    "input_file": "Series-6---ANOVA-and-ANCOVA.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-14-Bayesian methods - Series 5 of 10/",
    "title": "Series 5 of 10 -- Introduction to Selection of Variables using Bayesian Approach",
    "description": "Gently Introduce to Variable Selection using Bayesian Approach",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-02-14",
    "categories": [
      "Biostatistics",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nRead data\r\nModeling in\r\nJAGS\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake\\)\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend\\)\r\nmodel \\(SATT = \\beta_0 + \\beta_2 \\\r\nPrcntTake\\)\r\nmodel\r\n\\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake + \\beta_3 \\ StuTeaRat + \\beta_4 \\ Salary\\)\r\n\r\nInclusion Probabilities\r\nConclusions\r\nReferences\r\n\r\nIntroduction\r\nThis example is based on section 18.4 in Kruschke, 2015.\r\nA Bayesian approach variable selection in regression analysis can be\r\ndone by adding a binary parameter for each slope:\\[y_i = \\beta_0 + \\sum_j \\delta_j \\ \\beta_j\r\nx_{i,j},\\]\r\nwith \\[\\delta_j = 0, \\ 1.\\]\r\nEvery combination of values \\(\\delta_1, \\\r\n..., \\delta_k\\) gives a submodel. Simple priors for\r\ndelta-indicators are independent Bernoulli distributions: \\(\\delta \\sim \\text{dbern}(0.5)\\). Then\r\nposterior probabilities \\(P(\\delta_j = 1\\mid\r\nD)\\) show importance of the corresponding predictors \\(x_i\\).\r\nThe diagram of such model is an easy generalization of regression\r\nmodel.\r\nSince parameters \\(\\delta_j\\) are\r\ndiscrete, we need to use JAGS: library(runjags) instead of\r\nStan\r\n\r\nlibrary(runjags)\r\n\r\nRead data\r\nThe SAT\r\ndata includes:\r\nState: 50 states,\r\nSATV: SAT verbal; SATM: SAT math;\r\nSATT: SAT total score\r\nSpend: average spending per pupil\r\nPrcntTake: percentage of students who took the\r\ntest\r\nStuTeaRat: the average student teacher ratio in each\r\nstate and\r\nSalary: the average salary of the teachers\r\nThese latter four variables are also plausible predictors of SAT\r\nscore.\r\n\r\ndta <- read.csv(\"data/Guber1999data.csv\")\r\nnames(dta)\r\n[1] \"State\"     \"Spend\"     \"StuTeaRat\" \"Salary\"    \"PrcntTake\"\r\n[6] \"SATV\"      \"SATM\"      \"SATT\"     \r\nDT::datatable(dta)\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"],[\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"],[4.405,8.963,4.778,4.459,4.992,5.443,8.817,7.03,5.718,5.193,6.078,4.21,6.136,5.826,5.483,5.817,5.217,4.761,6.428,7.245,7.287,6.994,6,4.08,5.383,5.692,5.935,5.16,5.859,9.774,4.586,9.623,5.077,4.775,6.162,4.845,6.436,7.109,7.469,4.797,4.775,4.388,5.222,3.656,6.75,5.327,5.906,6.107,6.93,6.16],[17.2,17.6,19.3,17.1,24,18.4,14.4,16.6,19.1,16.3,17.9,19.1,17.3,17.5,15.8,15.1,17,16.8,13.8,17,14.8,20.1,17.5,17.5,15.5,16.3,14.5,18.7,15.6,13.8,17.2,15.2,16.2,15.3,16.6,15.5,19.9,17.1,14.7,16.4,14.4,18.6,15.7,24.3,13.8,14.6,20.2,14.8,15.9,14.9],[31.144,47.951,32.175,28.934,41.078,34.571,50.045,39.076,32.588,32.291,38.518,29.783,39.431,36.785,31.511,34.652,32.257,26.461,31.972,40.661,40.795,41.895,35.948,26.818,31.189,28.785,30.922,34.836,34.72,46.087,28.493,47.612,30.793,26.327,36.802,28.172,38.555,44.51,40.729,30.279,25.994,32.477,31.223,29.082,35.406,33.987,36.151,31.944,37.746,31.285],[8,47,27,6,45,29,81,68,48,65,57,15,13,58,5,9,11,9,68,64,80,11,9,4,9,21,9,30,70,70,11,74,60,5,23,9,51,70,70,58,5,12,47,4,68,65,48,17,9,10],[491,445,448,482,417,462,431,429,420,406,407,468,488,415,516,503,477,486,427,430,430,484,506,496,495,473,494,434,444,420,485,419,411,515,460,491,448,419,425,401,505,497,419,513,429,428,443,448,501,476],[538,489,496,523,485,518,477,468,469,448,482,511,560,467,583,557,522,535,469,479,477,549,579,540,550,536,556,483,491,478,530,473,454,592,515,536,499,461,463,443,563,543,474,563,472,468,494,484,572,525],[1029,934,944,1005,902,980,908,897,889,854,889,979,1048,882,1099,1060,999,1021,896,909,907,1033,1085,1036,1045,1009,1050,917,935,898,1015,892,865,1107,975,1027,947,880,888,844,1068,1040,893,1076,901,896,937,932,1073,1001]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>State<\\/th>\\n      <th>Spend<\\/th>\\n      <th>StuTeaRat<\\/th>\\n      <th>Salary<\\/th>\\n      <th>PrcntTake<\\/th>\\n      <th>SATV<\\/th>\\n      <th>SATM<\\/th>\\n      <th>SATT<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7,8]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nWe now let look at the correlation matrix among 4 predictors.\r\n\r\ndta %>%\r\n  select(Spend, PrcntTake, StuTeaRat, Salary) %>%\r\n  cor(.) %>%\r\n  round(., 3)\r\n           Spend PrcntTake StuTeaRat Salary\r\nSpend      1.000     0.593    -0.371  0.870\r\nPrcntTake  0.593     1.000    -0.213  0.617\r\nStuTeaRat -0.371    -0.213     1.000 -0.001\r\nSalary     0.870     0.617    -0.001  1.000\r\n\r\nThat Salary is strongly correlated with Spend, and therefore, a model\r\nthat includes both Salary and Spend will show a strong trade-off between\r\nthose predictors and consequently will show inflated uncertainty in the\r\nregression coefficients for either one. Should only one or the other be\r\nincluded, or both, or neither?\r\nOne more point, response variable is average high school SAT score\r\nper state. One of the predictors is amount of money spent by the state\r\non schools. Plotting amount of money spent against SAT scores shows\r\nnegative slope.\r\n\r\nplot(dta$Spend,dta$SATT)\r\n\r\n\r\nThus, should we cut funding for school? Because even with the\r\nfunding, SAT would be reduced disproportional with the budget\r\nspent!!\r\nModeling in JAGS\r\n\r\ny <- dta[,'SATT']\r\nx <- as.matrix(dta[ , c(\"Spend\",\"PrcntTake\",\"StuTeaRat\",\"Salary\")])\r\n\r\n#prepare data in JAGS\r\ndataList <- list(Ntotal = length(y),\r\n                 y = y,\r\n                 x = x,\r\n                 Nx = ncol(x))\r\n\r\nDescription of the model:\r\n\\[y_i = \\beta_0 + \\delta_j \\ \\beta_j\r\nx_{i,j} + \\epsilon_i \\\\\r\ny_i \\sim t(\\mu_i, \\sigma, \\nu) \\\\\r\n\\\\\r\n\\text{where mean, scale and degree of freedom, respectively, would be}\r\n\\\\\r\n\\mu_i = \\beta_0 + \\delta_j \\ \\beta_j x_{i,j} \\\\\r\n\\beta_0 \\sim N(0, \\frac{1}{1/2^2}) \\ ; \\ \\beta_j \\sim t(0,\r\n1/\\sigma_{\\beta}^2, \\nu = 1) \\\\\r\n\\sigma \\sim Unif(L, H) \\\\\r\n\\nu = \\gamma^{\\prime} + 1, \\ \\text{where} \\ \\gamma^{\\prime} \\sim\r\nexp(\\lambda) \\\\\r\n\\delta_j \\sim Bernoulli(0.5) \\ ; \\ \\sigma_{\\beta} \\sim\r\n\\text{Gamma(shape, rate)}\\]\r\n\r\nmodelString = \"\r\n    # Standardize the data:\r\n    data {\r\n        ym <- mean(y)\r\n        ysd <- sd(y)\r\n        for (i in 1:Ntotal) {\r\n            zy[i] <- (y[i] - ym) / ysd\r\n        }\r\n        for (j in 1:Nx) {\r\n            xm[j]  <- mean(x[,j])\r\n            xsd[j] <-   sd(x[,j])\r\n            for (i in 1:Ntotal) {\r\n                zx[i,j] <- (x[i,j] - xm[j]) / xsd[j]\r\n            }\r\n        }\r\n    }\r\n    # Specify the model for standardized data:\r\n    model {\r\n        for (i in 1:Ntotal) {\r\n            zy[i] ~ dt(zbeta0 + sum(delta[1:Nx] * zbeta[1:Nx] * zx[i,1:Nx] ) ,  1/zsigma^2 , nu)\r\n        }\r\n        # Priors vague on standardized scale:\r\n        zbeta0 ~ dnorm(0, 1/2^2)\r\n        for (j in 1:Nx) {\r\n            zbeta[j] ~ dt(0 , 1/sigmaBeta^2 , 1) \r\n            delta[j] ~ dbern( 0.5 )\r\n        }\r\n        zsigma ~ dunif(1.0E-5 , 1.0E+1)\r\n        \r\n        # sigmaBeta <- 2.0 ## uncomment one of the following specifications for sigmaBeta\r\n        # sigmaBeta ~ dunif( 1.0E-5 , 1.0E+2 )\r\n        sigmaBeta ~ dgamma(1.1051,0.1051) # mode 1.0, sd 10.0\r\n        # sigmaBeta <- 1/sqrt(tauBeta) ; tauBeta ~ dgamma(0.001,0.001) \r\n        nu ~ dexp(1/30.0)\r\n        \r\n        # Transform to original scale:\r\n        beta[1:Nx] <- (delta[1:Nx] * zbeta[1:Nx] / xsd[1:Nx])*ysd\r\n        beta0 <- zbeta0*ysd  + ym - sum(delta[1:Nx] * zbeta[1:Nx] * xm[1:Nx] / xsd[1:Nx])*ysd\r\n        sigma <- zsigma*ysd\r\n    }\r\n\"\r\n\r\n\r\nparameters <- c(\"beta0\",  \"beta\",  \"sigma\", \"delta\", \"sigmaBeta\", \"zbeta0\", \"zbeta\", \"zsigma\", \"nu\" )\r\nrunjagsMethod <- \"parallel\"  # change to \"rjags\" in case of working on 1-core CPU \r\n# run JAGS\r\n\r\n\r\nrunJagsOut <- run.jags(method = runjagsMethod,\r\n                       model = modelString, \r\n                       monitor = parameters, \r\n                       data = dataList,\r\n                       n.chains = 3, #nChains <- 3\r\n                       adapt = 500, #adaptSteps <- 500\r\n                       burnin = 1000, #burnInSteps <- 1000\r\n                       sample = ceiling(15000/3), #numSavedSteps <- 15000;  nChains <- 3\r\n                       thin = 25, #thinSteps <- 25\r\n                       summarise = FALSE,\r\n                       plots = FALSE)\r\n\r\n\r\n# generate summary info for all parameters\r\nsummary(runJagsOut)\r\nCalculating summary statistics...\r\nNote: The monitored variable 'delta[2]' appears to be\r\nnon-stochastic; it will not be included in the convergence\r\ndiagnostic\r\nCalculating the Gelman-Rubin statistic for 18 variables....\r\nError : The \"modeest\" package is required to calculate the mode of continuous variables\r\n               Lower95        Median      Upper95          Mean\r\nbeta0     944.66400000  1.011275e+03  1.11231e+03  1.018969e+03\r\nbeta[1]    -0.00128211  7.803640e+00  1.91766e+01  7.314723e+00\r\nbeta[2]    -3.26471000 -2.761835e+00 -2.22909e+00 -2.753613e+00\r\nbeta[3]    -5.57611000  0.000000e+00  4.47374e-03 -6.349370e-01\r\nbeta[4]    -0.26275600  0.000000e+00  3.55510e+00  3.305153e-01\r\nsigma      24.62630000  3.214530e+01  4.10201e+01  3.238811e+01\r\ndelta[1]    0.00000000  1.000000e+00  1.00000e+00  6.138000e-01\r\ndelta[2]    1.00000000  1.000000e+00  1.00000e+00  1.000000e+00\r\ndelta[3]    0.00000000  0.000000e+00  1.00000e+00  1.732667e-01\r\ndelta[4]    0.00000000  0.000000e+00  1.00000e+00  2.208667e-01\r\nsigmaBeta   0.02406400  1.127925e+00  7.94953e+00  2.198454e+00\r\nzbeta0     -0.12437500 -1.292225e-04  1.29719e-01  1.964852e-04\r\nzbeta[1]   -7.77130000  2.129285e-01  1.10354e+01  5.264458e-01\r\nzbeta[2]   -1.16775000 -9.878755e-01 -7.97319e-01 -9.849344e-01\r\nzbeta[3]  -24.01480000 -8.063585e-02  2.36377e+01 -2.338195e-01\r\nzbeta[4]  -17.73010000  1.036410e-01  1.87959e+01  3.378339e-01\r\nzsigma      0.32913900  4.296320e-01  5.48246e-01  4.328772e-01\r\nnu          1.91471000  2.685290e+01  9.61985e+01  3.558561e+01\r\n                   SD Mode        MCerr MC%ofSD SSeff       AC.250\r\nbeta0     43.89744488   NA 0.4821033688     1.1  8291  0.019090161\r\nbeta[1]    7.04248924   NA 0.1017558219     1.4  4790  0.064596992\r\nbeta[2]    0.27026073   NA 0.0033981576     1.3  6325  0.053310641\r\nbeta[3]    1.74666227   NA 0.0151369398     0.9 13315 -0.009957991\r\nbeta[4]    0.99368906   NA 0.0093047567     0.9 11405  0.010317963\r\nsigma      4.16357566   NA 0.0387315148     0.9 11556  0.016315434\r\ndelta[1]   0.48689359    1 0.0078742141     1.6  3823  0.085242124\r\ndelta[2]   0.00000000    1           NA      NA    NA           NA\r\ndelta[3]   0.37849026    0 0.0034208127     0.9 12242 -0.004375483\r\ndelta[4]   0.41484462    0 0.0038539951     0.9 11586  0.010031033\r\nsigmaBeta  3.24829579   NA 0.0687169662     2.1  2235  0.115133738\r\nzbeta0     0.06462281   NA 0.0005367156     0.8 14497 -0.007738634\r\nzbeta[1]   5.53722794   NA 0.2424652404     4.4   522  0.502781355\r\nzbeta[2]   0.09666902   NA 0.0012154802     1.3  6325  0.053310325\r\nzbeta[3]  18.13696053   NA 0.5572712553     3.1  1059  0.319423413\r\nzbeta[4]  14.61246772   NA 0.3817438159     2.6  1465  0.293091432\r\nzsigma     0.05564748   NA 0.0005176588     0.9 11556  0.016315483\r\nnu        30.03665106   NA 0.2442544957     0.8 15122 -0.003220931\r\n               psrf\r\nbeta0     1.0000092\r\nbeta[1]   1.0001584\r\nbeta[2]   1.0002489\r\nbeta[3]   1.0006953\r\nbeta[4]   1.0006186\r\nsigma     1.0000140\r\ndelta[1]  1.0006266\r\ndelta[2]         NA\r\ndelta[3]  1.0008028\r\ndelta[4]  1.0006483\r\nsigmaBeta 1.0016839\r\nzbeta0    1.0001875\r\nzbeta[1]  1.0614080\r\nzbeta[2]  1.0002489\r\nzbeta[3]  1.0456877\r\nzbeta[4]  1.0327248\r\nzsigma    1.0000147\r\nnu        0.9999383\r\n\r\n\r\n# plot all params\r\nplot(runJagsOut,\r\n     plot.type = c(\"trace\", \"ecdf\", \"histogram\", \"autocorr\"))\r\nGenerating summary statistics and plots (these will NOT be\r\nsaved for reuse)...\r\nCalculating summary statistics...\r\nNote: The monitored variable 'delta[2]' appears to be\r\nnon-stochastic; it will not be included in the convergence\r\ndiagnostic\r\nCalculating the Gelman-Rubin statistic for 18 variables....\r\nError : The \"modeest\" package is required to calculate the mode of continuous variables\r\n\r\n\r\nFind posterior probability of different combinations of predictors by\r\ncalculating frequencies with which they appeared in MCMC.\r\n\r\ntrajectoriesDelta <- as.matrix(runJagsOut$mcmc[,7:10])\r\nhead(trajectoriesDelta)\r\n     delta[1] delta[2] delta[3] delta[4]\r\n[1,]        0        1        0        0\r\n[2,]        1        1        0        0\r\n[3,]        1        1        0        0\r\n[4,]        1        1        0        0\r\n[5,]        1        1        1        0\r\n[6,]        0        1        0        0\r\nNchain <- nrow(trajectoriesDelta)\r\n\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake\\)\r\n\r\n(config1 <- sum(apply(trajectoriesDelta, 1, function(z) prod(z==c(1,1,0,0))))/Nchain)\r\n[1] 0.4738667\r\n\r\nmodel \\(SATT = \\beta_0 + \\beta_1 \\ Spend\\)\r\n\r\n(config2 <- sum(apply(trajectoriesDelta, 1,function(z) prod(z==c(1,0,0,0))))/Nchain)\r\n[1] 0\r\n\r\nmodel \\(SATT = \\beta_0 + \\beta_2 \\ PrcntTake\\)\r\n\r\n(config3 <- sum(apply(trajectoriesDelta, 1, function(z) prod(z==c(0,1,0,0))))/Nchain)\r\n[1] 0.2154667\r\n\r\nmodel\r\n\\(SATT = \\beta_0 + \\beta_1 \\ Spend + \\beta_2 \\\r\nPrcntTake + \\beta_3 \\ StuTeaRat + \\beta_4 \\ Salary\\)\r\n\r\n(config4 <- sum(apply(trajectoriesDelta, 1, function(z) prod(z==c(1,1,1,1))))/Nchain)\r\n[1] 0.02186667\r\n\r\nInclusion Probabilities\r\n\r\n(inclSpend<-sum(trajectoriesDelta[,1]==1)/Nchain) #Spend\r\n[1] 0.6138\r\n(inclPrcntTake<-sum(trajectoriesDelta[,2]==1)/Nchain) #PrcntTake\r\n[1] 1\r\n(inclStueTeaRat<-sum(trajectoriesDelta[,3]==1)/Nchain) #StueTeaRat\r\n[1] 0.1732667\r\n(inclSalary<-sum(trajectoriesDelta[,4]==1)/Nchain) #Salary\r\n[1] 0.2208667\r\n\r\nConclusions\r\nOut of 4 analyzed configurations of the model the most observed is\r\nconfiguration 1 (0.4738667).\r\nFor configuration 4 with all predictors observation rate is just\r\n0.0218667.\r\nOut of 4 variables two are selected with inclusion probabilities\r\nabove 0.5, those are Spend (0.6138) and\r\nPrcntTake (1).\r\nThe second variable PrcntTake is included with\r\nnon-stochastic probability of 1, standard deviation of \\(\\delta_2\\) is zero.\r\nWarning. Posterior probabilities of inclusion may be\r\nvery sensitive to prior information.\r\nReferences\r\nBayesian Methods, UC’s lecture\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-02-01T12:31:51-06:00",
    "input_file": "Series-5-of-10---Introduction-to-Variable-Selection-in-Bayesian-Approach.knit.md"
  },
  {
    "path": "posts/2022-01-31-Bayesian methods - Series 4 of 10/",
    "title": "Series 4 of 10 -- Fitting Linear Models - Multiple Regression",
    "description": "Review a simple linear regression using Bayesian Methods   \nHow to fit a linear multiple regression (Stan) using Bayesian Methods   \nCases of significant/insignificant, correlated and collinear predictors  \nShrinkage  \nBayesian model with or without shrinkage, ridge regression and lasso regression: An example of SAT scores",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-01-31",
    "categories": [
      "Biostatistics",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nReview a simple\r\nlinear regression in Bayes\r\nMultiple\r\nRegression\r\nExample 1: Two significant\r\npredictors\r\nReminder of\r\nStan\r\n\r\nExample 2: Insignificant\r\npredictor\r\nExample 3: Correlated\r\npredictors\r\nStrong\r\ncorrelation\r\nCollinearity\r\n\r\n\r\nShrinkage of regression\r\ncoefficients\r\nTwo significant predictors\r\nAnalysis and comparison\r\n\r\nInsignificant predictor\r\nAnalysis and comparison\r\n\r\nCorrelated predictors\r\nAnalysis and comparison\r\n\r\n\r\nIs school financing\r\nnecessary?\r\nNo-shrinkage\r\nShrinkage\r\nLinear model\r\nRidge and\r\nlasso\r\n\r\nFurther\r\nreading\r\n\r\nReview a simple\r\nlinear regression in Bayes\r\nWe can see a\r\npost how to fit a simple linear regression in both Frequentist\r\napproach and Bayesian methods. Now we move on to the linear multiple\r\nregression.\r\nMultiple Regression\r\nExample 1: Two significant\r\npredictors\r\nGenerate data for multiple linear regression with 2 independent\r\nsignificant predictors.\r\n\r\n# generate data\r\nset.seed(03182021)\r\nNtotal <- 500\r\nx <- cbind(rnorm(Ntotal, mean = 20, sd = 4), \r\n           rnorm(Ntotal, mean=10, sd = 6))\r\nNx <- ncol(x)\r\ny <- 4 + 1.1*x[,1] + 3*x[,2] + rnorm(Ntotal, mean = 0, sd = 1)\r\n\r\nCreate a data list.\r\n\r\ndataListRegression <- list(Ntotal = Ntotal, y = y, x = as.matrix(x), Nx = Nx)\r\n\r\nHere’s a model in the Frequentist method:\r\n\r\nsummary(lm(y~x))\r\n\r\nCall:\r\nlm(formula = y ~ x)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.83480 -0.63696  0.00131  0.69896  2.48803 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 4.565419   0.230456   19.81   <2e-16 ***\r\nx1          1.074206   0.010945   98.14   <2e-16 ***\r\nx2          2.995385   0.007954  376.57   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9759 on 497 degrees of freedom\r\nMultiple R-squared:  0.9969,    Adjusted R-squared:  0.9968 \r\nF-statistic: 7.895e+04 on 2 and 497 DF,  p-value: < 2.2e-16\r\n\r\nThe diagram of the model shows hierarchical structure with normal\r\npriors for intercept \\(\\beta_0\\) and\r\nslopes \\(\\beta_i\\), \\(i=1,2\\).\r\n\r\nDescription of the model:\r\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} +\r\n\\beta_2 x_{i,2} + \\epsilon_i \\\\\r\ny_i \\sim t(\\mu_i, \\sigma, \\nu) \\\\\r\n\\\\\r\n\\text{where mean, scale and degree of freedom, respectively, would be}\r\n\\\\\r\n\\mu_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} \\\\\r\n\\beta_0 \\sim N(M_0, S_0) \\ ; \\ \\beta_j \\sim N(M_1, S_1) \\\\\r\n\\sigma \\sim Unif(L, H) \\\\\r\n\\nu = \\gamma^{\\prime} + 1, \\ \\text{where} \\ \\gamma^{\\prime} \\sim\r\nexp(\\lambda)\\]\r\nNow, we come back to the problem. Here’s we write a model string:\r\nReminder of Stan: (see below, at the end of Example 1)\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int<lower=1> Nx;\r\n    vector[Ntotal] y;\r\n    matrix[Ntotal, Nx] x;\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    vector[Ntotal] zy; // normalized\r\n    vector[Nx] meanX;\r\n    vector[Nx] sdX;\r\n    matrix[Ntotal, Nx] zx; // normalized\r\n    \r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    zy = (y - meanY) / sdY;\r\n    for (j in 1:Nx) {\r\n        meanX[j] = mean(x[,j]);\r\n        sdX[j] = sd(x[,j]);\r\n        for ( i in 1:Ntotal ) {\r\n            zx[i,j] = (x[i,j] - meanX[j]) / sdX[j];\r\n        }\r\n    }\r\n}\r\nparameters {\r\n    real zbeta0;\r\n    vector[Nx] zbeta;\r\n    real<lower=0> nu;\r\n    real<lower=0> zsigma;\r\n}\r\ntransformed parameters{\r\n    vector[Ntotal] zy_hat;\r\n    zy_hat = zbeta0 + zx * zbeta;\r\n}\r\nmodel {\r\n    zbeta0 ~ normal(0, 2);\r\n    zbeta  ~ normal(0, 2);\r\n    nu ~ exponential(1/30.0);\r\n    zsigma ~ uniform(1.0E-5 , 1.0E+1);\r\n    zy ~ student_t(1+nu, zy_hat, zsigma);\r\n}\r\ngenerated quantities { \r\n    // Transform to original scale:\r\n    real beta0; \r\n    vector[Nx] beta;\r\n    real sigma;\r\n    // .* and ./ are element-wise product and divide\r\n    beta0 = zbeta0*sdY  + meanY - sdY * sum( zbeta .* meanX ./ sdX );\r\n    beta = sdY * ( zbeta ./ sdX );\r\n    sigma = zsigma * sdY;\r\n} \"\r\n\r\n\r\nRobustMultipleRegressionDso <- stan_model(model_code=modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(RobustMultipleRegressionDso, file=\"data/DSORobustMultRegr.Rds\")\r\nRobustMultipleRegressionDso <- readRDS(\"data/DSORobustMultRegr.Rds\")\r\n\r\nFit the model.\r\n\r\nfit1 <- sampling(RobustMultipleRegressionDso,\r\n                 data=dataListRegression,\r\n                 pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                 iter = 5000, chains = 2, cores = 2)\r\nstan_ac(fit1)\r\n\r\nstan_trace(fit1)\r\n\r\n\r\nLook at the results.\r\n\r\nsummary(fit1)$summary[,c(1,3,4,5,8,9)] #mean, sd, 2.5%, 97.5%, n_eff\r\n                mean           sd         2.5%         25%\r\nbeta0      4.5780310  0.231357219    4.1304172    4.422841\r\nbeta[1]    1.0738282  0.011006048    1.0521979    1.066428\r\nbeta[2]    2.9951527  0.008047017    2.9797451    2.989759\r\nnu        61.7987150 36.009712586   17.0095629   36.082778\r\nsigma      0.9611543  0.032896641    0.8978996    0.938909\r\nlp__    1014.2033841  1.585209650 1010.1168812 1013.426674\r\n              97.5%    n_eff\r\nbeta0      5.025614 7033.830\r\nbeta[1]    1.095054 6561.254\r\nbeta[2]    3.011046 6871.816\r\nnu       151.390448 4557.868\r\nsigma      1.027262 4328.823\r\nlp__    1016.293483 2600.401\r\npairs(fit1,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit1,pars=\"nu\")\r\n\r\nplot(fit1,pars=\"sigma\")\r\n\r\nplot(fit1,pars=\"beta0\")\r\n\r\nplot(fit1,pars=\"beta[1]\")\r\n\r\nplot(fit1,pars=\"beta[2]\")\r\n\r\n\r\nAnalyze fitted model using shinystan\r\n\r\nlaunch_shinystan(fit1)\r\n\r\nConclusions:\r\n\\(\\nu\\) (degree of freedom in\r\nt-distribution) is large enough to consider normal distribution:\r\n2.5% HDI level is 12.6257143, mean value is 49.9778865. Not surprising:\r\nwe simulated normal model \\(\\Rightarrow\\) normality parameter \\(\\nu\\)\r\nParameters \\(\\beta_0\\) and \\(\\beta_1\\) are significantly negatively\r\ncorrelated, as expected.\r\nParameters \\(\\beta_0\\) and \\(\\beta_2\\) are also negatively correlated,\r\nbut correlation is not so strong.\r\nAll parameter estimates are close to what we simulated.\r\nReminder of Stan\r\nA Stan program has three required “blocks”:\r\ndata block: where you declare the data types, their\r\ndimensions, any restrictions (i.e. upper = or lower = , which act as\r\nchecks for Stan), and their names. Any names you give to\r\nyour Stan program will also be the names used in other\r\nblocks.\r\nparameters block: This is where you indicate the\r\nparameters you want to model, their dimensions, restrictions, and name.\r\nFor a linear regression, we will want to model the intercept, any\r\nslopes, and the standard deviation of the errors around the regression\r\nline.\r\nmodel block: This is where you include any sampling\r\nstatements, including the “likelihood” (model) you are using. The model\r\nblock is where you indicate any prior distributions you want to include\r\nfor your parameters. If no prior is defined, Stan uses\r\ndefault priors with the specifications\r\nuniform(-infinity, +infinity). You can restrict priors\r\nusing upper or lower when declaring the parameters\r\n(i.e. <lower = 0> to make sure a parameter is\r\npositive). You can find more information about prior specification here.\r\nSampling is indicated by the ~ symbol, and Stan already\r\nincludes many common distributions as vectorized functions. You can\r\ncheck out the\r\nmanual for a comprehensive list and more information on the optional\r\nblocks you could include in your Stan model.\r\nThere are also four optional blocks:\r\nfunctions\r\ntransformed data: allows for preprocessing of the\r\ndata\r\ntransformed parameters: allows for parameter processing\r\nbefore the posterior is computed\r\nObjects declared in the “transformed parameters” block of a Stan\r\nprogram are:\r\nUnknown but are known given the values of the objects in the\r\nparameters block\r\nSaved in the output and hence should be of interest to the\r\nresearcher\r\nAre usually the arguments to the log-likelihood function that is\r\nevaluated in the model block, although in hierarchical\r\nmodels the line between the prior and the likelihood can be drawn in\r\nmultiple ways\r\n(if the third point is not the case, the object should usually be\r\ndeclared in the generated quantities block of a Stan\r\nprogram)\r\nThe purpose of declaring such things in the\r\ntransformed parameters block rather than the parameters\r\nblock is often to obtain more efficient sampling from the posterior\r\ndistribution. If there is a posterior PDF \\(f(\\theta \\mid \\text{data})\\), then for any\r\nobjective transformation from \\(\\alpha\\) to \\(\\theta\\), the posterior PDF of \\(\\alpha\\) is simply \\(f(\\)\\((\\)\\() \\mid\r\ndata)\\text{abs}|J|\\), where \\(|J|\\) is the determinant of the Jacobian\r\nmatrix of the transformation from \\(\\alpha\\) to \\(\\theta\\). Thus, you can make the same\r\ninferences about (functions of) \\(\\theta\\) either by drawing from the\r\nposterior whose PDF is \\(f(\\theta \\mid\r\n\\text{data})\\) where \\(\\theta\\)\r\nare the parameters or the posterior whose PDF is f(θ(α)|data)abs|J|\r\nwhere α are parameters and θ are transformed parameters. Since the\r\nposterior inferences about (functions of) θ are the same, you are free\r\nto choose a transformation that enhances the efficiency of the sampling\r\nby making α less correlated, unit scaled, more Gaussian, etc. than is\r\nθ.\r\n“generated quantities”\r\n\\(\\Rightarrow\\) Transformation to improve fit Comments are indicated by\r\n// in Stan. The\r\nwrite(\"model code\", \"file_name\") bit allows us to write the\r\nStan model in our R script and output the file to the working directory\r\n(or you can set a different file path)\r\nExample 2: Insignificant\r\npredictor\r\n\r\nRegression.Data <- as.matrix(read.csv(\"data/DtSim4RegANOVA.csv\", header=TRUE, sep=\",\"))\r\ntail(Regression.Data)\r\n          Output     Input1      Input2\r\n[495,] 2.4054442  0.9276934  0.07278244\r\n[496,] 1.8663026 -0.3678520  1.51715986\r\n[497,] 1.3590146  0.5369795  0.96209003\r\n[498,] 3.1836007  1.0171332 -0.56660564\r\n[499,] 2.3615061  1.1637966  0.07815352\r\n[500,] 0.8483407  1.1775607  1.59720356\r\n\r\nPrepare the data for Stan.\r\n\r\nNtotal <- nrow(Regression.Data)\r\nx <- Regression.Data[ ,2:3]\r\ntail(x)\r\n           Input1      Input2\r\n[495,]  0.9276934  0.07278244\r\n[496,] -0.3678520  1.51715986\r\n[497,]  0.5369795  0.96209003\r\n[498,]  1.0171332 -0.56660564\r\n[499,]  1.1637966  0.07815352\r\n[500,]  1.1775607  1.59720356\r\nNx <- ncol(x)\r\ny <- Regression.Data[ ,1]\r\ndataListInsig <- list(Ntotal=Ntotal, \r\n                      y=y, \r\n                      x=as.matrix(x), \r\n                      Nx=Nx)\r\n\r\nRun MCMC using the same DSO.\r\n\r\nfit2 <- sampling(RobustMultipleRegressionDso, data=dataListInsig,\r\n                 pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                 iter=5000, chains = 2, cores = 2)\r\n\r\n\r\nlaunch_shinystan(fit2)\r\n\r\nAnalyze the results.\r\n\r\nsummary(fit2)$summary[,c(1,3,4,8,9)] #mean, sd, 2.5%, 97.5%, n_eff\r\n                 mean          sd          2.5%         97.5%\r\nbeta0    1.217597e+00  0.05182680    1.11612951    1.31636194\r\nbeta[1]  7.999726e-01  0.02812159    0.74453626    0.85437808\r\nbeta[2]  9.416795e-03  0.02736786   -0.04397004    0.06323134\r\nnu       5.276307e+01 34.06776500   13.74259262  142.96095320\r\nsigma    5.979875e-01  0.02144590    0.55558445    0.64151495\r\nlp__    -1.817471e+02  1.53474481 -185.40442023 -179.67956203\r\n           n_eff\r\nbeta0   5838.281\r\nbeta[1] 6158.199\r\nbeta[2] 5787.610\r\nnu      4567.018\r\nsigma   4814.577\r\nlp__    2650.410\r\npairs(fit2,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit2,pars=\"nu\")\r\n\r\nplot(fit2,pars=\"sigma\")\r\n\r\nplot(fit2,pars=\"beta0\")\r\n\r\nplot(fit2,pars=\"beta[1]\")\r\n\r\nplot(fit2,pars=\"beta[2]\")\r\n\r\n\r\nWe see that parameter \\(\\beta_2\\) is\r\nnot significant.\r\nHowever, there is no strong correlation or redundancy between the\r\npredictors.\r\nCompare with the output of linear model\r\n\r\npairs(Regression.Data)\r\n\r\nsummary(lm(Output~., data=as.data.frame(Regression.Data)))\r\n\r\nCall:\r\nlm(formula = Output ~ ., data = as.data.frame(Regression.Data))\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.76631 -0.39358 -0.01411  0.40432  1.91861 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  1.21480    0.05179  23.458   <2e-16 ***\r\nInput1       0.80116    0.02819  28.423   <2e-16 ***\r\nInput2       0.00970    0.02787   0.348    0.728    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.6107 on 497 degrees of freedom\r\nMultiple R-squared:  0.6204,    Adjusted R-squared:  0.6188 \r\nF-statistic: 406.1 on 2 and 497 DF,  p-value: < 2.2e-16\r\n\r\nExample 3: Correlated\r\npredictors\r\nStrong correlation\r\n\r\nset.seed(03192021)\r\nNtotal <- 500\r\nx1 <- rnorm(Ntotal, mean = 20, sd = 4)\r\nx2 <- 1 - 1.5*x1 + rnorm(Ntotal, mean=0, sd = .1)\r\nx <- cbind(x1,x2)      \r\n\r\nplot(x)\r\n\r\nNx <- ncol(x)\r\ny <- 4 + .2*x[,1] + 3*x[,2]+rnorm(Ntotal, mean = 0, sd = 1)\r\nplot(x[,1],y)\r\n\r\nplot(x[,2],y)\r\n\r\nfitlm<-lm(y~x[,1]+x[,2])\r\nsummary(fitlm)\r\n\r\nCall:\r\nlm(formula = y ~ x[, 1] + x[, 2])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7765 -0.7585  0.0172  0.6577  3.1721 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.5619     0.4878   9.353  < 2e-16 ***\r\nx[, 1]       -0.5468     0.6707  -0.815    0.415    \r\nx[, 2]        2.5034     0.4479   5.589 3.76e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9988 on 497 degrees of freedom\r\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9962 \r\nF-statistic: 6.53e+04 on 2 and 497 DF,  p-value: < 2.2e-16\r\ndrop1(fitlm)\r\nSingle term deletions\r\n\r\nModel:\r\ny ~ x[, 1] + x[, 2]\r\n       Df Sum of Sq    RSS     AIC\r\n<none>              495.85  1.8338\r\nx[, 1]  1    0.6632 496.51  0.5021\r\nx[, 2]  1   31.1702 527.02 30.3165\r\n\r\n\r\ndataListShrink2 <- list(Ntotal=Ntotal, y=y, x=as.matrix(x), Nx=Nx)\r\n\r\nNote that actual coefficient for x[,1] is \\(+0.2\\), but slope on the plot plot(x[,1],y)\r\nis negative.\r\nAlso note that estimated model coefficients are different from\r\nactual because of correlation:\r\n\r\ncbind(actual=c(4,.2,3),estimated=fitlm$coefficients)\r\n            actual  estimated\r\n(Intercept)    4.0  4.5619222\r\nx[, 1]         0.2 -0.5468278\r\nx[, 2]         3.0  2.5034438\r\n\r\nRun the chains and analyze the results.\r\n\r\ntStart<-proc.time()\r\nfit3<-sampling(RobustMultipleRegressionDso,\r\n               data = dataListShrink2,\r\n               pars = c('beta0', 'beta', 'nu', 'sigma'),\r\n               iter = 5000, chains = 2, cores = 2)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.51    0.39   92.09 \r\n\r\nHow long did it take to run this MCMC? Why so\r\nlong?\r\nMonte Carlo methods assume that the samples are independent. This is\r\nusually not the case for sequential draws in Markov Chain Monte Carlo\r\n(MCMC) sampling, motivating the use of thinning. In thinning, we keep\r\nevery T sample (\\(P(x \\le t)\\)) and\r\nthrow away the other samples. For some types of MCMC (notably, Gibbs\r\nsampling), highly correlated variables result in highly correlated\r\nsamples (i.e., definitely not independent), requiring a large T to\r\ncompensate. Large values of T mean a large computational cost for each\r\neffective sample (i.e., for each sample that is kept).\r\nNote: The parameter thin allows the user to specify if and how\r\nmuch the MCMC chains should be thinned out before storing them. By\r\ndefault thin = 1 is used, which corresponds to keeping all values. A\r\nvalue thin = 10 would result in keeping every 10th value and discarding\r\nall other values.\r\nCheck convergence in shiny.\r\n\r\nlaunch_shinystan(fit3)\r\n\r\n\r\nstan_dens(fit3)\r\n\r\nstan_ac(fit3, separate_chains = T)\r\n\r\nsummary(fit3)$summary[,c(1,3,4,8,9)]\r\n               mean          sd        2.5%       97.5%    n_eff\r\nbeta0     4.5857595  0.48813790   3.6314232   5.5544965 2114.939\r\nbeta[1]  -0.5967437  0.67710628  -1.9314954   0.7388229 1713.891\r\nbeta[2]   2.4699613  0.45229741   1.5765319   3.3619226 1712.967\r\nnu       56.0426640 33.61895666  15.8382163 143.5659182 2718.137\r\nsigma     0.9816801  0.03355264   0.9166907   1.0483195 2731.866\r\nlp__    967.3463697  1.56455325 963.5132940 969.3727437 1911.020\r\npairs(fit3,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit3,pars=\"nu\")\r\n\r\nplot(fit3,pars=\"sigma\")\r\n\r\nplot(fit3,pars=\"beta0\")\r\n\r\nplot(fit3,pars=\"beta[1]\")\r\n\r\nplot(fit3,pars=\"beta[2]\")\r\n\r\n\r\n\r\nGeneral signs of collinear predictors:\r\n- High correlation between slopes (compensating sign)\r\n- Wide posterior distributions for slopes\r\n- Increased autocorrelation for slopes\r\n\r\n\r\npairs(cbind(y,x1,x2))\r\n\r\ncbind(actual=c(4,.2,3),estimatedLm=fitlm$coefficients,estimatedBayes=summary(fit3)$summary[1:3,1])\r\n            actual estimatedLm estimatedBayes\r\n(Intercept)    4.0   4.5619222      4.5857595\r\nx[, 1]         0.2  -0.5468278     -0.5967437\r\nx[, 2]         3.0   2.5034438      2.4699613\r\n\r\nLinear model shows the same information as Bayesian.\r\nCollinearity\r\nIn case when predictors have strong collinearity, linear model may\r\nstop working.\r\nSimulate the same model as in the previous section, but make predictors\r\ncollinear.\r\n\r\nset.seed(03192021)\r\nNtotal <- 500\r\nx1 <- rnorm(Ntotal, mean = 20, sd = 4)\r\nx2<-1-1.5*x1+rnorm(Ntotal, mean=0, sd = .000001) # sd closes to 0\r\nx<-cbind(x1,x2)           \r\nplot(x)\r\n\r\nNx <- ncol(x)\r\ny <- 4 + .2*x[,1] + 3*x[,2]+rnorm(Ntotal, mean = 0, sd = 1)\r\nplot(x[,1],y)\r\n\r\nplot(x[,2],y)\r\n\r\n\r\n\r\ndataListShrink2c <- list(Ntotal=Ntotal, y=y, x=as.matrix(x), Nx=Nx)\r\n(lmFit <- lm(y~x1+x2))\r\n\r\nCall:\r\nlm(formula = y ~ x1 + x2)\r\n\r\nCoefficients:\r\n(Intercept)           x1           x2  \r\n      7.094       -4.303           NA  \r\nsummary(lmFit)\r\n\r\nCall:\r\nlm(formula = y ~ x1 + x2)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7626 -0.7636  0.0244  0.6410  3.1747 \r\n\r\nCoefficients: (1 not defined because of singularities)\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  7.09394    0.24447   29.02   <2e-16 ***\r\nx1          -4.30334    0.01189 -361.96   <2e-16 ***\r\nx2                NA         NA      NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9991 on 498 degrees of freedom\r\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9962 \r\nF-statistic: 1.31e+05 on 1 and 498 DF,  p-value: < 2.2e-16\r\ndrop1(lmFit)\r\nSingle term deletions\r\n\r\nModel:\r\ny ~ x1 + x2\r\n       Df Sum of Sq    RSS    AIC\r\n<none>              497.08 1.0687\r\nx1      0 0.0001417 497.08 1.0688\r\nx2      0 0.0000000 497.08 1.0687\r\n\r\nLinear model stops working.\r\nSimulate Markov chains.\r\n\r\ncbind(actual=c(4,.2,3),estimated=fitlm$coefficients)\r\n            actual  estimated\r\n(Intercept)    4.0  4.5619222\r\nx[, 1]         0.2 -0.5468278\r\nx[, 2]         3.0  2.5034438\r\n\r\nRun the chains and analyze the results.\r\n\r\ntStart <- proc.time()\r\nfit3c <- sampling(RobustMultipleRegressionDso,\r\n                data=dataListShrink2c,\r\n                pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                iter=5000, chains = 1, cores = 2)\r\n\r\nSAMPLING FOR MODEL '8e2e8be6f09a2541ae94da0604a417ec' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 5000 [  0%]  (Warmup)\r\nChain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)\r\nChain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)\r\nChain 1: Iteration: 1500 / 5000 [ 30%]  (Warmup)\r\nChain 1: Iteration: 2000 / 5000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 2500 / 5000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 2501 / 5000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 5000 / 5000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 155.086 seconds (Warm-up)\r\nChain 1:                114.928 seconds (Sampling)\r\nChain 1:                270.014 seconds (Total)\r\nChain 1: \r\ntEnd <- proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n 261.57    0.17  270.09 \r\n\r\nWith collinear predictors model definitely takes much longer time to\r\nsimulate.\r\n\r\nstan_dens(fit3c)\r\n\r\nstan_ac(fit3c, separate_chains = T)\r\n\r\nsummary(fit3c)$summary[,c(1,3,4,8,9)]\r\n               mean          sd        2.5%      97.5%    n_eff\r\nbeta0     5.2869784  4.12513863  -2.5410297  13.834655 258.6426\r\nbeta[1]  -1.5985501  6.17402618 -14.3329219  10.098421 255.8424\r\nbeta[2]   1.8031011  4.11591117  -6.6807200   9.601809 255.7672\r\nnu       57.5235574 34.84527395  15.7981673 148.866690 605.1602\r\nsigma     0.9834476  0.03440985   0.9179121   1.052713 331.5993\r\nlp__    967.5976423  1.48319659 964.0943396 969.579397 845.7838\r\npairs(fit3c,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit3c,pars=\"nu\")\r\n\r\nplot(fit3c,pars=\"sigma\")\r\n\r\nplot(fit3c,pars=\"beta0\")\r\n\r\nplot(fit3c,pars=\"beta[1]\")\r\n\r\nplot(fit3c,pars=\"beta[2]\")\r\n\r\n\r\nMarkov chains may go over limit on tree depths (yellow dots on pairs\r\ngraph).\r\nBut Bayesian method still works. It shows that one or both of the slopes\r\nare not significantly different from zero.\r\nShrinkage of regression\r\ncoefficients\r\nWhen there are many candidate predictors in the model it may be\r\nuseful to “motivate” them to become closer to zero if they are not very\r\nstrong.\r\nOne way to do it is to:\r\nSet a prior distribution for slopes as t-Student instead of\r\nnormal;\r\nMake mean of that distribution equal to zero;\r\nMake normality parameter \\(\\nu\\)\r\nsmall and dispersion parameter \\(\\sigma\\) also small\r\n\r\nSmall \\(\\sigma\\) forces slopes to\r\nshrink towards zero mean. At the same time, small \\(\\nu\\) makes the tails fat enough to allow\r\nsome strong slopes to be outliers.\r\nParameter \\(\\sigma\\) of the prior\r\nfor regression coefficients \\(\\beta_j\\)\r\ncan be either fixed, or given its own prior and estimated.\r\nIn the former case, all coefficients will be forced to have the same\r\nregularizator, if it is random and estimated from the same data then\r\nthere is mutual influence between \\(\\sigma\\) and regression coefficients: if\r\nmany of them are close to zero then \\(\\sigma\\) is going to be smaller, which in\r\nturn pushes coefficients even closer to zero.\r\nThese approaches reminds us the Ridge/LASSO/Elastic net regression in\r\nMachine Learning models.\r\nTwo significant predictors\r\nUse the same data dataListRegression as in the above\r\nsection.\r\nDescribe the model.\r\n\\[y_i = \\beta_0 + \\beta_1 x_{i,1} +\r\n\\beta_2 x_{i,2} + \\epsilon_i \\\\\r\ny_i \\sim t(\\mu_i, \\sigma, \\nu) \\\\\r\n\\\\\r\n\\text{where mean, scale and degree of freedom, respectively, would be}\r\n\\\\\r\n\\mu_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} \\\\\r\n\\beta_0 \\sim N(M_0, S_0) \\ ; \\ \\beta_j \\sim t(\\mu_j, \\nu_j,\r\n\\sigma_{\\beta}) \\\\\r\n\\sigma_{\\beta} \\sim gamma(shape,rate) \\\\\r\n\\sigma \\sim Unif(L, H) \\\\\r\n\\nu = \\gamma^{\\prime} + 1, \\ \\text{where} \\ \\gamma^{\\prime} \\sim\r\nexp(\\lambda)\\]\r\n\r\nmodelString<-\"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int<lower=1> Nx;\r\n    vector[Ntotal] y;\r\n    matrix[Ntotal, Nx] x;\r\n}\r\ntransformed data {\r\n    real meanY;\r\n    real sdY;\r\n    vector[Ntotal] zy; // normalized\r\n    vector[Nx] meanX;\r\n    vector[Nx] sdX;\r\n    matrix[Ntotal, Nx] zx; // normalized\r\n    \r\n    meanY = mean(y);\r\n    sdY = sd(y);\r\n    zy = (y - meanY) / sdY;\r\n    for (j in 1:Nx) {\r\n        meanX[j] = mean(x[,j]);\r\n        sdX[j] = sd(x[,j]);\r\n        for (i in 1:Ntotal) {\r\n            zx[i,j] = (x[i,j] - meanX[j]) / sdX[j];\r\n        }\r\n    }\r\n}\r\nparameters {\r\n    real zbeta0;\r\n    real<lower=0> sigmaBeta;\r\n    vector[Nx] zbeta;\r\n    real<lower=0> nu;\r\n    real<lower=0> zsigma;\r\n}\r\ntransformed parameters{\r\n    vector[Ntotal] zy_hat;\r\n    zy_hat = zbeta0 + zx * zbeta;\r\n}\r\nmodel {\r\n    zbeta0 ~ normal(0, 2);\r\n    sigmaBeta ~ gamma(2.3,1.3); // mode=(alpha-1)/beta, var=alpha/beta^2\r\n    zbeta  ~ student_t(1.0/30.0, 0, sigmaBeta);\r\n    nu ~ exponential(1/30.0);\r\n    zsigma ~ uniform(1.0E-5 , 1.0E+1);\r\n    zy ~ student_t(1+nu, zy_hat, zsigma);\r\n}\r\ngenerated quantities { \r\n    // Transform to original scale:\r\n    real beta0; \r\n    vector[Nx] beta;\r\n    real sigma;\r\n    // .* and ./ are element-wise product and divide\r\n    beta0 = zbeta0*sdY  + meanY - sdY * sum( zbeta .* meanX ./ sdX );\r\n    beta = sdY * ( zbeta ./ sdX );\r\n    sigma = zsigma * sdY;\r\n} \"\r\n\r\nGamma distribution prior for sigmaBeta is selected to have relatively\r\nlow mode 1.\r\n\r\nxGamma <- seq(from = .00001, to = 10, by = .001)\r\nplot(xGamma,dgamma(xGamma,shape=2.3,rate=1.3),type=\"l\")\r\n\r\nxGamma[which.max(dgamma(xGamma,shape=2.3,rate=1.3))]\r\n[1] 1.00001\r\n\r\nCreate DSO.\r\n\r\nRegressionShrinkDso <- stan_model(model_code = modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# save(RegressionShrinkDso, file = \"data/DSOShrunkMultRegr.Rds\")\r\nload(\"data/DSOShrunkMultRegr.Rds\")\r\n\r\nGenerate Markov chains in case of 2 significant predictors.\r\n\r\ntStart<-proc.time()\r\n# fit model\r\nfit4 <- sampling(RegressionShrinkDso, \r\n             data=dataListRegression, \r\n             pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n             iter=5000, chains = 2, cores = 2\r\n)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.40    0.25   18.60 \r\n\r\nAnalyze fitted model using shinystan\r\n\r\nlaunch_shinystan(fit4)\r\n\r\n\r\nstan_dens(fit4)\r\n\r\nstan_ac(fit4, separate_chains = T)\r\n\r\nsummary(fit4)$summary[,c(1,3,4,8,9)]\r\n                  mean           sd         2.5%       97.5%    n_eff\r\nbeta0        4.5843612  0.228604011    4.1393299    5.027146 8748.136\r\nbeta[1]      1.0735281  0.010973133    1.0524275    1.095341 8200.715\r\nbeta[2]      2.9950789  0.007839406    2.9796239    3.009790 7482.511\r\nnu          61.3810270 35.734724611   17.0630928  149.281989 5570.822\r\nsigma        0.9613439  0.033176999    0.8953832    1.028195 5375.540\r\nsigmaBeta    1.3993861  0.899552223    0.2216032    3.632021 6406.796\r\nlp__      1010.3909262  1.749705508 1006.2179766 1012.779996 2263.114\r\npairs(fit4,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit4,pars=\"nu\")\r\n\r\nplot(fit4,pars=\"sigma\")\r\n\r\nplot(fit4,pars=\"beta0\")\r\n\r\nplot(fit4,pars=\"beta[1]\")\r\n\r\nplot(fit4,pars=\"beta[2]\")\r\n\r\n\r\nAnalysis and comparison\r\nCompare posterior mean values and 95% HDI with fit1\r\n(same model, but with no shrinkage).\r\n\r\ncbind(summary(fit1)$summary[1:3,c(1,4,8)],\r\n      summary(fit4)$summary[1:3,c(1,4,8)])\r\n            mean     2.5%    97.5%     mean     2.5%    97.5%\r\nbeta0   4.578031 4.130417 5.025614 4.584361 4.139330 5.027146\r\nbeta[1] 1.073828 1.052198 1.095054 1.073528 1.052427 1.095341\r\nbeta[2] 2.995153 2.979745 3.011046 2.995079 2.979624 3.009790\r\n\r\nMean values of both fits seem very similar.\r\nCheck widths of the HDI for coefficients.\r\n\r\ncbind(summary(fit1)$summary[1:3,c(8)]-summary(fit1)$summary[1:3,c(4)],\r\n      summary(fit4)$summary[1:3,c(8)]-summary(fit4)$summary[1:3,c(4)])\r\n              [,1]       [,2]\r\nbeta0   0.89519719 0.88781592\r\nbeta[1] 0.04285560 0.04291323\r\nbeta[2] 0.03130079 0.03016606\r\n\r\nShrinkage can be noticed after third digit of all coefficients.\r\nIn this example both slopes are significant and they practically did not\r\nshrink.\r\nFor comparison fit linear model, ridge and lasso regressions to the\r\nsame data.\r\n1- Linear model.\r\n\r\nlmFit<-lm(dataListRegression$y~dataListRegression$x[,1]+dataListRegression$x[,2])\r\n\r\n2- Ridge.\r\n\r\nlibrary(glmnet)\r\nset.seed(15)\r\ncv.outRidge <- cv.glmnet(x = dataListRegression$x, \r\n                         y = dataListRegression$y, \r\n                         alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 1.680471\r\nridgeFit <- glmnet(x = dataListRegression$x, y=dataListRegression$y,\r\n                   alpha = 0, lambda = bestlam, standardize = F)\r\n(ridge.coef <- predict(ridgeFit,type=\"coefficients\", s = bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 4.768742\r\nV1          1.068697\r\nV2          2.986144\r\n\r\n3- Lasso.\r\n\r\nset.seed(15)\r\ncv.outLasso <- cv.glmnet(x = dataListRegression$x, \r\n                         y = dataListRegression$y, \r\n                         alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 0.08363741\r\nlassoFit<-glmnet(x=dataListRegression$x,y=dataListRegression$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\n(lasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 4.689546\r\nV1          1.069233\r\nV2          2.992895\r\n\r\nCompare coefficients from all 5 models\r\n\r\ncomparison<-cbind(summary(fit1)$summary[1:3,c(1,4,8)],\r\n      summary(fit4)$summary[1:3,c(1,4,8)],\r\n      Ridge=ridge.coef,\r\n      Lasso=lasso.coef,\r\n      Linear=lmFit$coefficients)\r\ncolnames(comparison)<-c(paste(\"NoShrinkage\",c(\"mean\",\"2.5%\",\"97.5%\"),sep=\"_\"),\r\n                        paste(\"Shrinkage\",c(\"mean\",\"2.5%\",\"97.5%\"),sep=\"_\"),\r\n                        \"Ridge\",\"Lasso\",\"Linear\")\r\nt(comparison)\r\n9 x 3 sparse Matrix of class \"dgCMatrix\"\r\n                     beta0  beta[1]  beta[2]\r\nNoShrinkage_mean  4.578031 1.073828 2.995153\r\nNoShrinkage_2.5%  4.130417 1.052198 2.979745\r\nNoShrinkage_97.5% 5.025614 1.095054 3.011046\r\nShrinkage_mean    4.584361 1.073528 2.995079\r\nShrinkage_2.5%    4.139330 1.052427 2.979624\r\nShrinkage_97.5%   5.027146 1.095341 3.009790\r\nRidge             4.768742 1.068697 2.986144\r\nLasso             4.689546 1.069233 2.992895\r\nLinear            4.565419 1.074206 2.995385\r\n\r\nAll models show practically no shrinkage relative to linear\r\nmodel.\r\nBoth Ridge and Lasso regression have too high estimates of\r\nintercept.\r\nInsignificant predictor\r\nShrink estimates from data dataListInsig.\r\n\r\ntStart<-proc.time()\r\n# fit model\r\nfit5 <- sampling (RegressionShrinkDso, \r\n             data=dataListInsig, \r\n             pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n             iter=5000, chains = 2, cores = 2\r\n)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.44    0.17   18.02 \r\n\r\nWe can analyze fitted model with shinystan but\r\n\r\nstan_dens(fit5)\r\n\r\nstan_ac(fit5, separate_chains = T)\r\n\r\nsummary(fit5)$summary[,c(1,3,4,8,9)]\r\n                   mean          sd          2.5%         97.5%\r\nbeta0      1.221117e+00  0.05300762    1.11769338    1.32774329\r\nbeta[1]    7.988002e-01  0.02885518    0.74227587    0.85594136\r\nbeta[2]    7.983679e-03  0.02713646   -0.04570233    0.06256174\r\nnu         5.257628e+01 33.19871103   13.71139258  137.97027648\r\nsigma      5.977651e-01  0.02104643    0.55653680    0.63914284\r\nsigmaBeta  1.043781e+00  0.83037712    0.10282820    3.17391417\r\nlp__      -1.850178e+02  1.72628557 -189.19759519 -182.70019632\r\n             n_eff\r\nbeta0     6549.983\r\nbeta[1]   6984.701\r\nbeta[2]   6777.038\r\nnu        4536.272\r\nsigma     5500.091\r\nsigmaBeta 5994.464\r\nlp__      2157.592\r\npairs(fit5,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit5,pars=\"nu\")\r\n\r\nplot(fit5,pars=\"sigma\")\r\n\r\nplot(fit5,pars=\"beta0\")\r\n\r\nplot(fit5,pars=\"beta[1]\")\r\n\r\nplot(fit5,pars=\"beta[2]\")\r\n\r\n\r\nThis time posterior density of \\(\\beta_2\\) (beta[2]) is concentrated at\r\nzero.\r\nAnalysis and comparison\r\nCompare mean levels and HDI widths for fits with and without\r\nshrinkage.\r\n\r\ncbind(summary(fit2)$summary[1:3,c(1,4,8)],\r\n      summary(fit5)$summary[1:3,c(1,4,8)])\r\n               mean        2.5%      97.5%        mean        2.5%\r\nbeta0   1.217597421  1.11612951 1.31636194 1.221117106  1.11769338\r\nbeta[1] 0.799972585  0.74453626 0.85437808 0.798800184  0.74227587\r\nbeta[2] 0.009416795 -0.04397004 0.06323134 0.007983679 -0.04570233\r\n             97.5%\r\nbeta0   1.32774329\r\nbeta[1] 0.85594136\r\nbeta[2] 0.06256174\r\n\r\n\r\ncbind(summary(fit2)$summary[1:3,c(8)]-summary(fit2)$summary[1:3,c(4)],\r\n      summary(fit5)$summary[1:3,c(8)]-summary(fit5)$summary[1:3,c(4)])\r\n             [,1]      [,2]\r\nbeta0   0.2002324 0.2100499\r\nbeta[1] 0.1098418 0.1136655\r\nbeta[2] 0.1072014 0.1082641\r\n\r\nParameters shrunk a little more this time, second coefficient shrunk\r\nto zero.\r\nAgain, fit linear model, ridge and lasso regressions to the same\r\ndata.\r\n1- Linear model.\r\n\r\nlmFit<-lm(dataListInsig$y~dataListInsig$x[,1]+dataListInsig$x[,2])\r\n\r\n2- Ridge.\r\n\r\nset.seed(15)\r\ncv.outRidge=cv.glmnet(x=dataListInsig$x,y=dataListInsig$y,alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 0.07783229\r\nridgeFit<-glmnet(x=dataListInsig$x,y=dataListInsig$y,\r\n                 alpha=0,lambda=bestlam,standardize = F)\r\n(ridge.coef<-predict(ridgeFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                     s1\r\n(Intercept) 1.287098608\r\nInput1      0.739131424\r\nInput2      0.004155875\r\n\r\n3- Lasso.\r\n\r\nset.seed(15)\r\ncv.outLasso=cv.glmnet(x=dataListInsig$x,y=dataListInsig$y,alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 0.02067294\r\nlassoFit<-glmnet(x=dataListInsig$x,y=dataListInsig$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\n(lasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 1.249387\r\nInput1      0.778466\r\nInput2      .       \r\n\r\nCompare coefficients from all 3 models.\r\n\r\ncomparison<-cbind(summary(fit2)$summary[1:3,c(1)],\r\n      summary(fit5)$summary[1:3,c(1)],\r\n      Ridge=ridge.coef,\r\n      Lasso=lasso.coef,\r\n      Linear=lmFit$coefficients)\r\ncolnames(comparison)<-c(\"NoShrinkage\",\"Shrinkage\",\"Ridge\",\"Lasso\",\"Linear\")\r\nt(comparison)\r\n5 x 3 sparse Matrix of class \"dgCMatrix\"\r\n            (Intercept)    Input1      Input2\r\nNoShrinkage    1.217597 0.7999726 0.009416795\r\nShrinkage      1.221117 0.7988002 0.007983679\r\nRidge          1.287099 0.7391314 0.004155875\r\nLasso          1.249387 0.7784660 .          \r\nLinear         1.214802 0.8011573 0.009700042\r\n\r\nAll models correctly exclude second coefficient.\r\nRidge shrunk both slopes more than other models.\r\nThere is again tendency for Ridge and Lasso to overestimate\r\nintercept.\r\nCorrelated predictors\r\nShrink coefficients estimated from dataListShrink2.\r\n\r\ntStart<-proc.time()\r\n# fit model\r\nfit6 <- sampling (RegressionShrinkDso, \r\n             data=dataListShrink2, \r\n             pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n             iter=5000, chains = 2, cores = 2\r\n)\r\ntEnd<-proc.time()\r\ntEnd-tStart\r\n   user  system elapsed \r\n   0.54    0.10   91.97 \r\n\r\nWe could analyze model with shinystan but let’s check\r\ndensities, pairs and individual plots of parameters.\r\n\r\nstan_dens(fit6)\r\n\r\nstan_ac(fit6, separate_chains = T)\r\n\r\nsummary(fit6)$summary[,c(1,3,4,8,9)]\r\n                 mean          sd        2.5%       97.5%    n_eff\r\nbeta0       4.5437360  0.47198344   3.6607788   5.5095532 1845.144\r\nbeta[1]    -0.5261239  0.63908279  -1.8992740   0.6344545 1575.212\r\nbeta[2]     2.5172117  0.42658592   1.6035586   3.2916493 1575.367\r\nnu         56.5744375 34.07819768  14.7590894 143.3833364 3428.014\r\nsigma       0.9804926  0.03427887   0.9135965   1.0470397 3347.567\r\nsigmaBeta   1.2388283  0.85960126   0.1607706   3.3968145 3997.294\r\nlp__      963.8218087  1.75910037 959.4784596 966.1987987 2037.995\r\npairs(fit6,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit6,pars=\"nu\")\r\n\r\nplot(fit6,pars=\"sigma\")\r\n\r\nplot(fit6,pars=\"beta0\")\r\n\r\nplot(fit6,pars=\"beta[1]\")\r\n\r\nplot(fit6,pars=\"beta[2]\")\r\n\r\n\r\nAnalysis and comparison\r\nShow mean values and HDI.\r\n\r\ncbind(summary(fit3)$summary[1:3,c(1,4,8)],\r\n      summary(fit6)$summary[1:3,c(1,4,8)])\r\n              mean      2.5%     97.5%       mean      2.5%     97.5%\r\nbeta0    4.5857595  3.631423 5.5544965  4.5437360  3.660779 5.5095532\r\nbeta[1] -0.5967437 -1.931495 0.7388229 -0.5261239 -1.899274 0.6344545\r\nbeta[2]  2.4699613  1.576532 3.3619226  2.5172117  1.603559 3.2916493\r\ncbind(summary(fit3)$summary[1:3,c(8)]-summary(fit3)$summary[1:3,c(4)],\r\n      summary(fit6)$summary[1:3,c(8)]-summary(fit6)$summary[1:3,c(4)])\r\n            [,1]     [,2]\r\nbeta0   1.923073 1.848774\r\nbeta[1] 2.670318 2.533728\r\nbeta[2] 1.785391 1.688091\r\n\r\nIn this example \\(\\beta_1\\) shrunk\r\nmore significantly and is not different from zero.\r\nAt the same time \\(\\beta_2\\) has become\r\nmore different from zero.\r\nRegularization reinforced one of the two correlated predictors while\r\ndumping the other.\r\nAgain, fit linear model, ridge and lasso regressions to the same\r\ndata.\r\n1- Linear model.\r\n\r\nlmFit<-lm(dataListShrink2$y~dataListShrink2$x[,1]+dataListShrink2$x[,2])\r\nsummary(lmFit)\r\n\r\nCall:\r\nlm(formula = dataListShrink2$y ~ dataListShrink2$x[, 1] + dataListShrink2$x[, \r\n    2])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.7765 -0.7585  0.0172  0.6577  3.1721 \r\n\r\nCoefficients:\r\n                       Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)              4.5619     0.4878   9.353  < 2e-16 ***\r\ndataListShrink2$x[, 1]  -0.5468     0.6707  -0.815    0.415    \r\ndataListShrink2$x[, 2]   2.5034     0.4479   5.589 3.76e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9988 on 497 degrees of freedom\r\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9962 \r\nF-statistic: 6.53e+04 on 2 and 497 DF,  p-value: < 2.2e-16\r\n\r\n2- Ridge.\r\n\r\nset.seed(15)\r\ncv.outRidge=cv.glmnet(x=dataListShrink2$x,y=dataListShrink2$y,alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 1.61435\r\nridgeFit<-glmnet(x=dataListShrink2$x,y=dataListShrink2$y,\r\n                 alpha=0,lambda=bestlam,standardize = F)\r\n(ridge.coef<-predict(ridgeFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                   s1\r\n(Intercept)  4.943942\r\nx1          -1.425696\r\nx2           1.910632\r\n\r\n3- Lasso.\r\n\r\nset.seed(15)\r\ncv.outLasso=cv.glmnet(x=dataListShrink2$x,y=dataListShrink2$y,alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 0.1062134\r\nlassoFit<-glmnet(x=dataListShrink2$x,y=dataListShrink2$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\n(lasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam))\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                  s1\r\n(Intercept) 4.116010\r\nx1          .       \r\nx2          2.865189\r\n\r\nCompare coefficients from all 3 models.\r\n\r\ncomparison<-cbind(summary(fit3)$summary[1:3,c(1)],\r\n      summary(fit6)$summary[1:3,c(1)],\r\n      Ridge=ridge.coef,\r\n      Lasso=lasso.coef,\r\n      Linear=lmFit$coefficients)\r\ncolnames(comparison)<-c(\"NoShrinkage\",\"Shrinkage\",\"Ridge\",\"Lasso\",\"Linear\")\r\nt(comparison)\r\n5 x 3 sparse Matrix of class \"dgCMatrix\"\r\n            (Intercept)         x1       x2\r\nNoShrinkage    4.585760 -0.5967437 2.469961\r\nShrinkage      4.543736 -0.5261239 2.517212\r\nRidge          4.943942 -1.4256958 1.910632\r\nLasso          4.116010  .         2.865189\r\nLinear         4.561922 -0.5468278 2.503444\r\n\r\nAll models correctly exclude first slope.\r\nLasso does it decisively, making slope \\(\\beta_1\\) and exactly equal to \\(0\\).\r\nLasso also estimated intercept and \\(\\beta_2\\) more accurately than other\r\nmodels: recall that for this data set we simulate \\(\\beta_0=4, \\beta_2=3\\).\r\nIs school financing\r\nnecessary?\r\nAnalysis of SAT scores, example from Kruschke, 2015, section\r\n18.3.\r\nThese data are analyzed in the article\r\nby Deborah Lynn Guber.\r\nThe variables observed are:\r\nstates (State)\r\nthe mean SAT score by state (SATV, SATM and SATT),\r\namount of money spent by student (Spend),\r\npercent of students who take SAT (PrcntTake) and\r\nother variables (student to teacher ratio, teacher salary).\r\nRead the data from file Guber1999data.csv available at\r\nKruschke,\r\n2015.\r\n\r\nmyData = read.csv(\"data/Guber1999data.csv\")  # section 18.3 @ Kruschke\r\nhead(myData)\r\n       State Spend StuTeaRat Salary PrcntTake SATV SATM SATT\r\n1    Alabama 4.405      17.2 31.144         8  491  538 1029\r\n2     Alaska 8.963      17.6 47.951        47  445  489  934\r\n3    Arizona 4.778      19.3 32.175        27  448  496  944\r\n4   Arkansas 4.459      17.1 28.934         6  482  523 1005\r\n5 California 4.992      24.0 41.078        45  417  485  902\r\n6   Colorado 5.443      18.4 34.571        29  462  518  980\r\npairs(myData[,-c(1,6:7)])\r\n\r\nplot(myData$Spend,myData$SATT)\r\n\r\nsummary(lm(myData$SATT~myData$Spend))$coeff\r\n               Estimate Std. Error   t value     Pr(>|t|)\r\n(Intercept)  1089.29372  44.389950 24.539197 8.168276e-29\r\nmyData$Spend  -20.89217   7.328209 -2.850925 6.407965e-03\r\n\r\nThe plots show that mean SAT score is negatively correlated with\r\namount of money states spend per student. These results were used in hot\r\ndebates about spending money on education to support argument in favor\r\nof reducing public support for schools.\r\nPrepare the data.\r\nUse the 2 predictors from the file, plus add 12 randomly generated\r\nnuisance predictors.\r\n\r\nNtotal <- nrow(myData)\r\ny <- myData$SATT\r\nx <- cbind(myData$Spend, myData$PrcntTake)\r\ncolnames(x) <- c(\"Spend\",\"PrcntTake\");\r\ndataList2Predict <- list(Ntotal=Ntotal,y=y,x=x,Nx=ncol(x))\r\n# generate 12 spurious predictors:\r\nset.seed(47405)\r\nNxRand <- 12\r\nfor (xIdx in 1:NxRand) {\r\n    xRand = rnorm(Ntotal)\r\n    x = cbind(x, xRand )\r\n    colnames(x)[ncol(x)] = paste0(\"xRand\", xIdx)\r\n}\r\ndataListExtraPredict <- list(Ntotal=Ntotal,y=y,x=x,Nx=ncol(x))\r\n\r\nNo-shrinkage\r\nUse the same model as in the example of the first section:\r\nRobustMultipleRegressionDso.\r\nFirst, run the model with 2 predictors.\r\n\r\nfit_noshrink2Pred <- sampling (RobustMultipleRegressionDso, \r\n                          data=dataList2Predict, \r\n                          pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                          iter=5000, chains = 2, cores = 2)\r\nsummary(fit_noshrink2Pred)$summary[,c(1,4,8)]\r\n               mean       2.5%      97.5%\r\nbeta0   991.4007496 947.253200 1036.31532\r\nbeta[1]  12.8196529   4.235574   21.51116\r\nbeta[2]  -2.8747599  -3.308695   -2.44884\r\nnu       33.4879111   3.455822  114.56115\r\nsigma    31.6173398  24.425564   39.83589\r\nlp__     -0.1425401  -4.083100    1.96456\r\n\r\nIt is clear that the slope of Spend is significantly\r\npositive and slope of PrcntTake is significantly\r\nnegative.\r\nThis shows that the negative correlation between SAT scores and the\r\nmoney spent as seen from the scatterplot is illusory: fewer students\r\nfrom underfunded schools take SAT, but these are only students who apply\r\nfor colleges; students who potentially would receive low SAT scores do\r\nnot apply to college and do not take the test.\r\nRun MCMC for the model with additional nuisance predictors.\r\n\r\nfit_noshrinkExtra <- sampling (RobustMultipleRegressionDso, \r\n                          data=dataListExtraPredict, \r\n                          pars=c('beta0', 'beta', 'nu', 'sigma'),\r\n                          iter=5000, chains = 2, cores = 2)\r\n\r\nAnalyze the output with shinystan .\r\n\r\nlaunch_shinystan(fit_noshrinkExtra)\r\n\r\nHere are the results of MCMC.\r\n\r\nstan_ac(fit_noshrinkExtra, separate_chains = T)\r\n\r\npairs(fit_noshrinkExtra,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\"))\r\n\r\nplot(fit_noshrinkExtra,pars=c('beta'))\r\n\r\nstan_dens(fit_noshrinkExtra,pars=c(\"beta0\",\"beta\"))\r\n\r\n\r\nAll densities look symmetrical: mean values of posterior\r\ndistributions can be used as point estimates of betas.\r\n\r\nsummary(fit_noshrinkExtra)$summary[,c(1,4,8)]\r\n               mean       2.5%        97.5%\r\nbeta0    998.618086 952.442828 1044.8766561\r\nbeta[1]   10.234291   1.233829   19.2611967\r\nbeta[2]   -2.721493  -3.179022   -2.2554795\r\nbeta[3]    2.304234 -11.450917   15.7354294\r\nbeta[4]   -4.728814 -14.889133    5.4752469\r\nbeta[5]    6.250301  -7.252438   19.1997960\r\nbeta[6]   -5.696532 -15.225194    3.7955166\r\nbeta[7]    6.989249  -2.925123   16.8434431\r\nbeta[8]    1.908281  -8.072728   12.2999566\r\nbeta[9]    4.283368  -6.011922   14.5423902\r\nbeta[10]   2.651897 -11.337831   17.1438898\r\nbeta[11]  -4.221621 -13.803873    5.1213425\r\nbeta[12] -10.363432 -20.155584   -0.7626764\r\nbeta[13]  -1.894524 -12.602323    8.8656771\r\nbeta[14]   1.710674  -8.973928   12.1135095\r\nnu        31.187292   2.027864  105.5120668\r\nsigma     30.255439  21.367424   39.5905702\r\nlp__       1.240241  -6.398344    6.6903196\r\n\r\nThe variables corresponding to betas are:\r\n\r\ncolnames(x)\r\n [1] \"Spend\"     \"PrcntTake\" \"xRand1\"    \"xRand2\"    \"xRand3\"   \r\n [6] \"xRand4\"    \"xRand5\"    \"xRand6\"    \"xRand7\"    \"xRand8\"   \r\n[11] \"xRand9\"    \"xRand10\"   \"xRand11\"   \"xRand12\"  \r\n\r\nNote that the coefficient for variable Spend is still\r\npositive, but the left side of HDI interval is much closer to zero. The\r\ncoefficient for PrcntTake is still significantly\r\nnegative.\r\nOne of the nuisance predictors happened to be significantly negative:\r\nbeta[12].\r\nAs a result of adding nuisance predictors the accuracy of inference\r\nbecomes lower.\r\nShrinkage\r\nAnalyze the same data with the model encouraging shrinkage of\r\nparameters.\r\nFirst, fit the model without nuisance parameters.\r\n\r\nfit_shrink <- sampling (RegressionShrinkDso, \r\n                        data=dataList2Predict, \r\n                        pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n                        iter=5000, chains = 2, cores = 2)\r\n\r\nCheck convergence in shiny.\r\n\r\nlaunch_shinystan(fit_shrink)\r\n\r\n\r\npairs(fit_shrink,pars=c(\"beta0\",\"beta\",\"nu\",\"sigma\",\"sigmaBeta\"))\r\n\r\nplot(fit_shrink,pars=c('beta'))\r\n\r\nstan_dens(fit_shrink,pars=c('beta'))\r\n\r\nstan_ac(fit_shrink, separate_chains = T)\r\n\r\n\r\nCompare with the fit without nuisance parameters and without\r\nshrinkage.\r\n\r\ncbind(summary(fit_noshrink2Pred)$summary[1:4,c(1,4,8)],\r\n      summary(fit_shrink)$summary[1:4,c(1,4,8)])\r\n             mean       2.5%      97.5%       mean       2.5%\r\nbeta0   991.40075 947.253200 1036.31532 996.071254 951.012629\r\nbeta[1]  12.81965   4.235574   21.51116  11.770134   2.632324\r\nbeta[2]  -2.87476  -3.308695   -2.44884  -2.831416  -3.275607\r\nnu       33.48791   3.455822  114.56115  33.323074   3.756878\r\n              97.5%\r\nbeta0   1042.051358\r\nbeta[1]   20.514489\r\nbeta[2]   -2.361973\r\nnu       109.178594\r\n\r\nFirst variable shrunk closer to zero: mean value is smaller and left\r\nend of the 95%-HDI is closer to zero.\r\nNow fit the model with additional parameters.\r\n\r\nfit_shrinkExtra <- sampling (RegressionShrinkDso, \r\n                        data=dataListExtraPredict, \r\n                        pars=c('beta0', 'beta', 'nu', 'sigma', 'sigmaBeta'),\r\n                        iter=5000, chains = 2, cores = 2)\r\nstan_ac(fit_shrinkExtra, separate_chains = T)\r\n\r\npairs(fit_shrinkExtra,pars=c(\"beta0\",\"beta[1]\",\"beta[2]\",\"beta[3]\",\"beta[4]\",\"beta[11]\",\"beta[12]\"))\r\n\r\npairs(fit_shrinkExtra,pars=c(\"nu\",\"sigma\",\"sigmaBeta\"))\r\n\r\nplot(fit_shrinkExtra,pars=c('beta'))\r\n\r\nstan_dens(fit_shrinkExtra,pars=c('beta'))\r\n\r\n\r\nNote characteristic pinched tips of posterior densities for shrunk\r\nvariables.\r\n\r\nsummary(fit_shrinkExtra)$summary[,c(1:4,8)]\r\n                   mean     se_mean         sd          2.5%\r\nbeta0     1009.82602144 4.473582390 26.5068325  9.628679e+02\r\nbeta[1]      8.21954511 1.078486389  5.2814904 -2.802567e-01\r\nbeta[2]     -2.68755852 0.033018532  0.2279845 -3.164852e+00\r\nbeta[3]      1.16429414 0.538365949  3.1397865 -4.187581e+00\r\nbeta[4]     -1.16002937 0.229745431  2.6666174 -8.523255e+00\r\nbeta[5]      1.78068242 0.492660992  3.0628568 -2.483201e+00\r\nbeta[6]     -2.47265483 1.152596733  3.7247172 -1.093501e+01\r\nbeta[7]      2.76863569 1.135585515  3.9366726 -1.659493e+00\r\nbeta[8]      0.86387877 0.267798818  2.3636293 -2.570624e+00\r\nbeta[9]      1.36957487 0.314802537  2.8913159 -2.288281e+00\r\nbeta[10]    -1.39643294 1.838822871  3.7474360 -1.012010e+01\r\nbeta[11]    -1.00165428 0.236339260  2.3005006 -7.569141e+00\r\nbeta[12]    -6.61450384 0.710490722  5.4495075 -1.794561e+01\r\nbeta[13]    -0.38037233 0.100166767  1.9872390 -5.869640e+00\r\nbeta[14]     0.76980004 0.466670211  2.3077738 -3.803671e+00\r\nnu          40.07666808 5.561396626 34.6493271  3.403896e+00\r\nsigma       30.81178901 0.343535668  3.8888139  2.332439e+01\r\nsigmaBeta    0.01966483 0.002562242  0.0270782  5.482108e-04\r\nlp__        21.13792207 0.824755562  6.4649353  9.719103e+00\r\n                  97.5%\r\nbeta0     1058.28908450\r\nbeta[1]     18.23938239\r\nbeta[2]     -2.24414288\r\nbeta[3]      8.71009792\r\nbeta[4]      3.08755229\r\nbeta[5]      9.02094911\r\nbeta[6]      1.45812939\r\nbeta[7]     11.60701517\r\nbeta[8]      7.02230696\r\nbeta[9]      9.15217808\r\nbeta[10]     5.87438345\r\nbeta[11]     2.47309092\r\nbeta[12]     0.61143385\r\nbeta[13]     3.18343802\r\nbeta[14]     6.30005198\r\nnu         129.97213782\r\nsigma       38.72621370\r\nsigmaBeta    0.09534239\r\nlp__        35.39122557\r\n\r\nParameter beta[12] has shrunk to zero based on 95%-HDI\r\nas a result of regularized model.\r\nThis helped removing all nuisance parameters. But shrinkage also removed\r\nparameter beta[1] of variable Spend!!!\r\nLinear model\r\nCompare with linear model.\r\nWithout nuisance predictors:\r\n\r\nlmSAT<-lm(y~x[,1]+x[,2])\r\nsummary(lmSAT)\r\n\r\nCall:\r\nlm(formula = y ~ x[, 1] + x[, 2])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-88.400 -22.884   1.968  19.142  68.755 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 993.8317    21.8332  45.519  < 2e-16 ***\r\nx[, 1]       12.2865     4.2243   2.909  0.00553 ** \r\nx[, 2]       -2.8509     0.2151 -13.253  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 32.46 on 47 degrees of freedom\r\nMultiple R-squared:  0.8195,    Adjusted R-squared:  0.8118 \r\nF-statistic: 106.7 on 2 and 47 DF,  p-value: < 2.2e-16\r\nconfint(lmSAT)\r\n                 2.5 %      97.5 %\r\n(Intercept) 949.908859 1037.754459\r\nx[, 1]        3.788291   20.784746\r\nx[, 2]       -3.283679   -2.418179\r\n\r\nWith nuisance predictors:\r\n\r\nlmSATAll<-lm(y~.,data=as.data.frame(cbind(y,x)))\r\nsummary(lmSATAll)\r\n\r\nCall:\r\nlm(formula = y ~ ., data = as.data.frame(cbind(y, x)))\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-61.485 -17.643   1.093  15.349  64.549 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 1002.601     22.231  45.100  < 2e-16 ***\r\nSpend          9.503      4.313   2.203   0.0342 *  \r\nPrcntTake     -2.703      0.228 -11.853 8.29e-14 ***\r\nxRand1         2.164      6.842   0.316   0.7536    \r\nxRand2        -5.190      5.015  -1.035   0.3078    \r\nxRand3         6.424      6.599   0.974   0.3370    \r\nxRand4        -5.678      4.899  -1.159   0.2542    \r\nxRand5         7.363      4.957   1.485   0.1464    \r\nxRand6         1.606      5.074   0.316   0.7536    \r\nxRand7         3.909      5.034   0.777   0.4426    \r\nxRand8         3.060      7.072   0.433   0.6679    \r\nxRand9        -4.654      4.397  -1.058   0.2971    \r\nxRand10      -10.265      4.816  -2.131   0.0401 *  \r\nxRand11       -2.912      5.252  -0.555   0.5827    \r\nxRand12        1.334      5.258   0.254   0.8013    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.51 on 35 degrees of freedom\r\nMultiple R-squared:  0.8733,    Adjusted R-squared:  0.8226 \r\nF-statistic: 17.23 on 14 and 35 DF,  p-value: 1.083e-11\r\nconfint(lmSATAll)[2:3,2]-confint(lmSATAll)[2:3,1]\r\n     Spend  PrcntTake \r\n17.5101430  0.9258323 \r\nconfint(lmSAT)[2:3,2]-confint(lmSAT)[2:3,1]\r\n    x[, 1]     x[, 2] \r\n16.9964551  0.8655003 \r\n\r\nThese also show that addition of nuisance parameters widened\r\nconfidence intervals.\r\nRidge and lasso\r\n\r\nset.seed(15)\r\ncv.outRidge=cv.glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,alpha=0)\r\nplot(cv.outRidge)\r\n\r\n(bestlam <-cv.outRidge$lambda.min)\r\n[1] 6.570761\r\nridgeFit<-glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,\r\n                 alpha=0,lambda=bestlam,standardize = F)\r\nridge.coef<-predict(ridgeFit,type=\"coefficients\",s=bestlam)\r\nset.seed(15)\r\ncv.outLasso=cv.glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,alpha=1)\r\nplot(cv.outLasso)\r\n\r\n(bestlam <-cv.outLasso$lambda.min)\r\n[1] 7.732551\r\nlassoFit<-glmnet(x=dataListExtraPredict$x,y=dataListExtraPredict$y,\r\n                 alpha=1,lambda=bestlam,standardize = F)\r\nlasso.coef<-predict(lassoFit,type=\"coefficients\",s=bestlam)\r\ncomparison<-round(cbind(summary(lmSATAll)$coefficients[,c(1,4)],\r\n                        summary(fit_noshrinkExtra)$summary[1:15,c(1,4,8)],\r\n                        summary(fit_shrinkExtra)$summary[1:15,c(1,4,8)],\r\n                        ridge.coef, lasso.coef),3)\r\ncomparison<-as.matrix(comparison)\r\ncolnames(comparison)<-c(\"LM\",\"LM-Pv\",\"NoShrink\",\"NoShrink-L\",\"NoShrink-H\",\r\n                        \"Shrink\",\"Shrink-L\",\"Shrink-H\",\"Ridge\",\"Lasso\")\r\ncomparison\r\n                  LM LM-Pv NoShrink NoShrink-L NoShrink-H   Shrink\r\n(Intercept) 1002.601 0.000  998.618    952.443   1044.877 1009.826\r\nSpend          9.503 0.034   10.234      1.234     19.261    8.220\r\nPrcntTake     -2.703 0.000   -2.721     -3.179     -2.255   -2.688\r\nxRand1         2.164 0.754    2.304    -11.451     15.735    1.164\r\nxRand2        -5.190 0.308   -4.729    -14.889      5.475   -1.160\r\nxRand3         6.424 0.337    6.250     -7.252     19.200    1.781\r\nxRand4        -5.678 0.254   -5.697    -15.225      3.796   -2.473\r\nxRand5         7.363 0.146    6.989     -2.925     16.843    2.769\r\nxRand6         1.606 0.754    1.908     -8.073     12.300    0.864\r\nxRand7         3.909 0.443    4.283     -6.012     14.542    1.370\r\nxRand8         3.060 0.668    2.652    -11.338     17.144   -1.396\r\nxRand9        -4.654 0.297   -4.222    -13.804      5.121   -1.002\r\nxRand10      -10.265 0.040  -10.363    -20.156     -0.763   -6.615\r\nxRand11       -2.912 0.583   -1.895    -12.602      8.866   -0.380\r\nxRand12        1.334 0.801    1.711     -8.974     12.114    0.770\r\n            Shrink-L Shrink-H    Ridge    Lasso\r\n(Intercept)  962.868 1058.289 1005.862 1027.326\r\nSpend         -0.280   18.239    8.932    5.032\r\nPrcntTake     -3.165   -2.244   -2.697   -2.607\r\nxRand1        -4.188    8.710    2.061    0.000\r\nxRand2        -8.523    3.088   -5.024    0.000\r\nxRand3        -2.483    9.021    5.004    0.000\r\nxRand4       -10.935    1.458   -5.064    0.000\r\nxRand5        -1.659   11.607    6.686    0.000\r\nxRand6        -2.571    7.022    1.916    0.000\r\nxRand7        -2.288    9.152    3.828    0.000\r\nxRand8       -10.120    5.874    2.090    0.000\r\nxRand9        -7.569    2.473   -4.497    0.000\r\nxRand10      -17.946    0.611   -9.411   -4.142\r\nxRand11       -5.870    3.183   -2.396    0.000\r\nxRand12       -3.804    6.300    0.807    0.000\r\n\r\nNote that there is no way to extract from ridge and lasso regressions\r\nany measure for comparison with zero, like confidence intervals.\r\nLinear model keeps both Spend and PrcntTake\r\nand removes with 5% level all nuisance coefficients except\r\nxRand10\r\nBayesian model without shrinkage does the same.\r\nBayesian model with shrinkage shrinks to zero all artificial predictors,\r\nbut it also removes Spend.\r\nRidge in general is consistent with linear model, but it is not clear if\r\nit shrinks any parameters to zero or not. Lasso fails to shrink to zero\r\nseveral artificial parameters.\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-02-01T13:44:21-06:00",
    "input_file": "Series-4-of-10---Fitting-Linear-Models_Multiple-Regression.knit.md"
  },
  {
    "path": "posts/2022-01-15-Bayesian methods - Series 3 of 10/",
    "title": "Series 3 of 10 -- How to Compare Two Groups with Robust Bayesian Estimation",
    "description": "4 years ago, the argument about the stop relying 100% on null hypothesis significance testing (NHST) which was the P-VALUE. A very appealing alternative to NHST is Bayesian statistics, which in itself contains many approaches to statistical inference. In this post, I provide an introductory and practical tutorial to Bayesian parameter estimation in the context of comparing two independent groups' data based on the adaption of UC's lecture and Kruschke's textbook (Chapter 16).",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Biostatistics",
      "Bayesian Methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nSetting to\r\ncompare two groups (w/ no predictors)\r\nIQ data\r\nData.\r\nPrepare data\r\n\r\nCall\r\nStan\r\nNormal\r\nassumption\r\nStan in\r\nregular way\r\nCoda\r\nShinystan\r\n\r\nRobust\r\nassumption\r\n\r\nComparison of the groups\r\nFrequentist probability\r\napproach\r\nBayesian\r\napproach\r\nPlot 95% HDI intervals of\r\ndifference\r\n\r\nFrequentist\r\nprobability approach to Markov chains\r\n\r\nReferences\r\n\r\nSetting to\r\ncompare two groups (w/ no predictors)\r\nIQ data\r\nOne example of two-groups problem is testing effect of a drug when\r\none group receives a placebo and another receives the drug. The response\r\nvariable is result of IQ test.\r\nIn this example we estimate mean and standard deviation for two\r\ngroups using normal distribution, then\r\nrobust \\(t\\)-distribution with common\r\nnormality parameter \\(\\lambda\\).\r\nThe diagram of the model structure shows\r\nhow the model needs to be coded.Data.\r\n\r\ndta <- read.csv(\"data/IQdrug.csv\")\r\nDT::datatable(dta)\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\"],[102,107,92,101,110,68,119,106,99,103,90,93,79,89,137,119,126,110,71,114,100,95,91,99,97,106,106,129,115,124,137,73,69,95,102,116,111,134,102,110,139,112,122,84,129,112,127,106,113,109,208,114,107,50,169,133,50,97,139,72,100,144,112,109,98,106,101,100,111,117,104,106,89,84,88,94,78,108,102,95,99,90,116,97,107,102,91,94,95,86,108,115,108,88,102,102,120,112,100,105,105,88,82,111,96,92,109,91,92,123,61,59,105,184,82,138,99,93,93,72],[\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Smart Drug\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\",\"Placebo\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Score<\\/th>\\n      <th>Group<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":1},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nPrepare data\r\n\r\ny <- as.numeric(dta[,\"Score\"])\r\nx <- as.numeric(as.factor(dta[,\"Group\"]))\r\n\r\n(xLevels <- levels(as.factor(dta[,\"Group\"])))\r\n[1] \"Placebo\"    \"Smart Drug\"\r\nNtotal = length(y)\r\n\r\n# Specify the data in a list for JAGS/Stan:\r\ndataList = list(\r\n    y = y,\r\n    x = x,\r\n    Ntotal = Ntotal,\r\n    meanY = mean(y),\r\n    sdY = sd(y)\r\n)\r\n\r\nCall Stan\r\n\r\n#source(\"../DBDA2Eprograms/DBDA2E-utilities.R\")\r\nlibrary(rstan)\r\noptions(mc.cores = parallel::detectCores())\r\nrstan_options(auto_write = TRUE)\r\n\r\nNormal assumption\r\nAssuming that both groups samples have normal distributions, estimate\r\nthe parameters and compare them.\r\nWrite description of the model for stan.\r\n\\[\r\ny_{i\\mid j} \\sim N(\\mu_j, \\sigma^2_j) \\\\\r\n\\mu_j \\sim N(\\bar{Y}, \\frac{1}{100*SD_Y^2}) \\\\\r\n\\sigma_j \\sim uniform(\\frac{SD_Y}{1000}, SD_Y*1000)\r\n\\]\r\n\r\n# Use the description for Stan from file \"ch16.2.stan\"\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    normalSigma = sdY*100;\r\n}\r\nparameters {\r\n    real mu[2];\r\n    real<lower=0> sigma[2];\r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi);\r\n    mu ~ normal(meanY, normalSigma);\r\n    for ( i in 1:Ntotal ) {\r\n        y[i] ~ normal(mu[x[i]] , sigma[x[i]]);\r\n    }\r\n}\r\n\"\r\n\r\nIf running the description in modelString for the first\r\ntime create stanDSONormal, otherwise reuse it.\r\n\r\nstanDsoNormal <- stan_model(model_code=modelString)\r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n#saveRDS(stanDsoNormal, file=\"data/DSONormal1.Rds\")\r\nstanDsoNormal<-readRDS(\"data/DSONormal1.Rds\")\r\n\r\nRun MCMC.\r\n\r\nparameters = c(\"mu\",\"sigma\")     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4\r\nthinSteps = 1\r\nnumSavedSteps = 5000\r\nstanFitNormal <- sampling(stanDsoNormal,\r\n                   data = dataList,\r\n                   pars = parameters, # optional\r\n                   chains = nChains,\r\n                   iter = (ceiling(numSavedSteps/nChains)*thinSteps+burnInSteps),\r\n                   warmup = burnInSteps,\r\n                   init = \"random\", # optional\r\n                   thin = thinSteps)\r\n\r\nSave the fit object for further analysis.\r\n\r\n# save(stanFitNormal,file=\"data/StanNormalFit2Groups.Rdata\")\r\nload(\"data/StanNormalFit2Groups.Rdata\")\r\n\r\nThen, we can explore the results.\r\nStan in regular way\r\n\r\n# text statistics:\r\nprint (stanFitNormal)\r\nInference for Stan model: a4cb1c74477f9a8b1e476a89a6556245.\r\n4 chains, each with iter=2250; warmup=1000; thin=1; \r\npost-warmup draws per chain=1250, total post-warmup draws=5000.\r\n\r\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nmu[1]     100.04    0.03 2.43   95.33   98.39  100.01  101.71  104.76\r\nmu[2]     107.79    0.05 3.36  101.37  105.50  107.78  110.00  114.44\r\nsigma[1]   18.34    0.03 1.81   15.28   17.07   18.22   19.44   22.38\r\nsigma[2]   26.02    0.03 2.44   21.77   24.31   25.80   27.58   31.23\r\nlp__     -423.27    0.03 1.47 -426.95 -424.00 -422.93 -422.20 -421.44\r\n         n_eff Rhat\r\nmu[1]     5453    1\r\nmu[2]     5035    1\r\nsigma[1]  5004    1\r\nsigma[2]  5772    1\r\nlp__      2598    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Mon Jan 30 00:57:34 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\n# estimates & hdi:\r\nplot(stanFitNormal)\r\n\r\n# samples\r\nrstan::traceplot(stanFitNormal, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFitNormal, pars=c('mu','sigma'))\r\n\r\nstan_scat(stanFitNormal, c('mu[1]','mu[2]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[1]','sigma[1]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[1]','sigma[2]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[2]','sigma[1]'))\r\n\r\nstan_scat(stanFitNormal, c('mu[2]','sigma[2]'))\r\n\r\nstan_scat(stanFitNormal, c('sigma[1]','sigma[2]'))\r\n\r\nstan_hist(stanFitNormal)\r\n\r\nstan_dens(stanFitNormal)\r\n\r\n# autocorrelation:\r\nstan_ac(stanFitNormal, separate_chains = T)\r\n\r\nstan_diag(stanFitNormal,information = \"sample\",chain=0)\r\n\r\nstan_diag(stanFitNormal,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(stanFitNormal,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(stanFitNormal,information = \"divergence\",chain = 0)\r\n\r\n\r\nCoda\r\nIf we prefer output using coda class, reformat the chains into\r\ncoda:\r\n\r\nlibrary(coda)\r\nstan2coda <- function(stanFitNormal) {\r\n    # apply to all chains\r\n    mcmc.list(lapply(1:ncol(stanFitNormal), function(x) mcmc(as.array(stanFitNormal)[,x,])))\r\n}\r\ncodaSamples <- stan2coda(stanFitNormal)\r\nsummary(codaSamples)\r\n\r\nIterations = 1:1250\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 1250 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n            Mean    SD Naive SE Time-series SE\r\nmu[1]     100.04 2.431  0.03438        0.03166\r\nmu[2]     107.79 3.357  0.04748        0.04742\r\nsigma[1]   18.34 1.806  0.02554        0.02516\r\nsigma[2]   26.02 2.441  0.03452        0.03252\r\nlp__     -423.27 1.465  0.02072        0.02860\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%     25%     50%     75%   97.5%\r\nmu[1]      95.33   98.39  100.01  101.71  104.76\r\nmu[2]     101.37  105.50  107.78  110.00  114.44\r\nsigma[1]   15.28   17.07   18.22   19.44   22.38\r\nsigma[2]   21.77   24.31   25.80   27.58   31.23\r\nlp__     -426.95 -424.00 -422.93 -422.20 -421.44\r\nplot(codaSamples)\r\n\r\nautocorr.plot(codaSamples)\r\n\r\neffectiveSize(codaSamples)\r\n   mu[1]    mu[2] sigma[1] sigma[2]     lp__ \r\n6113.262 5058.326 5239.336 5666.196 2646.550 \r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n         Point est. Upper C.I.\r\nmu[1]             1          1\r\nmu[2]             1          1\r\nsigma[1]          1          1\r\nsigma[2]          1          1\r\nlp__              1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\nplot(density(codaSamples[[1]][,1]),xlim=c(10,120),ylim=c(0,.25),main=\"Posterior Densities\")  # mu[1], 1st chain\r\nlines(density(codaSamples[[1]][,2]))                         # mu[2], 1st chain\r\nlines(density(codaSamples[[1]][,3]))                         # sigma[1], 1st chain\r\nlines(density(codaSamples[[1]][,4]))                         # sigma[2], 1st chain\r\nlines(density(codaSamples[[2]][,1]),col=\"red\")               # mu[1], 2nd chain\r\nlines(density(codaSamples[[2]][,2]),col=\"red\")               # mu[2], 2nd chain\r\nlines(density(codaSamples[[2]][,3]),col=\"red\")               # sigma[1], 2nd chain\r\nlines(density(codaSamples[[2]][,4]),col=\"red\")               # sigma[2], 2nd chain\r\n\r\n\r\nShinystan\r\nOr you can use shinystan to analyze fitted model\r\n\r\nlibrary(shinystan)\r\n\r\nLaunch shiny application with the loaded object.\r\n\r\nlaunch_shinystan(stanFitNormal)\r\n\r\nRobust assumption\r\nUse the robust assumption of Student-t distribution instead of normal\r\ndistribution.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    int x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    real expLambda ;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    normalSigma = sdY*100;\r\n    expLambda = 1/29.0;\r\n}\r\nparameters {\r\n    real<lower=0> nuMinusOne;\r\n    real mu[2] ; // 2 groups\r\n    real<lower=0> sigma[2] ; // 2 groups\r\n}\r\ntransformed parameters {\r\n    real<lower=0> nu ;\r\n    nu = nuMinusOne + 1 ;\r\n}\r\nmodel {\r\n    sigma  ~ uniform( unifLo , unifHi ) ; // vectorized 2 groups\r\n    mu ~ normal( meanY , normalSigma ) ; // vectorized 2 groups\r\n    nuMinusOne ~ exponential( expLambda ) ;\r\n    for ( i in 1:Ntotal ) {\r\n      y[i] ~ student_t(nu, mu[x[i]] ,sigma[x[i]]) ; // nested index of group\r\n    }\r\n}\r\n\"\r\n\r\nIf running the description in modelString for the first time create\r\nstanDSO, otherwise reuse it.\r\n\r\nstanDsoRobust <- stan_model(model_code=modelString) \r\n\r\nIf saved DSO is used load it, then run the chains.\r\n\r\n# saveRDS(stanDsoRobust,file=\"data/DSORobust1.Rds\")\r\nstanDsoRobust<-readRDS(file=\"data/DSORobust1.Rds\")\r\n\r\nIf necessary, initialize chains. Parameter init can be:\r\none of digit 0, string “0” or “random”, a function that returns a list,\r\nor a list of initial parameter values with which to indicate how the\r\ninitial values of parameters are specified.\r\n“0”: initialize all to be zero on the unconstrained support\r\n“random”: randomly generated\r\nlist: a list of lists equal in length to the number of chains\r\n(parameter chains), where each list in the list of lists * specifies the\r\ninitial values of parameters by name for the corresponding chain.\r\nfunction: a function that returns a list for specifying the initial\r\nvalues of parameters for a chain. The function can take an optional\r\nparameter chain_id.\r\nSince Stan has pretty complicated parameter tuning process during\r\nwhich among other parameters it selects initial values, it may be a good\r\nidea to let Stan select default initial parameters until we get enough\r\nexperience.\r\nRun MCMC.\r\n\r\nparameters = c( \"mu\" , \"sigma\" , \"nu\" )     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps<-5000\r\n# Get MC sample of posterior:\r\nstanFitRobust <- sampling( object=stanDsoRobust , \r\n                   data = dataList , \r\n                   pars = parameters , # optional\r\n                   chains = nChains ,\r\n                   cores=nChains,\r\n                   iter = (ceiling(numSavedSteps/nChains)*thinSteps\r\n                            + burnInSteps ) , \r\n                   warmup = burnInSteps , \r\n                   init = \"random\" , # optional\r\n                   thin = thinSteps )\r\n\r\nSave the fit object.\r\n\r\n# save(stanFitRobust,file=\"data/StanRobustFit2Groups.Rdata\")\r\nload(\"data/StanRobustFit2Groups.Rdata\")\r\n\r\nExplore the results.\r\n\r\nprint(stanFitRobust)\r\nInference for Stan model: 3b61ff01c98dc80a0961c8bae33b0b6e.\r\n4 chains, each with iter=2250; warmup=1000; thin=1; \r\npost-warmup draws per chain=1250, total post-warmup draws=5000.\r\n\r\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nmu[1]      99.28    0.03 1.76   95.89   98.11   99.32  100.47  102.63\r\nmu[2]     107.14    0.04 2.69  101.78  105.36  107.10  108.95  112.28\r\nsigma[1]   11.32    0.03 1.75    8.28   10.11   11.21   12.40   15.06\r\nsigma[2]   17.87    0.04 2.71   12.99   16.01   17.72   19.57   23.55\r\nnu          3.87    0.03 1.71    1.91    2.82    3.51    4.42    8.15\r\nlp__     -451.38    0.03 1.67 -455.61 -452.22 -451.03 -450.15 -449.20\r\n         n_eff Rhat\r\nmu[1]     4781    1\r\nmu[2]     5127    1\r\nsigma[1]  4350    1\r\nsigma[2]  4679    1\r\nnu        3694    1\r\nlp__      2314    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Mon Jan 30 01:00:14 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(stanFitRobust)\r\n\r\nrstan::traceplot(stanFitRobust, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFitRobust, pars=c('nu','mu','sigma'))\r\n\r\nstan_scat(stanFitRobust, c('nu','mu[1]'))\r\n\r\nstan_scat(stanFitRobust, c('nu','mu[2]'))\r\n\r\nstan_scat(stanFitRobust, c('nu','sigma[1]'))\r\n\r\nstan_scat(stanFitRobust, c('nu','sigma[2]'))\r\n\r\nstan_scat(stanFitRobust, c('mu[1]','sigma[1]'))\r\n\r\nstan_scat(stanFitRobust, c('sigma[1]','sigma[2]'))\r\n\r\n\r\nNote correlation between the sigma parameters.Is there’s maybe no sign of correlation in non-robust\r\napproach!How can you explain positive correlation?\r\nThe difference of scales (i.e., \\(\\sigma_2 −\r\n\\sigma_1\\)) shows a credible nonzero difference (in the\r\nbelow), suggesting that the smart drug causes greater variance than\r\nthe placebo.\r\n\r\nstan_hist(stanFitRobust)\r\n\r\nstan_dens(stanFitRobust)\r\n\r\nstan_ac(stanFitRobust, separate_chains = T)\r\n\r\nstan_diag(stanFitRobust,information = \"sample\",chain=0)\r\n\r\nstan_diag(stanFitRobust,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"divergence\",chain = 0)\r\n\r\n\r\n\r\nlaunch_shinystan(stanFitRobust)\r\n\r\nComparison of the groups\r\nFrequentist probability\r\napproach\r\nFor comparison decide whether the two groups are different or not,\r\nusing the Frequentist approach.\r\nUse t-test with unequal variances:\r\n\r\nqqnorm(y[x==1])\r\nqqline(y[x==1])\r\n\r\nqqnorm(y[x==2])\r\nqqline(y[x==2])\r\n\r\n\r\n\r\nt.test(Score ~ Group , data=dta, var.equal=FALSE, paired=FALSE)\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  Score by Group\r\nt = -1.958, df = 111.44, p-value = 0.05273\r\nalternative hypothesis: true difference in means between group Placebo and group Smart Drug is not equal to 0\r\n95 percent confidence interval:\r\n -15.70602585   0.09366161\r\nsample estimates:\r\n   mean in group Placebo mean in group Smart Drug \r\n                100.0351                 107.8413 \r\n\r\nResult: with 5% error I level the null (equality)\r\nhypothesis cannot be rejected.\r\nHowever, the case is not so clear: the p-value is at the marginal\r\n(very close to 5%). The sample size is pretty small (120\r\nobservations in both groups), the result is not very\r\nconclusive.\r\nThe test relies on the assumption that distributions of both samples are\r\nGaussian. As we can see in qq-plots this assumption does not hold.\r\nBayesian approach\r\nNow use Bayesian approach based on robust estimation.\r\nCreate matrices of combined chains for the two means and two standard\r\ndeviations of the robust fit.\r\n\r\nsummary(stanFitRobust)\r\n$summary\r\n                mean    se_mean       sd        2.5%         25%\r\nmu[1]      99.278479 0.02541479 1.757258   95.892836   98.106053\r\nmu[2]     107.135439 0.03759197 2.691692  101.782362  105.356701\r\nsigma[1]   11.324726 0.02646412 1.745481    8.283891   10.114247\r\nsigma[2]   17.871701 0.03961980 2.710205   12.992997   16.008138\r\nnu          3.865007 0.02819681 1.713714    1.906517    2.816141\r\nlp__     -451.375158 0.03470814 1.669462 -455.612524 -452.215782\r\n                 50%         75%       97.5%    n_eff      Rhat\r\nmu[1]      99.318108  100.465290  102.634056 4780.774 1.0001983\r\nmu[2]     107.104442  108.951046  112.281358 5126.967 1.0002329\r\nsigma[1]   11.214309   12.404448   15.055811 4350.261 0.9992927\r\nsigma[2]   17.718447   19.571494   23.554055 4679.288 0.9999055\r\nnu          3.506651    4.418275    8.154027 3693.830 1.0003792\r\nlp__     -451.034452 -450.145225 -449.198416 2313.613 1.0004085\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n          stats\r\nparameter        mean       sd        2.5%         25%         50%\r\n  mu[1]      99.31321 1.794266   95.953253   98.116266   99.321257\r\n  mu[2]     106.96172 2.739928  101.452693  105.249842  106.924883\r\n  sigma[1]   11.33591 1.739136    8.295615   10.138611   11.217442\r\n  sigma[2]   17.87345 2.746624   13.081372   15.908501   17.549351\r\n  nu          3.89781 1.758589    1.897855    2.791728    3.522191\r\n  lp__     -451.39555 1.654445 -455.559983 -452.246843 -451.069865\r\n          stats\r\nparameter          75%       97.5%\r\n  mu[1]     100.487503  102.712333\r\n  mu[2]     108.803910  112.141803\r\n  sigma[1]   12.430281   15.144620\r\n  sigma[2]   19.648503   23.691793\r\n  nu          4.469162    8.370698\r\n  lp__     -450.199223 -449.224826\r\n\r\n, , chains = chain:2\r\n\r\n          stats\r\nparameter         mean       sd        2.5%         25%         50%\r\n  mu[1]      99.359740 1.727473   96.002888   98.183860   99.424128\r\n  mu[2]     107.205724 2.710996  101.960121  105.333992  107.264773\r\n  sigma[1]   11.328668 1.715013    8.365028   10.140853   11.193846\r\n  sigma[2]   17.821753 2.637216   13.271496   15.984582   17.685629\r\n  nu          3.789725 1.446564    1.982413    2.796458    3.463222\r\n  lp__     -451.324191 1.638725 -455.461092 -452.162620 -450.963830\r\n          stats\r\nparameter          75%       97.5%\r\n  mu[1]     100.586306  102.610716\r\n  mu[2]     109.024107  112.217721\r\n  sigma[1]   12.358057   15.070945\r\n  sigma[2]   19.416344   23.651606\r\n  nu          4.371135    7.443861\r\n  lp__     -450.105134 -449.218868\r\n\r\n, , chains = chain:3\r\n\r\n          stats\r\nparameter         mean       sd        2.5%         25%         50%\r\n  mu[1]      99.241089 1.715029   95.621161   98.141599   99.310472\r\n  mu[2]     107.198150 2.761142  101.738884  105.413202  107.184480\r\n  sigma[1]   11.324729 1.786161    8.129510   10.087825   11.237923\r\n  sigma[2]   17.834568 2.789681   12.584832   16.032342   17.729390\r\n  nu          3.847617 1.677111    1.854929    2.795824    3.477059\r\n  lp__     -451.449925 1.738399 -455.821321 -452.303253 -451.091065\r\n          stats\r\nparameter          75%      97.5%\r\n  mu[1]     100.432571  102.53221\r\n  mu[2]     109.058140  112.43690\r\n  sigma[1]   12.455750   15.07465\r\n  sigma[2]   19.478207   23.53294\r\n  nu          4.352777    8.40313\r\n  lp__     -450.148846 -449.15265\r\n\r\n, , chains = chain:4\r\n\r\n          stats\r\nparameter         mean       sd        2.5%         25%         50%\r\n  mu[1]      99.199874 1.788565   95.891846   97.960787   99.213313\r\n  mu[2]     107.176156 2.544973  102.262461  105.526175  107.097533\r\n  sigma[1]   11.309597 1.742850    8.295669   10.087199   11.227140\r\n  sigma[2]   17.957035 2.665734   13.114103   16.071792   17.869635\r\n  nu          3.924874 1.935407    1.912542    2.882155    3.564496\r\n  lp__     -451.330962 1.643141 -455.604148 -452.095591 -450.986033\r\n          stats\r\nparameter         75%       97.5%\r\n  mu[1]     100.36449  102.632627\r\n  mu[2]     108.81906  112.363129\r\n  sigma[1]   12.38710   14.913067\r\n  sigma[2]   19.68608   23.495363\r\n  nu          4.44298    8.213977\r\n  lp__     -450.10343 -449.228235\r\n\r\n\r\ny.dis1<-cbind(Mu=rstan::extract(stanFitRobust,pars=\"mu[1]\")$'mu[1]',\r\n            Sigma=rstan::extract(stanFitRobust,pars=\"sigma[1]\")$'sigma[1]')\r\ny.dis2<-cbind(Mu=rstan::extract(stanFitRobust,pars=\"mu[2]\")$'mu[2]',\r\n            Sigma=rstan::extract(stanFitRobust,pars=\"sigma[2]\")$'sigma[2]')\r\nden.Dis1<-density(y.dis1[, \"Mu\"])\r\nden.Dis2<-density(y.dis2[, \"Mu\"])\r\nplot(den.Dis1,col=\"blue\", xlim=c(90,120), main=\"Compare 2 distributions\")\r\nlines(den.Dis2,col=\"red\")\r\n\r\n\r\nTraditional Bayesian approach: look at HDIs.\r\n\r\nlibrary(HDInterval)\r\n\r\n\r\nhdi(cbind(y.dis1[,1], y.dis2[,1]), credMass=.9)\r\n           [,1]     [,2]\r\nlower  96.40157 102.6255\r\nupper 102.14760 111.4564\r\nattr(,\"credMass\")\r\n[1] 0.9\r\n\r\n\r\nhdi(cbind(y.dis1[,2], y.dis2[,2]), credMass=.85)\r\n          [,1]     [,2]\r\nlower  8.79751 13.88659\r\nupper 13.71127 21.54786\r\nattr(,\"credMass\")\r\n[1] 0.85\r\n\r\nThe 95% HDI intervals overlap for both parameters, but with reduced\r\ncredible mass level they can be distinguished.\r\nPlot 95% HDI intervals of\r\ndifference\r\nMean difference\r\n\r\nparamSampleVec <- y.dis2[,1] - y.dis1[,1]\r\nplotPost(paramSampleVec=paramSampleVec)\r\n\r\n\r\nScale difference\r\n\r\nparamSampleVec <- y.dis2[,2] - y.dis1[,2]\r\nplotPost(paramSampleVec=paramSampleVec)\r\n\r\n\r\nFrequentist\r\nprobability approach to Markov chains\r\nApply Frequentist approach to the chain samples.\r\nFirst, check if the samples for two standard deviations are\r\nsignificantly different or not:\r\n\r\nc(mean(y.dis1[,2]),mean(y.dis2[,2])) # mean values\r\n[1] 11.32473 17.87170\r\nc(sd(y.dis1[,2]),sd(y.dis2[,2]))     # standard deviations of samples of MCMC standard deviations\r\n[1] 1.745481 2.710205\r\nks.test(y.dis1[,2],y.dis2[,2])       # Kolmogorov-Smirnov test for posterior distributions of standard deviations\r\n\r\n    Two-sample Kolmogorov-Smirnov test\r\n\r\ndata:  y.dis1[, 2] and y.dis2[, 2]\r\nD = 0.862, p-value < 2.2e-16\r\nalternative hypothesis: two-sided\r\nden<-density(y.dis2[,2])\r\nplot(density(y.dis1[,2]),xlim=c(5,30))\r\nlines(den$x,den$y,col=\"red\")\r\n\r\nt.test(y.dis1[,2],y.dis2[,2], var.equal=F, paired=FALSE) #t-test for means of posterior distributions for standard deviations\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  y.dis1[, 2] and y.dis2[, 2]\r\nt = -143.61, df = 8537.3, p-value < 2.2e-16\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -6.636341 -6.457609\r\nsample estimates:\r\nmean of x mean of y \r\n 11.32473  17.87170 \r\n\r\nCheck shapes of the distributions of the mean and standard deviation\r\nparameters. How different are they between control and treated\r\ngroups?\r\nFor standard deviations:\r\n\r\nqqnorm(y.dis1[,2])  # control\r\nqqline(y.dis1[,2])\r\n\r\nqqnorm(y.dis2[,2])  # treatment\r\nqqline(y.dis2[,2])\r\n\r\n\r\nFor mean values:\r\n\r\nqqnorm(y.dis1[,1])  #control\r\nqqline(y.dis1[,1])\r\n\r\nqqnorm(y.dis2[,1])  # treatment\r\nqqline(y.dis2[,1])\r\n\r\n\r\nComparison of mean and standard deviations of the posterior sample\r\nfor standard deviations, Kolmogorov-Smirnov test, density plots and\r\nt-test for the two samples all indicate that the variances of the two\r\ngroups are different.\r\nThis means that we cannot apply ANOVA to compare the two group mean\r\nvalues directly.\r\nTry t-test for the two means of the posterior distributions with\r\ndifferent variances:\r\n\r\nt.test(y.dis1[,1],y.dis2[,1], var.equal=F, paired=FALSE)\r\n\r\n    Welch Two Sample t-test\r\n\r\ndata:  y.dis1[, 1] and y.dis2[, 1]\r\nt = -172.83, df = 8605.2, p-value < 2.2e-16\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -7.946073 -7.767847\r\nsample estimates:\r\nmean of x mean of y \r\n 99.27848 107.13544 \r\n\r\nThe null hypothesis of equality of the means of the posterior\r\ndistributions for the mean values of the two groups decisively rejected:\r\nwe are testing with a lot longer samples.\r\nPlot the images of the two groups in the mean-standard deviation\r\nparameter space:\r\n\r\nplot(y.dis1,xlim=c(92,118),ylim=c(5,33),col=\"red\",xlab=\"Mean\",ylab=\"St. Dev.\")\r\npoints(y.dis2,col=\"blue\")\r\n\r\n\r\nWe can see that by using at least 2 different methods proving that\r\nthere is a significant difference between the 2 groups shown on the\r\nplot.\r\nReferences\r\nJohn K. Kruschke, Journal of Experimental Psychology: General, 2013,\r\nv.142(2), pp.573-603. (doi: 10.1037/a0029146), website\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial\r\nwith r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic Press is\r\nan imprint of Elsevier.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-03T10:27:56-06:00",
    "input_file": "Series-3-of-10---Comparing-2-Groups.knit.md"
  },
  {
    "path": "posts/2022-01-01-Bayesian methods - Series 2 of 10/",
    "title": "Series 2 of 10 -- MCMC for Estimation of Gaussian parameters",
    "description": "Run MCMC on binomial model Gaussian distribution, one sample Hierarchical model, two groups of Gaussian observations",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2022-01-01",
    "categories": [
      "Biostatistics",
      "Bayesian Methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nReview of MCMC: binomial\r\nmodel\r\nData\r\nRunning MCMC\r\nChecking\r\nresults\r\nAnalytical solution\r\nHistograms of\r\naccepted and rejected values\r\n\r\n\r\nGaussian\r\ndistribution, one sample: known variance, unknown mean\r\nData\r\nFNP approach\r\nBayesian approach by\r\nformula, weak prior\r\nBayesian approach by\r\nformula, strong prior\r\nUsing\r\nJAGS to estimate mean of normal distribution with known variance\r\nData\r\nPreparation of the model for\r\nJAGS\r\nInitializing Markov chains\r\nSending information to\r\nJAGS\r\nRunning MCMC in JAGS:\r\nburn in and main run\r\nAnalyzing convergence\r\nAnalyzing and\r\ninterpreting the results\r\n\r\n\r\nHierarchical\r\nmodel: two groups of Gaussian observations\r\nData\r\nDifferent mean values,\r\ncommon weak prior\r\nModel\r\ndescription\r\nInitialization and\r\nsending the model to JAGS\r\nRunning the\r\nmodel\r\nAnalysis\r\n\r\nExperimenting with\r\nprior hyperparameters\r\nRun: hypeMean=130;\r\nhypeSD=0.2\r\nRun: hypeMean=160;\r\nhypeSD=0.2\r\n\r\nFNP\r\napproach: ANOVA\r\n\r\nFurther\r\nreading\r\n\r\nReview of MCMC: binomial\r\nmodel\r\nThe diagram of the model structure:\r\nsimplest binomial model with conjugate beta prior (adapted from\r\nTextbook).Model description contains 2 parts:\r\n\\[y_i \\sim \\text{Binom} (p = \\theta, \\\r\nsize = 1) \\\\\r\n\\theta \\sim \\text{Beta}(A_{prior},B_{prior})\\]\r\nwhere \\(\\theta\\) is the unknown\r\nprobability of success with prior beta distribution with fixed\r\nparameters \\(A,\\ B\\).\r\nTo check the results of simulation use analytical formulas for\r\nposterior distribution: if prior beta distribution has parameters \\(A_{prior},B_{prior}\\) and the data contain\r\nnumber of successes \\(s\\),\r\n\\[s = \\sum_{i=1}^k y_i\\]\r\nout of \\(k\\) observations then\r\nposterior beta distribution has parameters\r\n\\[A_{post} = A_{prior} + s; \\ B_{post} =\r\nB_{prior} + k − s\\]\r\nData\r\nCreate vector of \\(0\\) and \\(1\\) of length \\(k=20\\) and probability of success \\(\\theta = 0.6\\)\r\n\r\n# Y <- rbinom(41, size=1, p=.32)\r\nflips = 41\r\nheads = 13\r\na <- 10; b <- 10\r\n\r\nRecall that Metropolis-Hastings MCMC algorithm\r\nconsists of the following steps:\r\nGenerate new proposal using some convenient\r\ndistribution.\r\nDecide whether the proposal theta_prop is accepted\r\nor rejected\r\nRunning MCMC\r\n\r\nTo do that follow the logic:\r\nIf posterior probability density at theta_prop is\r\ngreater than posterior probability density at theta_curr\r\nthen accept theta_prop into the chain sample.\r\nIf posterior probability density at theta_prop is\r\nsmaller than posterior probability density at theta_curr\r\nthen accept theta_prop with probability\r\n\\[p_{dec} = \\frac{p(\\theta_{prop} \\mid\r\ny)}{p(\\theta_{curr} \\mid y)} = \\frac{p(y \\mid \\theta_{prop}) \\\r\np(\\theta_{prop} \\mid A, B)}{p(y \\mid \\theta_{curr}) \\ p(\\theta_{curr}\r\n\\mid A, B)} \\]\r\nand reject theta_prop with probability\r\n$1-p_{dec}$.\r\nAcceptance decision is made by simulating uniform random variable on\r\n\\(U_{[0,1]}\\) and comparing it with\r\n\\(p_{dec}\\). If \\(U < p_{dec}\\) then\r\ntheta_prop is accepted.\r\n\r\nmetropolis_algorithm <- function(samples, theta_seed, sd){\r\n   theta_curr <- theta_seed\r\n   # Create vector of NAs to store sampled parameters\r\n   posterior_thetas <- rep(NA, times = samples)\r\n   for (i in 1:samples){\r\n      # Typically new proposals are generated from Gaussian distribution centered at the current value \r\n      # of Markov chain with sufficiently small variance.\r\n      theta_prop <- rnorm(n = 1, mean = theta_curr, sd = sd) # Proposal distribution\r\n      # If the proposed parameter is outside its range then set it equal to its current\r\n      # value. Otherwise keep the proposed value\r\n      theta_prop <- ifelse((theta_prop < 0 | theta_prop > 1), theta_curr, theta_prop)\r\n      \r\n      # Bayes' numerators\r\n      posterior_prop <- dbeta(theta_prop, shape1 = a, shape2 = b) * dbinom(heads, size = flips, prob = theta_prop)\r\n      posterior_curr <- dbeta(theta_curr, shape1 = a, shape2 = b) * dbinom(heads, size = flips, prob = theta_curr)\r\n      # Calculate probability of accepting\r\n      p_accept_theta_prop <- min(posterior_prop/posterior_curr, 1.0)\r\n      rand_unif <- runif(n = 1)\r\n      # Probabilistically accept proposed theta\r\n      theta_select <- ifelse(p_accept_theta_prop > rand_unif, theta_prop, theta_curr)\r\n      posterior_thetas[i] <- theta_select\r\n      # Reset theta_curr for the next iteration of the loop\r\n      theta_curr <- theta_select\r\n   }\r\n   return(posterior_thetas)\r\n}\r\n\r\nChecking results\r\n\r\nset.seed(12423)\r\nposterior_thetas <- metropolis_algorithm(samples = 10000, theta_seed = 0.9, sd = 0.05)\r\n\r\nAfter simulating Markov chain check the results.\r\nAnalytical solution\r\nCompare prior parameters and posterior parameters given by\r\nformulas\r\nIt was due to since the beta distribution is conjugate to the\r\nbinomial distribution (above), we can see how close our sampled\r\nposterior distribution approximates the exact posterior for \\(\\theta\\). Recall that with a \\(Beta(10, 10)\\) prior and with \\(13\\) heads in \\(41\\) flips, the posterior distribution is\r\nalso beta distributed with \\(a =\r\n10+13\\) and \\(b = 10+(41−13)\\).\r\nSo, let’s overlay the exact posterior distribution on our previous graph\r\nalso.\r\n\r\nopar <- par()\r\npar(mar=c(2.5,3.5,3,2.1), mgp = c(1.7, 1, 0))\r\nd <- density(posterior_thetas)\r\nplot(d,\r\n     main = expression(paste('Kernel Density Plot for ', theta)),\r\n     xlab = expression(theta),\r\n     ylab = 'density',\r\n     yaxt = 'n',\r\n     cex.lab = 1.3\r\n     )\r\npolygon(d, col='dodgerblue1', border='dark blue')\r\n\r\nexactb <- rbeta(n = 10000, shape1 = 23, shape2 = 38)\r\nlines(density(exactb),                             # Plot of randomly drawn beta density\r\n     type = 's', col = 'red')\r\n\r\n\r\n\r\nhdi(posterior_thetas, credMass = 0.95)\r\n    lower     upper \r\n0.2563125 0.4986970 \r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nHistograms of\r\naccepted and rejected values\r\nGaussian\r\ndistribution, one sample: known variance, unknown mean\r\nData\r\nSimulate single sample from Gaussian distribution with\r\nunknown mean \\(\\theta\\)\r\nand known standard deviation \\(\\sigma\\).\r\n\r\nmu <- 120  \r\nsi <- 25\r\nnSample <- 200\r\nset.seed(05152022)\r\nY <- rnorm(nSample, mean = mu, sd = si)\r\nTheoretical.data <- c(Mean = mu, Sd = si)\r\nplot(Y)\r\n\r\n\r\nFNP approach\r\nEstimate mean of the distribution, check histogram, test hypothesis\r\n\\(H_0\\): \\(\\mu = 120\\) against two-sided alternative\r\n\\(H_a\\): \\(\\mu \\neq 120\\).\r\n\r\nmeanMLE <- mean(Y)\r\nhist(Y, freq = F)\r\n\r\nplot(density(Y))\r\n\r\n\r\n\r\n(zStat <- (meanMLE - Theoretical.data[\"Mean\"])/(Theoretical.data[\"Sd\"]/sqrt(nSample)))\r\n     Mean \r\n0.3157925 \r\n(sSidedP <- pnorm(zStat,lower.tail = F) + pnorm(-zStat,lower.tail = T))\r\n   Mean \r\n0.75216 \r\n\r\nFinally, distribution of the data estimated by FNP:\r\n\r\n(MLE.data <- c(Mean = meanMLE, Sd = Theoretical.data[\"Sd\"]))\r\n    Mean    Sd.Sd \r\n120.5582  25.0000 \r\n\r\nBayesian approach by\r\nformula, weak prior\r\nSet the model as \\[y_i \\sim N(\\theta,\r\n\\sigma)\\]\r\nwhere \\(\\theta\\) is unknown and\r\n\\(\\sigma = 25\\).\r\nSet prior distribution for the mean value as low informative Gaussian\r\nwith parameters \\(M = 100\\), \\(\\sigma_{\\mu} = 200\\), i.e. \r\n\\[ \\theta \\sim N(M,\r\n\\sigma_{\\mu})\\]\r\n\r\nM <- 100\r\npriorParamWeak <- c(Mean = M, Sd = 200)\r\n\r\nObtain posterior distribution using formulas for conjugate Gaussian\r\ndistribution:\r\n\\[ \\mu_{post} = \\frac{\\mu_{prior}\r\n\\gamma_{prior} + \\bar{y} \\gamma_{data}}{\\gamma_{prior} + \\gamma_{data}}\r\n= \\frac{M \\gamma_{prior} + \\bar{y} \\gamma_{data}}{\\gamma_{prior} +\r\n\\gamma_{data}} = M \\frac{\\gamma_{prior}}{\\gamma_{prior} + \\gamma_{data}}\r\n+ \\bar{y} \\frac{\\gamma_{data}}{\\gamma_{prior} + \\gamma_{data}}\r\n\\]\r\n\\[ \\sigma_{post}^2 =\r\n\\frac{1}{\\gamma_{prior} + \\gamma_{data}} \\]\r\nwhere \\(n\\) is the sample size and\r\n\\(\\gamma_{data}\\) and \\(\\gamma_{prior}\\) are data precision and\r\nprior precision, correspondingly:\r\n\\[ \\gamma_{data} ; \\gamma_{prior} =\r\n\\frac{1}{\\sigma_{\\mu}^2} \\]\r\n\r\nprecision_prior <- 1/200^2\r\nprecision_data <- nSample/25^2\r\npSigmasq <- 1/(precision_prior + precision_data)\r\npSd <- sqrt(pSigmasq)\r\npMean <- 100*precision_prior*pSigmasq + meanMLE*precision_data*pSigmasq\r\n(posterParamWeak <- c(Mean = pMean, Sd = pSd))\r\n      Mean         Sd \r\n120.556641   1.767698 \r\n\r\nBayesian estimate is obtained as \\(\\mu_{post}\\) = 120.5566415, showing no\r\ncompromise between likelihood 120.5582475 and prior 100.\r\nPlot theoretical (simulated), estimated by MLE and estimated by\r\nBayesian analysis densities.\r\n\r\nBayes.dataWeak <- c(posterParamWeak[\"Mean\"], Theoretical.data[\"Sd\"])\r\nX <- seq(from = 115, to = 125, by = 1)\r\nplot(X, dnorm(X, Theoretical.data[1], Theoretical.data[2]),\r\n     type=\"l\", xlab=\"X\", ylab=\"Density\")\r\nlines(X, dnorm(X,meanMLE,si), col=\"orange\", lwd=3)\r\nlines(X, dnorm(X,posterParamWeak[1],si), col=\"blue\")\r\nlegend(\"topright\",\r\n       legend = c(\"Theoretical\", \"MLE\", \"Bayesian\"),\r\n       col = c(\"black\", \"orange\", \"blue\"),\r\n       lty = 1)\r\n\r\n\r\nFor the sample length \\(200\\) and\r\nlow informative prior Bayesian estimate is very close to MLE in\r\ncomparison with the prior. In addition standard deviation of the\r\nposterior distribution collapsed from \\(200\\) to 1.7676979.\r\n\r\nrbind(Theoretical.data, MLE.data, Bayes.dataWeak)\r\n                     Mean Sd\r\nTheoretical.data 120.0000 25\r\nMLE.data         120.5582 25\r\nBayes.dataWeak   120.5566 25\r\nrbind(priorParamWeak, posterParamWeak)\r\n                    Mean         Sd\r\npriorParamWeak  100.0000 200.000000\r\nposterParamWeak 120.5566   1.767698\r\n\r\nBayesian approach by\r\nformula, strong prior\r\nRepeat Bayesian analysis with stronger prior \\(M = 100, \\  \\sigma_{\\mu} = 2\\).\r\n\r\npriorParamStrong <- c(Mean = M, Sd = 2)\r\nprecision_prior <- 1/2^2\r\nprecision_data <- nSample/25^2\r\npSigmasq <- 1/(precision_prior + precision_data)\r\npSd <- sqrt(pSigmasq)\r\npMean <- 100*precision_prior*pSigmasq + meanMLE*precision_data*pSigmasq\r\nposterParamStrong <- c(Mean = pMean, Sd = pSd)\r\nrbind(priorParamStrong, posterParamStrong)\r\n                      Mean       Sd\r\npriorParamStrong  100.0000 2.000000\r\nposterParamStrong 111.5415 1.324532\r\n\r\nBayesian estimate of the mean shifted towards mode of the prior\r\ndistribution: now the mean of posterior distribution is obtained by\r\n111.5414723\r\nShift of the lower level parameter towards mode of the higher level\r\nparameter is shrinkage.\r\nUsing\r\nJAGS to estimate mean of normal distribution with known variance\r\nData\r\nPrepare the data list dataList in the format required by\r\nJAGS\r\n\r\ndataList <- list(\"Y\" = Y, #data vector\r\n                 \"Ntotal\" = nSample) # length of the sample\r\n\r\nPreparation of the model for\r\nJAGS\r\nThe model diagram: \r\nInterpretation of the diagram starts from the bottom. Each arrow of\r\nthe diagram corresponds to a line of description code for the model.\r\nLine 1 of the model describes generation of the data according to the\r\nlikelihood function: \\(y_i \\sim N(\\mu,\r\n\\sigma)\\).\r\nLine 2 of the model describes generation of the parameter θ from the\r\nprior distribution: \\(\\mu \\sim N(M,\r\n\\sigma_{\\mu})\\).\r\nIn this case parameters of the prior distribution should be defined. For\r\nexample, like above set \\(M = 100,\r\n\\sigma_{\\mu} = 200\\).\r\nThe format of the model description for JAGS is a string of the\r\nform:\r\n\r\nmodel1NormalSample= \"\r\n  model { \r\n    # Description of data\r\n    for (i in 1:Ntotal){\r\n      Y[i] ~ dnorm(mu, 1/sigma^2) #tau = 1/sigma^2\r\n    }\r\n    # Description of prior\r\n    mu ~ dnorm(100 , Ntotal/200^2)\r\n    #tau <- pow(sigma, -2) #JAGS uses precision\r\n    sigma <- 25\r\n  }\r\n\"\r\n\r\nData are described as for loop.\r\nPrior is described as typical formula:\r\n“lower order parameter $\\sim$ density(higher order\r\nparameters)”.\r\nNote. In JAGS normal distribution is specified with\r\nmean value and precision, i.e. instead of\r\n$dnorm(\\mu, \\sigma)$ use\r\n$dnorm(\\mu, \\frac{1}{\\sigma^2})$.\r\nNote that variables names for the data and data length\r\nY, nSample in the description have to match\r\nthe names of the data list.\r\nThe next step of preparation of the model description for JAGS is\r\nsaving it to a temporary text file Tempmodel.txt.\r\n\r\nwriteLines(model1NormalSample, con=\"Tempmodel.txt\")\r\n\r\nInitializing Markov chains\r\nTo initialize trajectories define a list of lists with several values\r\nor create a function that returns an init value every time it is\r\ncalled.\r\nJAGS will call this function when it starts a new chain.\r\nGeneral syntax of initiation function is:\r\n\r\ninitsList<-function() {\r\n  # Definition of mu and sigma Init\r\n  upDown<-sample(c(1,-1),1)\r\n  m <- mean(Y)*(1+upDown*.05)\r\n   \r\n  list(mu = m)\r\n}\r\n\r\nFor example, select starting point from normal distribution centered\r\nat MLE with some reasonable standard deviation.\r\nCheck how initiation works by running the function several times.\r\n\r\ninitsList()\r\n$mu\r\n[1] 126.5862\r\n\r\nSending information to JAGS\r\nNext step is getting all information to JAGS using\r\njags.model() from library rjags.\r\nThis function transfers the data, the model specification and the\r\ninitial values to JAGS and requests JAGS to select appropriate sampling\r\nmethod.\r\n\r\njagsModel <- jags.model(file = \"TempModel.txt\", data = dataList, n.chains = 3, n.adapt = 500)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 200\r\n   Unobserved stochastic nodes: 1\r\n   Total graph size: 211\r\n\r\nInitializing model\r\n\r\nIn jags.model parameter n.chains specifies\r\nthe number of chains to run (defaults to 1) and parameter\r\nn.adapt sets the number of steps JAGS can take to tune the\r\nsampling method (defaults to 1000).\r\nThe object returned by jags.model contains all\r\ninformation that we need to communicate to JAGS about the problem in the\r\nformat suitable for JAGS.\r\nRunning MCMC in JAGS:\r\nburn in and main run\r\nNow run JAGS chains for 600 steps to complete burn in,\r\ni.e. transition to a stable distribution of Markov chain.\r\n\r\nupdate(jagsModel,n.iter=600)\r\n\r\nAfter completing burn in generate MCMC trajectories representing the\r\nposterior distribution for the model.\r\n\r\ncodaSamples <- coda.samples(jagsModel, variable.names=c(\"mu\"), n.iter = 3334)\r\nlist.samplers(jagsModel)\r\n$`bugs::ConjugateNormal`\r\n[1] \"mu\"\r\nhead(codaSamples)\r\n[[1]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n           mu\r\n[1,] 122.0174\r\n[2,] 120.6580\r\n[3,] 121.8346\r\n[4,] 121.1977\r\n[5,] 122.0095\r\n[6,] 123.1905\r\n[7,] 121.9572\r\n\r\n[[2]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n           mu\r\n[1,] 118.4939\r\n[2,] 118.1665\r\n[3,] 119.8600\r\n[4,] 123.0622\r\n[5,] 119.9395\r\n[6,] 121.2756\r\n[7,] 120.5576\r\n\r\n[[3]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n           mu\r\n[1,] 118.0154\r\n[2,] 121.2776\r\n[3,] 119.6511\r\n[4,] 119.7407\r\n[5,] 119.4734\r\n[6,] 121.0419\r\n[7,] 121.6894\r\n\r\nattr(,\"class\")\r\n[1] \"mcmc.list\"\r\n\r\nBesides the model specification object coda.samples()\r\ntakes a vector of character strings corresponding to the names of\r\nparameters to record variable.names and the number of\r\niterations to run n.iter.\r\nIn this example there are 3 chains, to the total number of iterations\r\nwill be about 10,000.\r\nUse function list.samplers to show the samplers applied to\r\nthe model.\r\nAnalyzing convergence\r\nAnalyze convergence of chains using following tools:\r\nSummary\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 601:3934\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 3334 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean             SD       Naive SE Time-series SE \r\n     120.21874        1.74212        0.01742        0.01764 \r\n\r\n2. Quantiles for each variable:\r\n\r\n 2.5%   25%   50%   75% 97.5% \r\n116.9 119.0 120.2 121.4 123.6 \r\n\r\nTraceplot\r\n\r\ncoda::traceplot(codaSamples)\r\n\r\ndensplot(codaSamples)\r\n\r\nplot(codaSamples)\r\n\r\n\r\nAutocorrelation and effective size\r\n\r\nautocorr.plot(codaSamples,ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n      mu \r\n9761.497 \r\n\r\nThe ESS number is consistent with the autocorrelation\r\nfunction.\r\nShrink factor\r\n\r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n   Point est. Upper C.I.\r\nmu          1       1.01\r\ngelman.plot(codaSamples)\r\n\r\n\r\nAnalyzing and\r\ninterpreting the results\r\nFind the mean values and standard deviations of posterior\r\ndistributions corresponding to different chains and compare them with\r\nMLE:\r\n\r\nchainMeans <- lapply(codaSamples,function(z) mean(z))\r\nchainSd <- lapply(codaSamples,function(z) sd(z))\r\nrbind(Means = chainMeans, SD = chainSd)\r\n      [,1]     [,2]     [,3]    \r\nMeans 120.1994 120.2765 120.1803\r\nSD    1.752507 1.755037 1.717595\r\n\r\nCompare the posterior densities generated by 3 chains with analytical\r\nposterior distribution:\r\n\r\n(l<-min(unlist(codaSamples))-.05)\r\n[1] 112.8358\r\n(h<-max(unlist(codaSamples))+.05)\r\n[1] 127.7117\r\nhistBreaks<-seq(l,h,by=.05)\r\npostHist<-lapply(codaSamples,hist,breaks=histBreaks)\r\n\r\n\r\n\r\nplot(postHist[[1]]$mids,postHist[[1]]$density,type=\"l\",\r\n     col=\"black\",lwd=2,ylab=\"Distribution Density\",xlab=\"Theta\")\r\nlines(postHist[[2]]$mids,postHist[[2]]$density,type=\"l\",col=\"red\",lwd=2)\r\nlines(postHist[[3]]$mids,postHist[[3]]$density,type=\"l\",col=\"blue\",lwd=2)\r\nlines(postHist[[3]]$mids,\r\n      dnorm(postHist[[3]]$mids,posterParamWeak[\"Mean\"],posterParamWeak[\"Sd\"]),\r\n      type=\"l\",col=\"green\",lwd=3)\r\nlegend(\"topright\",legend=c(\"Chain1\",\"Chain2\",\"Chain3\",\"Theoretical\"),col=c(\"black\",\"red\",\"blue\",\"green\"),lwd=2)\r\n\r\n\r\nHierarchical\r\nmodel: two groups of Gaussian observations\r\nConsider now two groups of Gaussian observations with unknown means\r\n\\(\\mu_1, \\mu_2\\) and same known\r\nstandard deviation \\(\\sigma = 25\\).\r\nData\r\nCreate data, prepare them for JAGS.\r\nKeep the first group sample as in the previous section: theoretical mean\r\n120 and theoretical known standard deviation 25.\r\nSimulate second sample with the same standard deviation and theoretical\r\nmean 150.\r\nCombine both samples together and add second column containing group\r\nnumber.\r\n\r\nset.seed(05162022)\r\nY2 <- rnorm(nSample, mean = 150, sd = 25)\r\nY2 <- rbind(cbind(Y, rep(1, nSample)), cbind(Y2, rep(2, nSample)))\r\ncolnames(Y2) <- c(\"y\",\"s\")\r\nden1 <- density(subset(Y2,Y2[,2]==1)[,1])\r\nden2 <- density(subset(Y2,Y2[,2]==2)[,1])\r\nplot(den1$x, den1$y, type=\"l\", ylim=c(0,.02))\r\nlines(den2$x, den2$y, col=\"blue\")\r\n\r\n\r\nThe sample from both groups now is Y2.\r\nCreate data for JAGS.\r\n\r\ny <- Y2[,\"y\"]\r\ns <- Y2[,\"s\"]\r\nnSample <- length(y)\r\nnGr <- length(unique(s))\r\ndataList <- list(y = y, s = s, nSample = nSample, nGr = nGr)\r\nnames(dataList)\r\n[1] \"y\"       \"s\"       \"nSample\" \"nGr\"    \r\n\r\nDifferent mean values,\r\ncommon weak prior\r\nIn this section consider model structure with common weak prior for\r\nmean values of both groups.\r\nThink about situations in which common prior is a reasonable\r\nassumption.\r\nThe model diagram convenient for coding it in JAGS or Stan looks\r\nlike.\r\n\r\nAn equivalent diagram may help understanding difference between\r\nhierarchical model of random effects and non-hierarchical model of fixed\r\neffects.\r\n\r\nSelect hyper-parameters of the common prior normal distribution as in\r\nthe previous section: \\(M=100, \\\r\n\\sigma_{\\mu}=200\\).\r\nModel description\r\nPrepare the model string.\r\nThe difference from one-group data description is that now prior\r\ndistribution for \\(\\mu\\) is described\r\nin a loop over all groups.\r\n\r\nmodel2NormalSample= \"\r\n  model { \r\n    # Description of data\r\n    for (i in 1:nSample){\r\n      y[i] ~ dnorm(mu[s[i]], 1/sigma^2) #tau = 1/sigma^2\r\n    }\r\n    # Description of prior\r\n    #mu[1:nGr] <- c(100,100)\r\n    for (j in 1:nGr) {      \r\n      mu[j] ~ dnorm(M, nSample/sigma_mu^2) \r\n    }\r\n    sigma <- 25\r\n    M <- 100\r\n    sigma_mu <- 200\r\n  }\r\n\"\r\nwriteLines(model2NormalSample, con=\"Tempmodel2.txt\")\r\n\r\nInitialization and\r\nsending the model to JAGS\r\nInitialize chains randomly around MLE.\r\n\r\ninitsList<-function() {\r\n  # Definition of mu and sigma Init\r\n   theta <- vector()\r\n   for (i in 1:nGr){\r\n      upDown<-sample(c(1,-1),1)\r\n      theta[i] <- mean(Y2[which(s==i)],)*(1+upDown*.05)\r\n   }\r\n  list(theta = theta)\r\n}\r\n# Check initiation\r\ninitsList()\r\n$theta\r\n[1] 126.5862 159.6764\r\n\r\nSend the model to JAGS.\r\nSet main parameters:\r\nDefine vector of parameters for monitoring and recording\r\nDefine number of steps for adaptation of samplers\r\nDefine number of burn-in steps\r\nDefine number of chains \\(n_{chains}\\)\r\nDefine number of saved steps of Markov chains \\(n_{steps}\\)\r\nDefine number of iterations per chain as \\(n_{iter} =\r\n\\frac{n_{steps}}{n_{chains}}\\)\r\nUse jags.model() to transfer information to JAGS. Arguments\r\nfor this function are: model string, data list, initiation function\r\nname, number of chains and number of steps for adaptation.\r\n\r\njagsModel2 <- jags.model(file = \"data/Tempmodel2.txt\", data = dataList, n.chains = 3, n.adapt = 500)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\n\r\nRunning the model\r\nRun burn in by using update().\r\n\r\nupdate(jagsModel2, n.iter = 600)\r\n\r\nMake the main run by using coda.samples().\r\n\r\nparameters <- c(\"mu\")\r\nnIter <- 10000\r\ncodaSamples2Groups1Prior = coda.samples(jagsModel2,\r\n                                        variable.names = parameters,\r\n                                        n.iter = nIter)\r\nhead(codaSamples2Groups1Prior)\r\n[[1]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n        mu[1]    mu[2]\r\n[1,] 118.6467 149.0570\r\n[2,] 122.5462 151.6248\r\n[3,] 118.8301 145.8869\r\n[4,] 120.6572 150.9358\r\n[5,] 121.1533 146.9245\r\n[6,] 119.5723 151.1555\r\n[7,] 121.0751 151.1671\r\n\r\n[[2]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n        mu[1]    mu[2]\r\n[1,] 122.9129 151.3536\r\n[2,] 120.3069 147.1769\r\n[3,] 115.7560 151.4156\r\n[4,] 120.6509 149.5604\r\n[5,] 117.8543 151.9259\r\n[6,] 118.6829 154.2494\r\n[7,] 119.4651 151.7435\r\n\r\n[[3]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 601 \r\nEnd = 607 \r\nThinning interval = 1 \r\n        mu[1]    mu[2]\r\n[1,] 118.0450 149.2746\r\n[2,] 119.8209 149.0857\r\n[3,] 125.1278 151.2497\r\n[4,] 116.9475 150.0177\r\n[5,] 117.3252 150.1579\r\n[6,] 118.6826 148.6312\r\n[7,] 120.5690 152.0444\r\n\r\nattr(,\"class\")\r\n[1] \"mcmc.list\"\r\nlist.samplers(jagsModel2)\r\n$`bugs::ConjugateNormal`\r\n[1] \"mu[2]\"\r\n\r\n$`bugs::ConjugateNormal`\r\n[1] \"mu[1]\"\r\n\r\nAnalysis\r\nAnalyze convergence.\r\n\r\nsummary(codaSamples2Groups1Prior)\r\n\r\nIterations = 601:10600\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 10000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n       Mean    SD Naive SE Time-series SE\r\nmu[1] 120.0 1.735  0.01002        0.01002\r\nmu[2] 150.5 1.735  0.01001        0.01001\r\n\r\n2. Quantiles for each variable:\r\n\r\n       2.5%   25%   50%   75% 97.5%\r\nmu[1] 116.5 118.8 120.0 121.1 123.4\r\nmu[2] 147.1 149.3 150.5 151.7 153.9\r\nplot(codaSamples2Groups1Prior)\r\n\r\nautocorr.plot(codaSamples2Groups1Prior, ask = F)\r\n\r\neffectiveSize(codaSamples2Groups1Prior)\r\n   mu[1]    mu[2] \r\n29965.46 30000.00 \r\ngelman.diag(codaSamples2Groups1Prior)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu[1]          1          1\r\nmu[2]          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples2Groups1Prior)\r\n\r\n\r\nCheck estimated means.\r\n\r\nmatrix(unlist(lapply(codaSamples2Groups1Prior,function(z) apply(z,2,mean))),ncol=3)\r\n         [,1]     [,2]     [,3]\r\n[1,] 119.9629 119.9299 119.9627\r\n[2,] 150.5279 150.4870 150.5058\r\n\r\nPlot posterior densities.\r\n\r\nplot(density(codaSamples2Groups1Prior[[1]][,1]),xlim=c(110,160),ylim=c(0,.25))\r\nlines(density(codaSamples2Groups1Prior[[1]][,2]))\r\nlines(density(codaSamples2Groups1Prior[[2]][,1]),col=\"orange\")\r\nlines(density(codaSamples2Groups1Prior[[2]][,2]),col=\"orange\")\r\nlines(density(codaSamples2Groups1Prior[[3]][,1]),col=\"blue\")\r\nlines(density(codaSamples2Groups1Prior[[3]][,2]),col=\"blue\")\r\n\r\n\r\nCalculate HDIs for each chain.\r\n\r\nlapply(codaSamples2Groups1Prior,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n         mu[1]    mu[2]\r\nlower 116.6673 147.1416\r\nupper 123.3728 153.8997\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n         mu[1]    mu[2]\r\nlower 116.6291 146.9730\r\nupper 123.4452 153.7687\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n         mu[1]    mu[2]\r\nlower 116.7187 147.1051\r\nupper 123.5352 153.9668\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nFind differences between \\(\\mu_1\\)\r\nand \\(\\mu_2\\).\r\n\r\nchainDiffs <- lapply(codaSamples2Groups1Prior, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs, function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n          var1\r\nlower 25.68099\r\nupper 35.32636\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n          var1\r\nlower 25.63348\r\nupper 35.25554\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n          var1\r\nlower 25.55780\r\nupper 35.19627\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nFind left 95% HDI boundaries for the chain differences and plot\r\nthem.\r\n\r\n(leftBounds<-unlist(lapply(chainDiffs,function(z) hdi(as.matrix(z))[1])))\r\n[1] 25.68099 25.63348 25.55780\r\nplot(density(chainDiffs[[1]]),xlim=c(15,35),ylim=c(0,.2),col=\"black\")\r\nlines(density(chainDiffs[[2]]),col=\"red\")\r\nlines(density(chainDiffs[[3]]),col=\"blue\")\r\nabline(v=leftBounds,col=c(\"black\",\"orange\",\"blue\"))\r\n\r\n\r\nFor the given sample and prior distribution mean values of the 2\r\ngroups are clearly different based on 95% HDI for all 3 chains.\r\nExperimenting with\r\nprior hyperparameters\r\nRepeat calculations with different prior hyperparameters.\r\nTo do that organize all steps of running MCMC with JAGS in a function\r\nrunMCMC2Groups <- function(prMean, prSD, dat), where\r\nprMean and prSD are the the prior\r\nhyperparameters and dat is the data list.\r\nNote that in order to pass arguments of the function to JAGS model\r\ndescription you need to append them to the data list:\r\nFirst put them in a list\r\nhyper <- list(hypeMean = prMean, hypeSD = prSD)\r\nThen combine list hyper with the data when run\r\njags.model():\r\njags.model(\"TEMPmodel.txt\", data=append(dat,hyper),...)\r\nAfter that parameters can be used in the description of the model as\r\ndnorm(hypeMean, 1/hypeSD^2).\r\nThe function should return the simulated chains.\r\n\r\nrunMCMC2Groups <- function(prMean, prSD, dat){\r\n   model2NormalSamplesb = \"\r\n      model {\r\n         for (i in 1:nSample){\r\n         y[i] ~ dnorm(theta[s[i]], 1/25^2)\r\n         }\r\n         for (sIdx in 1:nGr) {# Different thetas from same prior\r\n            theta[sIdx] ~ dnorm(hypeMean,1/hypeSD^2) \r\n         }\r\n      }\r\n   \" # close quote for modelString\r\nwriteLines(model2NormalSamplesb, con=\"data/TEMPmodel3.txt\")\r\n   \r\nparameters = c(\"theta\") # The parameters to be monitored\r\nadaptSteps = 500        # Number of steps to adapt the samplers\r\nburnInSteps = 500       # Number of steps to burn-in the chains\r\nnChains = 3             # nChains should be 2 or more for diagnostics \r\nnumSavedSteps <- 50000\r\nnIter = ceiling(numSavedSteps/nChains )\r\nhyper <- list(hypeMean = prMean, hypeSD=prSD)\r\n\r\n# Create, initialize, and adapt the model:\r\njagsModel = jags.model(\"data/TEMPmodel3.txt\", \r\n                       data = append(dat,hyper), \r\n                       inits = initsList, \r\n                       n.chains = nChains, \r\n                       n.adapt = adaptSteps)\r\n\r\nupdate(jagsModel , n.iter=burnInSteps)\r\ncodaSamplesResult <- coda.samples(jagsModel, \r\n                                  variable.names = parameters, \r\n                                  n.iter = nIter)\r\ncodaSamplesResult\r\n}\r\n\r\nRun: hypeMean=130; hypeSD=200\r\n\r\nRun.130.200 <- runMCMC2Groups(130, 200, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.130.200,function(z) apply(z, 2, mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n120.5531 152.0436 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n120.5736 152.0886 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n120.5378 152.0711 \r\nchainDiffs<-lapply(Run.130.200,function(z) z[,2]-z[,1])\r\nlapply(chainDiffs,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n          var1\r\nlower 26.66148\r\nupper 36.45065\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n          var1\r\nlower 26.49189\r\nupper 36.27758\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n          var1\r\nlower 26.75067\r\nupper 36.52194\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRun: hypeMean=130; hypeSD=0.2\r\n\r\nRun.130.2<-runMCMC2Groups(130, .2, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.130.2,function(z) apply(z,2,mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n129.8797 130.2782 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n129.8807 130.2812 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n129.8832 130.2785 \r\nchainDiffs <- lapply(Run.130.2, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            var1\r\nlower -0.1498324\r\nupper  0.9505163\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            var1\r\nlower -0.1391972\r\nupper  0.9614988\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            var1\r\nlower -0.1693627\r\nupper  0.9252938\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRun: hypeMean=100; hypeSD=0.2\r\n\r\nRun.100.2 <- runMCMC2Groups(100, .2, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.100.2, function(z) apply(z,2,mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n100.2627 100.6598 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n100.2617 100.6603 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n100.2594 100.6590 \r\nchainDiffs <- lapply(Run.100.2, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs, function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            var1\r\nlower -0.1525038\r\nupper  0.9457228\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            var1\r\nlower -0.1577104\r\nupper  0.9359693\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            var1\r\nlower -0.1635975\r\nupper  0.9365602\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRun: hypeMean=160; hypeSD=0.2\r\n\r\nRun.170.2 <- runMCMC2Groups(170, .2, dataList)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 400\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 813\r\n\r\nInitializing model\r\nlapply(Run.170.2,function(z) apply(z,2,mean))\r\n[[1]]\r\ntheta[1] theta[2] \r\n169.3763 169.7759 \r\n\r\n[[2]]\r\ntheta[1] theta[2] \r\n169.3778 169.7718 \r\n\r\n[[3]]\r\ntheta[1] theta[2] \r\n169.3746 169.7765 \r\nchainDiffs <- lapply(Run.170.2, function(z) z[,2]-z[,1])\r\nlapply(chainDiffs, function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            var1\r\nlower -0.1519031\r\nupper  0.9611009\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            var1\r\nlower -0.1523604\r\nupper  0.9345152\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            var1\r\nlower -0.1489817\r\nupper  0.9523275\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nExplain the results of running hierarchical model with different sets\r\nof hyperparameters:\r\n- more informative hypeSD would lead to narrow the difference between 2\r\nmeans\r\n- with informative hypeSD, the poster would be mainly affected by the\r\nprior\r\nFNP approach: ANOVA\r\nCompare results with fixed effects ANOVA model.\r\n\r\nhead(Y2)\r\n             y s\r\n[1,] 128.41478 1\r\n[2,]  99.92492 1\r\n[3,] 149.86236 1\r\n[4,] 178.42914 1\r\n[5,]  96.67612 1\r\n[6,] 115.41590 1\r\nmANOVA <- lm(y~as.factor(s), as.data.frame(Y2))\r\nsummary(mANOVA)\r\n\r\nCall:\r\nlm(formula = y ~ as.factor(s), data = as.data.frame(Y2))\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-71.077 -18.636   0.522  16.592  78.619 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)    120.558      1.782   67.65   <2e-16 ***\r\nas.factor(s)2   31.515      2.520   12.51   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.2 on 398 degrees of freedom\r\nMultiple R-squared:  0.2821,    Adjusted R-squared:  0.2803 \r\nF-statistic: 156.4 on 1 and 398 DF,  p-value: < 2.2e-16\r\nanova(mANOVA)\r\nAnalysis of Variance Table\r\n\r\nResponse: y\r\n              Df Sum Sq Mean Sq F value    Pr(>F)    \r\nas.factor(s)   1  99316   99316  156.37 < 2.2e-16 ***\r\nResiduals    398 252793     635                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": {},
    "last_modified": "2023-02-03T10:31:29-06:00",
    "input_file": "Series-2-of-10---MCMC-for-Estimation-of-Gaussian-parameters.knit.md"
  },
  {
    "path": "posts/2021-11-20-Bayesian methods - Series 1 of 10/",
    "title": "From Unconditional to Multivariate Bayesian Model Workshop -- Series 1 of 10 -- Gaussian vs Robust Estimation",
    "description": "Fitting an unconditional model (without predictors) Robust estimates -- Gaussian vs. t-distribution",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-11-20",
    "categories": [
      "Biostatistics",
      "Bayesian Methods",
      "R",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nTypical response\r\ndistribution\r\nFitting\r\nnormal model for 1 group with no predictors\r\nRunning in\r\nJAGS\r\nRunning in\r\nStan\r\n\r\nRobust estimation using\r\nt-distribution\r\nGeneralized Student\r\ndistribution\r\nJAGS\r\nStan\r\n\r\nFurther\r\nreading\r\n\r\nHow to fitting an unconditional model (without predictors)? From\r\nhere, we can see the philosophy of the Bayesian’s perspective.\r\nTypical response\r\ndistribution\r\nScale y\r\nDist. \\(y \\sim\r\nf(\\mu, ...)\\)\r\nInverse link \\(\\mu = g^{-1}(\\eta)\\)\r\nMetric\r\n\\(y \\sim\r\ndnorm(\\mu,\\sigma)\\)\r\n\\(\\mu =\r\n\\eta\\)\r\nBinary\r\n\\(y \\sim\r\ndbern(\\mu)\\)\r\n\\(\\mu =\r\nlogistic(\\eta)\\)\r\nNominal\r\n\\(y \\sim\r\ndmultinom(\\mu_1, ...,\\mu_p)\\)\r\n\\(\\mu_k =\r\n\\frac{exp(\\eta_k)}{\\sum_j exp(\\eta_j)}\\)\r\nOrdinal\r\n\\(y \\sim\r\ndmultinom(\\mu_1, ...,\\mu_p)\\)\r\n\\(\\mu =\r\n\\Phi(\\frac{\\theta_k - \\eta}{\\sigma}) - \\Phi(\\frac{\\theta_{k-1} -\r\n\\eta}{\\sigma})\\)\r\nCount\r\n\\(y \\sim\r\ndpois(\\mu)\\)\r\n\\(\\mu =\r\nexp(\\eta)\\)\r\nFitting\r\nnormal model for 1 group with no predictors\r\nWe just need to estimate both parameters of normal distribution for\r\nrandom variable \\(y\\).\r\nThe diagram of the model structure\r\n(adapted from Textbook)\r\n# preparing data\r\nset.seed(03172021)\r\ny <- rnorm(100, mean = 6, sd = 3)\r\nNtotal = length(y)\r\n\r\nCreate a data list, include sample mean and standard deviation in\r\nit.\r\n\r\ndataList = list(\r\n  y = y ,\r\n  Ntotal = Ntotal ,\r\n  mean_mu = mean(y) ,\r\n  sd_mu = sd(y)\r\n)\r\n\r\nRunning in JAGS\r\nSpecify the model.\r\nRecall that in JAGS [@RN733] normal distribution is specified by\r\nprecision \\(\\frac{1}{\\sigma^2}\\)\r\ninstead of standard deviation or variance. Select an uninformative prior\r\nfor \\(\\sigma\\) and normal (conjugate)\r\nprior for \\(\\mu\\).\r\nWhat do we suggest as parameters of the normal distribution based on\r\nthe sample? \\[\r\ny_i \\sim N(\\mu,\\tau), \\ \\text{where} \\ \\tau = \\frac{1}{\\sigma^2} \\\\\r\n\\mu \\sim N(M,S) \\\\\r\n\\sigma \\sim Uniform[Lw,Mx] \\ \\text{OR} \\ Gamma(\\alpha, \\beta)\r\n\\]\r\n\r\nmodelString = \"\r\n  model {\r\n    for (i in 1:Ntotal) {\r\n      y[i] ~ dnorm(mu , 1/sigma^2) #JAGS uses precision\r\n    }\r\n    # mu ~ dnorm(mean_mu , 1/(100*sd_mu)^2)\r\n    mu ~ dnorm(mean_mu , Ntotal/sd_mu^2) #JAGS uses precision\r\n    sigma ~ dunif(sd_mu/1000, sd_mu*1000)\r\n  }\r\n  \" # close quote for modelString\r\n\r\n\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"data/TEMPmodel.txt\")\r\n\r\nInitialize chains.\r\n\r\ninitsList <- function(){\r\n   upDown <- sample(c(1,-1),1)\r\n   m <- mean(y)*(1+upDown*.05)\r\n   s <- sd(y)*(1-upDown*.1) \r\n   list(mu = m, sigma = s)\r\n}\r\n\r\nRun the chains.\r\nSet parameters:\r\n\r\n# Create, initialize, and adapt the model:\r\nparameters = c(\"mu\", \"sigma\")     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnumSavedSteps = 50000\r\nnChains = 4 \r\nthinSteps = 1\r\nnIter = ceiling((numSavedSteps*thinSteps)/nChains )\r\n\r\nSend model to JAGS:\r\n\r\njagsModel = jags.model(\"data/TEMPmodel.txt\", data=dataList, inits=initsList,\r\n                       n.chains=nChains, n.adapt=adaptSteps)\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 100\r\n   Unobserved stochastic nodes: 2\r\n   Total graph size: 114\r\n\r\nInitializing model\r\n\r\nBurn in and run:\r\n\r\n# Burn-in:\r\nupdate(jagsModel, n.iter=burnInSteps)\r\n\r\n# Run it\r\n# The saved MCMC chain:\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters,\r\n                           n.iter=nIter, thin=thinSteps)\r\n\r\nCheck how chains converged.\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:14000\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 12500 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n       Mean     SD Naive SE Time-series SE\r\nmu    5.923 0.2265 0.001013       0.001001\r\nsigma 3.208 0.2325 0.001040       0.001407\r\n\r\n2. Quantiles for each variable:\r\n\r\n       2.5%   25%   50%   75% 97.5%\r\nmu    5.480 5.769 5.922 6.076 6.366\r\nsigma 2.792 3.044 3.194 3.356 3.698\r\n\r\nThe parameters are estimated close to what we simulated and very\r\nsimilar to what point estimation would give.\r\n\r\nmean(y)\r\n[1] 5.922201\r\nsd(y)\r\n[1] 3.173288\r\n\r\nThe plot of the samples and the densities of the parameters.\r\n\r\nplot(codaSamples)\r\n\r\n\r\nPlot autocorrelations.\r\n\r\nautocorr.plot(codaSamples, ask = F)\r\n\r\n\r\nAutocorrelation function plot shows that standard deviation effective\r\nsize must be pretty small.\r\nIn fact:\r\n\r\neffectiveSize(codaSamples)\r\n      mu    sigma \r\n51236.85 27414.72 \r\n\r\nShrink factor shows that even with long memory for standard deviation\r\ndistributions converged:\r\n\r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu             1          1\r\nsigma          1          1\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\n\r\nObserved HDIs of the chains:\r\n\r\nlapply(codaSamples,function(z) hdi(as.matrix(z)))\r\n[[1]]\r\n            mu    sigma\r\nlower 5.479068 2.774907\r\nupper 6.362111 3.677393\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n            mu    sigma\r\nlower 5.488275 2.766543\r\nupper 6.367365 3.661088\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n            mu    sigma\r\nlower 5.475716 2.778049\r\nupper 6.353808 3.677524\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[4]]\r\n            mu    sigma\r\nlower 5.489742 2.782062\r\nupper 6.386876 3.677348\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nRunning in Stan\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    real mean_mu;\r\n    real sd_mu;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    unifLo = sd_mu/100;\r\n    unifHi = sd_mu*100;\r\n    normalSigma = sd_mu*100;  // 100*10 times larger than MLE\r\n}\r\nparameters {\r\n    real mu;\r\n    real<lower=0> sigma;\r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi);\r\n    mu ~ normal(mean_mu, normalSigma); \r\n    y ~ normal(mu, sigma);\r\n}\r\n\" # close quote for modelString\r\n\r\nCreate a DSO and save it to disk to reuse later or just keep it in\r\nmemory.\r\n\r\nstanDso <- stan_model(model_code = modelString)\r\n\r\nRun chains by either using the existing DSO:\r\n\r\nstanFit <- sampling(object=stanDso,\r\n                    data = dataList,\r\n                    pars=c('mu', 'sigma'),\r\n                    chains = 2,\r\n                    cores= 2,\r\n                    iter = 5000,\r\n                    warmup = 200, \r\n                    thin = 1)\r\n\r\nOr alternatively, use the description of the model saved in\r\nch16_1.stan directly:\r\n\r\n# fit model\r\nfit <- stan(file = \"modelString.stan\", \r\n            data = list(Ntotal = length(y),\r\n                        y = y,\r\n                        meanY=mean(y),\r\n                        sdY=sd(y)), \r\n            pars=c('mu', 'sigma'),\r\n            # control=list(adapt_delta=0.99),\r\n            iter=5000, chains = 2, cores = 2\r\n            )\r\n\r\nObjects fit and stanFit should return very\r\nsimilar results. The difference between stan() and\r\nsampling() is in the argument object which is\r\nDSO. If you expect to repeat same calculations with different data\r\ncompiling a DSO and reusing it with sampling() is\r\nfaster.\r\n\r\n# text statistics:\r\nprint (stanFit)\r\nInference for Stan model: anon_model.\r\n2 chains, each with iter=5000; warmup=200; thin=1; \r\npost-warmup draws per chain=4800, total post-warmup draws=9600.\r\n\r\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nmu       5.92    0.00 0.32    5.30    5.71    5.92    6.14    6.57\r\nsigma    3.22    0.00 0.23    2.80    3.06    3.20    3.36    3.72\r\nlp__  -164.84    0.02 1.01 -167.54 -165.23 -164.53 -164.11 -163.85\r\n      n_eff Rhat\r\nmu     8953    1\r\nsigma  7630    1\r\nlp__   4176    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Feb 23 20:07:56 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\n# estimates & hdi:\r\nplot(stanFit)\r\n\r\n# samples\r\ntraceplot(stanFit, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFit, pars=c('mu','sigma'))\r\n\r\nstan_scat(stanFit, c('mu', 'sigma'))\r\n\r\nstan_hist(stanFit)\r\n\r\nstan_dens(stanFit)\r\n\r\n# autocorrelation:\r\nstan_ac(stanFit, separate_chains = T)\r\n\r\n\r\n\r\n# or work with familiar coda class:\r\nstan2coda <- function(fit) {\r\n    # apply to all chains\r\n    mcmc.list(lapply(1:ncol(fit), function(x) mcmc(as.array(fit)[,x,])))\r\n}\r\ncodaSamples <- stan2coda(stanFit)\r\nsummary(codaSamples)\r\n\r\nIterations = 1:4800\r\nThinning interval = 1 \r\nNumber of chains = 2 \r\nSample size per chain = 4800 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n          Mean     SD Naive SE Time-series SE\r\nmu       5.923 0.3225 0.003291       0.003330\r\nsigma    3.216 0.2344 0.002392       0.002609\r\nlp__  -164.839 1.0128 0.010337       0.015893\r\n\r\n2. Quantiles for each variable:\r\n\r\n          2.5%      25%      50%      75%    97.5%\r\nmu       5.298    5.707    5.922    6.137    6.566\r\nsigma    2.795    3.055    3.201    3.364    3.720\r\nlp__  -167.540 -165.228 -164.526 -164.113 -163.847\r\nplot(codaSamples)\r\n\r\nautocorr.plot(codaSamples) \r\n\r\neffectiveSize(codaSamples)\r\n      mu    sigma     lp__ \r\n9392.909 8296.156 4092.009 \r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu             1       1.00\r\nsigma          1       1.00\r\nlp__           1       1.01\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\n\r\n\r\nplot(density(codaSamples[[1]][,1]),xlim=c(0,8),ylim=c(0,3))  # mu, 1st chain\r\nlines(density(codaSamples[[1]][,2]))                         # sigma, 1st chain\r\nlines(density(codaSamples[[2]][,1]),col=\"red\")               # mu, 2nd chain\r\nlines(density(codaSamples[[2]][,2]),col=\"red\")               # sigma, 2nd chain\r\n\r\n\r\nOr you can use shinystan to do similar analysis of fitted model:\r\n\r\nlaunch_shinystan(fit)\r\n\r\nRobust estimation using\r\nt-distribution\r\nCreate a sample with heavy tails in order to check robust estimation\r\nof parameters of normal distribution.\r\nRefer this simulation of Leptokurtic\r\nDistributions.\r\n\r\nnSample<-1000\r\nsd.Values<-c(2,3.4,.8,2.6)\r\nsd.process<-rep(c(rep(sd.Values[1],50),\r\n                  rep(sd.Values[2],75),\r\n                  rep(sd.Values[3],75),\r\n                  rep(sd.Values[4],50)), 4)\r\n            \r\nplot(sd.process,type=\"l\")\r\n\r\n\r\nVariable sd.process is a deterministically changing\r\nstandard deviation. Simulating perfect normal and independent\r\nrealizations with such different standard deviations make a leptokurtic\r\ndistribution.\r\n\r\nset.seed(05062022)\r\ny <- rnorm(nSample)*sd.process\r\ny <- y[1:300]\r\nplot(y, type=\"l\")\r\n\r\nden <- density(y)\r\nplot(den)\r\nlines(den$x, dnorm(den$x, mean(y), sd(y)), col=\"red\")\r\n\r\n\r\n\r\nNtotal = length(y)\r\n\r\nDensity plot clearly shows fat tails that will be most likely\r\nidentified as outliers under the assumption of normal distribution.\r\nCreate data list.\r\n\r\ndataList = list(\r\n   y = y ,\r\n   Ntotal = Ntotal ,\r\n   mean_mu = mean(y) ,\r\n   sd_mu = sd(y)\r\n   )\r\n\r\nGeneralized Student\r\ndistribution\r\nStudent distribution with \\(\\nu\\)\r\ndegrees of freedom appears in a sampling scheme from Gaussian\r\ndistribution with unknown mean and variance.\r\nIn such case the variable \\[ X \\sim N(\\mu,\r\n\\sigma)\\] has the \\(z\\)-score\r\n\\[z = \\frac{X - \\hat{\\mu}}{s/\\sqrt{n}} =\r\n\\frac{X - \\hat{\\mu}}{\\hat{\\sigma}} \\] where \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i -\r\n\\hat{\\mu})^2\\), has\\(t\\)-distribution with \\(n-1\\) degrees of freedom.\r\nThen random variable \\[ X = \\hat{\\mu} +\r\n\\hat{\\sigma}z\\]\r\nhas generalized Student distribution with location parameter \\(\\hat{\\mu}\\), scale parameter \\(\\hat{\\sigma}\\) and the number of degrees of\r\nfreedom \\(\\nu\\).\r\nThe parameters \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\) are not exactly\r\ncorresponding to the corresponding parameters of Gaussian distribution,\r\nbut \\(\\hat{\\mu} \\rightarrow \\mu\\),\r\n\\(\\hat{\\sigma} \\rightarrow \\sigma\\) as\r\n\\(\\nu\\) becomes small and \\(t\\)-distribution becomes Gaussian.\r\nInterpretation of the parameters:\r\n\\(\\hat{\\mu}\\) - location\r\n\\(\\hat{\\sigma}\\) - scale\r\n\\(\\nu\\) - amount of mass in the\r\ntails.\r\nJAGS\r\n\r\nmodelString = \"\r\nmodel {\r\n   for ( i in 1:Ntotal ) {\r\n      y[i] ~ dt(mu,1/sigma^2,nu)\r\n   }\r\n   mu ~ dnorm( mean_mu , 100/sd_mu^2 )\r\n   sigma ~ dunif( sd_mu/1000 , sd_mu*1000 )\r\n   nu ~ dexp(1/30.0)\r\n}\r\n\" # close quote for modelString\r\n# Write out modelString to a text file\r\nwriteLines(modelString , con=\"data/TEMPmodel-jags.txt\")\r\n\r\nInitialize the model with MLE.\r\n\r\ninitsList <-function() {\r\n   upDown<-sample(c(1,-1),1)\r\n   m <- mean(y)*(1+upDown*.05)\r\n   s <- sd(y)*(1-upDown*.1) \r\n   list(mu = m, sigma = s, nu=2)\r\n   }\r\n\r\n\r\nparameters = c(\"mu\", \"sigma\", \"nu\")     # The parameters to be monitored\r\nadaptSteps = 500               # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 3 \r\nthinSteps = 1\r\nnumSavedSteps=50000\r\n(nIter = ceiling((numSavedSteps*thinSteps)/nChains))\r\n[1] 16667\r\n\r\n\r\n# Create, initialize, and adapt the model:\r\njagsModel = jags.model(\"data/TEMPmodel-jags.txt\", data=dataList, inits=initsList, \r\n                       n.chains=nChains, n.adapt=adaptSteps )\r\nCompiling model graph\r\n   Resolving undeclared variables\r\n   Allocating nodes\r\nGraph information:\r\n   Observed stochastic nodes: 300\r\n   Unobserved stochastic nodes: 3\r\n   Total graph size: 318\r\n\r\nInitializing model\r\n\r\nRun the chains.\r\n\r\n# Burn-in:\r\nupdate(jagsModel, n.iter=burnInSteps)\r\n# The saved MCMC chain:\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters , \r\n                           n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the results.\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:18167\r\nThinning interval = 1 \r\nNumber of chains = 3 \r\nSample size per chain = 16667 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n         Mean     SD  Naive SE Time-series SE\r\nmu    -0.2816 0.1115 0.0004988      0.0006814\r\nnu     4.1298 1.5167 0.0067826      0.0223954\r\nsigma  1.8116 0.1673 0.0007482      0.0021333\r\n\r\n2. Quantiles for each variable:\r\n\r\n         2.5%     25%     50%     75%    97.5%\r\nmu    -0.5031 -0.3565 -0.2804 -0.2055 -0.06739\r\nnu     2.3196  3.1388  3.7905  4.7075  7.98449\r\nsigma  1.5018  1.6962  1.8053  1.9221  2.15579\r\nmean(y)\r\n[1] -0.4222642\r\nsd(y)\r\n[1] 2.495025\r\n\r\nNote that the robust estimate of \\(\\mu\\) is similar, but \\(\\sigma\\) is significantly smaller.\r\n\r\nplot(codaSamples)\r\n\r\nsummary(sd.process)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   0.80    0.80    2.30    2.18    3.40    3.40 \r\nautocorr.plot(codaSamples,ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n       mu        nu     sigma \r\n26967.896  4580.058  6146.000 \r\ngelman.diag(codaSamples)\r\nPotential scale reduction factors:\r\n\r\n      Point est. Upper C.I.\r\nmu             1       1.00\r\nnu             1       1.01\r\nsigma          1       1.00\r\n\r\nMultivariate psrf\r\n\r\n1\r\ngelman.plot(codaSamples)\r\n\r\nhead(codaSamples[1])\r\n[[1]]\r\nMarkov Chain Monte Carlo (MCMC) output:\r\nStart = 1501 \r\nEnd = 1507 \r\nThinning interval = 1 \r\n              mu       nu    sigma\r\n[1,] -0.35538534 4.495681 1.958775\r\n[2,] -0.06611877 4.258681 1.793895\r\n[3,] -0.05521260 4.103952 1.789491\r\n[4,] -0.24838098 5.631748 1.801702\r\n[5,] -0.24537268 4.109628 1.863111\r\n[6,] -0.19055382 3.354896 2.028280\r\n[7,] -0.56653077 4.627866 1.839904\r\n\r\nattr(,\"class\")\r\n[1] \"mcmc.list\"\r\n(HDIofChains<-lapply(codaSamples,function(z) hdi(as.matrix(z))))\r\n[[1]]\r\n              mu       nu    sigma\r\nlower -0.5047759 2.070184 1.501770\r\nupper -0.0678133 7.153532 2.149501\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[2]]\r\n               mu       nu    sigma\r\nlower -0.49556250 2.028008 1.475841\r\nupper -0.07011458 6.820492 2.127922\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\n[[3]]\r\n               mu       nu    sigma\r\nlower -0.50150501 1.966809 1.497709\r\nupper -0.06232077 6.961558 2.143415\r\nattr(,\"credMass\")\r\n[1] 0.95\r\n\r\nNon-robust estimate of \\(\\sigma\\) is\r\noutside the HDI for all the chains.\r\nStan\r\nAdapt the model description to t-distribution with additional\r\nparameter \\(\\nu\\).\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real y[Ntotal];\r\n    real mean_mu;\r\n    real sd_mu;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real normalSigma;\r\n    real expLambda;         //New: parameter of prior for nu \r\n    unifLo = sd_mu/1000;\r\n    unifHi = sd_mu*1000;\r\n    normalSigma = sd_mu*100;\r\n    expLambda=1/29.0;      //New: setting value for expLambda\r\n}\r\nparameters {\r\n    real<lower=0> nuMinusOne; //New: definition of additional parameter nu\r\n    real mu;\r\n    real<lower=0> sigma;\r\n}\r\ntransformed parameters {\r\n    real<lower=0> nu;           //New: new parameter nu\r\n    nu=nuMinusOne+1;           //New: shifting nu to avoid zero\r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi);\r\n    mu ~ normal(mean_mu, normalSigma);\r\n    nuMinusOne~exponential(expLambda);      //New: exponential prior for nu\r\n    y ~ student_t(nu, mu, sigma);           //New: student_t distribution for nu\r\n}\r\n\" # close quote for modelString\r\n\r\nCreate DSO.\r\n\r\nstanDso <- stan_model(model_code = modelString)\r\n\r\nRun MCMC.\r\n\r\nstanFitRobust <- sampling(object=stanDso , \r\n                     data = dataList ,\r\n                     pars=c('nu','mu', 'sigma'),\r\n                     chains = 3,\r\n                     cores= 3,\r\n                     iter = 50000,\r\n                     warmup = 300, \r\n                     thin = 1)\r\n\r\nExplore the results.\r\n\r\n# text statistics:\r\nprint(stanFitRobust)\r\nInference for Stan model: anon_model.\r\n3 chains, each with iter=50000; warmup=300; thin=1; \r\npost-warmup draws per chain=49700, total post-warmup draws=149100.\r\n\r\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5%\r\nnu       4.04    0.01 1.54    2.30    3.10    3.72    4.58    7.69\r\nmu      -0.24    0.00 0.12   -0.49   -0.33   -0.24   -0.16    0.00\r\nsigma    1.80    0.00 0.16    1.50    1.69    1.80    1.91    2.14\r\nlp__  -514.38    0.01 1.26 -517.67 -514.95 -514.05 -513.46 -512.95\r\n      n_eff Rhat\r\nnu    45883    1\r\nmu    87613    1\r\nsigma 62456    1\r\nlp__  54355    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Feb 23 20:11:46 2023.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\n# estimates & hdi:\r\nplot(stanFitRobust)\r\n\r\n# samples\r\nclass(stanFitRobust)\r\n[1] \"stanfit\"\r\nattr(,\"package\")\r\n[1] \"rstan\"\r\nrstan::traceplot(stanFitRobust, ncol=1, inc_warmup=F)\r\n\r\npairs(stanFitRobust, pars=c('nu','mu','sigma'))\r\n\r\nstan_scat(stanFitRobust, c('nu','mu'))\r\n\r\nstan_scat(stanFitRobust, c('nu','sigma'))\r\n\r\nstan_scat(stanFitRobust, c('mu','sigma'))\r\n\r\nstan_hist(stanFitRobust)\r\n\r\nstan_dens(stanFitRobust)\r\n\r\n# autocorrelation:\r\nstan_ac(stanFitRobust, separate_chains = T)\r\n\r\nstan_diag(stanFitRobust,information = \"sample\")\r\n\r\nstan_diag(stanFitRobust,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(stanFitRobust,information = \"divergence\",chain = 0)\r\n\r\n\r\nThere seems to be a pattern in relationship between \\(\\sigma\\) and \\(\\nu\\).\r\nCheck if there is dependency.\r\n\r\nstanRobustChains <- extract(stanFitRobust)\r\nnames(stanRobustChains)\r\n[1] \"nu\"    \"mu\"    \"sigma\" \"lp__\" \r\nplot(stanRobustChains$nu,stanRobustChains$sigma)\r\n\r\nplot(rank(stanRobustChains$nu),rank(stanRobustChains$sigma))\r\n\r\n\r\nInterpret the dependency: Degrees of freedom \\(\\nu\\) are related to sample size \\((n-1)\\). If the df increases, it also\r\nstands that the sample size is increasing; the graph of the\r\nt-distribution will have skinnier tails, pushing the critical value\r\ntowards the mean.\r\n\\(\\Rightarrow\\) t-copulas (guess\r\nfrom empirical copulas)\r\nExplore shiny object.\r\n\r\nlaunch_shinystan(stanFitRobust)\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th\r\ned. ed.). Boston: McGraw-Hill Irwin.\r\nKruschke, John K. Doing Bayesian Data Analysis: a Tutorial with R,\r\nJAGS, and Stan. 2nd ed., Academic Press is an imprint of Elsevier,\r\n2015.\r\n",
    "preview": "posts/2021-11-20-Bayesian methods - Series 1 of 10/distill-preview.png",
    "last_modified": "2023-02-23T20:14:05-06:00",
    "input_file": "Series-1-of-10---Fitting-an-unconditional-models.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-24-LPA Chicago Neighborhoods/",
    "title": "Latent Profile Analysis of Chicago Neighborhoods",
    "description": "American Census Survey data;    \nFactor Analysis;    \nLatent Profile Analysis: Gaussian Mixture modeling;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-10-24",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Factor Analysis",
      "Mclust",
      "Tutorial",
      "Mapping"
    ],
    "contents": "\r\n\r\nContents\r\nFactor Analysis\r\nACS data\r\nDetermine Number of Factors to Extract\r\n\r\nLatent Profile Analysis\r\nData to run LPA\r\nCheck how many clusters should create?\r\nFitted a 4-components GMM with unconstrained covariance matrices\r\n\r\nApplying the Clustering\r\nMapping on Chicago Neighborhood map\r\nClassification by race-ethnicity\r\n\r\nRefs\r\n\r\n\r\nlibrary(mclust)\r\nlibrary(psych)\r\nlibrary(tidyverse)\r\nlibrary(nFactors)\r\nlibrary(kableExtra)\r\n\r\nFactor Analysis\r\nACS data\r\nThe summary of the data “Chicago Census Data by Tract_American Census Survey (ACS)”. Data includes Census tract characteristics of 2010-2014 5-year ACS estimates that easy to download from https://data.census.gov/cedsci/.\r\nWe will use the following variables in the exploratory factor analysis:\r\nAgeDependencyRatio: age dependency ratio\r\nLimitedEngProf5andUP: limited English proficiency\r\nLessThanHS: less than high school in education level\r\nUnemployment\r\nPctForeignBorn: percentage of foreign born\r\nFemaleHHPct: percentage of female per household\r\nMedianIncomeHH: median income per household\r\nAllOcc2009Pct: percentage of all occupants in 2009\r\nPctVacHousing: percentage of vacant housing\r\nPctBelowPoverty: percentage below poverty\r\nPublicAssistancePct: percentage of public assistance\r\n\r\ncensus <- read.csv(\"data/Chicago Census Data by Tract_ACS 2015.csv\", header = TRUE) %>% \r\n  rename(tract = ï..tract) %>% \r\n  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)\r\n\r\nsummary(census)\r\n     tract        AgeDependencyRatio LimitedEngProf5andUP\r\n Min.   : 10100   Min.   :  4.80     Min.   : 0.000      \r\n 1st Qu.:160825   1st Qu.: 39.80     1st Qu.: 1.825      \r\n Median :351250   Median : 54.95     Median : 8.400      \r\n Mean   :403946   Mean   : 52.89     Mean   :13.974      \r\n 3rd Qu.:670275   3rd Qu.: 65.28     3rd Qu.:23.950      \r\n Max.   :843900   Max.   :147.40     Max.   :64.000      \r\n                                                         \r\n   LessThanHS     Unemployment   PctForeignBorn     FemaleHHPct    \r\n Min.   : 0.00   Min.   : 0.00   Min.   :0.00000   Min.   :0.0000  \r\n 1st Qu.: 8.20   1st Qu.: 7.00   1st Qu.:0.03864   1st Qu.:0.0871  \r\n Median :16.30   Median :11.85   Median :0.15108   Median :0.1651  \r\n Mean   :18.38   Mean   :14.51   Mean   :0.18641   Mean   :0.2013  \r\n 3rd Qu.:26.20   3rd Qu.:19.80   3rd Qu.:0.31061   3rd Qu.:0.2986  \r\n Max.   :62.50   Max.   :91.90   Max.   :0.71263   Max.   :0.7047  \r\n                                                                   \r\n MedianIncomeHH   AllOcc2009Pct    PctVacHousing     PctBelowPoverty\r\n Min.   :  5000   Min.   :0.0000   Min.   :0.00000   Min.   : 0.40  \r\n 1st Qu.: 29402   1st Qu.:0.5341   1st Qu.:0.07766   1st Qu.:12.53  \r\n Median : 42321   Median :0.6285   Median :0.11732   Median :21.85  \r\n Mean   : 49658   Mean   :0.6284   Mean   :0.13969   Mean   :24.08  \r\n 3rd Qu.: 62056   3rd Qu.:0.7279   3rd Qu.:0.18202   3rd Qu.:33.80  \r\n Max.   :160455   Max.   :0.9406   Max.   :0.70764   Max.   :74.20  \r\n NA's   :1                                                          \r\n PublicAssistancePct\r\n Min.   :0.0000     \r\n 1st Qu.:0.0929     \r\n Median :0.2207     \r\n Mean   :0.2448     \r\n 3rd Qu.:0.3667     \r\n Max.   :0.7985     \r\n                    \r\n\r\n\r\n# recode into % to scale to other variables\r\ndf <- census %>% mutate(\r\n  PctForeignBorn = PctForeignBorn*100,\r\n  FemaleHHPct = FemaleHHPct*100,\r\n  # MedianIncomeHH1Kinv = (max(MedianIncomeHH, na.rm=TRUE) - MedianIncomeHH)/1000,\r\n  # we would not use 1Kinv because of weird distribution (multi-mode)\r\n  MedianIncomeHH1K = MedianIncomeHH/1000,\r\n  AllOcc2009Pct = AllOcc2009Pct*100,\r\n  PctVacHousing = PctVacHousing*100,\r\n  PublicAssistancePct = PublicAssistancePct*100\r\n) %>% \r\n  select(tract, AgeDependencyRatio, LimitedEngProf5andUP, LessThanHS, Unemployment, PctForeignBorn, FemaleHHPct, MedianIncomeHH1K, AllOcc2009Pct, PctVacHousing, PctBelowPoverty, PublicAssistancePct)\r\n\r\ndf.fa <- df[complete.cases(df),] %>% \r\n  select(-tract)# 1 missing case\r\n\r\n\r\nsummary(df.fa)\r\n AgeDependencyRatio LimitedEngProf5andUP   LessThanHS   \r\n Min.   :  4.8      Min.   : 0.00        Min.   : 0.00  \r\n 1st Qu.: 39.8      1st Qu.: 1.90        1st Qu.: 8.20  \r\n Median : 55.0      Median : 8.40        Median :16.30  \r\n Mean   : 52.9      Mean   :13.99        Mean   :18.39  \r\n 3rd Qu.: 65.3      3rd Qu.:24.00        3rd Qu.:26.20  \r\n Max.   :147.4      Max.   :64.00        Max.   :62.50  \r\n  Unemployment   PctForeignBorn    FemaleHHPct     MedianIncomeHH1K\r\n Min.   : 0.00   Min.   : 0.000   Min.   : 0.000   Min.   :  5.00  \r\n 1st Qu.: 7.00   1st Qu.: 3.858   1st Qu.: 8.704   1st Qu.: 29.40  \r\n Median :11.90   Median :15.115   Median :16.502   Median : 42.32  \r\n Mean   :14.52   Mean   :18.659   Mean   :20.122   Mean   : 49.66  \r\n 3rd Qu.:19.80   3rd Qu.:31.076   3rd Qu.:29.859   3rd Qu.: 62.06  \r\n Max.   :91.90   Max.   :71.263   Max.   :70.470   Max.   :160.46  \r\n AllOcc2009Pct   PctVacHousing    PctBelowPoverty PublicAssistancePct\r\n Min.   : 0.00   Min.   : 0.000   Min.   : 0.40   Min.   : 0.000     \r\n 1st Qu.:53.45   1st Qu.: 7.752   1st Qu.:12.50   1st Qu.: 9.252     \r\n Median :62.85   Median :11.722   Median :21.80   Median :22.002     \r\n Mean   :62.85   Mean   :13.898   Mean   :24.07   Mean   :24.440     \r\n 3rd Qu.:72.80   3rd Qu.:18.066   3rd Qu.:33.80   3rd Qu.:36.667     \r\n Max.   :94.06   Max.   :50.673   Max.   :74.20   Max.   :79.851     \r\n\r\nDetermine Number of Factors to Extract\r\n\r\nev <- eigen(cor(df.fa)) # get eigenvalues\r\nap <- parallel(subject=nrow(df.fa),var=ncol(df.fa), rep=100,cent=.05)\r\nnS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)\r\nplotnScree(nS) \r\n\r\n\r\nWhen we run a factor analysis, we need to decide on three things:\r\nthe number of factors\r\nthe method of estimation\r\nthe rotation\r\n\\(\\Rightarrow\\) Maximum Likelihood Factor Analysis (bullet 2) entering raw data and extracting 3 factors (bullet 1), with varimax rotation (bullet 3)\r\n\r\nfit.f <- factanal(x = df.fa, factors = 3, n.obs = 797, rotation=\"varimax\")\r\nprint(fit.f, digits=2, cutoff=.3, sort=TRUE)\r\n\r\nCall:\r\nfactanal(x = df.fa, factors = 3, n.obs = 797, rotation = \"varimax\")\r\n\r\nUniquenesses:\r\n  AgeDependencyRatio LimitedEngProf5andUP           LessThanHS \r\n                0.37                 0.00                 0.19 \r\n        Unemployment       PctForeignBorn          FemaleHHPct \r\n                0.29                 0.10                 0.18 \r\n    MedianIncomeHH1K        AllOcc2009Pct        PctVacHousing \r\n                0.29                 0.21                 0.60 \r\n     PctBelowPoverty  PublicAssistancePct \r\n                0.12                 0.07 \r\n\r\nLoadings:\r\n                     Factor1 Factor2 Factor3\r\nAgeDependencyRatio    0.56            0.56  \r\nUnemployment          0.80                  \r\nFemaleHHPct           0.84                  \r\nMedianIncomeHH1K     -0.80                  \r\nPctVacHousing         0.61                  \r\nPctBelowPoverty       0.93                  \r\nPublicAssistancePct   0.96                  \r\nLimitedEngProf5andUP          0.99          \r\nLessThanHS            0.53    0.69          \r\nPctForeignBorn                0.91          \r\nAllOcc2009Pct                         0.89  \r\n\r\n               Factor1 Factor2 Factor3\r\nSS loadings       4.82    2.43    1.33\r\nProportion Var    0.44    0.22    0.12\r\nCumulative Var    0.44    0.66    0.78\r\n\r\nTest of the hypothesis that 3 factors are sufficient.\r\nThe chi square statistic is 222.31 on 25 degrees of freedom.\r\nThe p-value is 1.46e-33 \r\n\r\nIn general, we’d like to see low uniquenesses or high communalities (subtract the uniquenesses from 1—The communality is the proportion of variance of the \\(i^{th}\\) variable contributed by the \\(m\\) (here’s 3) common factors)\r\nAt the conclusion of a factor analysis of census data, we might determine that the census measures 3 factors: (1) social poverty disparity strength, (2) education strength, (3) age disparity strength\r\nFactor analysis seeks to model the correlation matrix with fewer variables called factors. If we succeed with, said here, 3 factors, we are able to model the correlation matrix using only 3 variables instead of 11. Just remember these 3 variables, or factors, are unobserved. We give them names like “latent variables”. They are not subsets of our original variables.\r\nGiven the data matrix \\(X\\), consider it’s covariance matrix \\(cov(X)=\\sum\\) and obtain the matrix \\(V\\) of eigenvectors from \\(\\sum\\). This matrix \\(V\\) is what you call the loadings.\r\nThis set of eigenvectors define an orthogonal change of basis matrix that maximize the variance from X. This means that, if I project X into the subspace generated by V I will obtain a matrix U by simply solving XV=U. This matrix U is what you call the scores (for each factor, we would have its own score).\r\nIn our case, we would manipulate a little bit due to we use only variables in the first factor. Thus, if the absolute value of loadings less than or equal 0.5, it would not contribute to the score\r\n\r\ndf.fa$concdisadv <- as.matrix(df.fa) %*% (fit.f$loadings[,1]*(abs(fit.f$loadings[,1])>0.5))\r\n\r\n# print out the scores\r\nkable(df.fa[1:10,], caption = \"FA scores -- The concentrated disadvantage index scores\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:concentrated disadvantage–fa score)FA scores – The concentrated disadvantage index scores\r\n\r\n\r\nAgeDependencyRatio\r\n\r\n\r\nLimitedEngProf5andUP\r\n\r\n\r\nLessThanHS\r\n\r\n\r\nUnemployment\r\n\r\n\r\nPctForeignBorn\r\n\r\n\r\nFemaleHHPct\r\n\r\n\r\nMedianIncomeHH1K\r\n\r\n\r\nAllOcc2009Pct\r\n\r\n\r\nPctVacHousing\r\n\r\n\r\nPctBelowPoverty\r\n\r\n\r\nPublicAssistancePct\r\n\r\n\r\nconcdisadv\r\n\r\n\r\n30.6\r\n\r\n\r\n3.5\r\n\r\n\r\n6.6\r\n\r\n\r\n13.0\r\n\r\n\r\n15.09985\r\n\r\n\r\n13.586956\r\n\r\n\r\n32.188\r\n\r\n\r\n52.35507\r\n\r\n\r\n15.885714\r\n\r\n\r\n28.7\r\n\r\n\r\n24.36594\r\n\r\n\r\n76.40009\r\n\r\n\r\n39.4\r\n\r\n\r\n22.7\r\n\r\n\r\n21.0\r\n\r\n\r\n9.9\r\n\r\n\r\n35.50975\r\n\r\n\r\n18.823979\r\n\r\n\r\n39.122\r\n\r\n\r\n47.99542\r\n\r\n\r\n12.903226\r\n\r\n\r\n33.3\r\n\r\n\r\n27.56777\r\n\r\n\r\n90.84744\r\n\r\n\r\n47.7\r\n\r\n\r\n15.9\r\n\r\n\r\n16.3\r\n\r\n\r\n10.9\r\n\r\n\r\n31.07639\r\n\r\n\r\n11.643836\r\n\r\n\r\n27.318\r\n\r\n\r\n56.84932\r\n\r\n\r\n16.707417\r\n\r\n\r\n29.6\r\n\r\n\r\n24.65753\r\n\r\n\r\n93.39844\r\n\r\n\r\n44.4\r\n\r\n\r\n19.4\r\n\r\n\r\n22.2\r\n\r\n\r\n7.1\r\n\r\n\r\n32.23630\r\n\r\n\r\n5.439942\r\n\r\n\r\n37.111\r\n\r\n\r\n51.47864\r\n\r\n\r\n16.084559\r\n\r\n\r\n19.9\r\n\r\n\r\n18.76597\r\n\r\n\r\n63.53275\r\n\r\n\r\n23.4\r\n\r\n\r\n2.4\r\n\r\n\r\n0.9\r\n\r\n\r\n9.2\r\n\r\n\r\n13.57015\r\n\r\n\r\n6.634564\r\n\r\n\r\n38.384\r\n\r\n\r\n52.27394\r\n\r\n\r\n15.006821\r\n\r\n\r\n18.1\r\n\r\n\r\n13.32263\r\n\r\n\r\n34.51310\r\n\r\n\r\n23.7\r\n\r\n\r\n6.9\r\n\r\n\r\n7.4\r\n\r\n\r\n14.5\r\n\r\n\r\n23.37764\r\n\r\n\r\n7.255245\r\n\r\n\r\n25.309\r\n\r\n\r\n43.31294\r\n\r\n\r\n10.729614\r\n\r\n\r\n21.3\r\n\r\n\r\n20.89161\r\n\r\n\r\n60.98266\r\n\r\n\r\n16.2\r\n\r\n\r\n10.7\r\n\r\n\r\n13.6\r\n\r\n\r\n5.1\r\n\r\n\r\n31.99531\r\n\r\n\r\n5.332456\r\n\r\n\r\n35.822\r\n\r\n\r\n47.20211\r\n\r\n\r\n19.330855\r\n\r\n\r\n39.2\r\n\r\n\r\n17.18236\r\n\r\n\r\n60.81030\r\n\r\n\r\n54.1\r\n\r\n\r\n25.1\r\n\r\n\r\n23.0\r\n\r\n\r\n10.0\r\n\r\n\r\n31.82692\r\n\r\n\r\n2.746781\r\n\r\n\r\n19.813\r\n\r\n\r\n46.43777\r\n\r\n\r\n13.703704\r\n\r\n\r\n38.3\r\n\r\n\r\n30.21459\r\n\r\n\r\n109.96227\r\n\r\n\r\n32.7\r\n\r\n\r\n14.8\r\n\r\n\r\n13.1\r\n\r\n\r\n9.1\r\n\r\n\r\n25.23635\r\n\r\n\r\n10.178282\r\n\r\n\r\n37.338\r\n\r\n\r\n52.73906\r\n\r\n\r\n5.657492\r\n\r\n\r\n17.7\r\n\r\n\r\n13.16045\r\n\r\n\r\n43.67896\r\n\r\n\r\n52.7\r\n\r\n\r\n22.2\r\n\r\n\r\n18.4\r\n\r\n\r\n9.4\r\n\r\n\r\n34.15854\r\n\r\n\r\n9.987113\r\n\r\n\r\n58.319\r\n\r\n\r\n68.49227\r\n\r\n\r\n8.920188\r\n\r\n\r\n14.9\r\n\r\n\r\n23.45361\r\n\r\n\r\n50.27430\r\n\r\n\r\nLatent Profile Analysis\r\nData to run LPA\r\nThe latent profile analysis (normal mixture) will use variables loaded on factor 1 identified from the factor analysis:\r\nAgeDependencyRatio\r\nUnemployment\r\nFemaleHHPct\r\nMedianIncomeHH1K\r\nPctVacHousing\r\nPctBelowPoverty\r\nPublicAssistancePct\r\nLessThanHS\r\n\r\n\r\n\r\n    If we run Mclust for all 11 indicators, the data-driven would produce up to \r\n    8, 9 clusters. Indeed, I have checked the problem. That's the reason \r\n    we would use factor analysis to select covariates in the first components.\r\nCheck how many clusters should create?\r\n\r\nX <- as.matrix(df2[,-1])\r\nmod <- Mclust(X)\r\nsummary(mod$BIC)\r\nBest BIC values:\r\n            VVE,7        VVE,6        VVV,4\r\nBIC      -45583.8 -45597.13834 -45603.11491\r\nBIC diff      0.0    -13.33485    -19.31142\r\nplot(mod, what=\"BIC\", ylim=range(mod$BIC[,-(1:2)], na.rm=TRUE), legendArgs = list(x = \"bottomleft\")) \r\n\r\nsummary(mod)\r\n---------------------------------------------------- \r\nGaussian finite mixture model fitted by EM algorithm \r\n---------------------------------------------------- \r\n\r\nMclust VVE (ellipsoidal, equal orientation) model with 7 components: \r\n\r\n log-likelihood   n  df      BIC       ICL\r\n       -22304.2 797 146 -45583.8 -45709.47\r\n\r\nClustering table:\r\n  1   2   3   4   5   6   7 \r\n 74 143 129  19 253 101  78 \r\nhead(mod$classification)\r\n1 2 3 4 5 6 \r\n1 2 1 2 1 1 \r\nhead(mod$uncertainty)\r\n           1            2            3            4            5 \r\n0.0007328577 0.0256582102 0.0863792453 0.0139170110 0.0018607666 \r\n           6 \r\n0.0001025574 \r\nmod$uncertainty[mod$classification==4]\r\n          14           76          106          130          136 \r\n3.977713e-04 1.459000e-02 1.891587e-04 5.432918e-01 4.584159e-03 \r\n         164          337          349          395          399 \r\n2.167873e-10 4.375842e-06 1.160644e-03 3.932567e-02 4.171542e-01 \r\n         404          474          501          514          556 \r\n1.909584e-14 4.237307e-06 4.835498e-02 2.901384e-01 5.423051e-01 \r\n         606          615          730          735 \r\n3.456849e-01 4.081981e-01 0.000000e+00 1.035645e-06 \r\nplot(mod, what=\"classification\")\r\n\r\n\r\nIn the above Mclust() function call, the number of mixing components and the covariance parameterization are selected using the Bayesian Information Criterion (BIC). A summary showing the top-three models and a plot of the BIC traces (the first fig.) for all the models considered is then obtained.\r\nWe would use the indication of a 4-component mixture with covariances having different shapes, volume and orientation (VVV). The use can be confirmed by the second plot, there are no clear regions of 7 or 6-component on the plot.\r\nFitted a 4-components GMM with unconstrained covariance matrices\r\n\r\nmod.4 <- Mclust(df2[,-1], G=4) \r\nsummary(mod.4$BIC) \r\nBest BIC values:\r\n             VVV,4       VEV,4      VVE,4\r\nBIC      -45603.11 -45750.7443 -45805.360\r\nBIC diff      0.00   -147.6294   -202.245\r\nplot(mod.4, what=\"classification\")\r\n\r\nsummary(mod.4, parameters = TRUE)\r\n---------------------------------------------------- \r\nGaussian finite mixture model fitted by EM algorithm \r\n---------------------------------------------------- \r\n\r\nMclust VVV (ellipsoidal, varying volume, shape, and orientation)\r\nmodel with 4 components: \r\n\r\n log-likelihood   n  df       BIC       ICL\r\n      -22203.62 797 179 -45603.11 -45706.84\r\n\r\nClustering table:\r\n  1   2   3   4 \r\n 93 294 139 271 \r\n\r\nMixing probabilities:\r\n        1         2         3         4 \r\n0.1149777 0.3725480 0.1729934 0.3394809 \r\n\r\nMeans:\r\n                         [,1]     [,2]      [,3]     [,4]\r\nAgeDependencyRatio  23.610425 50.09446 48.738241 68.03065\r\nLessThanHS           2.443075 25.44106  9.556613 20.53996\r\nUnemployment         4.670161 11.64512  6.805765 24.94439\r\nFemaleHHPct          3.605628 15.91133  8.987856 36.01190\r\nMedianIncomeHH1K    94.399365 42.76009 74.934483 29.19450\r\nPctVacHousing       11.171810 11.36435  7.438423 20.89255\r\nPctBelowPoverty     10.542959 23.73358  9.221573 36.57643\r\nPublicAssistancePct  3.680982 22.34020  7.395136 42.46116\r\n\r\nVariances:\r\n[,,1]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          110.827954   4.505709   -2.5527991\r\nLessThanHS                    4.505709   4.825718    1.3439579\r\nUnemployment                 -2.552799   1.343958    7.4948966\r\nFemaleHHPct                   9.592752   2.963274    0.8724782\r\nMedianIncomeHH1K             21.948939  -6.215072   -5.8068859\r\nPctVacHousing                 3.617609   5.612807   13.4091673\r\nPctBelowPoverty             -11.759843   2.453223    4.0747698\r\nPublicAssistancePct           9.902373   4.939258    1.0298916\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    9.5927518       21.9489390      3.617609\r\nLessThanHS            2.9632739       -6.2150719      5.612807\r\nUnemployment          0.8724782       -5.8068859     13.409167\r\nFemaleHHPct           6.4553757        0.3348242      4.420610\r\nMedianIncomeHH1K      0.3348242      624.2423262      3.149840\r\nPctVacHousing         4.4206103        3.1498396     83.845803\r\nPctBelowPoverty       2.5312080      -45.4496868      8.420220\r\nPublicAssistancePct   6.0893513       -2.1182191      7.549918\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio       -11.759843            9.902373\r\nLessThanHS                 2.453223            4.939258\r\nUnemployment               4.074770            1.029892\r\nFemaleHHPct                2.531208            6.089351\r\nMedianIncomeHH1K         -45.449687           -2.118219\r\nPctVacHousing              8.420220            7.549918\r\nPctBelowPoverty           26.561832            3.884233\r\nPublicAssistancePct        3.884233           13.178735\r\n[,,2]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          214.300185 126.992111    16.865825\r\nLessThanHS                  126.992111 206.987885    12.301917\r\nUnemployment                 16.865825  12.301917    18.503181\r\nFemaleHHPct                  45.098013  48.787591     9.238511\r\nMedianIncomeHH1K            -23.094658 -56.231510   -11.054392\r\nPctVacHousing                -3.977919   9.033062     1.628468\r\nPctBelowPoverty               2.423802  33.812115     8.997776\r\nPublicAssistancePct          60.250147  80.617836    16.223181\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    45.098013       -23.094658     -3.977919\r\nLessThanHS            48.787591       -56.231510      9.033062\r\nUnemployment           9.238511       -11.054392      1.628468\r\nFemaleHHPct           45.928065        -7.903785      2.925860\r\nMedianIncomeHH1K      -7.903785       125.877832    -17.588453\r\nPctVacHousing          2.925860       -17.588453     21.535258\r\nPctBelowPoverty        9.689907       -73.612703     15.542755\r\nPublicAssistancePct   30.032800       -64.034777     12.149998\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio         2.423802            60.25015\r\nLessThanHS                33.812115            80.61784\r\nUnemployment               8.997776            16.22318\r\nFemaleHHPct                9.689907            30.03280\r\nMedianIncomeHH1K         -73.612703           -64.03478\r\nPctVacHousing             15.542755            12.15000\r\nPctBelowPoverty           85.926470            49.99227\r\nPublicAssistancePct       49.992266            90.50270\r\n[,,3]\r\n                    AgeDependencyRatio   LessThanHS Unemployment\r\nAgeDependencyRatio          294.197105 -12.24417798   16.7767540\r\nLessThanHS                  -12.244178  24.62164001    4.8263400\r\nUnemployment                 16.776754   4.82634001    8.0873820\r\nFemaleHHPct                   9.701178   9.04745430    6.1616615\r\nMedianIncomeHH1K             13.966827 -56.09164058  -21.9078480\r\nPctVacHousing                 5.163643  -0.03837313   -0.7952366\r\nPctBelowPoverty             -28.576144  13.88339665    4.2317608\r\nPublicAssistancePct          -7.729161  14.21609068    3.7741089\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio    9.7011782        13.966827   5.163642823\r\nLessThanHS            9.0474543       -56.091641  -0.038373126\r\nUnemployment          6.1616615       -21.907848  -0.795236625\r\nFemaleHHPct          19.6797355       -28.074212  -0.444020130\r\nMedianIncomeHH1K    -28.0742123       332.351159  -7.866381763\r\nPctVacHousing        -0.4440201        -7.866382  12.781134605\r\nPctBelowPoverty       5.6968101       -48.427943   1.159862443\r\nPublicAssistancePct   8.3047060       -43.608455  -0.005379067\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio       -28.576144        -7.729161059\r\nLessThanHS                13.883397        14.216090678\r\nUnemployment               4.231761         3.774108900\r\nFemaleHHPct                5.696810         8.304706000\r\nMedianIncomeHH1K         -48.427943       -43.608455461\r\nPctVacHousing              1.159862        -0.005379067\r\nPctBelowPoverty           24.687016        15.650477682\r\nPublicAssistancePct       15.650478        22.034142873\r\n[,,4]\r\n                    AgeDependencyRatio LessThanHS Unemployment\r\nAgeDependencyRatio          218.842790   25.84154     6.528224\r\nLessThanHS                   25.841542   67.06932    13.320989\r\nUnemployment                  6.528224   13.32099    95.039772\r\nFemaleHHPct                  21.881252   17.59145    37.153627\r\nMedianIncomeHH1K            -38.416907  -39.71887   -39.349411\r\nPctVacHousing                 3.741222   22.53556    23.335309\r\nPctBelowPoverty              58.648501   57.09121    65.051666\r\nPublicAssistancePct          43.325884   47.59794    63.689544\r\n                    FemaleHHPct MedianIncomeHH1K PctVacHousing\r\nAgeDependencyRatio     21.88125        -38.41691      3.741222\r\nLessThanHS             17.59145        -39.71887     22.535563\r\nUnemployment           37.15363        -39.34941     23.335309\r\nFemaleHHPct           115.25845        -28.87802     25.653844\r\nMedianIncomeHH1K      -28.87802        100.69000    -30.670522\r\nPctVacHousing          25.65384        -30.67052     90.092820\r\nPctBelowPoverty        73.51904       -110.34219     48.794240\r\nPublicAssistancePct    80.89187        -99.63891     42.436649\r\n                    PctBelowPoverty PublicAssistancePct\r\nAgeDependencyRatio         58.64850            43.32588\r\nLessThanHS                 57.09121            47.59794\r\nUnemployment               65.05167            63.68954\r\nFemaleHHPct                73.51904            80.89187\r\nMedianIncomeHH1K         -110.34219           -99.63891\r\nPctVacHousing              48.79424            42.43665\r\nPctBelowPoverty           184.51805           149.38841\r\nPublicAssistancePct       149.38841           178.56187\r\n# compare classification between mod and mod.4\r\n#table(mod$classification, mod.4$classification)\r\n\r\ndf2$ConcDisadv_cluster <- mod.4$classification\r\nwith(df2, table(ConcDisadv_cluster))\r\nConcDisadv_cluster\r\n  1   2   3   4 \r\n 93 294 139 271 \r\ndf2[,11:14] <- mod.4$z\r\nnames(df2)[11:14] <- c(\"ConcDisadv_prob1\", \"ConcDisadv_prob2\", \"ConcDisadv_prob3\", \"ConcDisadv_prob4\")\r\nkable(df2[20:50,], caption = \"Concentrated Disadvantage Clustering Selection based on the highest probability\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:4-comp VVV)Concentrated Disadvantage Clustering Selection based on the highest probability\r\n\r\n\r\n\r\n\r\ntract\r\n\r\n\r\nAgeDependencyRatio\r\n\r\n\r\nLessThanHS\r\n\r\n\r\nUnemployment\r\n\r\n\r\nFemaleHHPct\r\n\r\n\r\nMedianIncomeHH1K\r\n\r\n\r\nPctVacHousing\r\n\r\n\r\nPctBelowPoverty\r\n\r\n\r\nPublicAssistancePct\r\n\r\n\r\nConcDisadv_cluster\r\n\r\n\r\nConcDisadv_prob1\r\n\r\n\r\nConcDisadv_prob2\r\n\r\n\r\nConcDisadv_prob3\r\n\r\n\r\nConcDisadv_prob4\r\n\r\n\r\n20\r\n\r\n\r\n20701\r\n\r\n\r\n65.8\r\n\r\n\r\n12.9\r\n\r\n\r\n9.1\r\n\r\n\r\n13.217391\r\n\r\n\r\n68.750\r\n\r\n\r\n17.857143\r\n\r\n\r\n13.7\r\n\r\n\r\n8.869565\r\n\r\n\r\n3\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.0081983\r\n\r\n\r\n0.9918002\r\n\r\n\r\n0.0000015\r\n\r\n\r\n21\r\n\r\n\r\n20702\r\n\r\n\r\n57.6\r\n\r\n\r\n28.1\r\n\r\n\r\n4.7\r\n\r\n\r\n12.505623\r\n\r\n\r\n54.364\r\n\r\n\r\n13.837209\r\n\r\n\r\n19.8\r\n\r\n\r\n25.551057\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9998515\r\n\r\n\r\n0.0000269\r\n\r\n\r\n0.0001216\r\n\r\n\r\n22\r\n\r\n\r\n20801\r\n\r\n\r\n50.7\r\n\r\n\r\n24.9\r\n\r\n\r\n10.5\r\n\r\n\r\n13.394683\r\n\r\n\r\n45.263\r\n\r\n\r\n12.834225\r\n\r\n\r\n21.7\r\n\r\n\r\n20.858896\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9948329\r\n\r\n\r\n0.0043355\r\n\r\n\r\n0.0008316\r\n\r\n\r\n23\r\n\r\n\r\n20802\r\n\r\n\r\n45.5\r\n\r\n\r\n18.9\r\n\r\n\r\n7.4\r\n\r\n\r\n9.287777\r\n\r\n\r\n42.346\r\n\r\n\r\n14.870954\r\n\r\n\r\n29.1\r\n\r\n\r\n29.018287\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9992171\r\n\r\n\r\n0.0000094\r\n\r\n\r\n0.0007735\r\n\r\n\r\n24\r\n\r\n\r\n20901\r\n\r\n\r\n48.0\r\n\r\n\r\n16.0\r\n\r\n\r\n5.3\r\n\r\n\r\n15.280236\r\n\r\n\r\n38.799\r\n\r\n\r\n21.199442\r\n\r\n\r\n28.2\r\n\r\n\r\n19.115044\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9633811\r\n\r\n\r\n0.0000115\r\n\r\n\r\n0.0366074\r\n\r\n\r\n25\r\n\r\n\r\n20902\r\n\r\n\r\n55.0\r\n\r\n\r\n13.5\r\n\r\n\r\n5.3\r\n\r\n\r\n10.437052\r\n\r\n\r\n33.114\r\n\r\n\r\n9.982384\r\n\r\n\r\n26.7\r\n\r\n\r\n21.461187\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9775131\r\n\r\n\r\n0.0002441\r\n\r\n\r\n0.0222428\r\n\r\n\r\n26\r\n\r\n\r\n30101\r\n\r\n\r\n36.2\r\n\r\n\r\n5.0\r\n\r\n\r\n8.7\r\n\r\n\r\n7.772727\r\n\r\n\r\n41.188\r\n\r\n\r\n11.646586\r\n\r\n\r\n21.0\r\n\r\n\r\n12.954546\r\n\r\n\r\n2\r\n\r\n\r\n0.0123186\r\n\r\n\r\n0.9845259\r\n\r\n\r\n0.0029942\r\n\r\n\r\n0.0001613\r\n\r\n\r\n27\r\n\r\n\r\n30102\r\n\r\n\r\n23.5\r\n\r\n\r\n7.4\r\n\r\n\r\n14.9\r\n\r\n\r\n8.969132\r\n\r\n\r\n43.632\r\n\r\n\r\n18.779565\r\n\r\n\r\n14.5\r\n\r\n\r\n10.308678\r\n\r\n\r\n2\r\n\r\n\r\n0.0128167\r\n\r\n\r\n0.9713396\r\n\r\n\r\n0.0000043\r\n\r\n\r\n0.0158394\r\n\r\n\r\n28\r\n\r\n\r\n30103\r\n\r\n\r\n12.7\r\n\r\n\r\n14.6\r\n\r\n\r\n8.5\r\n\r\n\r\n0.000000\r\n\r\n\r\n37.383\r\n\r\n\r\n10.844371\r\n\r\n\r\n19.5\r\n\r\n\r\n11.142061\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.7999190\r\n\r\n\r\n0.2000236\r\n\r\n\r\n0.0000574\r\n\r\n\r\n29\r\n\r\n\r\n30104\r\n\r\n\r\n9.5\r\n\r\n\r\n9.9\r\n\r\n\r\n14.4\r\n\r\n\r\n1.628106\r\n\r\n\r\n36.529\r\n\r\n\r\n8.542320\r\n\r\n\r\n39.8\r\n\r\n\r\n21.679520\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9999999\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000001\r\n\r\n\r\n30\r\n\r\n\r\n30200\r\n\r\n\r\n33.0\r\n\r\n\r\n5.5\r\n\r\n\r\n7.5\r\n\r\n\r\n3.708333\r\n\r\n\r\n67.700\r\n\r\n\r\n11.699779\r\n\r\n\r\n10.5\r\n\r\n\r\n2.500000\r\n\r\n\r\n1\r\n\r\n\r\n0.5761337\r\n\r\n\r\n0.0351601\r\n\r\n\r\n0.3887062\r\n\r\n\r\n0.0000000\r\n\r\n\r\n31\r\n\r\n\r\n30300\r\n\r\n\r\n42.8\r\n\r\n\r\n18.2\r\n\r\n\r\n8.2\r\n\r\n\r\n7.496464\r\n\r\n\r\n36.020\r\n\r\n\r\n8.062419\r\n\r\n\r\n24.8\r\n\r\n\r\n19.024045\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9637376\r\n\r\n\r\n0.0352999\r\n\r\n\r\n0.0009626\r\n\r\n\r\n32\r\n\r\n\r\n30400\r\n\r\n\r\n44.3\r\n\r\n\r\n12.3\r\n\r\n\r\n18.1\r\n\r\n\r\n14.767255\r\n\r\n\r\n41.250\r\n\r\n\r\n8.247423\r\n\r\n\r\n27.8\r\n\r\n\r\n30.577849\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9820020\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0179980\r\n\r\n\r\n33\r\n\r\n\r\n30500\r\n\r\n\r\n29.1\r\n\r\n\r\n13.3\r\n\r\n\r\n2.7\r\n\r\n\r\n6.931383\r\n\r\n\r\n62.453\r\n\r\n\r\n11.087024\r\n\r\n\r\n15.7\r\n\r\n\r\n5.781957\r\n\r\n\r\n3\r\n\r\n\r\n0.0000003\r\n\r\n\r\n0.2383814\r\n\r\n\r\n0.7616182\r\n\r\n\r\n0.0000001\r\n\r\n\r\n34\r\n\r\n\r\n30601\r\n\r\n\r\n35.0\r\n\r\n\r\n6.8\r\n\r\n\r\n9.7\r\n\r\n\r\n2.740409\r\n\r\n\r\n38.822\r\n\r\n\r\n13.079255\r\n\r\n\r\n21.4\r\n\r\n\r\n15.695067\r\n\r\n\r\n2\r\n\r\n\r\n0.0000029\r\n\r\n\r\n0.9992705\r\n\r\n\r\n0.0004956\r\n\r\n\r\n0.0002309\r\n\r\n\r\n35\r\n\r\n\r\n30603\r\n\r\n\r\n29.2\r\n\r\n\r\n7.2\r\n\r\n\r\n10.5\r\n\r\n\r\n11.488251\r\n\r\n\r\n28.631\r\n\r\n\r\n17.159337\r\n\r\n\r\n35.5\r\n\r\n\r\n20.626632\r\n\r\n\r\n2\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.9999831\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000169\r\n\r\n\r\n36\r\n\r\n\r\n30604\r\n\r\n\r\n34.4\r\n\r\n\r\n4.1\r\n\r\n\r\n6.0\r\n\r\n\r\n3.826531\r\n\r\n\r\n32.069\r\n\r\n\r\n12.421805\r\n\r\n\r\n22.2\r\n\r\n\r\n12.551020\r\n\r\n\r\n2\r\n\r\n\r\n0.0148111\r\n\r\n\r\n0.9846784\r\n\r\n\r\n0.0004018\r\n\r\n\r\n0.0001087\r\n\r\n\r\n37\r\n\r\n\r\n30701\r\n\r\n\r\n31.7\r\n\r\n\r\n15.5\r\n\r\n\r\n15.1\r\n\r\n\r\n6.310160\r\n\r\n\r\n33.504\r\n\r\n\r\n3.409091\r\n\r\n\r\n31.0\r\n\r\n\r\n14.866310\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9996054\r\n\r\n\r\n0.0000713\r\n\r\n\r\n0.0003233\r\n\r\n\r\n38\r\n\r\n\r\n30702\r\n\r\n\r\n39.6\r\n\r\n\r\n1.5\r\n\r\n\r\n7.8\r\n\r\n\r\n3.538928\r\n\r\n\r\n48.870\r\n\r\n\r\n15.107296\r\n\r\n\r\n7.8\r\n\r\n\r\n1.718908\r\n\r\n\r\n1\r\n\r\n\r\n0.9065519\r\n\r\n\r\n0.0830127\r\n\r\n\r\n0.0098448\r\n\r\n\r\n0.0005907\r\n\r\n\r\n39\r\n\r\n\r\n30703\r\n\r\n\r\n35.6\r\n\r\n\r\n5.0\r\n\r\n\r\n6.3\r\n\r\n\r\n5.128205\r\n\r\n\r\n46.174\r\n\r\n\r\n12.883117\r\n\r\n\r\n13.7\r\n\r\n\r\n17.829457\r\n\r\n\r\n2\r\n\r\n\r\n0.0001535\r\n\r\n\r\n0.9958622\r\n\r\n\r\n0.0020986\r\n\r\n\r\n0.0018857\r\n\r\n\r\n40\r\n\r\n\r\n30706\r\n\r\n\r\n54.6\r\n\r\n\r\n8.8\r\n\r\n\r\n2.2\r\n\r\n\r\n5.118755\r\n\r\n\r\n36.362\r\n\r\n\r\n8.914584\r\n\r\n\r\n21.8\r\n\r\n\r\n11.916462\r\n\r\n\r\n2\r\n\r\n\r\n0.0004362\r\n\r\n\r\n0.9797384\r\n\r\n\r\n0.0005719\r\n\r\n\r\n0.0192536\r\n\r\n\r\n41\r\n\r\n\r\n30800\r\n\r\n\r\n39.2\r\n\r\n\r\n6.7\r\n\r\n\r\n9.5\r\n\r\n\r\n3.619909\r\n\r\n\r\n70.361\r\n\r\n\r\n12.262902\r\n\r\n\r\n7.5\r\n\r\n\r\n2.664656\r\n\r\n\r\n3\r\n\r\n\r\n0.0819534\r\n\r\n\r\n0.0953799\r\n\r\n\r\n0.8226666\r\n\r\n\r\n0.0000001\r\n\r\n\r\n42\r\n\r\n\r\n30900\r\n\r\n\r\n26.7\r\n\r\n\r\n7.2\r\n\r\n\r\n2.6\r\n\r\n\r\n9.686411\r\n\r\n\r\n80.657\r\n\r\n\r\n5.716163\r\n\r\n\r\n3.2\r\n\r\n\r\n10.871080\r\n\r\n\r\n3\r\n\r\n\r\n0.1331410\r\n\r\n\r\n0.0036901\r\n\r\n\r\n0.8631689\r\n\r\n\r\n0.0000000\r\n\r\n\r\n43\r\n\r\n\r\n31000\r\n\r\n\r\n30.1\r\n\r\n\r\n5.6\r\n\r\n\r\n5.9\r\n\r\n\r\n8.908202\r\n\r\n\r\n66.875\r\n\r\n\r\n11.191510\r\n\r\n\r\n5.8\r\n\r\n\r\n5.377512\r\n\r\n\r\n3\r\n\r\n\r\n0.0880318\r\n\r\n\r\n0.1053896\r\n\r\n\r\n0.8065776\r\n\r\n\r\n0.0000009\r\n\r\n\r\n44\r\n\r\n\r\n31100\r\n\r\n\r\n21.8\r\n\r\n\r\n12.9\r\n\r\n\r\n7.9\r\n\r\n\r\n5.386221\r\n\r\n\r\n57.994\r\n\r\n\r\n4.543643\r\n\r\n\r\n8.6\r\n\r\n\r\n7.891440\r\n\r\n\r\n3\r\n\r\n\r\n0.0000005\r\n\r\n\r\n0.0988186\r\n\r\n\r\n0.9011727\r\n\r\n\r\n0.0000082\r\n\r\n\r\n45\r\n\r\n\r\n31200\r\n\r\n\r\n23.6\r\n\r\n\r\n24.0\r\n\r\n\r\n10.9\r\n\r\n\r\n6.720122\r\n\r\n\r\n20.887\r\n\r\n\r\n16.111467\r\n\r\n\r\n34.9\r\n\r\n\r\n42.802596\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9878027\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0121973\r\n\r\n\r\n46\r\n\r\n\r\n31300\r\n\r\n\r\n57.4\r\n\r\n\r\n10.7\r\n\r\n\r\n10.0\r\n\r\n\r\n11.997074\r\n\r\n\r\n35.349\r\n\r\n\r\n3.392226\r\n\r\n\r\n23.6\r\n\r\n\r\n25.042672\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.8653997\r\n\r\n\r\n0.0003787\r\n\r\n\r\n0.1342215\r\n\r\n\r\n47\r\n\r\n\r\n31400\r\n\r\n\r\n27.4\r\n\r\n\r\n4.9\r\n\r\n\r\n4.0\r\n\r\n\r\n6.587404\r\n\r\n\r\n74.737\r\n\r\n\r\n9.508578\r\n\r\n\r\n8.1\r\n\r\n\r\n3.566838\r\n\r\n\r\n3\r\n\r\n\r\n0.3567877\r\n\r\n\r\n0.0026885\r\n\r\n\r\n0.6405238\r\n\r\n\r\n0.0000000\r\n\r\n\r\n48\r\n\r\n\r\n31501\r\n\r\n\r\n47.9\r\n\r\n\r\n15.1\r\n\r\n\r\n11.9\r\n\r\n\r\n10.254854\r\n\r\n\r\n24.643\r\n\r\n\r\n14.300572\r\n\r\n\r\n50.7\r\n\r\n\r\n36.165048\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9993333\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0006667\r\n\r\n\r\n49\r\n\r\n\r\n31502\r\n\r\n\r\n34.5\r\n\r\n\r\n7.6\r\n\r\n\r\n16.1\r\n\r\n\r\n6.619280\r\n\r\n\r\n21.265\r\n\r\n\r\n9.670947\r\n\r\n\r\n32.5\r\n\r\n\r\n32.119058\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9972245\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0027755\r\n\r\n\r\n50\r\n\r\n\r\n31700\r\n\r\n\r\n20.0\r\n\r\n\r\n8.2\r\n\r\n\r\n12.7\r\n\r\n\r\n9.429569\r\n\r\n\r\n40.129\r\n\r\n\r\n10.520833\r\n\r\n\r\n33.8\r\n\r\n\r\n22.002328\r\n\r\n\r\n2\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.9999976\r\n\r\n\r\n0.0000001\r\n\r\n\r\n0.0000023\r\n\r\n\r\nHere is the interesting point. Even though, we would see the mismatch among the clustering with the indicators, for example, cluster 4 and 3 may have the contradict between income, less than HS and poverty level. But overall, we will apply on the real cases, the clustering reflexes the correct situations. We can see below 2 applications on mapping and on classifying on race-ethnicity.\r\nApplying the Clustering\r\nMapping on Chicago Neighborhood map\r\nWith the level:\r\n1: not/mild disadvantage\r\n2, 3: medium disadvantage\r\n4: highly disadvantage\r\nWe can map on Chicago Neighborhood map (using ArcGIS) as such\r\n\r\nClassification by race-ethnicity\r\nSave for later on!!!\r\nRefs\r\nPugach, Oksana. 2018. “Latent Profile Analysis of Chicago Neighborhoods” Unpublished Work. Chalk Talk - Institute for Health Resereach; Policy - Methodology Research Core.\r\nUniversity of Virginia Library, Research Data Services + Sciences. “Getting Started with Factor Analysis” - https://data.library.virginia.edu/getting-started-with-factor-analysis/\r\nStats Stackexchange. “How to calculate the loading matrix from the score matrix and a data matrix X (PCA)?” - https://stats.stackexchange.com/questions/447952/how-to-calculate-the-loading-matrix-from-the-score-matrix-and-a-data-matrix-x-p\r\nLuca Scrucca. 2020. “A quick tour of mclust” - https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html\r\nLuca Scrucca, Michael Fop, T. Brendan Murphy,Adrian E. Raftery. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” R J. 2016 August ; 8(1): 289-317\r\n\r\n\r\n",
    "preview": "posts/2021-10-24-LPA Chicago Neighborhoods/distill-preview.png",
    "last_modified": "2021-10-25T11:17:54-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-25-selection-bias/",
    "title": "Selection bias",
    "description": "Understanding: definition, examples;    \nApplying in DAG;    \nGeneral solution;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-08-25",
    "categories": [
      "Causal Inference",
      "DiagrammeR"
    ],
    "contents": "\r\n\r\nContents\r\nSelection bias: set up\r\nSelection bias happens when? ~ Examples\r\nWhat’s the target population?\r\nWhy not?\r\nConditioning on a collider\r\nCommon structure\r\nIs the selected group different?\r\nConditioning on a collider\r\n\r\nA note about confounding\r\nMore examples\r\nWhat to do?\r\n\r\nSelection bias occurs when some part of the target population is not in the sampled population, or, more generally, when some population units are sampled at a different rate than intended by the investigator. A good sample will be as free from selection bias as possible.\r\n— Sharon L. Lohr, Sampling: Design and Analysis 2nd\r\nSelection bias happens in different fields. Everything can be “selection bias”\r\nModel selection\r\nProblems with statistical inference\r\n\r\nConfounding\r\nIn certain fields… “selection into treatment”\r\n\r\nNon-generalizability/transportability\r\nEffect in sample not the same as in target population\r\n\r\nCollider stratification (worst form of selection bias)\r\nBias for causal effects even within sample and under the null\r\n\r\nSome other names refer to “selection bias”\r\nBerkson’s bias\r\nHealthy worker effect\r\nCensoring/truncation\r\nNon-response bias\r\nPrevalent-user bias\r\nVolunteer bias\r\nIncidence-prevalence bias\r\nIndex-event bias\r\nSurvivor bias\r\nSelection bias: set up\r\n\\(X\\): exposure of interest\\(Y\\): outcome of interest\\(S\\): selection into study (S = 1 if selected)\r\nWe can estimate \\[ RR^s_{XY} = \\frac{Pr(Y = 1 \\mid X = 1; S = 1)}{Pr(Y = 1 \\mid X = 0; S = 1)} \\]\r\nwhich may not equal \\(RR^t_{XY}\\)\r\n\\(s\\): subject to bias\\(t\\): true\r\nWhat is \\(RR^t_{XY}\\)? \\(RR^t_{XY}\\) is the true causal effect in the target population.\r\n\r\nWe will assume that if we estimated \\(\\frac{Pr(Y =1\\mid X=1)}{Pr(Y =1\\mid X=0)}\\) , this is what we’d get\r\nSelection bias happens when? ~ Examples\r\n— (Hernán, Hernández-Díaz, and Robins 2004)\r\nConsider a randomized trial of anti-retroviral therapy (\\(X\\)) among people living with HIV, with a goal of preventing the development of AIDS (\\(Y\\))\r\n\\(\\frac{Pr(Y = 1 \\mid X = 1)}{Pr(Y = 1 \\mid X = 0)}\\) is the risk ratio among people randomized to the intervention arm vs. standard of care\r\nIf some people drop out of the study, we estimate \\(\\frac{Pr(Y = 1 \\mid X = 1; S = 1)}{Pr(Y = 1 \\mid X = 0; S = 1)}\\)\r\n\r\n\r\nhide\r\nlibrary(DiagrammeR) #grViz\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode[shape=none]\r\nX \r\n\r\nnode [shape = box,\r\n      fontname = Helvetica]\r\nS\r\n\r\nnode[shape=none]\r\nY\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode[shape=none]\\nX \\n\\nnode [shape = box,\\n      fontname = Helvetica]\\nS\\n\\nnode[shape=none]\\nY\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\nWhat’s the target population?\r\nTarget population The complete collection of observations we want to study. Defining the target population is an important and often difficult part of the study. For example, in a political poll, should the target population be all adults eligible to vote? All registered voters? All persons who voted in the last election? The choice of target population will profoundly affect the statistics that result.\r\n— Sharon L. Lohr, Sampling: Design and Analysis, 2nd\r\nThe study participants are not a random sample of all people living with HIV … is that a problem?\r\nPerhaps, if we’re trying to estimate how effective treatment would be in another context.\r\nBut not when it comes to estimating valid causal effects. With complete follow-up, we can estimate the effect of the drug in the target population from which the participants came.\r\nWithout loss to follow-up, we can’t even estimate that — not to mention generalize to another context.\r\nWhy not?\r\nThe participants who were lost to follow-up are not a random sample of all participants\r\nPerhaps the most severely immunocompromised people have trouble coming to study visits\r\nThey are also at higher risk of developing AIDS\r\nPerhaps people experiencing side effects of treatment no longer want to participate\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U;\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  S -> U\r\n  Y -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> S\r\n  S -> Y [color = white]\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U;\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  S -> U\\n  Y -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> S\\n  S -> Y [color = white]\\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConditioning on a collider\r\nSelection bias can occur when a non-causal X-Y path is opened by conditioning on \\(S\\)\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U;\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back, color = red]\r\n  S -> U \r\n  Y -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> S [color = red]\r\n  S -> Y [color = white]\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U;\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back, color = red]\\n  S -> U \\n  Y -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> S [color = red]\\n  S -> Y [color = white]\\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nCommon structure\r\nDoes Zika virus infection (\\(X\\)) increase the risk of microcephaly (\\(Y\\))?\r\nWe only assess microcephaly among live births (\\(S\\) = 1).\r\nElective terminations are not included (\\(S\\) = 0).\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode[shape=none]\r\nX \r\n\r\nnode [shape = box,\r\n      fontname = Helvetica]\r\nS\r\n\r\nnode[shape=none]\r\nY\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode[shape=none]\\nX \\n\\nnode [shape = box,\\n      fontname = Helvetica]\\nS\\n\\nnode[shape=none]\\nY\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nIs the selected group different?\r\nWe might assume that\r\nPeople who have more exposure to the virus are more likely to choose to end their pregnancies (worried about risks)\r\nPeople with less access to health care are less likely to have access to abortion services\r\nThere are factors that affect risk of microcephaly that are correlated with access to health care\r\nConditioning on a collider\r\nIntuitively, pregnancies are either in our study if:\r\nThey have lower-than-average probability of exposure to Zika virus (but average risk of microcephaly)\r\nThe have higher-than-average risk of microcephaly (but average risk of Zika exposure)\r\nThe already low-risk pregnancies also have lower exposure to the virus…. It looks like exposure to Zika virus is associated with microcephaly.\r\nA note about confounding\r\nThere are also confounders of the \\(X - Y\\) relationship, of course, since this study is observational\r\ni.e., other reasons why people might be simultaneously at high risk of Zika exposure and microcephaly\r\n\r\nSome of those might be the same factors causing selection bias\r\nIf we properly adjust for them to control confounding, we also control selection bias\r\n\r\nIf there are additional factors leading to selection bias that aren’t confounders, we may not plan to measure or adjust for them\r\nWe’ll tie confounding and selection bias (and misclassification!) together at the end\r\nMore examples\r\nConsider a pharmacoepidemiology study comparing outcomes (\\(Y\\)) in current users vs. never users of a drug (\\(X\\)):\r\nIf people more at risk (\\(U_2\\)) of outcome \\(Y\\) also have more side effects \\(U_1\\), they are more likely to discontinue the drug and not be included in the study (\\(S\\) = 0).\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U1; U2\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  U1 -> U2 \r\n  Y -> U2\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  X -> U1 -> S\r\n  S -> Y [color = white]\r\n  \r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U1; U2\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  U1 -> U2 \\n  Y -> U2\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  X -> U1 -> S\\n  S -> Y [color = white]\\n  \\n  edge[color=gray]\\n  X -> Y \\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConsider a case-control study of coffee consumption (\\(X\\)) and pancreatic cancer (\\(Y\\)), in which controls are chosen from the same hospital:\r\nSelection is based on case status (\\(Y\\)). If controls with gastrointestinal disease are used (\\(U\\)), the fact that they are more likely to avoid coffee can make coffee look like it causes cancer.\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  X -> U \r\n  S -> U\r\n}\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  edge[color=gray]\r\n  X -> Y \r\n  edge[color=black]\r\n  Y -> S\r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  X -> U \\n  S -> U\\n}\\n\\nsubgraph C{\\n  rank=same;\\n  edge[color=gray]\\n  X -> Y \\n  edge[color=black]\\n  Y -> S\\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nConsider a study of the effect of antidepressants (\\(X\\)) on lung cancer (\\(Y\\)) among people with coronary artery disease (\\(S\\)):\r\nIf depression (\\(U1\\)) causes \\(X\\) and \\(S\\), and smoking (\\(U_2\\)) causes \\(S\\) and \\(Y\\) , selection bias (“\\(M\\)-bias”) can result.\r\n\r\n\r\nhide\r\ngrViz(\"\r\ndigraph causal{\r\n\r\nnode [shape = box]\r\nS\r\n\r\nnode[shape=none]\r\nX; Y; U1; U2\r\n\r\nsubgraph C{\r\n  rank=same;\r\n  edge[color=gray]\r\n  X -> Y \r\n}\r\n\r\nsubgraph U{\r\n  rankdir=TB; edge[dir=back]\r\n  X -> U1\r\n  Y -> U2\r\n  S -> U1\r\n  S -> U2\r\n}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\nnode [shape = box]\\nS\\n\\nnode[shape=none]\\nX; Y; U1; U2\\n\\nsubgraph C{\\n  rank=same;\\n  edge[color=gray]\\n  X -> Y \\n}\\n\\nsubgraph U{\\n  rankdir=TB; edge[dir=back]\\n  X -> U1\\n  Y -> U2\\n  S -> U1\\n  S -> U2\\n}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n— (Hernán, Hernández-Díaz, and Robins 2004) and (Smith 2020)\r\nWhat to do?\r\nGo back to the drawing board\r\nDesign a better study …\r\nTrack down those lost to follow-up\r\n\r\nAttempt to correct for it\r\nMeasure the variables causing bias\r\nModeling assumptions\r\n\r\nRedefine question\r\nConsider interpretability\r\n\r\nOR\r\nSensitivity analysis!\r\n\r\n\r\nHernán, Miguel A., Sonia Hernández-Díaz, and James M. Robins. 2004. “A Structural Approach to Selection Bias.” Journal Article. Epidemiology (Cambridge, Mass.) 15 (5): 615–25. https://doi.org/10.1097/01.ede.0000135174.63482.43.\r\n\r\n\r\nSmith, Louisa H. 2020. “Selection Mechanisms and Their Consequences: Understanding and Addressing Selection Bias.” Journal Article. Current Epidemiology Reports 7 (4): 179–89. https://doi.org/10.1007/s40471-020-00241-6.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-26T09:32:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-03-matching in R-causal-inference/",
    "title": "Matching in R",
    "description": "Observational study;    \nCausal Inference;    \nMatching;    \nR;",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-07-03",
    "categories": [
      "Biostatistics",
      "Causal Inference",
      "Tutorial",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nResearch\r\nquestions\r\nGoal\r\nContext\r\nData\r\nCovariates with Missing\r\nValues\r\n\r\nPropensity\r\nScore\r\nDistance\r\nMatrix\r\nConstructing the Match (B. B. Hansen\r\n2007)\r\nChecking Covariate Balance\r\nOutcomes\r\nFurther\r\nreading\r\n\r\n\r\nlibrary(DOS) # Design of Observational Studies\r\nlibrary(tidyverse)\r\nlibrary(kableExtra)\r\nlibrary(optmatch)\r\n\r\nMultivariate matching is involved in 3 steps:\r\n(i) creating a distance matrix,\r\n(ii) adding a propensity score caliper to the distance matrix, and\r\n(iii) finding an optimal match.\r\nMatching in R created largely due to the efforts of Ben Hansen (B. B. Hansen 2007), who created an R\r\nfunction, fullmatch, to do optimal matching from Fortan code created by\r\nDemetri Bertsekas D. P.\r\nBertsekas (1981).\r\nResearch questions\r\nDoes financial aid increase college attendance?\r\nGoal\r\nFour matched samples will be constructed: The\r\nconstruction of one of the four matched samples is presented in\r\nstep-by-step detail.\r\nContext\r\nDuring 1979-1981\r\nSocial Security Student Benefit Program provided a substantial\r\ntuition benefit for a child of a deceased father, before the\r\ncancellation in 1982.\r\nAnalyzing the impact on college attendance and completed\r\nschooling of the elimination of the Social Security Student Benefit\r\nProgram in 1982\r\nFrom 1965 to 1982, the Social Security Administration paid for\r\nmillions of students to go to college. Under this program, the 18- to\r\n22-year-old children of deceased, disabled, or retired Social Security\r\nbeneficiaries received monthly payments while enrolled full time in\r\ncollege.\r\nIn 1981, Congress voted to eliminate the program. Then was\r\ncancelled in 1982\r\nThe program’s demise provides an opportunity to measure the\r\nincentive effects of financial aid. Using difference-in-differences\r\nmethodology, and proxying for benefit eligibility with the death of a\r\nparent during an individual’s childhood, found that ? the\r\nelimination of the Social Security student benefit program reduced\r\ncollege attendance probabilities\r\nData\r\nData Xb with 2820 rows and 8 columns:\r\n- faminc: family income in units of $10,000\r\n- incmiss: income missing (1 if family income is missing, 0\r\no.w.)\r\n- black: 1 if black, 0 o.w.\r\n- hispanic: 1 if hispanic, 0 o.w.\r\n- afqtpct: the Armed Forces Qualifications Test\r\n- edm: mother’s education (1 for less than high school, 2\r\nfor high school, 3 for some college, 4 for BA degree or more)\r\n- edmissm: mother’s education missing (1 if missing, 0\r\no.w.)\r\n- female: gender (1 for female, 0 for male)\r\n\r\nImputation: faminc is set to 2 when\r\nincmiss = 1 indicating that faminc is missing,\r\nand edm is set to 0 when edmissm=1 indicating\r\nthat edm is missing\r\n\r\n\r\nAFQT was missing for less than 2% of subjects, and these subjects are\r\nnot used in the matching. With this restriction, there are 131 high\r\nschool seniors with deceased fathers and 2689 other high school seniors\r\nin the 1979-1981 cohort (131+2689 = 2820)\r\n\r\nThe treatment zb, where is 1 if the father is deceased, and\r\n0 o.w.\r\ndata(\"dynarski\")\r\ndim(dynarski)\r\n[1] 2820   10\r\nkable(head(dynarski, n=20), caption=\"First 20 people in the 1979-1981 cohort, the treatment zb and the eight covariates Xb\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 1: First 20 people in the 1979-1981 cohort, the treatment zb and\r\nthe eight covariates Xb\r\n\r\n\r\nid\r\n\r\n\r\nzb\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n5.3497560\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n71.901660\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2.4627060\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.158050\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n9.4876030\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.785250\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n0\r\n\r\n\r\n0.4388592\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90.617160\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n6\r\n\r\n\r\n0\r\n\r\n\r\n10.4363600\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n81.761160\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n7\r\n\r\n\r\n0\r\n\r\n\r\n6.2694180\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n97.917710\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\n3.2204620\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.916710\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n11\r\n\r\n\r\n0\r\n\r\n\r\n9.4719470\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n20.446560\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n12\r\n\r\n\r\n0\r\n\r\n\r\n2.7167480\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n57.175110\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n14\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n8.278976\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n15\r\n\r\n\r\n0\r\n\r\n\r\n7.1157020\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n50.928250\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n16\r\n\r\n\r\n0\r\n\r\n\r\n7.7669970\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n71.149020\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n17\r\n\r\n\r\n0\r\n\r\n\r\n12.5388400\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n74.912190\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n18\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n98.394380\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n21\r\n\r\n\r\n0\r\n\r\n\r\n9.4041260\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n98.720520\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n23\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.724040\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n24\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n73.281490\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n29\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1.781234\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n31\r\n\r\n\r\n0\r\n\r\n\r\n2.0000000\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n42.900150\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n34\r\n\r\n\r\n1\r\n\r\n\r\n2.6090910\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.623680\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n#write.csv(dynarski, file=\"dynarski.csv\")\r\n\r\n\r\nzb<-dynarski$zb\r\nzbf<-factor(zb,levels=c(1,0),labels=c(\"Father deceased\",\"Father not deceased\"))\r\ntable(zbf)\r\nzbf\r\n    Father deceased Father not deceased \r\n                131                2689 \r\nXb<-dynarski[,3:10]\r\n\r\n\\(\\Rightarrow\\) The 131 seniors with\r\ndeceased fathers will each be matched to 10 controls whose fathers were\r\nnot deceased.\r\nCovariates with Missing\r\nValues\r\n\r\nwith(dynarski, table(incmiss))\r\nincmiss\r\n   0    1 \r\n2282  538 \r\nincmiss.per<-round(table(dynarski$incmiss)[2]/dim(dynarski)[1]*100,1)\r\npaste0(\"Percentage of income missing: \", incmiss.per)\r\n[1] \"Percentage of income missing: 19.1\"\r\nwith(dynarski, table(edmissm))\r\nedmissm\r\n   0    1 \r\n2704  116 \r\nedmissm.per<-round(table(dynarski$edmissm)[2]/dim(dynarski)[1]*100,1)\r\npaste0(\"Percentage of mother's education missing: \", edmissm.per)\r\n[1] \"Percentage of mother's education missing: 4.1\"\r\n\r\nDiscussion of the missing covariate values will be later on another\r\npost.\r\nPropensity Score\r\nThe propensity score is estimated using a logit model.\r\n\r\n# Estimate the propensity score\r\np <- glm(zb ~ Xb$faminc + Xb$incmiss + Xb$black + Xb$hisp + Xb$afqtpct + Xb$edmissm + Xb$edm + Xb$female, family = binomial)$fitted.values\r\n\r\n\\(\\Rightarrow\\) The vector\r\np contains the 2820 estimated propensity scores, \\(\\hat{e}(\\mathbf{x}_l)\\), \\(l\\) = 1, 2, …, 2820.\r\n\r\nIt is reasonable to be able to improve the model: perhaps including\r\ninteraction terms or transformations or polynomials or whatnot.\r\n\r\nEstimated propensity scores for 131 seniors with deceased fathers and\r\n2689 other seniors in the 1979-1981 cohort, before the Social Security\r\nStudent Benefit Program was eliminated.\r\n\r\nboxplot(p~zbf, ylab=\"Propensity score\", main=\"1979-1981 Cohort\")\r\n\r\n\r\n\r\ndynarski$p <- p\r\nggplot(dynarski, aes(p, fill = zbf)) + \r\n  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity') +\r\n  ggtitle(\"Figure of Overlay Histograms of PS in 2 Groups\") +\r\n  theme_bw()\r\n\r\n\r\n\\(\\Longrightarrow\\) the median \\(\\hat{e}(\\mathbf{x})\\) in the treated group\r\n(0.064) is about equal to the upper quartile among potential controls\r\n(0.063). Nonetheless, the distributions in Figure of Overlay Histogram\r\nof PS in 2 Groups overlap substantially, so matching appears to be\r\npossible.\r\nDistance Matrix\r\nConstructing in 2 steps:\r\n- Step 1: computing the rankbased Mahalanobis distance \\(\\rightarrow\\) 131x2689 distance\r\nmatrix\r\n#Robust Mahalanobis distance matrix, treated x control\r\ndmat<-smahal(zb,Xb)\r\ndim(dmat)\r\n[1]  131 2689\r\nkable(round(dmat[1:5,1:5],2), caption=\"First five rows and columns of the 131×2689 distance matrix using the rank-based Mahalanobis distance\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 2: First five rows and columns of the 131×2689 distance matrix\r\nusing the rank-based Mahalanobis distance\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n3.86\r\n\r\n\r\n2.61\r\n\r\n\r\n5.06\r\n\r\n\r\n6.86\r\n\r\n\r\n6.72\r\n\r\n\r\n20\r\n\r\n\r\n3.47\r\n\r\n\r\n3.03\r\n\r\n\r\n7.58\r\n\r\n\r\n6.23\r\n\r\n\r\n5.82\r\n\r\n\r\n108\r\n\r\n\r\n9.60\r\n\r\n\r\n19.47\r\n\r\n\r\n20.02\r\n\r\n\r\n24.62\r\n\r\n\r\n13.03\r\n\r\n\r\n126\r\n\r\n\r\n6.81\r\n\r\n\r\n8.05\r\n\r\n\r\n12.93\r\n\r\n\r\n10.74\r\n\r\n\r\n9.88\r\n\r\n\r\n145\r\n\r\n\r\n8.70\r\n\r\n\r\n15.09\r\n\r\n\r\n17.74\r\n\r\n\r\n18.86\r\n\r\n\r\n12.37\r\n\r\n\r\nStep 2: adding the caliper on the propensity score. The caliper was\r\nat \\(0.2 \\times\r\nsd(\\hat{e}(\\mathbf{x}))\\).\r\n\r\n#Add a caliper on the propensity score using a penalty function\r\ndmat<-addcaliper(dmat,zb,p,caliper=.2)\r\ndim(dmat)\r\n[1]  131 2689\r\nkable(round(dmat[1:5,1:5],2), caption=\"First five rows and columns of the 131×2689 distance matrix after adding the propensity score calipers\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 3: First five rows and columns of the 131×2689 distance\r\nmatrix after adding the propensity score calipers\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n7\r\n\r\n\r\n18.60\r\n\r\n\r\n20.64\r\n\r\n\r\n42.04\r\n\r\n\r\n79.91\r\n\r\n\r\n46.10\r\n\r\n\r\n20\r\n\r\n\r\n46.32\r\n\r\n\r\n3.03\r\n\r\n\r\n72.66\r\n\r\n\r\n51.18\r\n\r\n\r\n73.30\r\n\r\n\r\n108\r\n\r\n\r\n82.94\r\n\r\n\r\n47.40\r\n\r\n\r\n115.60\r\n\r\n\r\n39.07\r\n\r\n\r\n111.01\r\n\r\n\r\n126\r\n\r\n\r\n57.81\r\n\r\n\r\n13.64\r\n\r\n\r\n86.16\r\n\r\n\r\n47.54\r\n\r\n\r\n85.51\r\n\r\n\r\n145\r\n\r\n\r\n8.70\r\n\r\n\r\n54.51\r\n\r\n\r\n33.32\r\n\r\n\r\n113.31\r\n\r\n\r\n30.34\r\n\r\n\r\n\\(\\Rightarrow\\) Among these 25\r\nentries, only 2 respected the caliper and did not incur a penalty (3.03\r\nand 8.70).\r\nConstructing the Match (B. B. Hansen 2007)\r\nMatching ten controls to each senior with a deceased father: The\r\nfullmatch function needs to know the distance matrix, here\r\ndmat, matching 10-to-1 means 10x131 will be used, omitting\r\n2689−10x131 = 1379, which is the omit fraction be: 51%.\r\n\r\nm<-fullmatch(dmat, data=dynarski, min.controls=10, max.controls=10, omit.fraction=1379/2689)\r\n\r\nThere is an entry in m for each of the 2820 seniors\r\n\r\nlength(m)\r\n[1] 2820\r\n\r\nThe first ten entries in m are\r\n\r\nm[1:10]\r\n    1     2     3     4     5     6     7     8     9    10 \r\n1.129 1.100  <NA>  1.54  <NA> 1.121 1.114  <NA> 1.111  1.87 \r\n\r\ni.e. the first senior of the 2820 seniors is in matched set #34 and\r\nthe second senior is in matched set #10. The third senior was one of the\r\n1379 unmatched controls; this is the meaning of the zero in m.01. The\r\nfourth senior is in matched set #87, the fifth is unmatched, and so\r\non.\r\nThe function matched(·) indicates who is matched. The first ten\r\nentries of matched(m) are\r\n\r\nmatched(m)[1:10]\r\n    1     2     3     4     5     6     7     8     9    10 \r\n TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE \r\n\r\nthe first two seniors were matched but the third was not, and so\r\non.\r\nThere are 1441 matched seniors, where 1441 = 131×11, because the 131\r\nseniors with deceased fathers were each matched to ten controls, making\r\n131 matched sets each of size 11.\r\n\r\nsum(matched(m))\r\n[1] 1441\r\nsum(matched(m))/11\r\n[1] 131\r\n\r\nThe first 3 matched sets are in Table below. The first match consists\r\nof 11 female high school seniors, neither black nor hispanic, whose\r\nmothers had a high school education, with family incomes between $30,000\r\nand $40,000, mostly with test scores between 59% and 77%. In the second\r\nmatched set, incomes were lower but test scores were higher. And so\r\non.\r\n\r\n# Housekeeping\r\nim<-as.integer(m) # matched set from 1 to 131\r\ndynarski<-cbind(dynarski,im) \r\ndm<-dynarski[matched(m),] # only select matched cases, note that matched(m): T\\F\r\ndm<-dm[order(dm$im,1-dm$zb),] # sort data according to matched set then zb decreasing\r\n\r\n# Table of matched set example\r\n#which(dm$id==10) # [1] 188\r\n#which(dm$id==396) # [1] 23\r\n#which(dm$id==3051) # [1] 1068\r\n\r\nkable(rbind(dm[188:198,-c(11,12)], dm[23:33,-c(11,12)],dm[1068:1078,-c(11,12)]), caption=\"The 3 of 131 matched sets, each set containing one treated subject and 10 matched controls\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\n(#tab:matched set ex)The 3 of 131 matched sets, each set containing one\r\ntreated subject and 10 matched controls\r\n\r\n\r\n\r\n\r\nid\r\n\r\n\r\nzb\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\n7\r\n\r\n\r\n10\r\n\r\n\r\n1\r\n\r\n\r\n3.220462\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.91671\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n239\r\n\r\n\r\n350\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n59.58355\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n252\r\n\r\n\r\n365\r\n\r\n\r\n0\r\n\r\n\r\n3.599340\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n61.23934\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n322\r\n\r\n\r\n465\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n56.37230\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n361\r\n\r\n\r\n518\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n67.00953\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n383\r\n\r\n\r\n550\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n63.49724\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n928\r\n\r\n\r\n1294\r\n\r\n\r\n0\r\n\r\n\r\n3.557851\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n67.31059\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1481\r\n\r\n\r\n2072\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n64.97742\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1489\r\n\r\n\r\n2082\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n63.62268\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1560\r\n\r\n\r\n2183\r\n\r\n\r\n0\r\n\r\n\r\n3.970631\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n76.51781\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2810\r\n\r\n\r\n3965\r\n\r\n\r\n0\r\n\r\n\r\n3.761650\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n72.57903\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n268\r\n\r\n\r\n396\r\n\r\n\r\n1\r\n\r\n\r\n2.371901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n88.50979\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2.462706\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.15805\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n101\r\n\r\n\r\n147\r\n\r\n\r\n0\r\n\r\n\r\n2.273267\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n77.09483\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n375\r\n\r\n\r\n537\r\n\r\n\r\n0\r\n\r\n\r\n2.595314\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n95.96086\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n665\r\n\r\n\r\n933\r\n\r\n\r\n0\r\n\r\n\r\n2.846281\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n96.11139\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n698\r\n\r\n\r\n974\r\n\r\n\r\n0\r\n\r\n\r\n1.897521\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n99.59859\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n707\r\n\r\n\r\n987\r\n\r\n\r\n0\r\n\r\n\r\n2.134711\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n81.18414\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1394\r\n\r\n\r\n1947\r\n\r\n\r\n0\r\n\r\n\r\n2.050298\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n91.44506\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n1518\r\n\r\n\r\n2124\r\n\r\n\r\n0\r\n\r\n\r\n2.298786\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n72.40341\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n1877\r\n\r\n\r\n2618\r\n\r\n\r\n0\r\n\r\n\r\n2.211560\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n68.91621\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2816\r\n\r\n\r\n3975\r\n\r\n\r\n0\r\n\r\n\r\n2.371901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n90.74260\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n1\r\n\r\n\r\n2182\r\n\r\n\r\n3051\r\n\r\n\r\n1\r\n\r\n\r\n3.409901\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n62.87004\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n427\r\n\r\n\r\n606\r\n\r\n\r\n0\r\n\r\n\r\n4.179612\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n81.73608\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n472\r\n\r\n\r\n664\r\n\r\n\r\n0\r\n\r\n\r\n4.388592\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n91.57050\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n628\r\n\r\n\r\n884\r\n\r\n\r\n0\r\n\r\n\r\n2.846281\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n48.77070\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n713\r\n\r\n\r\n995\r\n\r\n\r\n0\r\n\r\n\r\n3.134709\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n55.11791\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n724\r\n\r\n\r\n1008\r\n\r\n\r\n0\r\n\r\n\r\n3.320661\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n51.60562\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1006\r\n\r\n\r\n1399\r\n\r\n\r\n0\r\n\r\n\r\n3.439256\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n90.01505\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2087\r\n\r\n\r\n2908\r\n\r\n\r\n0\r\n\r\n\r\n3.795041\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n80.28098\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n2328\r\n\r\n\r\n3262\r\n\r\n\r\n0\r\n\r\n\r\n3.788779\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n57.27546\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n2424\r\n\r\n\r\n3400\r\n\r\n\r\n0\r\n\r\n\r\n2.925728\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n80.88309\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n0\r\n\r\n\r\n2576\r\n\r\n\r\n3624\r\n\r\n\r\n0\r\n\r\n\r\n3.320661\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n74.08430\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nChecking Covariate Balance\r\nNote that: for covariate \\(k\\),\r\n- \\(\\bar{x}_{tk}\\) is the mean in the\r\nfirst group in the comparison,\r\n- \\(\\bar{x}_{ck}\\) is the mean in the\r\nsecond group in the comparison, and\r\n- \\(\\bar{x}_{mck}\\) is the mean in the\r\nmatched comparison group formed from the second group\r\n- \\(sd_{bk}\\) is the standardized\r\ndifference in means before matching and\r\n- \\(sd_{mk}\\) is the standardized\r\ndifference in means after matching; they have the same denominator but\r\ndifferent numerators\r\n\r\n#fd <- dynarski[dynarski$zb==1,-c(1,2,12)]\r\n#round(apply(fd, 2, FUN=mean),2)\r\n\r\nd.tk<-dynarski %>%\r\n  filter(zb==1) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im)\r\nx.tk<-d.tk %>% summarise_all(mean, na.rm=TRUE)\r\ns.tk<-d.tk %>% summarise_all(sd, na.rm=TRUE)\r\n\r\nd.ck<-dynarski %>%\r\n  filter(zb==0) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im)\r\nx.ck<-d.ck %>% summarise_all(mean, na.rm=TRUE)\r\ns.ck<-d.ck %>% summarise_all(sd, na.rm=TRUE)\r\n\r\nsd.bk<-abs(x.tk-x.ck)/sqrt((s.tk^2+s.ck^2)/2)\r\n\r\nd.cmk<-dm %>%\r\n  filter(zb==0) %>%\r\n  mutate(faminc=ifelse(incmiss==1,NA, faminc), \r\n         edm=ifelse(edmissm==1, NA, edm)) %>%\r\n  select(-id,-zb,-im) \r\nx.cmk<-d.cmk %>% summarise_all(mean, na.rm=TRUE)\r\nsd.cmk<-abs(x.tk-x.cmk)/sqrt((s.tk^2+s.ck^2)/2)\r\n\r\nset<-round(rbind(x.tk,x.cmk,x.ck,sd.bk,sd.cmk),2)\r\nrow.names(set) <- c(\"x.tk\",\"x.cmk\",\"x.ck\",\"sd.bk\",\"sd.cmk\")\r\n\r\nkable(set, caption=\"Balance on covariates before and after matching\") %>%\r\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\r\n\r\nTable 4: Balance on covariates before and after matching\r\n\r\n\r\n\r\n\r\nfaminc\r\n\r\n\r\nincmiss\r\n\r\n\r\nblack\r\n\r\n\r\nhisp\r\n\r\n\r\nafqtpct\r\n\r\n\r\nedmissm\r\n\r\n\r\nedm\r\n\r\n\r\nfemale\r\n\r\n\r\np\r\n\r\n\r\nx.tk\r\n\r\n\r\n2.78\r\n\r\n\r\n0.15\r\n\r\n\r\n0.35\r\n\r\n\r\n0.15\r\n\r\n\r\n49.58\r\n\r\n\r\n0.08\r\n\r\n\r\n1.62\r\n\r\n\r\n0.49\r\n\r\n\r\n0.07\r\n\r\n\r\nx.cmk\r\n\r\n\r\n2.77\r\n\r\n\r\n0.15\r\n\r\n\r\n0.34\r\n\r\n\r\n0.15\r\n\r\n\r\n49.10\r\n\r\n\r\n0.04\r\n\r\n\r\n1.61\r\n\r\n\r\n0.50\r\n\r\n\r\n0.06\r\n\r\n\r\nx.ck\r\n\r\n\r\n4.58\r\n\r\n\r\n0.19\r\n\r\n\r\n0.29\r\n\r\n\r\n0.15\r\n\r\n\r\n52.39\r\n\r\n\r\n0.04\r\n\r\n\r\n1.91\r\n\r\n\r\n0.50\r\n\r\n\r\n0.05\r\n\r\n\r\nsd.bk\r\n\r\n\r\n0.71\r\n\r\n\r\n0.11\r\n\r\n\r\n0.13\r\n\r\n\r\n0.02\r\n\r\n\r\n0.10\r\n\r\n\r\n0.19\r\n\r\n\r\n0.33\r\n\r\n\r\n0.03\r\n\r\n\r\n0.67\r\n\r\n\r\nsd.cmk\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n0.02\r\n\r\n\r\n0.19\r\n\r\n\r\n0.02\r\n\r\n\r\n0.02\r\n\r\n\r\n0.09\r\n\r\n\r\n\\(\\Longrightarrow\\) They were quite\r\ndifferent before matching but were much closer after matching. The\r\nfamily income of seniors with deceased fathers was lower, they were more\r\noften black, and their mothers had less education. Between 1979-1981,\r\nAFQT test scores decline. The imbalances are much smaller after\r\nmatching.\r\nOutcomes\r\nThe outcomes were not provided in the Dynarski’s dataset. But I\r\nguessed we can report the proportions and MH-odds ratio between 2 groups\r\nafter matching.\r\nIn short, during 1979-1981, when the Social Security Student Benefit\r\nProgram provided tuition aid to students of a deceased Social Security\r\nbeneficiary, seniors with deceased fathers were more likely to\r\nattend college and complete one year of college than were\r\nmatched controls, with an odds ratio of about 1.65, but\r\nthere is no sign of this in 1982-1983 after the program ended (the\r\nlater can be checked because of lack the data for analysis).\r\nStandardized differences in covariate means before and after matching\r\nin matched comparisons. The boxplot displays standardized differences in\r\nmeans, for the nine covariates and the propensity score.\r\n\r\nm<-c(0,1)\r\nd.sd<-rbind(sd.bk,sd.cmk)\r\nd.sd<-cbind(d.sd,m)\r\nd.sd.long<-d.sd %>%\r\n  pivot_longer(-m,names_to=\"absstdzdiff\", values_to=\"sd\")\r\nd.sd.long$m<-factor(d.sd.long$m,levels=c(0,1),labels=c(\"Unmatched\",\"Matched\"))\r\nboxplot(sd~m, data=d.sd.long, xlab=\"\", ylab=\"Absolute Standardized Difference\", main=\"1979-1981 Cohort: FD vs. FND\")\r\n\r\n\r\nFurther reading\r\nDynarski (1999)’s fine\r\nstudy\r\nB. B. Hansen (2007)’s\r\nfullmatch function\r\nBen B. Hansen and Klopfer\r\n(2006), Paul R. Rosenbaum (1989), P. R. Rosenbaum (1991) showed how\r\nfullmatch is doing\r\nMing and Rosenbaum (2001) showed\r\nproc assign in SAS\r\n\r\n\r\nBertsekas. 1991. Linear Network Optimization. Book. Cambridge,\r\nMA: MIT Press. http://web.mit.edu/dimitrib/www/net.html.\r\n\r\n\r\nBertsekas, Dimitri P. 1981. “A New Algorithm for the Assignment\r\nProblem.” Journal Article. Mathematical Programming 21\r\n(1): 152–71. https://doi.org/10.1007/BF01584237.\r\n\r\n\r\nDynarski, Susan M. 1999. “Does Aid Matter? Measuring the Effect of\r\nStudent Aid on College Attendance and Completion.” Journal\r\nArticle, 1 online resource. http://HZ9PJ6FE4T.search.serialssolutions.com/?V=1.0&L=HZ9PJ6FE4T&S=JCs&C=TC_008322359&T=marc&tab=BOOKS\r\nAvailable only to UIC users.\r\n\r\n\r\nHansen, B. B. 2007. “Optmatch: Flexible, Optimal Matching for\r\nObservational Studies.” Journal Article. R News 7:\r\n18–24.\r\n\r\n\r\nHansen, Ben B., and Stephanie Olsen Klopfer. 2006. “Optimal Full\r\nMatching and Related Designs via Network Flows.” Journal Article.\r\nJournal of Computational and Graphical Statistics 15 (3):\r\n609–27. www.jstor.org/stable/27594200.\r\n\r\n\r\nMing, Kewei, and Paul R. Rosenbaum. 2001. “A Note on Optimal\r\nMatching with Variable Controls Using the Assignment Algorithm.”\r\nJournal Article. Journal of Computational and Graphical\r\nStatistics 10 (3): 455–63. www.jstor.org/stable/1391099.\r\n\r\n\r\nRosenbaum, P. R. 1991. “A Characterization of Optimal Designs for\r\nObservational Studies.” Journal Article. Journal of the Royal\r\nStatistical Society. Series B (Methodological) 53 (3): 597–610. www.jstor.org/stable/2345589.\r\n\r\n\r\nRosenbaum, Paul R. 1989. “Optimal Matching for Observational\r\nStudies.” Journal Article. Journal of the American\r\nStatistical Association 84 (408): 1024–32. https://doi.org/10.2307/2290079.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-01-30T01:04:36-06:00",
    "input_file": "Matching-in-R.knit.md"
  },
  {
    "path": "posts/2021-06-09-mets-using-nhanesdata/",
    "title": "Metabolic Syndrome Prevalence across time using NHANES Data",
    "description": "How to correctly approach NHANES data to make estimates that are representative of the population    \nDefine Metabolic Syndrome based on ATP III  \nLook at the MetS prevalence over time",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [
      "Biostatistics",
      "NHANES approach",
      "R",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nNHANES Cycle 2013/2014\r\nImport data from NHANES cycle 2013/2014\r\nData Processing\r\nDefine metabolic syndrome\r\nDefine sub-population of interest\r\nMetS proportion in the random sample (unweighted)\r\nWith all age groups\r\nWith all age groups & none missing of MetS indicator\r\nWith age 20+ & none missing of MetS indicator\r\nWith age 20+ and ind.non_missing is TRUE:\r\n\r\nMetS prevalence with weighted\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n\r\nCycle 2015/2016\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\nCycle 2017/2018\r\nProportion and SE, for adults aged 20+\r\nProportion and SE, for adults age groups\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\nSummary\r\nReferences\r\n\r\nI need to use these packages:\r\ntidyverse: data management (tidyr, dplyr) and plots (ggplot2)\r\nforeign: reading xport files\r\nsurvey: for using survey weights # survey::svydesign, svymean, svyglm\r\n\r\nlibrary(tidyverse)\r\nlibrary(foreign) \r\nlibrary(survey) \r\noptions(survey.lonely.psu='adjust')\r\nlibrary(kableExtra)\r\n\r\nDisplay Version Information\r\n\r\n#cat(\"R package versions:\\n\")\r\nfor (p in c(\"base\", \"survey\",\"tidyverse\")) { \r\n  cat(p, \": \", as.character(packageVersion(p)), \"\\n\")\r\n}\r\nbase :  4.0.4 \r\nsurvey :  4.0 \r\ntidyverse :  1.3.1 \r\n\r\nNHANES Cycle 2013/2014\r\nImport data from NHANES cycle 2013/2014\r\nCode for data import was adapted from a NHANES tutorials example.\r\nfunction to download the required survey cycles for a component file\r\n\r\ndownloadNHANES <- function(fileprefix){\r\n  print (fileprefix)\r\n  outdf <- data.frame(NULL)\r\n  for (j in 1:length(letters)){\r\n    urlstring <- paste('https://wwwn.cdc.gov/nchs/nhanes/',yrs[j],'/',fileprefix,letters[j],'.XPT', sep='')\r\n    download.file(urlstring, tf <- tempfile(), mode=\"wb\")\r\n    tmpframe <- foreign::read.xport(tf)\r\n    outdf <- bind_rows(outdf, tmpframe)\r\n  }\r\n  return(outdf)\r\n}\r\n\r\nImport NHANES cycle 2013/2014\r\n\r\n# Specify the survey cycles required, with corresponding file suffixes\r\nyrs <- c('2013-2014')\r\nletters <- c('_h')\r\n\r\n# Download data for each component\r\n# (1) Demographic (DEMO)\r\nDEMO <- downloadNHANES('DEMO') # 10175\r\n# (2) Blood Pressure & Cholesterol\r\nBPQ <- downloadNHANES('BPQ') # 6464\r\n# (3) Diabetes \r\nDIQ <- downloadNHANES('DIQ') # 9770\r\n# (4) Blood Pressure\r\nBPX <- downloadNHANES('BPX') # 9813\r\n# (5) Body Measures\r\nBMX <- downloadNHANES('BMX') # 9813\r\n# (6) Cholesterol - High - Density Lipoprotein\r\nHDL <- downloadNHANES('HDL') # 8291\r\n# (7) Cholesterol - Low-Density Lipoproteins (LDL) & Triglycerides (TRIGLY_J)\r\nTRIGLY <- downloadNHANES('TRIGLY') # 3329\r\n# (8) Plasma Fasting Glucose (GLU_J)\r\nGLU <- downloadNHANES('GLU') # 3329\r\n\r\nMerge all datasets, preserve all observations\r\nNHANES Data Structure\r\n# Merging using \"piping\" \r\nnhanesdata.merged <- left_join(DEMO, BPQ, by=\"SEQN\") %>% \r\n  left_join(DIQ, by=\"SEQN\") %>% \r\n  left_join(BPX, by='SEQN') %>% \r\n  left_join(BMX, by='SEQN') %>%\r\n  left_join(HDL, by='SEQN') %>%\r\n  left_join(TRIGLY, by='SEQN') %>%\r\n  left_join(GLU, by='SEQN') # 10175 obs\r\n\r\nData Processing\r\nSelect necessary variables\r\nCreate indicator for overall summary one\r\nCreate age categories for adults aged 18 and over: ages 18-39, 40-59, 60 and over\r\nRe-code the levels of variables: gender, ethnicity/race, marital status, medication prescription due to ‘refuse to answer’ or ‘don’t know’ (to missing)\r\n\r\nnhanes.vbls <- nhanesdata.merged %>%\r\n  select(SEQN, # Respondent sequence number\r\n         RIDAGEYR, # Age in years at screening\r\n         RIAGENDR, # Gender\r\n\r\n         RIDRETH3, # Race/Hispanic origin w/ NH Asian\r\n         DMDMARTL, # Marital status\r\n         DMDEDUC2, # Education level - Adults 20+\r\n         \r\n         # Survey design variables\r\n         SDMVPSU, # Masked variance pseudo-PSU\r\n         SDMVSTRA, # Masked variance pseudo-stratum\r\n         WTMEC2YR, # Full sample 2 year MEC exam weight\r\n         WTINT2YR, # Full sample 2 year interview weight\r\n         \r\n         BPXSY1, # Systolic: Blood pres (1st rdg) mm Hg\r\n         BPXDI1, # Diastolic: Blood pres (1st rdg) mm Hg\r\n         BPXSY2, # Systolic: Blood pres (2nd rdg) mm Hg\r\n         BPXDI2, # Diastolic: Blood pres (2nd rdg) mm Hg\r\n         BPXSY3, # Systolic: Blood pres (3rd rdg) mm Hg\r\n         BPXDI3, # Diastolic: Blood pres (3rd rdg) mm Hg\r\n         BPXSY4, # Systolic: Blood pres (4th rdg) mm Hg\r\n         BPXDI4, # Diastolic: Blood pres (4th rdg) mm Hg\r\n         LBXGLU, # Fasting Glucose (mg/dL)\r\n         LBDHDD, # Direct HDL-Cholesterol (mg/dL)\r\n         LBXTR, # Triglyceride (mg/dL)\r\n         BMXWAIST, # Waist Circumference (cm)\r\n         \r\n         BPQ090D, # Told to take prescription for cholesterol\r\n         BPQ040A, # Taking prescription for hypertension\r\n         DIQ070 # Take diabetic pills to lower blood sugar\r\n         ) %>%\r\n  mutate(# create indicator for overall summary\r\n         one = 1,\r\n         id = SEQN,\r\n         psu = SDMVPSU, strata = SDMVSTRA, \r\n         persWeight = WTINT2YR, persWeightMEC= WTMEC2YR,\r\n         age = RIDAGEYR,\r\n         # Create age categories for adults aged 18 and over: ages 18-39, 40-59, 60 and over\r\n        ageCat = cut(RIDAGEYR,\r\n                    breaks = c(19, 29, 39, 49, 59, 69, Inf),\r\n                    labels = c('20-29','30-39', '40-49', '50-59', '60-69', '70+')),\r\n         female = factor(if_else(RIAGENDR == 1, 0, 1)), # 1:male ==> 0\r\n         ethnicity = case_when(\r\n           RIDRETH3 %in% c(1,2) ~ 0, # Hispanic\r\n           RIDRETH3 == 3 ~ 1, # Non-Hispanic White\r\n           RIDRETH3 == 4 ~ 2, # Non-Hispanic Black\r\n           RIDRETH3 == 6 ~ 3, # Non-Hispanic Asian\r\n           RIDRETH3 == 7 ~ 4, # Multi-Racial\r\n         is.na(RIDRETH3) ~ NA_real_\r\n         ),\r\n         chol.prsn = case_when(\r\n           BPQ090D == 2 ~ 0, # No\r\n           BPQ090D == 1 ~ 1, # Yes\r\n           BPQ090D %in% c(7,9) ~ NA_real_\r\n         ),\r\n         bp.prsn = case_when(\r\n           BPQ040A == 2 ~ 0, # No\r\n           BPQ040A == 1 ~ 1, # Yes\r\n           BPQ040A %in% c(7,9) ~ NA_real_\r\n         ),\r\n         glu.prsn = case_when(\r\n           DIQ070 == 2 ~ 0, # No\r\n           DIQ070 == 1 ~ 1, # Yes\r\n           DIQ070 %in% c(7,9) ~ NA_real_\r\n         ),\r\n         dia = rowMeans(.[, c(\"BPXDI1\",\"BPXDI2\",\"BPXDI3\",\"BPXDI4\")], na.rm = TRUE),\r\n         sys = rowMeans(.[, c(\"BPXSY1\",\"BPXSY2\",\"BPXSY3\",\"BPXSY4\")], na.rm = TRUE),\r\n         glu = LBXGLU,\r\n         hdl = LBDHDD,\r\n         tri = LBXTR,\r\n         waist = BMXWAIST\r\n        ) %>%\r\n  select(one, id, psu, strata, persWeight, persWeightMEC, age, ageCat, female, ethnicity, sys, dia, glu, hdl, tri, waist, bp.prsn, chol.prsn, glu.prsn) # 10175\r\n\r\nDefine metabolic syndrome\r\nAccording to Alberti et al. (2009), the presence of any 3 of 5 risk factors constitutes a diagnosis of metabolic syndrome:\r\nelevated waist circumference (≥88 cm for women and ≥102 cm for men),\r\nelevated triglycerides (≥150 mg/dL) or drug treatment for elevated triglycerides (there was no variable indicating the medication for lower triglycerides),\r\nlow HDL cholesterol (<40 mg/dL for men and <50 mg/dL for women) or drug treatment for low HDL cholesterol,\r\nelevated blood pressure (systolic ≥130 mm Hg, or diastolic ≥85 mm Hg, or both) or antihypertensive drug treatment for a history of hypertension, and\r\nelevated fasting glucose (≥100 mg/dL) or drug treatment for elevated glucose.” (Moore et al., 2017; Preventing Chronic Disease).\r\nBesides, I intentionally created the indicators for missing data: - return NA if all indicators are NA for mets.sum: when I summed up all 5 MetS indicators, if all are NA, so sum should be NA (not 0)\r\n- ind.non_missing: return TRUE if none of indicators are none-NA, otherwise FALSE\r\nFinally, mets.ind (MetS indicator) would be binary of 1 and 0 if sum of 5 indicators above greater or equal to 3.\r\n\r\nnhanes.MetS <- nhanes.vbls %>%\r\n  mutate(waist.ind = case_when(waist >= 88  & female == 1 ~ 1, #condition 1\r\n                               waist >= 102 & female == 0 ~ 1, #condition 2\r\n                               is.na(waist) | is.na(female) ~ NA_real_, #condition 3\r\n                               TRUE ~ 0), #all other cases\r\n         tri.ind = case_when(tri >= 150 | chol.prsn == 1 ~ 1,\r\n                             is.na(tri) ~ NA_real_,\r\n                             TRUE ~0),\r\n         hdl.ind = case_when((hdl < 40 & female == 0) | chol.prsn == 1 ~ 1,\r\n                             (hdl < 50 & female == 1) | chol.prsn == 1 ~ 1,\r\n                             is.na(hdl) | is.na(female) ~ NA_real_,\r\n                             TRUE ~ 0),\r\n         bp.ind = case_when(sys >= 130 | dia >= 85 | bp.prsn == 1 ~ 1,\r\n                            is.na(sys) & is.na(dia) ~ NA_real_,\r\n                            TRUE ~ 0\r\n                            ),\r\n         glu.ind = case_when(glu >= 100 | glu.prsn == 1 ~ 1,\r\n                             is.na(glu) ~ NA_real_,\r\n                             TRUE ~ 0)\r\n         ) # 10175 obs\r\n\r\n\r\n\r\nnhanes.MetS$mets.sum = rowSums(nhanes.MetS[20:24], na.rm = T) # from waist.ind to glu.ind # 20:24\r\nnhanes.MetS$mets.sum[rowSums(!is.na(nhanes.MetS[20:24])) == 0] <- NA # return NA if all indicators are NA\r\nnhanes.MetS$ind.non_missing <- !rowSums(!is.na(nhanes.MetS[20:24])==0) # create an indicator in which none of criteria is none NA\r\nnhanes.MetS$mets.ind <- ifelse(nhanes.MetS$mets.sum >= 3, 1, 0) # 10175 obs\r\n\r\nHere we can get the sense of data for all indicators created: \r\nDefine sub-population of interest\r\nage of 20+ Reasons for the consistency in comparison:Ford, Giles, and Dietz (2002) used NHANES III, 1988 to 1994 based on 2001 Adult Treatment Panel III (ATP III) criteria (same as Moore et al.) with age groups.\r\n\r\nSmiley, King, and Bidulescu (2019) did not specify the age range but they used the same criteria for adults (e.g. “Metabolic syndrome was defined according to NCEP ATPIII (National Cholesterol Education Program Adult Treatment Panel III) criteria.”).\r\ninAnalysis1: age of 20+ and none of missing of MetS indicatorinAnalysis2: age of 20+ and none of indicators are none-NA\r\n\r\n\r\nnhanes.MetS <- nhanes.MetS %>% \r\n  mutate(\r\n    #  Define sub-population of interest \r\n    inAnalysis1 = (age >=20 & !is.na(mets.ind)),\r\n    inAnalysis2 = (age >=20 & ind.non_missing)\r\n    ) # 10175 obs\r\n\r\n\r\nsaveRDS(nhanes.MetS, file = \"data/nhanesMetS13_14snd.rds\")\r\n\r\nOur sample size would be if I subset data on:\r\ninAnlysis1: 5649\r\ninAnlysis2: 2587\r\nMetS proportion in the random sample (unweighted)\r\nWith all age groups\r\n\r\nnhanes.MetS %>%\r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n6940\r\n\r\n\r\n68.2\r\n\r\n\r\n1\r\n\r\n\r\n2184\r\n\r\n\r\n21.5\r\n\r\n\r\nNA\r\n\r\n\r\n1051\r\n\r\n\r\n10.3\r\n\r\n\r\nWith all age groups & none missing of MetS indicator\r\n\r\nnhanes.MetS %>%\r\n  filter(!is.na(mets.ind)) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n6940\r\n\r\n\r\n76.1\r\n\r\n\r\n1\r\n\r\n\r\n2184\r\n\r\n\r\n23.9\r\n\r\n\r\nWith age 20+ & none missing of MetS indicator\r\n\r\nnhanes.MetS %>% \r\n  filter(age>=20 & !is.na(mets.ind)) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n3511\r\n\r\n\r\n62.2\r\n\r\n\r\n1\r\n\r\n\r\n2138\r\n\r\n\r\n37.8\r\n\r\n\r\nWith age 20+ and ind.non_missing is TRUE:\r\n\r\nNote: ind.non_missing was created an indicator in which none of criteria is none NA\r\n\r\n\r\nnhanes.MetS %>% \r\n  filter(age>=20 & ind.non_missing) %>% \r\n  group_by(mets.ind) %>% \r\n  summarise(n=n()) %>% \r\n  mutate(prop=round(n/sum(n)*100,1)) %>%\r\n  kbl(col.names = c('Metabolic syndrome', 'Freq', 'Percent')) %>% \r\n  kable_material(c(\"striped\", \"hover\"))\r\n\r\nMetabolic syndrome\r\n\r\n\r\nFreq\r\n\r\n\r\nPercent\r\n\r\n\r\n0\r\n\r\n\r\n1273\r\n\r\n\r\n49.2\r\n\r\n\r\n1\r\n\r\n\r\n1314\r\n\r\n\r\n50.8\r\n\r\n\r\nMetS prevalence with weighted\r\nNote:  \r\npsu = SDMVPSU, \r\nstrata = SDMVSTRA, \r\npersWeight = WTINT2YR, \r\npersWeightMEC= WTMEC2YR\r\nApply weights then subset data\r\n\r\n# Define survey design for overall dataset \r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\n# Create a survey design object for the subset of interest \r\n# Subsetting the original survey design object ensures we keep the design information about the number of clusters and strata\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1       36 0.8\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      7.0 1.0\r\n30-39  30-39     21.6 1.0\r\n40-49  40-49     30.0 1.3\r\n50-59  50-59     45.7 2.1\r\n60-69  60-69     59.8 2.8\r\n70+      70+     67.2 1.4\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     30.1 1.1\r\n1         1     39.0 1.1\r\n2         2     34.1 1.6\r\n3         3     23.4 2.8\r\n4         4     28.3 3.9\r\n\r\nCycle 2015/2016\r\nI processed the same procedures as the cycle 2013/2014. Then saved as RDS file. Here I just showed the prevalence after reading the RDS data into R using weights.\r\n\r\nnhanes.MetS <- readRDS(file = \"data/nhanesMetS15_16.rds\")\r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1     37.7 1.8\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      9.2 1.5\r\n30-39  30-39     19.3 1.4\r\n40-49  40-49     33.2 3.2\r\n50-59  50-59     48.1 2.5\r\n60-69  60-69     61.7 2.4\r\n70+      70+     66.7 2.5\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     34.0 1.8\r\n1         1     40.1 2.4\r\n2         2     34.0 1.2\r\n3         3     23.9 2.8\r\n4         4     43.9 4.1\r\n\r\nCycle 2017/2018\r\n\r\nnhanes.MetS <- readRDS(file = \"data/nhanesMetS17_18.rds\")\r\nNHANES_all <- svydesign(data=nhanes.MetS, id=~psu, strata=~strata, weights=~persWeightMEC, nest=TRUE)\r\nNHANES <- subset(NHANES_all, inAnalysis1==1)\r\n\r\nProportion and SE, for adults aged 20+\r\n\r\n#' Proportion and standard error, for adults aged 20 and over\r\nsvyby(~mets.ind, ~one, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  one mets.ind  se\r\n1   1     38.2 1.3\r\n\r\nProportion and SE, for adults age groups\r\n\r\n#' Proportion and standard error, for adults age groups\r\nsvyby(~mets.ind, ~ageCat, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n      ageCat mets.ind  se\r\n20-29  20-29      9.0 1.5\r\n30-39  30-39     18.4 1.6\r\n40-49  40-49     31.1 1.8\r\n50-59  50-59     49.2 3.4\r\n60-69  60-69     60.5 2.8\r\n70+      70+     71.3 1.4\r\n\r\nProportion and SE, for adults aged 20+ by race and Hispanic origin\r\n\r\n#' Proportion and standard error, for adults aged 20+ by race and Hispanic origin\r\nsvyby(~mets.ind, ~ethnicity, NHANES, svymean) %>% mutate(mets.ind = round(mets.ind*100, digits=1), se=round(se*100, digits=1))\r\n  ethnicity mets.ind  se\r\n0         0     34.5 1.7\r\n1         1     40.3 1.7\r\n2         2     33.5 1.7\r\n3         3     32.1 2.3\r\n4         4     42.5 4.4\r\n\r\nSummary\r\n\r\nCycle <- c(\"2013-2014\",\"2015-2016\",\"2017-2018\")\r\nPrevalence <- c(36, 37.7, 38.2)\r\nyear.prl <- as.tibble(data.frame(Cycle, Prevalence))\r\n\r\nAge.Categories <- c('20-29','30-39','40-49','50-59','60-69','70+')\r\nPrevalence.Cycle.13_14 <- c(7,21.6,30,45.7,59.8,67.2)\r\nPrevalence.Cycle.15_16 <- c(9.2,19.3,33.2,48.1,61.7,66.7)\r\nPrevalence.Cycle.17_18 <- c(9,18.4,31.1,49.2,60.5,71.3)\r\nage.g.p <- as.tibble(data.frame(Age.Categories,Prevalence.Cycle.13_14,Prevalence.Cycle.15_16,Prevalence.Cycle.17_18))\r\nage.g.p.l <- gather(age.g.p, Prevalence, Percent, Prevalence.Cycle.13_14:Prevalence.Cycle.17_18)\r\n\r\nRace.Ethnic.Categories <- c('Hispanic','White','Black','Asian','Multi-Racial')\r\nPrevalence.Cycle.13_14 <- c(34.0,40.1,34.0,23.9,43.9)\r\nPrevalence.Cycle.15_16 <- c(34.0,40.1,34.0,23.9,43.9)\r\nPrevalence.Cycle.17_18 <- c(34.5,40.3,33.5,32.1,42.5)\r\nr.e.g.p <- as.tibble(data.frame(Race.Ethnic.Categories,Prevalence.Cycle.13_14,Prevalence.Cycle.15_16,Prevalence.Cycle.17_18))\r\nr.e.g.p.l <- gather(r.e.g.p, Prevalence, Percent, Prevalence.Cycle.13_14:Prevalence.Cycle.17_18)\r\n\r\n\r\nyear.prl %>% ggplot(aes(x = Cycle, y = Prevalence)) + \r\n  geom_point() +\r\n  geom_line(aes(group=1)) +\r\n  geom_text(label=paste0(Prevalence,\"%\"), hjust=1.5, vjust=0) +\r\n  ylim(30, 45) +\r\n  theme_bw()\r\n\r\n\r\n\r\nage.g.p.l %>% ggplot(aes(fill=Prevalence, y=Percent, x=Age.Categories)) + \r\n  geom_bar(position=\"dodge\", stat=\"identity\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nr.e.g.p.l %>% ggplot(aes(fill=Prevalence, y=Percent, x=Race.Ethnic.Categories)) + \r\n  geom_bar(position=\"dodge\", stat=\"identity\") +\r\n  theme_bw()\r\n\r\n\r\nMetabolic syndrome at the cycle 2013-2014 was close to Smiley, King, and Bidulescu (2019) (NHANES 2013-2014 with 31.5% in which the authors defined MetS using the criteria established by Lee, Gurka and DeBoer (Expert Rev. Cardiovasc. Ther, 2016), which takes into account racial and age-specific differences in populations, resulting in a complex formula to determine a score so they perhaps got the lower MetS cases), and Moore, Chaudhary, and Akinyemiju (2017) reported 34.2% in 2007–2012 (also applying in NHANES data).\r\nMetabolic syndrome is on the rise.\r\nThe rise emphasized on the older age groups.\r\nMore recently, Asian groups rose the most in metabolic syndrome.\r\nReferences\r\n\r\n\r\nAlberti, K. G. M. M., Robert H. Eckel, Scott M. Grundy, Paul Z. Zimmet, James I. Cleeman, Karen A. Donato, Jean-Charles Fruchart, W. Philip T. James, Catherine M. Loria, and Jr. Smith Sidney C. 2009. “Harmonizing the Metabolic Syndrome: A Joint Interim Statement of the International Diabetes Federation Task Force on Epidemiology and Prevention; National Heart, Lung, and Blood Institute; American Heart Association; World Heart Federation; International Atherosclerosis Society; and International Association for the Study of Obesity.” Journal Article. Circulation 120 (16): 1640–45. https://doi.org/10.1161/CIRCULATIONAHA.109.192644.\r\n\r\n\r\nFord, Earl S., Wayne H. Giles, and William H. Dietz. 2002. “Prevalence of the Metabolic Syndrome Among US Adults: Findings from the Third National Health and Nutrition Examination Survey.” Journal Article. JAMA : The Journal of the American Medical Association 287 (3): 356–59. https://doi.org/10.1001/jama.287.3.356.\r\n\r\n\r\nMoore, Justin Xavier, Ninad Chaudhary, and Tomi Akinyemiju. 2017. “Metabolic Syndrome Prevalence by Race/Ethnicity and Sex in the United States, National Health and Nutrition Examination Survey, 1988-2012.” Journal Article. Preventing Chronic Disease 14: E24–24. https://doi.org/10.5888/pcd14.160287.\r\n\r\n\r\nSmiley, Abbas, David King, and Aurelian Bidulescu. 2019. “The Association Between Sleep Duration and Metabolic Syndrome: The NHANES 2013/2014.” Journal Article. Nutrients 11 (11): 2582. https://doi.org/10.3390/nu11112582.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-09-mets-using-nhanesdata/distill-preview.png",
    "last_modified": "2021-06-16T13:11:13-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-26-Mplus-as-a-knitr-engine/",
    "title": "Set up to run Mplus inside Rmarkdown",
    "description": "Mplus as a knitr engine in Rmarkdown  \nMplusAutomation: a brief guide",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Mplus",
      "R",
      "MplusAutomation",
      "Tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nGoals\r\nSet up Mplus in Rmarkdown\r\nGentle introduction to MplusAutomation\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nLatent Profile Analysis using Mplus\r\nRun direct in Rmarkdown\r\nUsing runModels\r\n\r\nCompare the model fit\r\n\r\n\r\nGoals\r\nThis post I aim to set up the Mplus engine using knitr package in Rmarkdown.\r\nAfter that, I run a simple model example to demonstrate how Mplus works within Rmarkdown. (Therefore, I do not intent to explain the model results and fit here.)\r\nSet up Mplus in Rmarkdown\r\nGlobally we can change the engine interpreters for one/multiple engine(s), we applied for Mplus:\r\n\r\nknitr::opts_chunk$set(engine.path = list(\r\n  mplus = \"C:/Program Files/Mplus/Mplus\"\r\n))\r\n\r\nNext step, we register a Mplus custom language engine:\r\n\r\nknitr::knit_engines$set(mplus = function(options) {\r\n    code <- paste(options$code, collapse = \"\\n\")\r\n    fileConn<-file(\"formplus.inp\")\r\n    writeLines(code, fileConn)\r\n    close(fileConn)\r\n    out  <- system2(\"C:/Program Files/Mplus/Mplus\", \"formplus.inp\")\r\n    fileConnOutput <- file(\"formplus.out\")\r\n    mplusOutput <- readLines(fileConnOutput)\r\n    knitr::engine_output(options, code, mplusOutput)\r\n})\r\n\r\nFor more language engines, we can refer demos from Yihui Xie’s post.\r\nFurther reading, we may read a Rich Jones’ post on rpubs\r\nGentle introduction to MplusAutomation\r\nThe MplusAutomation package for R (Hallquist and Wiley 2018):\r\nCreating many similar syntax files:\r\nSimulations with different sample sizes\r\nExcluding different parts of a sample\r\n\r\nRunning batches of input files\r\nExtracting and tabulating model parameters and test statistics.\r\nFour core routines support these aims:\r\ncreateModels\r\nrunModels\r\nreadModels\r\ncompareModels\r\nThe MplusAutomation package can be installed within R using the following call:\r\n\r\nif (!require(MplusAutomation)) install.packages(\"MplusAutomation\")\r\nlibrary(MplusAutomation)\r\n\r\nRun a LPA via MplusAutomation\r\nGenerating data\r\nI use Edgar Anderson’s Iris Data with 150 cases (rows) and 5 variables named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.\r\niris data set gives the measurements in cm of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris: Iris setosa, versicolor, and virginica.\r\n\r\n?iris\r\n\r\n\r\nhead(iris)\r\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1          5.1         3.5          1.4         0.2  setosa\r\n2          4.9         3.0          1.4         0.2  setosa\r\n3          4.7         3.2          1.3         0.2  setosa\r\n4          4.6         3.1          1.5         0.2  setosa\r\n5          5.0         3.6          1.4         0.2  setosa\r\n6          5.4         3.9          1.7         0.4  setosa\r\n\r\nprepareMPlusData() function prepares a data file in the MPlus’ format, namely, a tab-separated .dat file with no column names.\r\n\r\nprepareMplusData(iris[, -5], \"iris.dat\", inpfile=TRUE)\r\n\r\nLatent Profile Analysis using Mplus\r\nWe can directly run in Rmarkdown or apply runModels to run the .inp files.\r\nRun direct in Rmarkdown\r\nOne for which we estimate different means between 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:33 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n*** WARNING in MODEL command\r\n  All variables are uncorrelated with all other variables within class.\r\n  Check that this is what is intended.\r\n   2 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -488.915  253358           2\r\n            -488.915  68985            17\r\n            -488.915  76974            16\r\n            -488.915  573096           20\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       13\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -488.915\r\n          H0 Scaling Correction Factor      0.9851\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                    1003.830\r\n          Bayesian (BIC)                  1042.968\r\n          Sample-Size Adjusted BIC        1001.825\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.27148          0.33514\r\n       2         99.72852          0.66486\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         0.991\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.003    0.997\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   0.995    0.005\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1      5.216    0.000\r\n    2    -13.816    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n Means\r\n    X1                 5.006      0.049    102.032      0.000\r\n    X2                 3.423      0.055     61.909      0.000\r\n    X3                 1.471      0.026     55.788      0.000\r\n    X4                 0.250      0.016     15.938      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nLatent Class 2\r\n\r\n Means\r\n    X1                 6.265      0.068     92.358      0.000\r\n    X2                 2.873      0.034     85.125      0.000\r\n    X3                 4.911      0.085     57.798      0.000\r\n    X4                 1.678      0.043     38.643      0.000\r\n\r\n Variances\r\n    X1                 0.328      0.042      7.853      0.000\r\n    X2                 0.121      0.017      7.347      0.000\r\n    X3                 0.459      0.063      7.340      0.000\r\n    X4                 0.123      0.013      9.126      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.685      0.175     -3.924      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.810E-03\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -741.018\r\n          2 Times the Loglikelihood Difference            504.205\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             18.069\r\n          Standard Deviation                               25.180\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           484.852\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:33:15\r\n        Ending Time:  22:33:16\r\n       Elapsed Time:  00:00:01\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means between the 2 profiles and the model is specified to estimate the correlation (or covariance) for the variables\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n\r\n    %c#1%\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nMplus VERSION 8.6\r\nMUTHEN & MUTHEN\r\n05/26/2021  10:33 PM\r\n\r\nINPUT INSTRUCTIONS\r\n\r\n  TITLE: iris LPA\r\n\r\n  DATA:\r\n      File is iris.dat\r\n\r\n  VARIABLE:\r\n\r\n      Names are x1, x2, x3, x4;\r\n\r\n      Classes = c(2) ;\r\n\r\n  MODEL:\r\n\r\n      %overall%\r\n\r\n      x1 x2 x3 x4;\r\n\r\n      x1 WITH x2-x4;\r\n      x2 WITH x3-x4;\r\n      x3 WITH x4;\r\n\r\n      %c#1%\r\n\r\n      [x1-x4];\r\n\r\n      %c#2%\r\n\r\n      [x1-x4];\r\n\r\n  ANALYSIS:\r\n      Type is mixture;\r\n\r\n  OUTPUT:\r\n      Tech11;\r\n\r\n\r\n\r\n*** WARNING in DATA command\r\n  Statement not terminated by a semicolon:\r\n  File is iris.dat\r\n   1 WARNING(S) FOUND IN THE INPUT INSTRUCTIONS\r\n\r\n\r\n\r\niris LPA\r\n\r\nSUMMARY OF ANALYSIS\r\n\r\nNumber of groups                                                 1\r\nNumber of observations                                         150\r\n\r\nNumber of dependent variables                                    4\r\nNumber of independent variables                                  0\r\nNumber of continuous latent variables                            0\r\nNumber of categorical latent variables                           1\r\n\r\nObserved dependent variables\r\n\r\n  Continuous\r\n   X1          X2          X3          X4\r\n\r\nCategorical latent variables\r\n   C\r\n\r\n\r\nEstimator                                                      MLR\r\nInformation matrix                                        OBSERVED\r\nOptimization Specifications for the Quasi-Newton Algorithm for\r\nContinuous Outcomes\r\n  Maximum number of iterations                                 100\r\n  Convergence criterion                                  0.100D-05\r\nOptimization Specifications for the EM Algorithm\r\n  Maximum number of iterations                                 500\r\n  Convergence criteria\r\n    Loglikelihood change                                 0.100D-06\r\n    Relative loglikelihood change                        0.100D-06\r\n    Derivative                                           0.100D-05\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCategorical Latent variables\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\nOptimization Specifications for the M step of the EM Algorithm for\r\nCensored, Binary or Ordered Categorical (Ordinal), Unordered\r\nCategorical (Nominal) and Count Outcomes\r\n  Number of M step iterations                                    1\r\n  M step convergence criterion                           0.100D-05\r\n  Basis for M step termination                           ITERATION\r\n  Maximum value for logit thresholds                            15\r\n  Minimum value for logit thresholds                           -15\r\n  Minimum expected cell size for chi-square              0.100D-01\r\nOptimization algorithm                                         EMA\r\nRandom Starts Specifications\r\n  Number of initial stage random starts                         20\r\n  Number of final stage optimizations                            4\r\n  Number of initial stage iterations                            10\r\n  Initial stage convergence criterion                    0.100D+01\r\n  Random starts scale                                    0.500D+01\r\n  Random seed for generating random starts                       0\r\n\r\nInput data file(s)\r\n  iris.dat\r\nInput data format  FREE\r\n\r\n\r\n\r\nUNIVARIATE SAMPLE STATISTICS\r\n\r\n     UNIVARIATE HIGHER-ORDER MOMENT DESCRIPTIVE STATISTICS\r\n\r\n         Variable/         Mean/     Skewness/   Minimum/ % with                Percentiles\r\n        Sample Size      Variance    Kurtosis    Maximum  Min/Max      20%/60%    40%/80%    Median\r\n\r\n     X1                    5.843       0.312       4.300    0.67%       5.000      5.600      5.800\r\n             150.000       0.681      -0.574       7.900    0.67%       6.100      6.500\r\n     X2                    3.057       0.316       2.000    0.67%       2.700      3.000      3.000\r\n             150.000       0.189       0.181       4.400    0.67%       3.100      3.400\r\n     X3                    3.758      -0.272       1.000    0.67%       1.500      3.900      4.350\r\n             150.000       3.096      -1.396       6.900    0.67%       4.600      5.300\r\n     X4                    1.199      -0.102       0.100    3.33%       0.200      1.100      1.300\r\n             150.000       0.577      -1.336       2.500    2.00%       1.500      1.900\r\n\r\nRANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES\r\n\r\nFinal stage loglikelihood values at local maxima, seeds, and initial stage start numbers:\r\n\r\n            -296.448  533738           11\r\n            -296.448  68985            17\r\n            -296.448  unperturbed      0\r\n            -296.448  27071            15\r\n\r\n\r\n\r\nTHE BEST LOGLIKELIHOOD VALUE HAS BEEN REPLICATED.  RERUN WITH AT LEAST TWICE THE\r\nRANDOM STARTS TO CHECK THAT THE BEST LOGLIKELIHOOD IS STILL OBTAINED AND REPLICATED.\r\n\r\n\r\nTHE MODEL ESTIMATION TERMINATED NORMALLY\r\n\r\n\r\n\r\nMODEL FIT INFORMATION\r\n\r\nNumber of Free Parameters                       19\r\n\r\nLoglikelihood\r\n\r\n          H0 Value                        -296.448\r\n          H0 Scaling Correction Factor      1.0304\r\n            for MLR\r\n\r\nInformation Criteria\r\n\r\n          Akaike (AIC)                     630.895\r\n          Bayesian (BIC)                   688.097\r\n          Sample-Size Adjusted BIC         627.966\r\n            (n* = (n + 2) / 24)\r\n\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THE ESTIMATED MODEL\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON ESTIMATED POSTERIOR PROBABILITIES\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1         50.00004          0.33333\r\n       2         99.99996          0.66667\r\n\r\n\r\nFINAL CLASS COUNTS AND PROPORTIONS FOR THE LATENT CLASSES\r\nBASED ON THEIR MOST LIKELY LATENT CLASS MEMBERSHIP\r\n\r\nClass Counts and Proportions\r\n\r\n    Latent\r\n   Classes\r\n\r\n       1               50          0.33333\r\n       2              100          0.66667\r\n\r\n\r\nCLASSIFICATION QUALITY\r\n\r\n     Entropy                         1.000\r\n\r\n\r\nAverage Latent Class Probabilities for Most Likely Latent Class Membership (Row)\r\nby Latent Class (Column)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nClassification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n           1        2\r\n\r\n    1   1.000    0.000\r\n    2   0.000    1.000\r\n\r\n\r\nLogits for the Classification Probabilities for the Most Likely Latent Class Membership (Column)\r\nby Latent Class (Row)\r\n\r\n              1        2\r\n\r\n    1     11.903    0.000\r\n    2    -12.706    0.000\r\n\r\n\r\nMODEL RESULTS\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\nLatent Class 1\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 5.006      0.049    101.442      0.000\r\n    X2                 3.428      0.053     64.589      0.000\r\n    X3                 1.462      0.024     60.137      0.000\r\n    X4                 0.246      0.015     16.674      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nLatent Class 2\r\n\r\n X1       WITH\r\n    X2                 0.113      0.019      5.805      0.000\r\n    X3                 0.305      0.050      6.104      0.000\r\n    X4                 0.114      0.019      6.112      0.000\r\n\r\n X2       WITH\r\n    X3                 0.098      0.022      4.359      0.000\r\n    X4                 0.056      0.010      5.330      0.000\r\n\r\n X3       WITH\r\n    X4                 0.193      0.024      8.175      0.000\r\n\r\n Means\r\n    X1                 6.262      0.066     94.947      0.000\r\n    X2                 2.872      0.033     86.743      0.000\r\n    X3                 4.906      0.082     59.719      0.000\r\n    X4                 1.676      0.042     39.652      0.000\r\n\r\n Variances\r\n    X1                 0.331      0.042      7.870      0.000\r\n    X2                 0.120      0.016      7.574      0.000\r\n    X3                 0.460      0.063      7.333      0.000\r\n    X4                 0.123      0.013      9.137      0.000\r\n\r\nCategorical Latent Variables\r\n\r\n Means\r\n    C#1               -0.693      0.173     -4.002      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.149E-04\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nTECHNICAL 11 OUTPUT\r\n\r\n     Random Starts Specifications for the k-1 Class Analysis Model\r\n        Number of initial stage random starts                  20\r\n        Number of final stage optimizations                     4\r\n\r\n     VUONG-LO-MENDELL-RUBIN LIKELIHOOD RATIO TEST FOR 1 (H0) VERSUS 2 CLASSES\r\n\r\n          H0 Loglikelihood Value                         -379.915\r\n          2 Times the Loglikelihood Difference            166.934\r\n          Difference in the Number of Parameters                5\r\n          Mean                                             11.054\r\n          Standard Deviation                                9.531\r\n          P-Value                                          0.0000\r\n\r\n     LO-MENDELL-RUBIN ADJUSTED LRT TEST\r\n\r\n          Value                                           160.527\r\n          P-Value                                          0.0000\r\n\r\n     Beginning Time:  22:33:16\r\n        Ending Time:  22:33:17\r\n       Elapsed Time:  00:00:01\r\n\r\n\r\n\r\nMUTHEN & MUTHEN\r\n3463 Stoner Ave.\r\nLos Angeles, CA  90066\r\n\r\nTel: (310) 391-9971\r\nFax: (310) 391-8971\r\nWeb: www.StatModel.com\r\nSupport: Support@StatModel.com\r\n\r\nCopyright (c) 1998-2021 Muthen & Muthen\r\n\r\nOne for which we estimate different means and the model is specified to different covariances (and variable variances) between the 2 profiles\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1, x2, x3, x4;\r\n\r\n    Classes = c(2) ;\r\n            \r\nMODEL:\r\n\r\n    %c#1%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\n    %c#2%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    x1 WITH x2-x4;\r\n    x2 WITH x3-x4;\r\n    x3 WITH x4;\r\n    \r\n    [x1-x4];\r\n    \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\n\r\n\r\nThe disadvantage is that Rmarkdown shows a lengthy output, making “eye strain” to track the necessary information.\r\n\r\nUsing runModels\r\nSave each chunk of the following models (either in a .txt file or in MPlus style using a .inp file type), then run with runModels\r\n\r\n# Model 1\r\nrunModels(\"2-iris-LPA_means.inp\")\r\n\r\nRunning model: 2-iris-LPA_means.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means.inp\" \r\n# Model 2\r\nrunModels(\"2-iris-LPA_means_correlated.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated.inp\" \r\n# Model 3\r\nrunModels(\"2-iris-LPA_means_correlated_free_variances.inp\")\r\n\r\nRunning model: 2-iris-LPA_means_correlated_free_variances.inp \r\nSystem command: C:\\WINDOWS\\system32\\cmd.exe /c cd \"C:\\Users\\minhh\\Box Sync\\Dissertation\\hai-mn.github.io\\_posts\\2021-05-26-Mplus-as-a-knitr-engine\" && \"Mplus\" \"2-iris-LPA_means_correlated_free_variances.inp\" \r\n\r\n\r\nm1 <- readModels(\"2-iris-LPA_means.out\")\r\nReading model:  2-iris-LPA_means.out \r\nm2 <- readModels(\"2-iris-LPA_means_correlated.out\")\r\nReading model:  2-iris-LPA_means_correlated.out \r\nm3 <- readModels(\"2-iris-LPA_means_correlated_free_variances.out\")\r\nReading model:  2-iris-LPA_means_correlated_free_variances.out \r\n\r\nCompare the model fit\r\nNow, we inspect the fit statistics and other summary information for the three models:\r\n\r\nm1$summaries$BIC\r\n[1] 1042.968\r\nm2$summaries$BIC\r\n[1] 688.097\r\nm3$summaries$BIC\r\n[1] 574.018\r\n\r\nAnd examine parameters:\r\n\r\nm1$parameters[[1]][-nrow(m1$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1        Means    X1 5.006 0.049 102.032    0           1\r\n2        Means    X2 3.423 0.055  61.909    0           1\r\n3        Means    X3 1.471 0.026  55.788    0           1\r\n4        Means    X4 0.250 0.016  15.938    0           1\r\n5    Variances    X1 0.328 0.042   7.853    0           1\r\n6    Variances    X2 0.121 0.017   7.347    0           1\r\n7    Variances    X3 0.459 0.063   7.340    0           1\r\n8    Variances    X4 0.123 0.013   9.126    0           1\r\n9        Means    X1 6.265 0.068  92.358    0           2\r\n10       Means    X2 2.873 0.034  85.125    0           2\r\n11       Means    X3 4.911 0.085  57.798    0           2\r\n12       Means    X4 1.678 0.043  38.643    0           2\r\n13   Variances    X1 0.328 0.042   7.853    0           2\r\n14   Variances    X2 0.121 0.017   7.347    0           2\r\n15   Variances    X3 0.459 0.063   7.340    0           2\r\n16   Variances    X4 0.123 0.013   9.126    0           2\r\nm2$parameters[[1]][-nrow(m2$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se pval LatentClass\r\n1      X1.WITH    X2 0.113 0.019   5.805    0           1\r\n2      X1.WITH    X3 0.305 0.050   6.104    0           1\r\n3      X1.WITH    X4 0.114 0.019   6.112    0           1\r\n4      X2.WITH    X3 0.098 0.022   4.359    0           1\r\n5      X2.WITH    X4 0.056 0.010   5.330    0           1\r\n6      X3.WITH    X4 0.193 0.024   8.175    0           1\r\n7        Means    X1 5.006 0.049 101.442    0           1\r\n8        Means    X2 3.428 0.053  64.589    0           1\r\n9        Means    X3 1.462 0.024  60.137    0           1\r\n10       Means    X4 0.246 0.015  16.674    0           1\r\n11   Variances    X1 0.331 0.042   7.870    0           1\r\n12   Variances    X2 0.120 0.016   7.574    0           1\r\n13   Variances    X3 0.460 0.063   7.333    0           1\r\n14   Variances    X4 0.123 0.013   9.137    0           1\r\n15     X1.WITH    X2 0.113 0.019   5.805    0           2\r\n16     X1.WITH    X3 0.305 0.050   6.104    0           2\r\n17     X1.WITH    X4 0.114 0.019   6.112    0           2\r\n18     X2.WITH    X3 0.098 0.022   4.359    0           2\r\n19     X2.WITH    X4 0.056 0.010   5.330    0           2\r\n20     X3.WITH    X4 0.193 0.024   8.175    0           2\r\n21       Means    X1 6.262 0.066  94.947    0           2\r\n22       Means    X2 2.872 0.033  86.743    0           2\r\n23       Means    X3 4.906 0.082  59.719    0           2\r\n24       Means    X4 1.676 0.042  39.652    0           2\r\n25   Variances    X1 0.331 0.042   7.870    0           2\r\n26   Variances    X2 0.120 0.016   7.574    0           2\r\n27   Variances    X3 0.460 0.063   7.333    0           2\r\n28   Variances    X4 0.123 0.013   9.137    0           2\r\nm3$parameters[[1]][-nrow(m3$parameters[[1]]), ]\r\n   paramHeader param   est    se  est_se  pval LatentClass\r\n1      X1.WITH    X2 0.097 0.022   4.469 0.000           1\r\n2      X1.WITH    X3 0.016 0.010   1.655 0.098           1\r\n3      X1.WITH    X4 0.010 0.004   2.486 0.013           1\r\n4      X2.WITH    X3 0.011 0.008   1.418 0.156           1\r\n5      X2.WITH    X4 0.009 0.005   1.763 0.078           1\r\n6      X3.WITH    X4 0.006 0.003   2.316 0.021           1\r\n7        Means    X1 5.006 0.049 101.439 0.000           1\r\n8        Means    X2 3.428 0.053  64.591 0.000           1\r\n9        Means    X3 1.462 0.024  60.132 0.000           1\r\n10       Means    X4 0.246 0.015  16.673 0.000           1\r\n11   Variances    X1 0.122 0.022   5.498 0.000           1\r\n12   Variances    X2 0.141 0.033   4.267 0.000           1\r\n13   Variances    X3 0.030 0.007   4.222 0.000           1\r\n14   Variances    X4 0.011 0.003   3.816 0.000           1\r\n15     X1.WITH    X2 0.121 0.027   4.467 0.000           2\r\n16     X1.WITH    X3 0.449 0.070   6.377 0.000           2\r\n17     X1.WITH    X4 0.166 0.026   6.282 0.000           2\r\n18     X2.WITH    X3 0.141 0.033   4.330 0.000           2\r\n19     X2.WITH    X4 0.079 0.015   5.295 0.000           2\r\n20     X3.WITH    X4 0.286 0.031   9.107 0.000           2\r\n21       Means    X1 6.262 0.066  94.948 0.000           2\r\n22       Means    X2 2.872 0.033  86.743 0.000           2\r\n23       Means    X3 4.906 0.082  59.719 0.000           2\r\n24       Means    X4 1.676 0.042  39.652 0.000           2\r\n25   Variances    X1 0.435 0.059   7.332 0.000           2\r\n26   Variances    X2 0.110 0.017   6.442 0.000           2\r\n27   Variances    X3 0.675 0.086   7.822 0.000           2\r\n28   Variances    X4 0.179 0.018  10.148 0.000           2\r\n\r\nOne last thing, I want to mention about the createModels() which can creates a set of models using a template. We can save file as mplus_iris_lpa_template.txt\r\n[[init]]\r\niterators = classes;\r\nclasses = 1:9;\r\nfilename = \"[[classes]]-iris-LPA.inp\";\r\noutputDirectory = the_dir;\r\n[[/init]]\r\n\r\nTITLE: iris LPA\r\n\r\nDATA:\r\n    File is iris.dat\r\n    \r\nVARIABLE: \r\n\r\n    Names are x1 x2 x3 x4;\r\n\r\n    Classes = c([[classes]]) ;\r\n\r\nMODEL:\r\n    \r\n    %overall%\r\n    \r\n    x1 x2 x3 x4; \r\n    \r\n    [x1-x4];\r\n\r\n            \r\nANALYSIS: \r\n    Type is mixture;\r\n            \r\nOUTPUT:\r\n    Tech11;\r\nHere is an example that would create models with different numbers of profiles, from 1 to 9.\r\n\r\n#Set the folder containing mplus.txt as working\r\ncreateModels(\"mplus_iris_lpa_template.txt\")\r\n\r\n#Set the folder containing input files as working directory\r\nrunModels()\r\n\r\n#Set the folder containing output files as working directory\r\nmodels_list <- readModels()\r\noutputs<-extractModelParameters()\r\n\r\nFurther reading, we can look at a Josh Rosenberg’s post and the Mplus website\r\nMore information about the MplusAutomation package in GitHub.\r\n\r\n\r\nHallquist, M. N., and J. F. Wiley. 2018. “MplusAutomation: An r Package for Facilitating Large-Scale Latent Variable Analyses in Mplus.” Journal Article. Struct Equ Modeling 25 (4): 621–38. https://doi.org/10.1080/10705511.2017.1402334.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-26T22:33:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/",
    "title": "A Note on 5-day Workshop on Mplus ~ Day 1",
    "description": "A Crash Course on Social Psychology Research from Michael Zyphur: Regression - Pathway analysis - Model fit  \nCombination of using Mplus and R  \nDrawing a pathway graph/causal graph using `DiagrammeR` package",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [
      "Biostatistics",
      "Psychology/Sociology",
      "Mplus",
      "Pathway/Causal Graph"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nCourse outline\r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nA variance \\(\\sigma^2\\)\r\nGraphical\r\nMultivariate Parameters\r\nCovariance \\(\\sigma_{xy}\\)\r\nA Linear Regression Model\r\nIn Terms of Distributions?\r\nWhy Regression? Causality\r\nControlling for variables\r\nWith Path Diagrams\r\nSummary\r\n\r\nPart 2: Mplus and Estimation\r\nData\r\nVariable\r\nDefine\r\nAnalysis\r\nEstimation and Inference\r\nModel\r\nOutput, Savedata, Plot, Montecarlo\r\nSummary\r\n\r\nPart 3: Path Analysis\r\nHistory\r\nShow case\r\nStructural Model Specification\r\nModel diagram\r\nReport\r\n\r\nWhat Does This Mean? How to Specify It?\r\nSummary\r\n\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nML Estimator\r\nModel Build\r\nModel Diagnostics\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nAbsolute Fit: SRMR\r\nAbsolute Fit: RMSEA\r\nRelative Fit: CFI\r\nRelative Fit: TLI/NNFI\r\nInformation Criteria: AIC\r\nInformation Criteria: BIC\r\n\r\nModification Indices\r\nBayesian Estimates of Fit\r\nPosterior Predictive Checking\r\nDeviance Information Criterion\r\n\r\nModel Selection\r\n\r\nFurther Readings\r\nPart 1\r\nPart 2\r\nPart 3\r\nPart 4\r\n\r\n\r\nMotivation\r\nWhile working on project Harmony at MRC-IHRP, my primary task is building an R package to build the tables and plots from alignment analysis designed for the multi-factor categorical case by extracting the Mplus output’s information. Since I have a chance to expose myself to Mplus, why don’t I self-learn a new tool and share it on my blog?\r\nThe Mplus and I see that a change to dive into Social Psychology Research. What on earth? The same concepts with statistics but a whole new world of terminology!!!\r\nThese can separate into five parts (equivalent to 5 days, then I divided into five topics) of taking notes on Structural Equation and Multilevel Modeling in the Mplus workshop of Prof. Michael Zyphur.\r\nHere’s a first part (1) of the so-called ‘A Crash Course on Social Psychology Research.’\r\n\r\nCourse outline\r\nStructural Equation and Multilevel Modeling in Mplus\r\nDay 1 - Introducing Mplus\r\nRegression, Covariation, and Statistical Models\r\nMplus and Parameter Estimation\r\nPath Analysis\r\nModel Fit and Model Selection\r\n\r\nDay 2 - Path Analysis\r\nMediation\r\nInstrumental Variable Methods\r\nModeration\r\nModerated Mediation\r\n\r\nDay 3 - SEM\r\nLatent Variables\r\nConfirmatory Factor Analysis\r\nStructural Equation Modeling\r\nModel Identification\r\n\r\nDay 4 - MLM\r\nMultilevel Data and Regression\r\nMultilevel Path Analysis\r\nMultilevel Confirmatory Factor Analysis and Structural Equation Modeling\r\nRandom Slopes\r\n\r\nDay 5 - LGM\r\nLongitudinal Data and Processes\r\nLatent Growth Models as Multilevel Models\r\nLatent Growth Models as Structural Equation Models\r\nDynamic Latent Growth Modeling\r\n\r\nThe material can be downloaded here\r\nReminder:\r\n  - Path analysis: regression for observed variables  \r\n  - CFA: regression from latent --> observed variables  \r\n  - SEM: regression among latent variables  \r\n  - Multilevel models: regression at multiple 'levels'  \r\n  - Letant growth: model change with latent variables  \r\nPart 1: Means, (Co)Variances, Regression\r\nDistributions Imply Rules\r\nParameters depend on the distributions we model\r\nAssuming a normal distribution for y, we have:\r\nMean \\(\\mu_y\\): a location parameter\r\nVariance \\(\\sigma^2_y\\) : a scale parameter\r\n\r\n\r\np <- seq(-5,5,by=0.1)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(.2)), aes(colour = \"mu=0,sigma2=.2\")) + \r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(1.0)), aes(colour = \"mu=0,sigma2=1.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=0, sd=sqrt(5.0)), aes(colour = \"mu=0,sigma2=5.0\")) +\r\n  stat_function(fun=dnorm, args=list(mean=-2, sd=sqrt(2.0)), aes(colour = \"mu=-2,sigma2=2\")) +\r\n  scale_y_continuous(limits=c(0,1.0)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Normal Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\nA variance \\(\\sigma^2\\)\r\nTells us the spread of scores along a variable\r\nTo estimate a variance, we assume variation is ‘random’ or due to ‘chance’\r\nThis allows using distributions & their rules\r\nEither way, a variable varies (by definition)\r\nTo model a variable y, we model its variation\r\nTo speak of a variable is to speak of variance!\r\n\r\nGraphical\r\nWe’ll show models with diagrams\r\nSquares and rectangles are observed variables\r\nVariance is double-headed arrow on one variable\r\nFor concision, we will be selective about what we diagram\r\n\r\n\r\nTo refer to y’s variance in Mplus, we will write “y”\r\nMultivariate Parameters\r\nFor multiple variables x and y:\r\nMultivartiate mean vector \\(\\mu\\) for x and y\r\nCovariance \\(\\Sigma\\) as matrix of 2 variances and a covariance\r\n\r\n\r\n#generate the data\r\ngibbs<-function (n, rho) {\r\n    mat <- matrix(ncol = 2, nrow = n)\r\n    x <- 0\r\n    y <- 0\r\n    mat[1, ] <- c(x, y)\r\n    for (i in 2:n) {\r\n        x <- rnorm(1, rho * y, (1 - rho^2))\r\n        y <- rnorm(1, rho * x, (1 - rho^2))\r\n        mat[i, ] <- c(x, y)\r\n    }\r\n    mat\r\n}\r\nbvn <- gibbs(10000, 0.98)\r\n\r\n#setup\r\nlibrary(rgl) # plot3d, quads3d, lines3d, grid3d, par3d, axes3d, box3d, mtext3d\r\nlibrary(car) # dataEllipse\r\n\r\n#process the data\r\nhx <- hist(bvn[,2], plot=FALSE)\r\nhxs <- hx$density / sum(hx$density)\r\nhy <- hist(bvn[,1], plot=FALSE)\r\nhys <- hy$density / sum(hy$density)\r\n\r\n## [xy]max: so that there's no overlap in the adjoining corner\r\nxmax <- tail(hx$breaks, n=1) + diff(tail(hx$breaks, n=2))\r\nymax <- tail(hy$breaks, n=1) + diff(tail(hy$breaks, n=2))\r\nzmax <- max(hxs, hys)\r\n\r\n#Basic scatterplot on the floor\r\n## the base scatterplot\r\nplot3d(bvn[,2], bvn[,1], 0, zlim=c(0, zmax), pch='.',\r\n       xlab='X', ylab='Y', zlab='', axes=FALSE)\r\npar3d(scale=c(1,1,3))\r\n\r\n#Histograms on the back walls\r\n## manually create each histogram\r\nfor (ii in seq_along(hx$counts)) {\r\n    quads3d(hx$breaks[ii]*c(.9,.9,.1,.1) + hx$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            rep(ymax, 4),\r\n            hxs[ii]*c(0,1,1,0), color='gray80')\r\n}\r\nfor (ii in seq_along(hy$counts)) {\r\n    quads3d(rep(xmax, 4),\r\n            hy$breaks[ii]*c(.9,.9,.1,.1) + hy$breaks[ii+1]*c(.1,.1,.9,.9),\r\n            hys[ii]*c(0,1,1,0), color='gray80')\r\n}\r\n\r\n#Summary Lines\r\n## I use these to ensure the lines are plotted \"in front of\" the\r\n## respective dot/hist\r\nbb <- par3d('bbox')\r\ninset <- 0.02 # percent off of the floor/wall for lines\r\nx1 <- bb[1] + (1-inset)*diff(bb[1:2])\r\ny1 <- bb[3] + (1-inset)*diff(bb[3:4])\r\nz1 <- bb[5] + inset*diff(bb[5:6])\r\n\r\n## even with draw=FALSE, dataEllipse still pops up a dev, so I create\r\n## a dummy dev and destroy it ... better way to do this?\r\n###dev.new()\r\nde <- dataEllipse(bvn[,1], bvn[,2], draw=FALSE, levels=0.95)\r\n###dev.off()\r\n\r\n## the ellipse\r\nlines3d(de[,2], de[,1], z1, color='green', lwd=3)\r\n\r\n## the two density curves, probability-style\r\ndenx <- density(bvn[,2])\r\nlines3d(denx$x, rep(y1, length(denx$x)), denx$y / sum(hx$density), col='red', lwd=3)\r\ndeny <- density(bvn[,1])\r\nlines3d(rep(x1, length(deny$x)), deny$x, deny$y / sum(hy$density), col='blue', lwd=3)\r\n\r\n#Beautifications\r\ngrid3d(c('x+', 'y+', 'z-'), n=10)\r\nbox3d()\r\naxes3d(edges=c('x-', 'y-', 'z+'))\r\noutset <- 1.2 # place text outside of bbox *this* percentage\r\nmtext3d('P(X)', edge='x+', pos=c(0, ymax, outset * zmax))\r\nmtext3d('P(Y)', edge='y+', pos=c(xmax, 0, outset * zmax))\r\n\r\n\r\nCovariance \\(\\sigma_{xy}\\)\r\nTells us how two variables ‘hang together’\r\nCorrelation is just a standardized covariance\r\n\r\nIf we assume variation is random …\r\nCovariance implies a non-causal relationship\r\nNo predictor or outcome, x and y simply co-vary\r\nBlue eyes & blond hair?\r\n\r\n\r\n\r\nIn Mplus we will write “y with x” or “x with y” If the relationship is non-causal, does it matter?\r\n\r\nA Linear Regression Model\r\nWhat is the rule \\(\\beta\\) ? Our linear model\r\n\\[ y_i = \\nu + \\beta x_i + \\epsilon_i \\]\r\n\\(y\\) and \\(x\\) are variables\r\n\\(\\epsilon\\) is a variable: unobserved residual/error\r\n\\(\\nu\\) and \\(\\beta\\) are constants/fixed that we estimate\r\nIn Terms of Distributions?\r\ny is decomposed into parts\r\nHow is the mean of \\(y\\) modeled?\\[ \\nu = \\mu_y — \\beta \\mu_x \\]\r\nHow is \\(y\\),\\(x\\) covariance modeled? \\[ \\beta = \\frac{\\sigma_{xy}}{\\sigma^2_x} \\]\r\nHow is the variance of \\(y\\) modeled? \\[  \\sigma^2_y =  \\sigma^2_y R^2 + \\sigma^2_{\\epsilon} \\]\r\nWhy Regression? Causality\r\nCausal statements are helpful for action\r\nConditions for \\(x \\rightarrow y\\) causality\r\n\\(x\\) precedes \\(y\\) in time\r\n\\(x\\) and \\(y\\) are related\r\n\\(x \\rightarrow y\\) effect isn’t due to a third variable z\r\n\r\nRegression assesses the second, and helps with the third\r\nControlling for variables\r\nRemoving the effect of \\(z\\) in \\(x \\rightarrow y\\) effect is\r\nControlling for \\(z\\)\r\nHolding \\(z\\) constant (it no longer varies)\r\nThe ‘independent effect’ of \\(x\\) on \\(y\\)\r\nWe ‘partial out’ the effect of \\(z\\)\r\n\r\nThese all mean the same thing:\r\nWe somehow make \\(z\\) irrelevant for \\(x \\rightarrow y\\)\r\nAllows estimating independent effects\r\n\\[ y_i = \\nu + \\beta_1 x_i + \\beta_2 z_i+ \\epsilon_i \\]\r\nEffects are independent and additive\r\nRead it left to right: \\(y\\) depends ON \\(x\\) \\(z\\)\r\nIn Mplus simply write “y ON x z”\r\nDoes it matter if “y ON x z” or “y ON z x” ?\r\nEach \\(\\beta\\) is just a slope, and \\(\\nu\\) is intercept on y-axis\r\nWith Path Diagrams\r\nRegression effects & residual as single-headed arrows\r\n\r\nResidual indicated as single-headed arrow w/out predictor\r\nRegress “y ON x z” reads the equation left-to-right\r\nNote: Predictors correlated by default in regression\r\nThe whole point is that the predictors are correlated\r\n\r\nSummary\r\nWe estimate rules/parameters to test theory\r\nMeans, (co)variances, & causal effects\r\n\r\nMplus has a simply language for these\r\nName of variable “y” refers to variance\r\n“With” refers to covariance\r\n“On” refers to a regression slope\r\n\r\nWe use multiple predictors to control for each (i.e., we estimate independent effects)\r\nPart 2: Mplus and Estimation\r\nMplus Team\r\nBengt Muthen\r\nFormer president of psychometrics society\r\nPh.D. in stats (Utrecht)\r\nAdviser: Joreskög\r\n\r\nLinda Muthen\r\nPh.D. in education (UCLA)\r\n\r\nTihomir Asparouhov\r\nPh.D. in stats (Cal Tech)\r\n\r\nStatmodel.com has a message board!\r\nstatmodel.com/discussion/messages/board-topics.html\r\n\r\nMplus Overview\r\nSolves for unknowns in regression equations:\r\nAll observed/manifest variables\r\nPath Analysis and traditional regression\r\n\r\n\r\nContinuous latent variables\r\nSEM (latent variables are factors)\r\nMultilevel/random effects/hierarchical linear models\r\nSurvival and other analyses with latent frailties/liabilities\r\n\r\nDiscrete latent variables (i.e., categorical)\r\nLatent profile analysis: continuous observed variables\r\nLatent class (cluster) analysis: discrete observed variables\r\nVarious forms of finite mixture models (LTA/Markov)\r\n\r\nMplus Features\r\nObserved variables\r\nContinuous: normal, skew-normal, t-distributed, skew-t, censored\r\nCategorical & Count: ordered, nominal, zero-inflated\r\n\r\nLinking functions: Identity, logit, probit\r\nEstimators\r\nULS, WLS, GLS, ML, MLR, MLF, BAYES, etc…\r\n\r\nSimulation with Monte Carlo & Bootstrap\r\nRead the manual – many examples!\r\nMplus Caveat\r\nInput is done by hand\r\nThere is a diagrammer, but we want a stick shift!\r\n\r\nVery little input is needed\r\nThis means many default options are used\r\n\r\nEach part of Mplus input has defaults\r\nCheck the manual, read model descriptions, look at output & parameter matrices (\\(TECH_1\\) option)\r\n\r\nExample\r\nTitle: \r\n    Start input section with the heading and a colon, then type specific commands/options and end each statement with a semi-colon;\r\n    * To ignore a command line, start it with an asteriks\r\nData: \r\n    File is data.dat;\r\nVariable: \r\n    Names are employ salary education height weight;\r\n    Usevariables are employ salary height education;\r\nAnalysis:\r\n    Estimator = ML;\r\nModel:\r\n    Employ salary education on height;\r\nOutput:\r\n    Standardized;\r\nInput Command Headings\r\nTitle: Ch. 15\r\nData: Ch. 15\r\nVariable: Ch. 15\r\nDefine: Ch. 15\r\nAnalysis: Ch. 16\r\nModel: Ch. 17\r\nModel Indirect, Model Constraint, Model Priors\r\n\r\nOutput: Ch. 18\r\nSavedata: Ch. 18\r\nPlot: Ch. 18\r\nMonteCarlo: Ch. 19\r\nData\r\nIndicates location of datafile\r\nStore in same folder as Mplus input file\r\n\r\nIndicates format of data\r\nIndividual data or summary data\r\nCorrelations, SDs, means\r\n\r\nDefault is individual data\r\n\r\nMplus requires numeric data only\r\nDo not save data with variable names\r\nFor missing data I use -999\r\nCan save from SPSS, Stata, Excel, etc…\r\nMust be tab-delimited or CSV\r\nEach variable is a column, separated by tabs or commas\r\nMissing data are -999\r\nWhere are the variables names?\r\n\r\n\r\nContains\r\nMplus input files\r\n\r\nIndividual example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Individual Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is individual.dat;\r\nAnalysis: \r\nEstimator = ml;\r\n!Estimator = Bayes;\r\nVariable: \r\nnames are y x1 x2;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1 TECH8;\r\n\r\nSummary example.inp\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/Summary Example.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is summary.dat;\r\nType = means stdeviations correlation;\r\nNobservations = 500;\r\nVariable: \r\nnames are y x1 x2;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\ny on x1 x2;\r\nOutput:\r\nStandardized sampstat TECH1;\r\n\r\nIndividual dataset\r\nIndividual.dat\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/individual.dat', n = 20), sep = '\\n')\r\n   -0.354517     0.573051    -0.175230\r\n    0.561655    -0.368095     1.090042\r\n    0.315551    -0.577052     0.425472\r\n    3.347049     1.088520     1.149353\r\n   -0.122389    -0.694153    -0.766538\r\n   -0.251276    -0.017487    -1.367410\r\n   -0.517996    -0.817974    -1.559255\r\n    1.888854    -0.658335     1.007614\r\n    0.461254     0.463916    -0.898300\r\n    2.237483     1.533398     0.180512\r\n    0.480991    -0.096545    -0.352276\r\n    0.165901    -1.341994    -1.445909\r\n    1.864947     1.027419     0.677408\r\n   -0.466245    -0.138712    -0.759287\r\n    2.567804     0.483444     0.959731\r\n   -0.024201    -0.507631    -0.517296\r\n   -1.912698     0.761720    -1.901134\r\n   -1.350069    -0.736562     2.318569\r\n    0.433773     0.723880     0.111837\r\n   -0.977083     0.155868    -0.897112\r\n\r\nSummary dataset\r\nSummary.dat\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 2 (Mplus and Estimation)/summary.dat'), sep = '\\n')\r\n.485    .001    -.042\r\n1.552   1.046   .978\r\n1.0\r\n.665    1.0\r\n.427    .028    1.0\r\n\r\n\r\nNote: Great! We can get the parameter estimates from the data with means, standard deviations and correlation matrix besides the `individual data. Furthermore, we can simulate data from ‘summary’ information (using Monte Carlo methods).\r\nOther options allowed (see manual)\r\nVariable\r\nAll info about our variables\r\nNames are arbitrary, but the number of names must match the number of columns in data file\r\nNames are y x1 x2;\r\n\r\nType of variable distribution\r\nContinuous normal assumed, but you can say other\r\n\r\nWhich variables will we use?\r\nUsevariables are y x1 x2;\r\n\r\nWeighting variables\r\nMany other options\r\n\r\nDefine\r\nCreates new variables that can be used\r\nYou must list them with Usevariables are\r\n\r\nLogical & arithmetic operators & functions\r\nConditional statements (to recode)\r\nIF ___ then ___;\r\n\r\nTransformations of various kinds\r\nCentering (grand-mean or group-mean)\r\nCluster_Mean, Sum, Mean, Cut\r\n\r\nDefine: x1x2 = x1*x2;\r\nAnalysis\r\nOptions\r\nType = nature of model desired\r\nType = General is default\r\nType = Twolevel or Threelevel means multilevel\r\nType = Random implies random slopes\r\n\r\n\r\nEstimators, Algorithms\r\nEstimator = ML, or Estimator = Bayes\r\n\r\nBootstrapping\r\nLinking functions (logit or probit)\r\nMany options for ML and Bayes estimation\r\nEstimation and Inference\r\nFrequentist: Estimation uses data & model\r\nTypical estimators, such as OLS or ML\r\nIterative process trying to find best parameter estimates\r\nIteratively alters estimates to find the ‘most likely’ values\r\nThis means probability of data is being maximized (i.e., ML)\r\n\r\nConvergence achieved when estimates change very little\r\nMplus gives the estimates, SE, p-values, & CIs upon request\r\nBut, even ‘most likely’ estimates may not be very likely\r\nYour model and its estimates may be the best of a bad lot!\r\nThis is why we’ll look at fit statistics\r\n\r\nBayes: Estimation uses data, model, & prior prob.\r\nDefault is ‘uninformative prior’, so results # ML\r\nIterative process that computes ‘posterior probabilities’\r\nIt uses MCMC estimation in at least two separate ‘chains’\r\nThe estimates produce a posterior distribution for each parameter\r\n\r\nConvergence achieved when chains agree (PSR < 1.05)\r\nMedian/mode of each distribution is like an ML estimate\r\nThe SD is like the SE\r\nGives Bayesian p-value and ‘credibility interval’\r\n\r\nAgain, bad models/estimates may agree across chains\r\nModel\r\nModel: ON Command\r\nON refers to a regression slope \\(\\beta\\)\r\ny ON x; \\[ y_i = \\nu + \\beta_1 x_i + \\epsilon_i \\]\r\nRegress y on x\r\nReads the equation from left to right\r\n\r\n\r\nThink of “y ON x” as \\(x \\rightarrow y\\)\r\nFreely estimates a \\(\\beta\\)\r\nThe single-headed arrow from nowhere is a residual\r\n\r\n\r\nModel: WITH Command\r\nWITH refers to covariance \\(\\Theta\\) or \\(\\Psi\\)\r\ny1 WITH y2;\r\nEstimates covariance among y1 and y2\r\n\r\nWhat if y1 and y2 are dependent variables?\r\nThen we estimate a residual covariance\r\n\r\n\r\n\r\nModel: BY Command\r\nBY refers to factor loadings (slopes)\r\nf1 BY y1 y2 y3;\r\nlatent variable “f1” is indicated by y1, y2, y3\r\n\r\nHuh BY y1 y2 y3\r\nlatent variable is called “Huh”… name is irrelevant\r\n\r\nFreely estimate \\(\\lambda\\) for each observed variable\r\nExcept first loading is fixed to 1.0 by default\r\n\r\n\r\n\r\nModel notation\r\nTypical SEM notation for vectors/matrices\r\n\\(B\\): matrix of regression coefficients, elements \\(\\beta\\)\r\n\\(\\Lambda\\): matrix of factor loadings, elements \\(\\lambda\\)\r\n\\(\\eta\\): vector of latents (factors/random effects)\r\n\\(\\Psi\\): matrix of latent (co)variances, elements ψ\r\n\\(\\Theta\\): matrix of observed (co)variances, elements \\(\\theta\\)\r\n\\(\\nu\\): intercepts, \\(\\alpha\\) is latent intercepts or means\r\n\r\nAll commands will tell Mplus to either:\r\nFreely estimate an element in vector/matrix\r\nThis is what our Model commands do\r\n\r\nFix an element in vector/matrix to some value\r\nMplus defaults often do this for us\r\n\r\nMplus notation for freeing and fixing estimates\r\ny ON x;\r\n* freely-estimated \\(\\beta\\)y ON x@.5;\r\n* \\(\\beta\\) is NOT estimated, but fixed to .5[y@0];\r\n* an intercept for y1 constrained to 0.0f1 BY y*;\r\n* * = freely estimate, so estimates factor loading for y on factor “f1”f1 BY y1-y5@1;\r\n* factor loadings for “f1” = 1 for variables y1, y2, y3, y4, y5\r\nLabeling and constraining/fixing estimates\r\nLabels are put in parenthesesy ON x (b1);\r\nNow, the slope is called “b1” and can be used later\r\n\r\nIf parameters have the same label, it’s like they’re the same thingy ON x z (b2);\r\nNow x\\(\\rightarrow\\)y and z\\(\\rightarrow\\)y slopes are constrained to equality\r\nMplus will estimate them but keep them equal\r\n\r\nModel Constraint\r\nHere we play with labeled parameters\r\nAllows linear/non-linear model constraints\r\nb1+b2=0;\r\n\r\nPlay with parameters labeled in Model command\r\n\r\nToo much creativity to describe, but:\r\nCan create new “phantom” parameters\r\nSee manual for the “New” command\r\nCheck example 5.21\r\n\r\n\r\nTITLE:     this is an example of a two-group twin\r\n           model for  continuous outcomes using parameter constraints\r\n\r\nDATA:      FILE = ex5.21.dat;\r\n\r\nVARIABLE:  NAMES = y1 y2 g;\r\n           GROUPING = g(1 = mz 2 = dz);\r\n\r\nMODEL:     [y1-y2]    (1);\r\n           y1-y2      (var);\r\n           y1 WITH y2 (covmz);\r\n\r\nMODEL dz:  y1 WITH y2 (covdz);\r\n\r\nMODEL CONSTRAINT:\r\n           NEW(a c e h);\r\n           var = a**2 + c**2 + e**2;\r\n           covmz = a**2 + c**2;\r\n           covdz = 0.5*a**2 + c**2;\r\n           h = a**2/(a**2 + c**2 + e**2);\r\nModel Priors\r\nFor Bayes, we can specify prior probabilities\r\nThese are distributions…\r\nModel:y2 ON y1 (b1);\r\nModel Priors:b1~N(.25, 1);\r\nWe say “b1” is distributed as (\\(\\sim\\)) normal (\\(N\\)) with mean and variance (\\(\\mu, \\sigma^2\\)) of .25 and 1\r\nMplus has defaults that are ‘diffuse priors’\r\nEg, regression coefficients are \\(\\sim N(0,10^10)\\)\r\n\r\nModel Indirect\r\nComputes mediation effects to show\r\n“Decomposition” of total and indirect effects\r\nUse bootstrapping in Analysis command with ML to get bootstrapped indirect effects\r\nBayesian estimation gives distribution of effects, so no bootstrapping required\r\n\r\nOutput, Savedata, Plot, Montecarlo\r\nOutput\r\nTECH, Standardized\r\n\r\nSavedata\r\nEstimated data: factor scores, co(var) matrices\r\n\r\nPlot\r\nHelpful for various models\r\n\r\nMonteCarlo\r\nData generation facility… allows parametric bootstrap\r\nMakes running simulations a snap\r\nEmpirically-derived estimates of power\r\nCan publish from this if you’re inclined\r\n\r\n\r\nSummary\r\nOur job is to\r\nTell Mplus about data and estimation method\r\nSpecify a statistical model to reflect our theory\r\nUse ML or Bayes to estimate parameters\r\nUse results to make inferences\r\nPart 3: Path Analysis\r\nHistory\r\nDeveloped by Sewell Wright in 1918 (1921)\r\nGeneticist modeling relations of family members\r\nBrother Philip borrows to create ‘simultaneous equations’ with IVs in 1928 (S&D of flaxseed)\r\n\r\nTaken up in health, biological, and social sciences to model complex relationships\r\nSubsumes simultaneous equation analysis\r\n\r\nShow case\r\nStructural Model Specification\r\nSpecify causal model of interest\r\nGrandey & Cropanzano (1999). The conservation of resources model applied to work–family conflict and strain. Journal of Vocational Behavior, 54, 350-370.\r\n\r\nCausal effects (regression)\r\nWork role stress \\(\\rightarrow\\) Job distress\r\nWork role stress \\(\\rightarrow\\) Work-family conflict\r\nWork-family conflict \\(\\rightarrow\\) Job distress\r\nJob distress \\(\\rightarrow\\) Turnover intentions\r\nJob distress \\(\\rightarrow\\) Life distress\r\n\r\nStochastic, non-causal relationships\r\nTurnover intentions\\(\\leftrightarrow\\)Life distress\r\n\r\nModel diagram\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>>]\r\nTI->LD[dir=both, label=<&psi;>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>>]\\nTI->LD[dir=both, label=<&psi;>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nVariable:\r\nNames are WRS FRS WFC FWC JD FD LD TI PPH SE;\r\nUsevariables are WRS TI WFC JD LD;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOR\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nTI WFC on WRS; LD on WFC;\r\n\r\nNote: for 2 dependent variables which are not predicting anything else, just the outcomes. Mplus will automatically covariance for us. If not estimate the residual covariance would only hurt the model fit. In other words, if there is a residual covariance there and we do not estimat them, it would drive the model fit down.\r\nCode in Mplus:\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS TI WFC JD LD;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS; ! left-side: dependent vbl; right-side: predictors\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\nNote: here’s again we use the ‘summary’ data from the article instead of ‘individual’ data (don’t have it!!!)\r\nMplus output\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999_Hai.out'), sep = '\\n')\r\nMODEL RESULTS ! Unstandardized Output\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000 ! 1-unit increase on JD, .536 increase on TI\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880 ! Residual covariances\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nQUALITY OF NUMERICAL RESULTS\r\n\r\n     Condition Number for the Information Matrix              0.141E-02\r\n       (ratio of smallest to largest eigenvalue)\r\n\r\n\r\nSTANDARDIZED MODEL RESULTS \r\n\r\n\r\nSTDYX Standardization ! Standardized Output (STDYX)\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.075      0.092      0.813      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.376      0.073      5.178      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.490      0.066      7.408      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTDY Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.438      0.086      5.114      0.000\r\n    WRS                0.094      0.115      0.814      0.416\r\n\r\n LD       ON\r\n    JD                 0.628      0.058     10.848      0.000\r\n    WFC                0.218      0.066      3.273      0.001\r\n\r\n JD       ON\r\n    WRS                0.472      0.089      5.279      0.000\r\n    WFC                0.376      0.073      5.178      0.000\r\n\r\n WFC      ON\r\n    WRS                0.615      0.078      7.844      0.000\r\n\r\n LD       WITH\r\n    TI                -0.013      0.087     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.547      0.300      1.823      0.068\r\n    WFC                1.806      0.327      5.528      0.000\r\n    JD                 0.688      0.288      2.389      0.017\r\n    LD                 0.425      0.228      1.864      0.062\r\n\r\n Residual Variances\r\n    TI                 0.766      0.065     11.870      0.000\r\n    WFC                0.760      0.065     11.724      0.000\r\n    JD                 0.579      0.065      8.854      0.000\r\n    LD                 0.405      0.054      7.446      0.000\r\n\r\n\r\nSTD Standardization\r\n\r\n                                                    Two-Tailed\r\n                    Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n TI       ON\r\n    JD                 0.536      0.112      4.767      0.000\r\n    WRS                0.092      0.114      0.812      0.417\r\n\r\n LD       ON\r\n    JD                 0.682      0.073      9.399      0.000\r\n    WFC                0.186      0.057      3.255      0.001\r\n\r\n JD       ON\r\n    WRS                0.381      0.077      4.947      0.000\r\n    WFC                0.296      0.060      4.947      0.000\r\n\r\n WFC      ON\r\n    WRS                0.631      0.098      6.458      0.000\r\n\r\n LD       WITH\r\n    TI                -0.006      0.042     -0.151      0.880\r\n\r\n Intercepts\r\n    TI                 0.539      0.277      1.950      0.051\r\n    WFC                1.853      0.256      7.228      0.000\r\n    JD                 0.555      0.208      2.669      0.008\r\n    LD                 0.373      0.185      2.019      0.043\r\n\r\n Residual Variances\r\n    TI                 0.745      0.092      8.124      0.000\r\n    WFC                0.800      0.098      8.124      0.000\r\n    JD                 0.377      0.046      8.124      0.000\r\n    LD                 0.311      0.038      8.124      0.000\r\n\r\n\r\nR-SQUARE\r\n\r\n    Observed                                        Two-Tailed\r\n    Variable        Estimate       S.E.  Est./S.E.    P-Value\r\n\r\n    TI                 0.234      0.065      3.631      0.000\r\n    WFC                0.240      0.065      3.704      0.000\r\n    JD                 0.421      0.065      6.436      0.000\r\n    LD                 0.595      0.054     10.947      0.000\r\n\r\n\r\nTECHNICAL 1 OUTPUT\r\n\r\n     PARAMETER SPECIFICATION\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  0             0             0             0             0\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             0             0             0\r\n WFC                0             0             0             0             0\r\n JD                 0             0             0             0             0\r\n LD                 0             0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0\r\n WFC                0             0\r\n JD                 0             0             0\r\n LD                 0             0             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                  1             2             3             4             0\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                 0             0             5             0             6\r\n WFC                0             0             0             0             7\r\n JD                 0             8             0             0             9\r\n LD                 0            10            11             0             0\r\n WRS                0             0             0             0             0\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI                12\r\n WFC                0            13\r\n JD                 0             0            14\r\n LD                15             0             0            16\r\n WRS                0             0             0             0             0\r\n\r\n     STARTING VALUES\r\n\r\n           NU\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                0.000         0.000         0.000         0.000         0.000\r\n\r\n           LAMBDA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             1.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         1.000         0.000         0.000         0.000\r\n JD             0.000         0.000         1.000         0.000         0.000\r\n LD             0.000         0.000         0.000         1.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         1.000\r\n\r\n           THETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000\r\n WFC            0.000         0.000\r\n JD             0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           ALPHA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n                2.120         3.430         2.520         2.730         2.500\r\n\r\n           BETA\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.000         0.000         0.000         0.000         0.000\r\n WFC            0.000         0.000         0.000         0.000         0.000\r\n JD             0.000         0.000         0.000         0.000         0.000\r\n LD             0.000         0.000         0.000         0.000         0.000\r\n WRS            0.000         0.000         0.000         0.000         0.000\r\n\r\n           PSI\r\n              TI            WFC           JD            LD            WRS\r\n              ________      ________      ________      ________      ________\r\n TI             0.490\r\n WFC            0.000         0.530\r\n JD             0.000         0.000         0.328\r\n LD             0.000         0.000         0.000         0.387\r\n WRS            0.000         0.000         0.000         0.000         0.635\r\n\r\nWhat’s \\(R^2\\) for our variables?\r\n(1 – standardized residual variance)\r\nCan also just look at bottom of output\r\nReport\r\n\r\ngrViz(\"\r\ndigraph causal{\r\n\r\n  # a 'graph' statement\r\n  graph [overlap = true, fontsize = 10]\r\n\r\n  # several 'node' statements\r\n  node  [shape = box,\r\n         fontname = Helvetica]\r\nWRS [label = 'Work Role Stress (WRS)']\r\nWFC [label = 'Work Family Conflict (WFC)']\r\nJD [label = 'Job Distress (JD)']\r\nTI [label = 'Turnover Intentions (TI)']\r\nLD [label = 'Life Distress (LD)']\r\n\r\n# Edges\r\nedge[color=black, arrowhead=vee]\r\nWRS->WFC [label=<&beta;<SUB>1<\/SUB>=.63/.49**>]\r\nWRS->TI [label=<&#946;<SUB>2<\/SUB>=.09/.08>]\r\nWRS->JD [label=<&#946;<SUB>3<\/SUB>=.38/0.38**>]\r\nWFC->JD [label=<&#946;<SUB>4<\/SUB>=.30/.38**>]\r\nWFC->LD [label=<&#946;<SUB>5<\/SUB>=.19/.22**>]\r\nJD->TI [label=<&#946;<SUB>6<\/SUB>=.54/.44**>]\r\nJD->LD [label=<&#946;<SUB>7<\/SUB>=.68/.63**>]\r\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\r\nd1->WFC\r\nd1 [shape=plaintext,label='']\r\nd2->JD\r\nd2 [shape=plaintext,label='']\r\nd3->TI\r\nd3 [shape=plaintext,label='']\r\nd4->LD\r\nd4 [shape=plaintext,label='']\r\n\r\n{rank = same; WRS; WFC}\r\n{rank = same; TI; LD}\r\n}\")\r\n\r\n{\"x\":{\"diagram\":\"\\ndigraph causal{\\n\\n  # a \\\"graph\\\" statement\\n  graph [overlap = true, fontsize = 10]\\n\\n  # several \\\"node\\\" statements\\n  node  [shape = box,\\n         fontname = Helvetica]\\nWRS [label = \\\"Work Role Stress (WRS)\\\"]\\nWFC [label = \\\"Work Family Conflict (WFC)\\\"]\\nJD [label = \\\"Job Distress (JD)\\\"]\\nTI [label = \\\"Turnover Intentions (TI)\\\"]\\nLD [label = \\\"Life Distress (LD)\\\"]\\n\\n# Edges\\nedge[color=black, arrowhead=vee]\\nWRS->WFC [label=<&beta;<SUB>1<\\/SUB>=.63/.49**>]\\nWRS->TI [label=<&#946;<SUB>2<\\/SUB>=.09/.08>]\\nWRS->JD [label=<&#946;<SUB>3<\\/SUB>=.38/0.38**>]\\nWFC->JD [label=<&#946;<SUB>4<\\/SUB>=.30/.38**>]\\nWFC->LD [label=<&#946;<SUB>5<\\/SUB>=.19/.22**>]\\nJD->TI [label=<&#946;<SUB>6<\\/SUB>=.54/.44**>]\\nJD->LD [label=<&#946;<SUB>7<\\/SUB>=.68/.63**>]\\nTI->LD[dir=both, label=<&psi;=-.006/-.013>]\\nd1->WFC\\nd1 [shape=plaintext,label=\\\"\\\"]\\nd2->JD\\nd2 [shape=plaintext,label=\\\"\\\"]\\nd3->TI\\nd3 [shape=plaintext,label=\\\"\\\"]\\nd4->LD\\nd4 [shape=plaintext,label=\\\"\\\"]\\n\\n{rank = same; WRS; WFC}\\n{rank = same; TI; LD}\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\nFor example: \\(\\beta_7\\) = .68/.63**, i.e. unstandardized/standardized both Y and X estimates and significance of p-value\r\nWhat Does This Mean? How to Specify It?\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (no covariance).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\n!Estimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nTI with LD@0;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model1).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nWRS on WFC JD TI;\r\nWFC on JD LD;\r\nJD on TI LD;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\n\r\n\r\ncat(readLines('Examples/Day 1, Session 3 (Path Analysis)/Grandey & Cropanzano 1999 (new model2).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData: \r\nFile is Grandey & Cropanzano 1999.txt;\r\nType = means stdeviations correlation;\r\nNobservations = 132;\r\nVariable:\r\nnames are \r\nWRS ! work role stress\r\nFRS ! family role stress\r\nWFC ! work-family conflict\r\nFWC ! family-work conflict\r\nJD ! job distress\r\nFD ! family distress\r\nLD ! life distress\r\nTI ! turnover intentions\r\nPPH ! poor physical health\r\nSE; ! self-esteem\r\n\r\nUsevariables are \r\nWRS WFC JD LD TI;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI LD on JD;\r\nJD on WRS WFC;\r\nWRS with WFC;\r\nOutput:\r\nStandardized Sampstat TECH1 TECH8;\r\n\r\nSummary\r\nPath models specify regression/covariation among observed variables\r\nRegression represents causality ON\r\nSo reduces magnitude of DV’s variance\r\n\r\nCovariation simply indicates WITH\r\nSo does not account for variance (there is no IV/DV)\r\n\r\nWe can draw and specify any diagram\r\nMplus then gives us parameter estimates\r\nPart 4: Model Fit, Selection, Modification, & Equivalence\r\nModel fit\r\nLots of literature & many indices for ML estimator\r\nIssue of Personality & Individual Differences (2007), lead by Barrett\r\nMany cite Hu & Bentler (1999)\r\n\r\nAbsolute fit\r\nHow well does our model explain our observed data?\r\n\r\nRelative/Incremental/Comparative fit\r\nCompares fit of different models using same dataset\r\n\r\nInformation Criteria\r\nMeasure info entropy, or info lost when using an estimated model rather than the data\r\n\r\nBayesians have less work on the topic …\r\nIn statistics programs we get fit from:\r\nAnalysis/Estimated model (\\(H_0\\) in Mplus output)\r\nModel we specify and estimate\r\n\r\nBaseline/Independence/Null model\r\nWorst fitting model (or at least all covariances = 0)\r\n\r\nUnrestricted/Saturated (\\(H_1\\) in Mplus output)\r\nBest fitting model (all parameters freely estimated)\r\n\r\n\r\nFor comparing models that we estimate:\r\nNull model (\\(H_0\\)): more-constrained model\r\nFewer parameters estimated, so more parsimonious\r\n\r\nAlternate model (\\(H_1\\)): less-constrained model\r\n\r\nML Estimator\r\nInformation we have vs. what’s estimated\r\nHow many parameters implied by our data?\r\nk means, k variances, and k\\(\\times\\)(k-1)/2 covariances\r\n\r\nHow many parameters are estimated?\r\nIntercepts, (residual) variances, slopes/covariances\r\n\r\n\r\nThe difference is our degrees of freedom (df)\r\nWhat about data/model fit ?\r\nFor ML, we have model log Likelihood (LL)\r\nIt turns out, -2\\(\\times\\)LL is \\(\\chi^2\\) distributed\r\nThe difference between two \\(\\chi^2\\) values is \\(\\chi^2\\) distributed\r\n\r\n\r\nModel Build\r\nAnalysis/Estimated model (\\(H_0\\))\r\nUnrestricted/Saturated/Alternate model (\\(H_1\\))\r\n\r\n\r\nModel:TI on JD WRS;LD on JD WFC;JD on WRS WFC;WFC on WRS;\r\nModel:JD LD TI WRS WFC with JD LD TI WRS WFC;\r\nBaseline/Null Model\r\n\r\nModel:\r\nModel Diagnostics\r\nContrasting Models\r\nChi-square testing of nested models\r\nCan use change in relative fit indices\r\nNon-nested model comparisons possible with AIC/BIC\r\nAbsolute Fit: A Models’ \\(\\chi^2\\)\r\nUnrestricted vs Analysis model gives us \\(\\chi^2\\)\r\n\\(df\\) is difference in estimated model & unrestricted\r\n\r\nIf our model is good, the difference should be small\r\nConducting NHST with the \\(\\chi^2\\)\r\nWhat’s the null vs. alternate hypothesis?\r\n\r\nIssues\r\nSensitive to sample size\r\nRarely taken seriously… unless it’s non-significant!\r\nWe want to “accept the null” ???\r\n\r\nIf you want to look good on a relative basis… ??\r\nUsing \\(\\chi^2\\) to Compare Estimated Models\r\nGet \\(\\chi^2\\) and \\(df\\) for different estimated models\r\nSubtract \\(\\chi^2\\) and subtract \\(df\\), use diff. for NHST: \\(\\Delta \\chi^2\\) or \\(-2\\times LL\\)\r\nRecall the null is model with fewer parameters\r\n\r\nModel must be nested: null must be subset of alt.\r\nAbsolute Fit: SRMR\r\nStandardized Root Mean Square Residual\r\nStandardized difference between observed and model-implied data\r\nResidual in this case is the difference\r\n\r\nLess than .05 is good, Hu & Bentler say that .08 isn’t too bad…\r\nAbsolute Fit: RMSEA\r\nRoot mean squared error of approximation\r\nLike an adjusted root mean square standardized residual\r\n\r\nTakes model parsimony into account\r\nPenalized for estimating too many parameters\r\nPuts a positive value on df\r\n\r\n\\[ \\frac{\\sqrt{\\chi^2_{Estimated} - df_{Estimated}}}{\\sqrt{df\\times (N-1)}}\\]\r\nLess than .06 or .07 good (CI usually given)\r\nSteiger (2007), Understanding the limitations of global fit…\r\n\r\nRelative Fit: CFI\r\nComparative Fit Index\r\nCompares \\(\\chi^2\\) of estimated model to \\(\\chi^2\\) of baseline\r\nAdjusts for \\(df\\) to reward parsimony\r\n\r\n\\[ \\frac{\\sqrt{(\\chi^2_{Baseline} - df_{Baseline}) - (\\chi^2_{Estimated} - df_{Estimated})}}{\\sqrt{\\chi^2_{Baseline} - df_{Baseline}}}\\]\r\n.95 or higher generally accepted as cut-off\r\nRelative Fit: TLI/NNFI\r\nTucker-Lewis Index/Non-Normed Fit Index\r\nSimilar to CFI but different penalization\r\n\\[ \\frac{\\chi^2_{Baseline}/df_{Baseline} - \\chi^2_{Estimated}/df_{Estimated}}{\\chi^2_{Baseline}/ df_{Baseline} - 1}\\]\r\nWill always be smaller than CFI\r\nTo some decimal place\r\n\r\n.95 often thought of as cut-off\r\nInformation Criteria: AIC\r\nAkaikes Information Criterion\r\nk is number of estimated parameter\r\nA weighting of accuracy versus model complexity\r\nOnly useful for comparing two estimated models\r\nLower values are better\r\n\\[ \\chi^2_{Estimated} + k\\times (k-1) - 2\\times df_{Estimated} \\]\r\nInformation Criteria: BIC\r\nBayesian Information Criterion & Adjusted BIC\r\nSimilar to AIC, except weights for sample size\r\nOnly useful for comparing two estimated models\r\nSmaller values are better—BIC, then aBIC here:\r\n\\[ \\chi^2_{Estimated} + ln(N)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\n\\[ \\chi^2_{Estimated} + ln((N+2)/24)\\times k\\times (k-1)/2 - df_{Estimated} \\]\r\nFor both AIC and BIC, no significance tests… see recommendations in interpreting differences\r\nThese are all Global Measures\r\nThey collapse across all model parts\r\nSome authors call for separate fit measures\r\nWhatever… fit is necessary but insufficient\r\nThe model must make theoretical sense\r\n\r\nModification Indices\r\nMODINDICES command in Mplus\r\nIndicate change in model fit by estimating additional parameters\r\nHeated debates here\r\nFor frequentists this capitalizes on chance\r\nCan lead to nonsensical models, bias/variance tradeoff\r\n\r\nUseful for understanding sources of misfit\r\nLet’s try it with our path model…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (ML) modindices.inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = ML;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8 MODINDICES(1);\r\n\r\nBayesian Estimates of Fit\r\nNot as well developed\r\nBayes was unpopular for much of \\(20^{th}\\) Century\r\n\r\nLess obvious how to proceed\r\nThe idea of sampling from a population not formalized in the same way (so no \\(\\chi^2\\))\r\n\r\nMplus offers a few that are useful…\r\n\r\ncat(readLines('Examples/Day 1, Session 4 (Model Fit)/Grandey & Cropanzano 1999 (Bayes).inp'), sep = '\\n')\r\nTitle: \r\nBogus Mplus text;\r\nData:\r\nFile is Grandey & Cropanzano 1999 MonteCarlo.dat;\r\nVariable:\r\nnames are JD LD TI WRS WFC;\r\nAnalysis:\r\nEstimator = Bayes;\r\nfbiterations=10000;\r\nProcessors=2;\r\nModel:\r\nTI on JD WRS;\r\nLD on JD WFC;\r\nJD on WRS WFC;\r\nWFC on WRS;\r\nOutput:\r\nStandardized sampstat Tech1 Tech8;\r\n\r\nPosterior Predictive Checking\r\nSteps required during model estimation\r\nSample parameters from posteriors\r\n\r\nGenerate data with parameters\r\n\r\nEstimate fit of generated vs. observed data\r\n\r\n\r\nIf model is good, generated data should fit better/worse about half the time\r\nGet Predictive Posterior p-value (PPP)\r\nHow often the generated data fit better\r\nPPP < .05 bad, PPP ≈ .50 good, PPP > .95 weird\r\n\r\nDeviance Information Criterion\r\nLike other information criteria\r\nPenalizes for model complexity, rewards fit\r\nSmaller values = less deviance (i.e., better fit)\r\nCan compare models with different priors\r\nModel Selection\r\nHow do we choose the models we publish?\r\nSelecting models\r\nCan build them based on scripted steps9,10\r\nGive your model of interest a shot and look at fit\r\nIf that doesn’t work, try different theory-driven models\r\nLook at sources of misfit and sort out what’s going on\r\n\r\nIs this an ethical issue?\r\nScientific models are serious things\r\nPeople/groups use results for their own purposes\r\nWhen considering which model to estimate, consider the social & political implications\r\n\r\nFurther Readings\r\nPart 1\r\nGrace, J. B., & Bollen, K. A. (2005). Interpreting the results from multiple regression and structural equation models. Bulletin of the Ecological Society of America, 86, 283-295.\r\nCohen, Cohen, West, & Aiken (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences (3rd). Lawrence Erlbaum & Associates.\r\nMyers, Well, & Lorch (2010). Research Design and Statistical Analysis (3rd). Routledge Academic.\r\nTabachnick & Fidell (2006). Using Multivariate Statistics (5th). Allyn & Bacon.\r\nFisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society of London, Series A 222, 309–368.\r\nPart 2\r\nMplus User’s Guide and Diagrammer Documentation\r\nPart 3\r\nStreiner, D. L. 2005. Finding Our Way: An Introduction to Path Analysis. Canadian Journal of Psychiatry, 50, 115-122.\r\nWright, S. (1921). Correlation and causation. J. Agricultural Research 20: 557–585.\r\nWright, S. (1934). The method of path coefficients. Annals of Mathematical Statistics 5: 161–215.\r\nAngrist, J. D., & Krueger, A. B. (2001). Instrumental variables and the search for identification: From supply and demand to natural experiments. Journal of Economics Perspectives, 15: 69-85.\r\nMuthén, B. (2002). Beyond SEM: General latent variable modeling. Behaviormetrika, 29, 81-117.\r\nPart 4\r\nHu & Bentler (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives, Structural Equation Modeling, 6(1), 1-55.\r\nPersonality and Individual Differences, Volume 42, Issue 5. 2007.\r\nNeyman, J., & Pearson, E. S. 1933. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society, A, 231, 289-337.\r\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology, 25, 111-16\r\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773-795.\r\nMcDonald, R. P. (2010). Structural models and the art of approximation. Perspectives on Psychological Science, 5, 675-686.\r\nCheung, G. W., & Rensvold, R. B. (1999). Testing factorial invariance across groups: a reconceptualization and proposed new method. Journal of Management, 25, 1–27.\r\nCheung, G. W., & Rensvold, R. B. (2002). Evaluating goodness-of-fit indexes for testing measurement invariance. Structural Equation Modeling, 9, 233–255.\r\nAnderson, J. C., & Gerbing, D. W. (1988). Structural equation modeling in practice: A review and recommended two-step approach. Psychological Bulletin, 103, 411-423.\r\nMulaik, S. A., & Millsap, R. E. (2000). Doing the four-step right. Structural Equation Modeling, 7, 36-73. (see other papers in the same issue).\r\nShapin, S. 2008. The scientific life: A moral history of a late modern vocation. Chicago University Press.\r\nPorter, T. M. 1995. Trust in Numbers: The Pursuit of Objectivity in Science and Public Life.\r\nPoovey, M. 1997. A History of the Modern Fact: Problems of Knowledge in the Sciences of Wealth and Society. Chicago.\r\nMcCloskey, D. N. 1992. If you’re so smart: The narrative of economic expertise. University of Chicago Press.\r\nMcCloskey, D. N. 1992. The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives.\r\nAsparouhov, T., Muthén, B. & Morin, A. J. S. 2015. Bayesian structural equation modeling with cross-loadings and residual covariances: Comments on Stromeyer et al. Journal of Management, 41, 1561-1577.\r\n\r\n\r\n",
    "preview": "posts/2021-05-11-5-day-mplus-workshop-michael-zyphur-day-1/distill-preview.png",
    "last_modified": "2021-05-26T22:32:56-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-22-Simple-linear-regression-in-Bayesian-way/",
    "title": "Fitting a Simple Linear Regression in Bayesian Context",
    "description": "How to fit a linear regression using Bayesian Methods  \nConsider a Bayesian model fit as a remedial measures for influential case",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "R",
      "Bayesian Methods",
      "JAGS/Stan"
    ],
    "contents": "\r\n\r\nContents\r\nFrequentist\r\napproach in simple linear regression\r\nAn\r\nexample with data\r\nFit a model\r\nModel fit diagnostics\r\nMessage\r\ntake-away\r\n\r\nBayesian\r\napproach\r\nIntroduction\r\nto a regression model in Bayesian way\r\nMCMC in JAGS\r\nDescribe\r\nthe model.\r\nExplore the MCMC object\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nExplore the MCMC object\r\n\r\nUsing fitted\r\nregression model for prediction\r\n\r\nCompare the fit between FA\r\nand BA\r\nFurther\r\nreading\r\n\r\nFrequentist\r\napproach in simple linear regression\r\nAn example with data\r\nI use the data in Kruschke\r\n(2015) which\r\nhas\r\n- male\r\n- height\r\n- weight\r\nWe will use height to predict weight of a\r\nperson\r\n\r\nDT::datatable(dta, \r\n          rownames = FALSE,\r\n          filter = list(position = \"top\"))\r\n\r\n{\"x\":{\"filter\":\"top\",\"vertical\":false,\"filterHTML\":\"<tr>\\n  <td data-type=\\\"integer\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none;position: absolute;width: 200px;opacity: 1\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none;position: absolute;width: 200px;opacity: 1\\\">\\n      <div data-min=\\\"54.6\\\" data-max=\\\"76\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none;position: absolute;width: 200px;opacity: 1\\\">\\n      <div data-min=\\\"89\\\" data-max=\\\"356.8\\\" data-scale=\\\"1\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[0,0,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,0,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1],[64,62.3,67.9,64.2,64.8,57.5,65.6,70.2,63.9,71.1,66.5,68.1,62.9,75.1,64.6,69.2,68.1,72.6,63.2,64.1,64.1,71.5,76,69.7,73.3,61.7,66.4,65.7,68.3,66.9,62.4,64.5,60.6,70.8,61,66,59.6,70.1,66.6,59.8,68.5,61.4,64.7,67.4,68.3,67.3,62.5,72.4,64.4,70.6,66.3,65.9,61.1,67.8,64.4,71.2,64.5,65.4,67,70.9,62.4,70.4,64.6,69.6,65.9,70.6,65.4,73.9,70.1,64.8,59.7,69.2,61.4,70.4,71.1,60.8,68.3,67.4,63.5,66.8,73.4,64.8,68.9,66.5,64.7,66.2,69.1,66,60.9,72.4,67.5,71.6,70.6,64.7,72.7,64.7,67.9,65.9,54.6,64.1,68.3,72.9,64,62.1,67.3,60.4,63.4,63.3,61.4,67.7,65.1,67.5,67,65.6,70.9,61.5,59.4,59.7,69,71.7,65.7,67.2,65.9,64.9,73.6,70.5,63.6,64.7,69.4,69.2,64.5,70.9,63.6,73.7,67.6,65.9,68.1,64.1,61.6,71.2,66.7,71.1,68.6,62.8,67.6,63.1,65.2,67.4,64.3,59.9,61.9,63.4,59.8,67.7,64.5,67.4,66.1,75.1,67.9,65.7,64.9,64.8,55.3,63,64.1,64.3,63.8,65.1,71.8,66.7,64.5,60.7,59.6,68.4,63.2,66,62.5,67.9,70.2,70.3,67.1,66.2,62.1,72.4,65.2,72.1,65.6,67.2,66.2,66,63.6,65,57.5,63.4,67.2,62.8,65.3,66.7,70.3,68.6,66.4,62.9,57.6,65.3,70.9,68,65.4,67.4,62.8,73.3,64.6,70,68.1,69.4,71.9,67.8,63.5,69.8,66.1,62,71.5,71.9,72.6,64.3,71.7,70.1,64.3,63.9,74.6,65.5,63.1,66.7,65.2,62,71,69.4,69.8,66.6,71,74.7,63.6,69.8,58.9,67.9,65.2,70.6,59.8,71.2,71.5,63.7,65.1,64.8,66.8,64.5,68.6,65.4,66.5,68.8,70.4,57,60.6,62.8,73.3,73.2,61.9,66.4,69.9,60.8,65.9,61.8,66.5,64,68.2,71.8,70.4,63,60.1,66.3,67.1,61.5,66.4,63.8,69.9,66.1,66.2,66.5,70.2,64,69,65.7,69.5,64.1,61.2,62.6,67.9,68.5,69.2,65.9,68.3,70.2],[136.4,215.1,173.6,117.3,123.3,96.5,178.3,191.1,158,193.9,127.1,147.9,119,204.4,143.4,124.4,140.9,164.7,139.8,110.2,134.1,193.6,180,155,188.2,187.4,139.2,147.9,178.6,111.1,119.2,184.4,100.1,207.3,159.8,120.7,102.8,195.7,130.1,156.5,113.7,119.1,142.8,179.9,166.3,135.4,118.9,173.9,117.8,192.6,122,129.5,116.9,177.1,160.1,199.5,111,177.4,187.9,177.1,185.3,223.5,128.4,184.4,122.1,216.8,173.8,197.8,181.1,136.6,105,150.1,125,172.2,143.9,132,110.7,155.9,174.9,176.6,167.1,133.5,211.4,150.6,144.7,120.7,222.5,168.4,134.3,182.8,187.2,193.4,195.2,131.1,204.5,108.6,128.1,152.7,120.3,183.5,210.6,163.1,143.9,114.4,170.5,93.2,147.8,161.2,114.6,158,144.5,184,225,116.9,183.2,131.5,217.1,123.6,145.9,170.6,133.5,165.9,134.1,111.1,201.1,156.3,161.5,145,159.8,149,222,149.7,130.6,242.5,150,191.3,164.1,135.6,139.4,114.8,191.6,194.7,170,146.9,186.1,125.8,96.2,156.8,117.6,149.6,125.4,140.7,150.2,156.5,137.6,140.3,174.7,186.1,191.3,135.2,130.5,137.1,166.5,280.5,126,128.3,166.5,124,120.8,193.8,157,190.8,110.9,185.9,163.2,167.9,119.8,122,178.8,181.1,168.8,120.9,96.5,210.4,139.3,187,146.5,141.8,162,173.8,160.7,125.3,125.9,193.7,234.9,156.9,221.2,241.6,170.4,152.6,172.8,176.6,123.5,202.1,182.2,190.9,146.6,158.5,140.5,168,182,275.2,164.1,153.5,159.5,141.7,194.1,149.6,157.5,149.9,210.8,154.5,206.5,123.4,166,160.8,167.6,141.9,186.2,155.7,148.7,132.6,356.8,164.9,187.2,169.6,178.8,202.2,158.9,186.1,169.7,147.6,110.6,146.8,146.2,164.8,115.7,191.5,198.6,143.7,126.7,123.1,231.5,134.7,159.4,141.5,144.5,150.5,173.7,215.5,146.4,133.6,240.2,216.6,89,213.6,211.3,140.4,135.7,151.7,198.3,158.7,218,172.7,161.9,130.3,123,157,131.2,108.7,202.7,131.9,164.5,179.2,154.4,120.1,189.7,160.2,145.7,186.2,144.8,147.4,120.8,134.9,164.8,205.9,172.5,130.8,146.5,173.8]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>male<\\/th>\\n      <th>height<\\/th>\\n      <th>weight<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[0,1,2]}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\nFit a model\r\n\r\nfit <- lm(dta$weight ~ dta$height)\r\nsummary(fit)\r\n\r\nCall:\r\nlm(formula = dta$weight ~ dta$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-63.95 -21.17  -5.26  16.24 201.94 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) -104.7832    31.5056  -3.326 0.000992 ***\r\ndta$height     3.9822     0.4737   8.406 1.77e-15 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 31.59 on 298 degrees of freedom\r\nMultiple R-squared:  0.1917,    Adjusted R-squared:  0.189 \r\nF-statistic: 70.66 on 1 and 298 DF,  p-value: 1.769e-15\r\n\r\nThen, we can see the fitted regression line overlay with data\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Scatter Plot between Height and Weight by Gender\",\r\n     pch = as.numeric(dta$male), col = as.factor(dta$male))\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\n\r\nModel fit diagnostics\r\nThere are four assumptions associated with a linear regression\r\nmodel:\r\nLinearity: The\r\nrelationship between X and the mean of Y is linear.\r\nHomoscedasticity:\r\nThe variance of residual is the same for any value of X.\r\nIndependence:\r\nObservations are independent of each other.\r\nNormality: For any\r\nfixed value of X, Y is normally distributed.\r\nI check the model fit by plotting:\r\nplot of residuals against predictor variable (how to check\r\nresiduals have non-linear patterns or not)\r\nnormal probability plot (how to check residuals are normally\r\ndistributed: residuals follow a straight line well or do they deviate\r\nseverely; it’s good if residuals are lined well on the straight dashed\r\nline.)\r\nplot of square root of standardized residual absolute value (how\r\nto check the assumption of equal variance (homoscedasticity): if\r\nresiduals are spread equally along the ranges of predictors; it’s good\r\nif we see a horizontal line with equally (randomly) spread\r\npoints.)\r\nAnd to look at the outlier and leverage via\r\nplot standardized residuals vs leverage\r\nResiduals are the difference between the observed score and the\r\npredicted score.\r\n\\[ e_i = Y^{obs}_i -\r\n\\hat{Y}_i\\]\r\nResiduals come in three varieties:\r\nRaw Residuals: The difference between the raw\r\nobserved score and the predicted score.\r\nStandardized Residuals: These are the raw residuals\r\ndivided by the standard error of estimate.\r\nStudentized Residuals: These are raw residuals\r\ndivided by the standard error of the residual with that case deleted.\r\nThese are sometimes called studentized deleted residuals or studentized\r\njackknifed residuals.\r\n\r\npar(mfrow=c(2,2))\r\nplot(fit)\r\n\r\n\r\nLastly, we look at the influential points\r\n\r\nn <- dim(dta)[1]\r\ncooksD <- cooks.distance(fit)\r\n#identify influential points\r\n(influential_obs <- as.numeric(names(cooksD)[(cooksD > (4/n))]))\r\n [1]   2 117 134 140 163 164 169 172 212 233 260 263\r\n#plot cooksD\r\nplot(cooksD, pch=\"*\", cex=2, main=\"Influential Obs by Cooks distance\")  \r\nabline(h = 4/n, col=\"red\")  # add cutoff line\r\ntext(x=1:length(cooksD), y=cooksD, labels=ifelse((cooksD>(4/n)),names(cooksD),\"\"), col=\"red\", pos = 4)\r\n\r\n\r\nTill now, we can say that the linear regression assumption is\r\nviolated in this case, e.g. error is not following the normal\r\ndistribution. Therefore, how about we delete the influential points and\r\nre-fit the model:\r\n\r\ndta.outliers_removed <- dta[-influential_obs, ]\r\n\r\nfit.outliers_removed <- lm(dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\nsummary(fit.outliers_removed)\r\n\r\nCall:\r\nlm(formula = dta.outliers_removed$weight ~ dta.outliers_removed$height)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n-55.16 -18.87  -3.18  16.55  83.27 \r\n\r\nCoefficients:\r\n                             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)                 -155.9116    26.8300  -5.811 1.65e-08 ***\r\ndta.outliers_removed$height    4.7112     0.4032  11.685  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 25.36 on 286 degrees of freedom\r\nMultiple R-squared:  0.3231,    Adjusted R-squared:  0.3208 \r\nF-statistic: 136.5 on 1 and 286 DF,  p-value: < 2.2e-16\r\n\r\nSomehow, we saw the model assumption is satisfied when the\r\ninfluential cases removed.\r\n\r\npar(mfrow=c(1,2))\r\nplot(fit.outliers_removed, which=c(1,2))\r\n\r\n\r\nThe regression line changes when we remove the influential\r\nobservations. Dangerous!!!\r\n\r\npar(mfrow=c(1,2))\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 3)\r\n\r\nplot(dta.outliers_removed$height, dta.outliers_removed$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"Outliers removed\")\r\nabline(fit.outliers_removed, col = \"orange\", lwd = 3)\r\n\r\n\r\nBut, the action of deleting of influential cases is often not the\r\nsolution due to produce the bias estimates.\r\nThrough this case, we have reviewed:\r\nInfluential points = Outliers &\r\nLeverage\r\nA point that makes a lot of difference in a regression case, is\r\ncalled ‘an influential point’. Usually influential points have two\r\ncharacteristics:\r\nThey are outliers, i.e. graphically\r\nthey are far from the pattern described by the other\r\npoints, that means that the relationship between x and y is\r\ndifferent for that point than for the other points.\r\nThey are in a position of high\r\nleverage, meaning that the value of the variable x is far from\r\nthe mean. Observations with very low or very high values of\r\nx are in positions of high leverage.\r\nRemind knowledge\r\nDiscrepancy: Difference between the predicted and observed\r\nvalue\r\n- Measured by Studentized Residuals\r\nLeverage: high leverage if it has “extreme” predictor x values\r\n- Measured by Hat Value\r\nInfluence: Assesses how much regression equation would change if an\r\nobservation/potential outlier was dropped from the analysis\r\n- Measured by Cook’s Distance, Difference in Fit (DFFITS), or Difference\r\nin coefficients (DFBETAS)\r\nMessage take-away\r\nRemoving influential cases is not the optimal solution.\r\nIn the real-life analysis, we can use other types of regression:\r\nRidge regression\r\nRobust regression\r\nIRLS Robust regression\r\nLowess method\r\nRegression trees\r\nand etc…\r\n\r\nBayesian approach\r\nIntroduction\r\nto a regression model in Bayesian way\r\n\\[ y = \\beta_0 + \\beta_1 x + \\epsilon\r\n\\]\r\nWith Bayesian approach distribution of \\(\\epsilon\\) does not have to be Gaussian\r\n(normal), we are going to use robust assumption.\r\nParameterization of the model for MCMC\r\n(adapted from Kruschke (2015))Bayes theorem for this model:\r\n\\[\r\np(\\beta_0, \\beta_1, \\sigma, \\gamma \\mid D) = \\frac{p(D \\mid \\beta_0,\r\n\\beta_1, \\sigma, \\gamma) \\ p(\\beta_0, \\beta_1, \\sigma, \\gamma)}{\\int\r\n\\int \\int \\int p(D \\mid \\beta_0, \\beta_1, \\sigma, \\gamma) \\ p(\\beta_0,\r\n\\beta_1, \\sigma, \\gamma) \\ d\\beta_0 \\ d\\beta_1 \\ d\\sigma \\ d\\gamma}\r\n\\]\r\nCreate the data list.\r\n\r\ny <- dta$weight\r\nx <- dta$height\r\ndataList <- list(x = x, y = y)\r\n\r\nMCMC in JAGS\r\nDescribe the model.\r\nBased on the Normal distribution (demonstrating\r\npurpose, not run)\r\n\r\nmodstring_norm = \"\r\n# Specify the Normal model for none-standardized data:\r\nmodel {\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ dnorm(mu[i], prec)\r\n        mu[i] = b[1] + b[2]*log_income[i] \r\n    }\r\n    \r\n    for (i in 1:2) {\r\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\r\n    }\r\n    \r\n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\r\n    sig2 = 1.0 / prec\r\n    sig = sqrt(sig2)\r\n} \"\r\n\r\nBased on the Student-t distribution, robust\r\nassumption\r\n\r\n modelString = \"\r\n# Standardize the data:\r\ndata {\r\n    Ntotal <- length(y)\r\n    xm <- mean(x)\r\n    ym <- mean(y)\r\n    xsd <- sd(x)\r\n    ysd <- sd(y)\r\n    for ( i in 1:length(y) ) {\r\n      zx[i] <- (x[i] - xm) / xsd\r\n      zy[i] <- (y[i] - ym) / ysd\r\n    }\r\n}\r\n# Specify the model for standardized data:\r\nmodel {\r\n    for ( i in 1:Ntotal ) {\r\n      zy[i] ~ dt( zbeta0 + zbeta1 * zx[i] , 1/zsigma^2 , nu )\r\n    }\r\n    # Priors vague on standardized scale:\r\n    zbeta0 ~ dnorm(0, 1/(10)^2 )  \r\n    zbeta1 ~ dnorm(0, 1/(10)^2 )\r\n    zsigma ~ dunif(1.0E-3, 1.0E+3 )\r\n    nu ~ dexp(1/30.0)\r\n    # Transform to original scale:\r\n    beta1 <- zbeta1 * ysd / xsd  \r\n    beta0 <- zbeta0 * ysd  + ym - zbeta1 * xm * ysd / xsd \r\n    sigma <- zsigma * ysd\r\n}\r\n\"\r\n# Write out modelString to a text file\r\nwriteLines(modelString, con=\"TEMPmodel.txt\")\r\n\r\nIn the tutorial, we just want to execute the model specified by\r\nt-student distribution.\r\nEvery arrow has a corresponding line in the descriptive\r\ndiagram.\r\nVariable names starting with “z” mean that these\r\nvariables are standardized (z-scores).\r\nThe intention of using z-scores in JAGS is to overcome a problem\r\nof correlation of the parameters (as the simulation the correlation\r\nbetween \\(\\beta_0\\) and \\(\\beta_1\\)).\r\nStrong correlation creates thin and long shape on scatter-plot of the\r\nvariables which makes Gibbs sampling very slow and inefficient.\r\nBut remember to scale back to the original measures.\r\nHMC implemented in Stan does not have this problem. This can be\r\napplied to STAN in all situation !!!\r\n\r\nparameters = c(\"beta0\" ,  \"beta1\" ,  \"sigma\", \r\n              \"zbeta0\" , \"zbeta1\" , \"zsigma\", \"nu\")\r\nadaptSteps = 500  # Number of steps to \"tune\" the samplers\r\nburnInSteps = 1000\r\nnChains = 4 \r\nthinSteps = 1\r\nnumSavedSteps=20000\r\nnIter = ceiling((numSavedSteps*thinSteps ) / nChains )\r\njagsModel = jags.model(\"TEMPmodel.txt\", data=dataList ,\r\n                      n.chains=nChains, n.adapt=adaptSteps)\r\nupdate(jagsModel, n.iter=burnInSteps)\r\ncodaSamples = coda.samples(jagsModel, variable.names=parameters, \r\n                          n.iter=nIter, thin=thinSteps)\r\n\r\nExplore the MCMC object\r\n\r\nsummary(codaSamples)\r\n\r\nIterations = 1501:6500\r\nThinning interval = 1 \r\nNumber of chains = 4 \r\nSample size per chain = 5000 \r\n\r\n1. Empirical mean and standard deviation for each variable,\r\n   plus standard error of the mean:\r\n\r\n             Mean       SD  Naive SE Time-series SE\r\nbeta0  -139.96225 27.61760 0.1952859      0.2180717\r\nbeta1     4.46120  0.41330 0.0029225      0.0032290\r\nnu        5.37901  1.62796 0.0115114      0.0236101\r\nsigma    23.97943  1.65132 0.0116766      0.0204363\r\nzbeta0   -0.09632  0.04806 0.0003398      0.0004283\r\nzbeta1    0.49046  0.04544 0.0003213      0.0003550\r\nzsigma    0.68372  0.04708 0.0003329      0.0005827\r\n\r\n2. Quantiles for each variable:\r\n\r\n            2.5%       25%        50%       75%      97.5%\r\nbeta0  -193.3580 -158.7464 -140.16434 -121.3445 -8.624e+01\r\nbeta1     3.6527    4.1830    4.46491    4.7407  5.257e+00\r\nnu        3.1509    4.2578    5.05987    6.1485  9.410e+00\r\nsigma    20.8323   22.8476   23.94254   25.0519  2.728e+01\r\nzbeta0   -0.1888   -0.1282   -0.09682   -0.0644 -4.115e-04\r\nzbeta1    0.4016    0.4599    0.49087    0.5212  5.779e-01\r\nzsigma    0.5940    0.6514    0.68267    0.7143  7.779e-01\r\nplot(codaSamples, trace=TRUE, density=FALSE) # note: many graphs\r\n\r\nautocorr.plot(codaSamples, ask=F)\r\n\r\neffectiveSize(codaSamples)\r\n    beta0     beta1        nu     sigma    zbeta0    zbeta1    zsigma \r\n16054.484 16401.066  4768.900  6535.965 12892.980 16401.066  6535.965 \r\n#gelman.diag(codaSamples)\r\ngelman.plot(codaSamples)   # lines may return error ==> Most likely reason: collinearity of parameters \r\n\r\n(HDIofChains <- lapply(codaSamples, function(z) cbind(Mu = hdi(codaSamples[[1]][,1]), Sd = hdi(codaSamples[[1]][,2]))))\r\n[[1]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[2]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[3]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\n[[4]]\r\n            var1     var1\r\nlower -192.46444 3.660595\r\nupper  -85.49078 5.257061\r\n\r\nLook at strong correlation between beta0 and beta1 which slows Gibbs\r\nsampling down.\r\n\r\nhead(as.matrix(codaSamples[[1]]))\r\n         beta0    beta1       nu    sigma      zbeta0    zbeta1\r\n[1,] -168.7337 4.890447 5.623225 23.90661 -0.10407253 0.5376553\r\n[2,] -158.6082 4.744414 6.371647 24.56065 -0.09181883 0.5216004\r\n[3,] -166.9662 4.873701 6.346842 22.92533 -0.08537616 0.5358143\r\n[4,] -161.6680 4.795882 3.209357 23.00506 -0.08162923 0.5272588\r\n[5,] -160.5706 4.801212 3.153090 22.15810 -0.04024801 0.5278448\r\n[6,] -155.9331 4.684888 4.394775 21.44653 -0.12823063 0.5150561\r\n        zsigma\r\n[1,] 0.6816415\r\n[2,] 0.7002899\r\n[3,] 0.6536626\r\n[4,] 0.6559359\r\n[5,] 0.6317868\r\n[6,] 0.6114980\r\npairs(as.matrix(codaSamples[[1]])[,1:4])\r\n\r\n\r\nMCMC in Stan\r\nDescribe the model in Stan\r\nIn order to give a vague priors to slope and intercept consider the\r\nfollowing arguments:\r\nThe largest possible value of slope is\r\n\\[ \\frac{\\sigma_y}{\\sigma_x} \\]\r\nwhen variables \\(x\\) and \\(y\\) are perfectly correlated.\r\nThen standard deviation of the slope parameter \\(\\beta_1\\) should be large enough to make\r\nthe maximum value easily achievable.\r\nSize of intercept is defined by value of\r\n\\[ E[X] \\frac{\\sigma_y}{\\sigma_x}\r\n\\]\r\nSo, the prior should have enough width to include this value.\r\n\r\nmodelString = \"\r\ndata {\r\n    int<lower=1> Ntotal;\r\n    real x[Ntotal];\r\n    real y[Ntotal];\r\n    real meanY;\r\n    real sdY;\r\n    real meanX;\r\n    real sdX;\r\n}\r\ntransformed data {\r\n    real unifLo;\r\n    real unifHi;\r\n    real expLambda;\r\n    real beta0sigma;\r\n    real beta1sigma;\r\n    unifLo = sdY/1000;\r\n    unifHi = sdY*1000;\r\n    expLambda = 1/30.0;\r\n    beta1sigma = 10*fabs(sdY/sdX);\r\n    beta0sigma = 10*(sdY^2+sdX^2)    / 10*fabs(meanX*sdY/sdX);\r\n}\r\nparameters {\r\n    real beta0;\r\n    real beta1;\r\n    real<lower=0> nu; \r\n    real<lower=0> sigma; \r\n}\r\nmodel {\r\n    sigma ~ uniform(unifLo, unifHi); \r\n    nu ~ exponential(expLambda);\r\n    beta0 ~ normal(0, beta0sigma);\r\n    beta1 ~ normal(0, beta1sigma);\r\n    for (i in 1:Ntotal) {\r\n        y[i] ~ student_t(nu, beta0 + beta1 * x[i], sigma);\r\n    }\r\n}\r\n\"\r\n\r\n\r\nstanDsoRobustReg <- stan_model(model_code=modelString) \r\n\r\n\r\ndat<-list(Ntotal=length(dta$weight), \r\n          y=dta$weight, \r\n          meanY=mean(dta$weight),\r\n          sdY=sd(dta$weight),\r\n          x=dta$height,\r\n          meanX=mean(dta$height),\r\n          sdX=sd(dta$height))\r\n\r\n\r\nfitSimRegStan <- sampling(stanDsoRobustReg, \r\n             data=dat, \r\n             pars=c('beta0', 'beta1', 'nu', 'sigma'),\r\n             iter=5000, chains = 4, cores = 4)\r\n\r\nSave the fitted object.\r\nExplore the MCMC object\r\n\r\nprint(fitSimRegStan)\r\nInference for Stan model: 99bd7b261ff9240bf0c6d7b1b21831d6.\r\n4 chains, each with iter=5000; warmup=2500; thin=1; \r\npost-warmup draws per chain=2500, total post-warmup draws=10000.\r\n\r\n          mean se_mean    sd     2.5%      25%      50%      75%\r\nbeta0  -139.35    0.50 27.89  -194.81  -157.61  -139.51  -120.74\r\nbeta1     4.45    0.01  0.42     3.64     4.17     4.45     4.72\r\nnu        5.37    0.03  1.61     3.15     4.25     5.07     6.16\r\nsigma    23.98    0.03  1.66    20.81    22.84    23.96    25.07\r\nlp__  -1265.00    0.02  1.43 -1268.55 -1265.74 -1264.68 -1263.94\r\n         97.5% n_eff Rhat\r\nbeta0   -85.12  3072    1\r\nbeta1     5.28  3117    1\r\nnu        9.39  3420    1\r\nsigma    27.31  3579    1\r\nlp__  -1263.20  3332    1\r\n\r\nSamples were drawn using NUTS(diag_e) at Thu Apr 22 15:10:38 2021.\r\nFor each parameter, n_eff is a crude measure of effective sample size,\r\nand Rhat is the potential scale reduction factor on split chains (at \r\nconvergence, Rhat=1).\r\nplot(fitSimRegStan)\r\n\r\nrstan::traceplot(fitSimRegStan, ncol=1, inc_warmup=F)\r\n\r\npairs(fitSimRegStan, pars=c('nu','beta0','beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','beta1'))\r\n\r\nstan_scat(fitSimRegStan, c('beta1','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('beta0','sigma'))\r\n\r\nstan_scat(fitSimRegStan, c('nu','sigma'))\r\n\r\nstan_dens(fitSimRegStan)\r\n\r\nstan_ac(fitSimRegStan, separate_chains = T)\r\n\r\nstan_diag(fitSimRegStan,information = \"sample\",chain=0)\r\n\r\nstan_diag(fitSimRegStan,information = \"stepsize\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"treedepth\",chain = 0)\r\n\r\nstan_diag(fitSimRegStan,information = \"divergence\",chain = 0)\r\n\r\n\r\nWork with shinystan object.\r\n\r\nlaunch_shinystan(fitSimRegStan)\r\n\r\nUsing fitted\r\nregression model for prediction\r\nRecall that the data in this example contains predictor height and\r\noutput weight for a group of people from Ht-Wt.csv (data\r\nabove).\r\nPlot all heights observed in the sample and check the summary of the\r\nvariable.\r\n\r\nplot(1:length(dat$x),dat$x)\r\n\r\nsummary(dat$x)\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  54.60   64.00   66.20   66.39   69.20   76.00 \r\n\r\nCan we predict weight of a person who is 50 or 80 inches tall?\r\nTo do this we can go through all pairs of simulated parameters (\\(\\beta_0\\), \\(\\beta_1\\)) and use them to simulate \\(y(50)\\) and \\(y(80)\\).\r\nThis gives distribution of predicted values.\r\n\r\nsummary(fitSimRegStan)\r\n$summary\r\n              mean     se_mean         sd         2.5%          25%\r\nbeta0  -139.354196 0.503202352 27.8915373  -194.808900  -157.605829\r\nbeta1     4.451593 0.007474065  0.4173043     3.636611     4.172879\r\nnu        5.366643 0.027556563  1.6115181     3.151177     4.246276\r\nsigma    23.982791 0.027670259  1.6554405    20.814300    22.838675\r\nlp__  -1265.002519 0.024734258  1.4276951 -1268.552672 -1265.738311\r\n               50%          75%        97.5%    n_eff     Rhat\r\nbeta0  -139.512830  -120.737878   -85.121838 3072.271 1.000770\r\nbeta1     4.454489     4.723609     5.282330 3117.396 1.000699\r\nnu        5.069790     6.157253     9.391004 3419.954 1.000994\r\nsigma    23.957637    25.072876    27.313013 3579.322 1.000550\r\nlp__  -1264.682923 -1263.938424 -1263.195323 3331.756 1.000423\r\n\r\n$c_summary\r\n, , chains = chain:1\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.496987 27.9128445  -195.452362  -159.028578\r\n    beta1     4.468322  0.4176474     3.643375     4.180768\r\n    nu        5.333266  1.6129391     3.146164     4.239934\r\n    sigma    23.996371  1.6756857    20.745408    22.833622\r\n    lp__  -1264.973734  1.3931068 -1268.262683 -1265.705478\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.690732  -121.306242   -85.591653\r\n    beta1     4.473340     4.745248     5.301112\r\n    nu        5.036853     6.074138     9.402810\r\n    sigma    23.966000    25.101448    27.242245\r\n    lp__  -1264.658401 -1263.936454 -1263.201543\r\n\r\n, , chains = chain:2\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -140.119919 27.7217798  -195.188356  -158.408294\r\n    beta1     4.462118  0.4146886     3.668582     4.177839\r\n    nu        5.396927  1.5988486     3.155521     4.277927\r\n    sigma    23.990640  1.6168350    21.056831    22.846187\r\n    lp__  -1264.946317  1.4072435 -1268.606024 -1265.657309\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -140.000666  -121.163812   -86.601618\r\n    beta1     4.462406     4.731267     5.288368\r\n    nu        5.129432     6.181671     9.348566\r\n    sigma    23.918413    25.056950    27.345459\r\n    lp__  -1264.644540 -1263.891813 -1263.183147\r\n\r\n, , chains = chain:3\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -137.758043 28.0473599  -193.976570  -156.709899\r\n    beta1     4.428479  0.4196264     3.620966     4.142890\r\n    nu        5.273822  1.5474425     3.130205     4.160287\r\n    sigma    23.905531  1.6471567    20.672028    22.808588\r\n    lp__  -1265.023664  1.3965268 -1268.555138 -1265.759590\r\n         stats\r\nparameter          50%         75%        97.5%\r\n    beta0  -137.963462  -118.57774   -83.786093\r\n    beta1     4.430414     4.70807     5.278970\r\n    nu        4.999793     6.07770     9.087928\r\n    sigma    23.906472    24.98220    27.219559\r\n    lp__  -1264.712841 -1263.99760 -1263.196266\r\n\r\n, , chains = chain:4\r\n\r\n         stats\r\nparameter         mean         sd         2.5%          25%\r\n    beta0  -139.041834 27.8184750  -193.491107  -156.858089\r\n    beta1     4.447452  0.4163603     3.625069     4.186441\r\n    nu        5.462557  1.6789159     3.209953     4.299050\r\n    sigma    24.038623  1.6794892    20.805817    22.868034\r\n    lp__  -1265.066361  1.5085815 -1268.785015 -1265.858500\r\n         stats\r\nparameter          50%          75%        97.5%\r\n    beta0  -139.288564  -121.639438   -83.861266\r\n    beta1     4.449144     4.707789     5.268918\r\n    nu        5.106526     6.256394     9.576797\r\n    sigma    24.014382    25.132032    27.449061\r\n    lp__  -1264.738186 -1263.933356 -1263.202000\r\nregParam<-cbind(Beta0=rstan::extract(fitSimRegStan,pars=\"beta0\")$'beta0',\r\n                Beta1=rstan::extract(fitSimRegStan,pars=\"beta1\")$'beta1')\r\nhead(regParam)\r\n         Beta0    Beta1\r\n[1,] -174.7224 4.962250\r\n[2,] -118.6827 4.113129\r\n[3,] -152.0820 4.701561\r\n[4,] -147.0165 4.564773\r\n[5,] -159.9549 4.751269\r\n[6,] -126.6841 4.251516\r\npredX50<-apply(regParam,1,function(z) z%*%c(1,50))\r\npredX80<-apply(regParam,1,function(z) z%*%c(1,80))\r\n\r\nPlot both distributions, look at their summaries and HDIs.\r\n\r\nsuppressWarnings(library(HDInterval))\r\nden<-density(predX50)\r\nplot(density(predX80),xlim=c(60,240))\r\nlines(den$x,den$y)\r\n\r\nsummary(cbind(predX50,predX80))\r\n    predX50          predX80     \r\n Min.   : 55.42   Min.   :195.2  \r\n 1st Qu.: 78.48   1st Qu.:212.9  \r\n Median : 83.19   Median :216.8  \r\n Mean   : 83.23   Mean   :216.8  \r\n 3rd Qu.: 87.97   3rd Qu.:220.6  \r\n Max.   :111.77   Max.   :240.3  \r\nrbind(predX50=hdi(predX50),predX80=hdi(predX80))\r\n            lower     upper\r\npredX50  69.09173  97.20656\r\npredX80 205.32725 228.15029\r\n\r\nBoth JAGS and Stan produced the identical\r\nresults.\r\nCompare the fit between FA\r\nand BA\r\n\r\nplot(dta$height, dta$weight, ylab = \"Weight (lbs)\", xlab = \"Height (inches)\",\r\n     main = \"With Outliers\")\r\nabline(fit, col = \"orange\", lwd = 2)\r\nabline(a=-139.96225, b= 4.46120, col = \"blue\", lwd = 1)\r\nabline(fit.outliers_removed, col = \"red\", lty=\"dashed\", lwd =1)\r\n\r\n\r\n\r\nWith influential\r\nWithout influential\r\nBayesian approach w/ robust\r\nassumption\r\n\r\norange, solid\r\nred,\r\ndashed\r\nblue,\r\nsolid\r\nIntercept\r\n-104.78\r\n-155.91\r\n-139.96\r\nSlope\r\n3.98\r\n4.71\r\n4.46\r\n\r\nGreat! From the comparison, we can see that using Bayesian Methods to\r\nfit simple linear regression can be robust when the traditional\r\nregression has the influential points.\r\n\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., &\r\nRubin, D. (2013). Bayesian Data Analysis, Third Edition, 3rd Edition\r\n(3rd edition ed.): CRC Press.\r\nKutner, M. H. (2005). Applied linear statistical models (5th\r\ned. ed.). Boston: McGraw-Hill Irwin.\r\n\r\n\r\nKruschke, John K. 2015. Doing Bayesian Data Analysis : A Tutorial\r\nwith r, JAGS, and Stan. Book. 2E [edition]. Amsterdam: Academic\r\nPress is an imprint of Elsevier.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-03T10:42:14-06:00",
    "input_file": "Simple-Linear-Regression-in-Bayes.knit.md"
  },
  {
    "path": "posts/2021-04-17-machine learning-discriminant-analysis/",
    "title": "Discriminant Analysis -- A Classification by Maximizing Class Separation",
    "description": "An Gentle Introduction of Discriminant Analysis & Its Applicant",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nLinear Discriminant Analysis\r\nWhy Discriminant Analysis\r\nQuadratic Discriminant Analysis\r\nLDA vs QDA\r\nBuilding our first linear and quadratic discriminant models\r\nCreating the task and learner, and training the LDA model\r\nCreating the task and learner, and training the QDA model\r\n\r\nReferences\r\n\r\nLinear Discriminant Analysis\r\nApproach for multiclass classification.\r\nA discriminant is a function that takes an input vector x and assigns to one of the multiple classes.\r\nModel the distribution of the predictors \\(X\\) in each response class\r\nUse Bayes theorem to flip these around into estimates for\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nWhen the distributions are assumed to be normal, the LDA model is very similar in form to logistic regression.\r\nWhy Discriminant Analysis\r\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\r\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\r\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\r\nQuadratic Discriminant Analysis\r\n\\[ Pr(y =𝑘\\mid 𝑋=𝑥) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\]\r\nwhere\r\n\\(\\pi_k\\): overall prior probability that a randomly chosen observation comes from the \\(𝑘\\)-th class\\(f_k(x)\\): the density function of \\(𝑥\\)\r\nLDA = \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\sum\\) in each class.\r\nQDA = With Gaussians but different \\(\\sum_k\\) in each class, we get quadratic discriminant analysis.\r\nNOTE = By proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\r\nLDA vs QDA\r\nThe strengths of the LDA and QDA algorithms are:\r\nThey can reduce a high-dimensional feature space into a much more manageable number\r\nCan be used for classification or as a preprocessing (dimension reduction) technique to other classification algorithms that may perform better on the dataset\r\nQDA can learn curved decision boundaries between classes (this isn’t the case for LDA)\r\nThe weaknesses of the LDA and QDA algorithms are:\r\nThey can only handle continuous predictors (although recoding a categorical variable as numeric may help in some cases)\r\nThey assume the data are normally distributed across the predictors. If the data are not, performance will suffer\r\nLDA can only learn linear decision boundaries between classes (this isn’t the case for QDA)\r\nLDA assumes equal covariances of the classes and performance will suffer if this isn’t the case (this isn’t the case for QDA)\r\nQDA is more flexbile than LDA, and so can be more prone to overfitting\r\nWhen we should apply LDA vs QDA\r\nLDA needs lot less parameters than QDA.\r\nLDA is a much less flexible classifier than QDA \\(\\Rightarrow\\) substantially low variance.\r\nIf LDA’s assumption of common covariance matrix is poor, then LDA has high bias.\r\nLDA better bet if training set is small so reducing variance is important.\r\nQDA better bet if training set is large so variance of classifier not a major concern.\r\nBuilding our first linear and quadratic discriminant models\r\nWe have a tibble containing 178 cases and 14 variables of measurements made on various wine bottles: data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\r\nThe analysis determined the quantities of 13 constituents (Alcohol, Malic acid, Ash, Alcalinit of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline)found in each of the three types of wines.\r\n\r\n\r\n#install.packages(\"mlr\")\r\nlibrary(mlr)\r\nlibrary(tidyverse)\r\n#install.packages(\"HDclassif\")\r\ndata(wine, package = \"HDclassif\")\r\nwineTib <- as_tibble(wine)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   class    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10\r\n   <int> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n 1     1  14.2  1.71  2.43  15.6   127  2.8   3.06 0.28   2.29  5.64\r\n 2     1  13.2  1.78  2.14  11.2   100  2.65  2.76 0.26   1.28  4.38\r\n 3     1  13.2  2.36  2.67  18.6   101  2.8   3.24 0.3    2.81  5.68\r\n 4     1  14.4  1.95  2.5   16.8   113  3.85  3.49 0.24   2.18  7.8 \r\n 5     1  13.2  2.59  2.87  21     118  2.8   2.69 0.39   1.82  4.32\r\n 6     1  14.2  1.76  2.45  15.2   112  3.27  3.39 0.34   1.97  6.75\r\n 7     1  14.4  1.87  2.45  14.6    96  2.5   2.52 0.3    1.98  5.25\r\n 8     1  14.1  2.15  2.61  17.6   121  2.6   2.51 0.31   1.25  5.05\r\n 9     1  14.8  1.64  2.17  14      97  2.8   2.98 0.290  1.98  5.2 \r\n10     1  13.9  1.35  2.27  16      98  2.98  3.15 0.22   1.85  7.22\r\n# ... with 168 more rows, and 3 more variables: V11 <dbl>, V12 <dbl>,\r\n#   V13 <int>\r\n\r\n\r\n\r\nnames(wineTib) <- c(\"Class\", \"Alco\", \"Malic\", \"Ash\", \"Alk\", \"Mag\",\r\n                    \"Phe\", \"Flav\", \"Non_flav\", \"Proan\", \"Col\", \"Hue\",\r\n                    \"OD\", \"Prol\")\r\nwineTib$Class <- as.factor(wineTib$Class)\r\nwineTib\r\n\r\n\r\n# A tibble: 178 x 14\r\n   Class  Alco Malic   Ash   Alk   Mag   Phe  Flav Non_flav Proan\r\n   <fct> <dbl> <dbl> <dbl> <dbl> <int> <dbl> <dbl>    <dbl> <dbl>\r\n 1 1      14.2  1.71  2.43  15.6   127  2.8   3.06    0.28   2.29\r\n 2 1      13.2  1.78  2.14  11.2   100  2.65  2.76    0.26   1.28\r\n 3 1      13.2  2.36  2.67  18.6   101  2.8   3.24    0.3    2.81\r\n 4 1      14.4  1.95  2.5   16.8   113  3.85  3.49    0.24   2.18\r\n 5 1      13.2  2.59  2.87  21     118  2.8   2.69    0.39   1.82\r\n 6 1      14.2  1.76  2.45  15.2   112  3.27  3.39    0.34   1.97\r\n 7 1      14.4  1.87  2.45  14.6    96  2.5   2.52    0.3    1.98\r\n 8 1      14.1  2.15  2.61  17.6   121  2.6   2.51    0.31   1.25\r\n 9 1      14.8  1.64  2.17  14      97  2.8   2.98    0.290  1.98\r\n10 1      13.9  1.35  2.27  16      98  2.98  3.15    0.22   1.85\r\n# ... with 168 more rows, and 4 more variables: Col <dbl>, Hue <dbl>,\r\n#   OD <dbl>, Prol <int>\r\n\r\nWe got:\r\n- 13 continuous measurements made on 178 bottles of wine, where each measurement is the amount of a different compound/element in the wine.\r\n- Class: vineyard the bottle comes from.\r\n\r\n\r\nwineUntidy <- gather(wineTib, \"Variable\", \"Value\", -Class)\r\nggplot(wineUntidy, aes(Class, Value)) +\r\n  facet_wrap(~ Variable, scales = \"free_y\") +\r\n  geom_boxplot() +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nBox and whisker plots of each continuous variable in the data against vineyard number. For the box and whiskers: the thick horizontal line represents the median, the box represents the interquartile range (IQR), the whiskers represent the Tukey range (1.5 times the IQR above and below the quartiles), and the dots represent data outside of the Tukey range.   \r\nCreating the task and learner, and training the LDA model\r\n\r\n\r\nwineTask <- makeClassifTask(data = wineTib, target = \"Class\")\r\nlda <- makeLearner(\"classif.lda\")\r\nldaModel <- train(lda, wineTask)\r\n\r\n\r\n\r\nExtracting discriminant function values for each case\r\n\r\n\r\nldaModelData <- getLearnerModel(ldaModel)\r\nldaPreds <- predict(ldaModelData)$x\r\nhead(ldaPreds)\r\n\r\n\r\n        LD1       LD2\r\n1 -4.700244 1.9791383\r\n2 -4.301958 1.1704129\r\n3 -3.420720 1.4291014\r\n4 -4.205754 4.0028715\r\n5 -1.509982 0.4512239\r\n6 -4.518689 3.2131376\r\n\r\nPlotting the discriminant function values against each other\r\n\r\n\r\nwineTib %>%\r\n  mutate(LD1 = ldaPreds[, 1],\r\n         LD2 = ldaPreds[, 2]) %>%\r\n  ggplot(aes(LD1, LD2, col = Class)) + \r\n    geom_point() +\r\n    stat_ellipse() +\r\n    theme_bw()\r\n\r\n\r\n\r\n\r\nCreating the task and learner, and training the QDA model\r\n\r\n\r\nqda <- makeLearner(\"classif.qda\")\r\nqdaModel <- train(qda, wineTask)\r\n\r\n\r\n\r\nCross-validating the LDA and QDA models\r\n\r\n\r\nkFold <- makeResampleDesc(method = \"RepCV\", folds = 10, reps = 50,\r\nstratify = TRUE)\r\n\r\nldaCV <- resample(learner = lda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nqdaCV <- resample(learner = qda, task = wineTask, resampling = kFold,\r\nmeasures = list(mmce, acc))\r\n\r\nldaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n    0.01133544     0.98866456 \r\n\r\nqdaCV$aggr\r\n\r\n\r\nmmce.test.mean  acc.test.mean \r\n   0.008314886    0.991685114 \r\n\r\nOur LDA model correctly classified 98.8% of wine bottles, on average! There isn’t much room for improvement here, but\r\nour QDA model managed to correctly classify 99.2% of cases!\r\nCalculating confusion matrices\r\n\r\n\r\ncalculateConfusionMatrix(ldaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.999/1e+00 0.001/9e-04 0.000/0e+00 0.001      \r\n  2      0.010/1e-02 0.977/1e+00 0.014/2e-02 0.023      \r\n  3      0.000/0e+00 0.007/5e-03 0.993/1e+00 0.007      \r\n  -err.-       0.011       0.005       0.020 0.01       \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2947    3    0      3\r\n  2        34 3468   48     82\r\n  3         0   16 2384     16\r\n  -err.-   34   19   48    101\r\n\r\ncalculateConfusionMatrix(qdaCV$pred, relative = TRUE)\r\n\r\n\r\nRelative confusion matrix (normalized by row/column):\r\n        predicted\r\ntrue     1           2           3           -err.-     \r\n  1      0.994/0.984 0.006/0.005 0.000/0.000 0.006      \r\n  2      0.014/0.016 0.986/0.993 0.000/0.000 0.014      \r\n  3      0.000/0.000 0.004/0.003 0.996/1.000 0.004      \r\n  -err.-       0.016       0.007       0.000 0.008      \r\n\r\n\r\nAbsolute confusion matrix:\r\n        predicted\r\ntrue        1    2    3 -err.-\r\n  1      2933   17    0     17\r\n  2        48 3502    0     48\r\n  3         0    9 2391      9\r\n  -err.-   48   26    0     74\r\n\r\nPredicting which vineyard the poisoned wine came from\r\n\r\n\r\npoisoned <- tibble(Alco = 13, Malic = 2, Ash = 2.2, Alk = 19, Mag = 100,\r\n                   Phe = 2.3, Flav = 2.5, Non_flav = 0.35, Proan = 1.7,\r\n                   Col = 4, Hue = 1.1, OD = 3, Prol = 750)\r\npredict(qdaModel, newdata = poisoned)\r\n\r\n\r\nPrediction: 1 observations\r\npredict.type: response\r\nthreshold: \r\ntime: 0.00\r\n  response\r\n1        1\r\n\r\nThe model predicts that the poisoned bottle came from vineyard 1.\r\nHere’s we ends the analytic example.\r\nReferences\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-18T10:46:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-14-machine-learning-&-predictive-analytics/",
    "title": "Machine Learning & Predictive Analytics",
    "description": "An Overview of Machine Learning & Predictive Analytics",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [
      "Biostatistics",
      "Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Machine Learning\r\nLearning Types\r\nSupervised\r\nUnsupervised Learning\r\nSemi-supervised Learning\r\n\r\nActive Learning\r\nReinforcement Learning\r\nTransfer Learning\r\n\r\nUniversal Workflow of ML\r\nML Terminologies\r\nData Assumptions\r\nOverfitting and Underfitting\r\nParametric & Nonparametric Models\r\nRegression Analysis\r\n\r\nUse R or Python for machine learning?\r\nMachine Learning with mlr Package in R\r\n\r\nReferences\r\n\r\nWhat is Machine Learning\r\nMeaningful data transformations from input to output data.\r\nTransformations: represent or encode the data (RGB or HSV for color pixel).\r\nLearning is automatic search for better data representations.\r\nSearch through a predefined space of possibilities using guidance from feedback signal.\r\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E”\r\n\r\n-Tom Mitchell, Machine Learning, McGraw Hill, 1997\r\n\r\n\r\nExperience E, Task T, Performance P\r\n1. Chess: T: playing chess, P: % of games won, E: playing practice games against itself.\r\n2. Driving: T: driving a vehicle, P: avgdistance before error, E: sequence of images and steering commands recoded during manual driving.\r\n3. Handwriting Recognition: T: recognizing and classifying handwritten words in images, P: % of correctly classified words, E: DB of handwritten words with given classifications.\r\n\r\nLearning Types\r\nSupervised\r\nUnsupervised\r\nSemi-supervised\r\nReinforcement\r\nTransfer\r\nActive\r\nSupervised\r\nThe majority of practical machine learning uses supervised learning.\r\nSupervised learning is where you have input variables (\\(x\\)) and an output variable (\\(y\\)) and you use an algorithm to learn the mapping function from the input to the output.\\[ y = f(x) \\]\r\nThe goal is to approximate the mapping function so well that when you have new input data \\(x\\) that you can predict the output variables \\(y\\) for that data.\r\nIt is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\r\nWe know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance.\r\nSupervised Learning Examples:\r\nLinear Regression\r\nLogistic Regression\r\nK-NN (k-Nearest Neighbors)\r\nSupport Vector Machines (SVMs)\r\nDecision Tress and Random Forests\r\nNeural Networks\r\nUnsupervised Learning\r\nUnsupervised learning is where you only have input data \\(x\\) and no corresponding output variables.\r\nThe goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\r\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data.\r\nUnsupervised Learning problems can be further grouped into Clustering and Association Problems.\r\nClustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\r\nAssociation: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy \\(A\\) also tend to buy \\(B\\).\r\nUnsupervised Learning Examples:\r\nClustering\r\nK-Means\r\nHierarchical Cluster Analysis (HCA)\r\nExpectation Maximization\r\n\r\nVisualization and Dimensionality Reduction\r\nPrincipal Component Analysis (PCA)\r\nKernel PCA\r\nt-distributed Stochastic Neighbor Embedding (t-SNE)\r\n\r\nAssociation Rule\r\nApriori\r\n\r\nNeural Networks\r\nAutoencoders\r\nBoltzmann machines\r\n\r\nSemi-supervised Learning\r\nSemi-supervised learning is halfway between supervised and unsupervised learning.\r\nTraditional classification methods use labeled data to build classifiers.\r\nThe labeled training sets used as input in Supervised learning is very certain and properly defined.\r\nHowever, they are limited, expensive and takes a lot of time to generate them.\r\nOn the other hand, unlabeled data is cheap and is readily available in large volumes.\r\nHence, semi-supervised learning is learning from a combination of both labeled and unlabeled data\r\nWhere we make use of a combination of small amount of labeled data and large amount of unlabeled data to increase the accuracy of our classifiers.\r\nActive Learning\r\nActive learning (sometimes called “query learning” or “optimal experimental design” in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence.\r\nThe key hypothesis is that if the learning algorithm is allowed to choose the data from which it learns—to be “curious,” if you will—it will perform better with less training.\r\nActive learning is a special case of semi-supervised learning.\r\nReinforcement Learning\r\nReinforcement Learning is learning what to do and how to map situations to actions.\r\nThe end result is to maximize the numerical reward signal.\r\nThe learner is not told which action to take, but instead must discover which action will yield the maximum reward\r\nTransfer Learning\r\nA machine learning technique where a model trained on one task is re-purposed on a second related task.\r\nAn optimization that allows rapid progress or improved performance when modeling the second task.\r\nUniversal Workflow of ML\r\nDefine the problem\r\nAssemble dataset\r\nChoose a metric to quantify project outcome\r\nDecide on how to calculate the metric\r\nPrepare dataset\r\nDefine standard baseline\r\nDevelop model that beats baseline\r\nIdeal model is at the border of overfit and underfit–cross the border to know where it is so overfit model\r\nRegularize model and tune hyperparameters\r\nML Terminologies\r\nDataset\r\nTraining –Learn the parameters\r\nValidation –select hyperparameters\r\nTest –test the model aka generalization error\r\n\r\nBatch –set of examples used in one iteration of model training.\r\nMini-batch –A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference.\r\nEpoch –A full training pass over the entire data set such that each example has been seen once.\r\nIteration –A single update of a model’s weights during training.\r\nData Assumptions\r\nTraining and test data are from the same probability distribution.\r\nTraining and test data are iid.\r\nOverfitting and Underfitting\r\nOverfitting –model fits very well to the training data, aka detects patterns in the noise also\r\nDetect:\r\nLow training error, high generalization error.\r\n\r\nRemedies:\r\nReduce model capacity by removing features and/or parameters.\r\nGet more training data.\r\nImprove training data quality by reducing noise.\r\n\r\n\r\nUnderfitting–model too simple to detect patterns in the data\r\nDetect\r\nHigh training error.\r\n\r\nRemedies:\r\nIncrease model capacity by adding more parameters and/or features.\r\nReduce model constraints.\r\n\r\n\r\nParametric & Nonparametric Models\r\n\\[ 𝑦= 𝑓(𝑥) \\]\r\nEstimate the unknown function 𝑓 as \\(\\hat{f}\\)\r\nParametric Models:\r\nAssume the functional form or shape of 𝑓\r\nApply methodology to train model\r\nAdvantage –simple estimation\r\nDisadvantage – \\(\\hat{f}\\)may be far from true 𝑓\r\n\r\nNonparametric Models:\r\nNo assumption on the functional form or shape of 𝑓\r\nEstimate to fit as close as possible to the data\r\nAdvantage –can accurately fit a wide range of possible shapes of 𝑓\r\nDisadvantage –need large datasets (since there is no fixed # of params to estimate)\r\n\r\nRegression Analysis\r\nOLS\r\nMSE\r\nComputational Complexity of matrix inversion\r\nComplete training set\r\n\r\nBatch Gradient Descent\r\nCost function (MSE)\r\nLearning rate hyperparameter\r\nPartial derivative\r\nComplete training set\r\n\r\nStochastic Gradient Descent\r\nMini-batch Gradient Descent\r\nLinear Regression with OLS\r\n\\[ 𝑦= \\theta^T𝑋\\]\r\nThe cost function minimization is a closed-form solution called the Normal Equation: \\[ \\hat{\\theta} = (X^T . X)^{-1} X^T.y  \\]\r\nAdvantage –equation is linear with size of training set so it can handle large training sets efficiently.\r\nDisadvantage –\r\ncomputational complexity of inverting a matrix that increases with size of training set.\r\ndifficult to do online learning with new data arriving regularly (need to recalculate estimates), i.e. no iterative parameter updates.\r\n\r\nUse R or Python for machine learning?\r\nThere is something of a rivalry between the two most commonly used data science languages: R and Python. Of course, there are no machine learning tasks which are only possible to apply in one language or the other.\r\nR:\r\nR is geared specifically for mathematical and statistical applications, i.e. R can focus purely on data, but may feel restricted if they ever need to build applications based on their models.\r\nCurrently, there are modern tools in R designed specifically to make data science tasks simple and human-readable, such as those from the tidyverse.\r\nPreviously, ML algorithms in R were scattered across multiple packages, written by different authors. But R has now followed suit, with the caret and mlr packages (which stands for machine learning in R). While quite similar in purpose and functionality to caret, mlr package provides an interface for a large number of machine learning algorithms, and allows you to perform extremely complicated machine learning tasks with very little coding.\r\nPython:\r\nFirst of all, some of the more cutting-edge deep learning approaches are easier to apply in Python (they tend to be written in Python first and implemented in R later).\r\nPython, while very good for data science, is a more general purpose programming language.\r\nProponents of python could use this as en example of why it was better suited for machine learning, as it has the well known scikit-learn package which has a plethora of machine learning algorithms built into it.\r\nGoogle Trends demonstrates the search interest relative to the highest point on the chart for the given region and over the past 5 years.\r\n\r\n trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05z1_\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0212jm\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0jgqg\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/09gbxjr\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"},{\"keyword\":\"/m/0j3djl7\",\"geo\":\"\",\"time\":\"2016-04-17 2021-04-17\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=today%205-y&q=%2Fm%2F05z1_,%2Fm%2F0212jm,%2Fm%2F0jgqg,%2Fm%2F09gbxjr,%2Fm%2F0j3djl7\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Machine Learning with mlr Package in R\r\nR users got mlr package similar to Scikit-Learn from Python. The package synthesizes all the ML functions from other packages in which we can perform most of ML tasks. mlr package has several algorithms in its bouquet. These algorithms have been categorized into regression, classification, clustering, survival, multiclassification and cost sensitive classification:\r\n\r\n\r\nlibrary(mlr)\r\nlistLearners(\"classif\")[c(\"class\",\"package\")]\r\n\r\n\r\n                            class                   package\r\n1                     classif.ada                 ada,rpart\r\n2              classif.adaboostm1                     RWeka\r\n3             classif.bartMachine               bartMachine\r\n4                classif.binomial                     stats\r\n5                classif.boosting              adabag,rpart\r\n6                     classif.bst                 bst,rpart\r\n7                     classif.C50                       C50\r\n8                 classif.cforest                     party\r\n9              classif.clusterSVM        SwarmSVM,LiblineaR\r\n10                  classif.ctree                     party\r\n11               classif.cvglmnet                    glmnet\r\n12                 classif.dbnDNN                   deepnet\r\n13                  classif.dcSVM            SwarmSVM,e1071\r\n14                  classif.earth               earth,stats\r\n15                 classif.evtree                    evtree\r\n16             classif.extraTrees                extraTrees\r\n17             classif.fdausc.glm                   fda.usc\r\n18          classif.fdausc.kernel                   fda.usc\r\n19             classif.fdausc.knn                   fda.usc\r\n20              classif.fdausc.np                   fda.usc\r\n21                classif.FDboost            FDboost,mboost\r\n22            classif.featureless                       mlr\r\n23                   classif.fgam                    refund\r\n24                    classif.fnn                       FNN\r\n25               classif.gamboost                    mboost\r\n26               classif.gaterSVM                  SwarmSVM\r\n27                classif.gausspr                   kernlab\r\n28                    classif.gbm                       gbm\r\n29                  classif.geoDA               DiscriMiner\r\n30               classif.glmboost                    mboost\r\n31                 classif.glmnet                    glmnet\r\n32       classif.h2o.deeplearning                       h2o\r\n33                classif.h2o.gbm                       h2o\r\n34                classif.h2o.glm                       h2o\r\n35       classif.h2o.randomForest                       h2o\r\n36                    classif.IBk                     RWeka\r\n37                    classif.J48                     RWeka\r\n38                   classif.JRip                     RWeka\r\n39                   classif.kknn                      kknn\r\n40                    classif.knn                     class\r\n41                   classif.ksvm                   kernlab\r\n42                    classif.lda                      MASS\r\n43       classif.LiblineaRL1L2SVC                 LiblineaR\r\n44      classif.LiblineaRL1LogReg                 LiblineaR\r\n45       classif.LiblineaRL2L1SVC                 LiblineaR\r\n46      classif.LiblineaRL2LogReg                 LiblineaR\r\n47         classif.LiblineaRL2SVC                 LiblineaR\r\n48 classif.LiblineaRMultiClassSVC                 LiblineaR\r\n49                  classif.linDA               DiscriMiner\r\n50                 classif.logreg                     stats\r\n51                  classif.lssvm                   kernlab\r\n52                   classif.lvq1                     class\r\n53                    classif.mda                       mda\r\n54                    classif.mlp                     RSNNS\r\n55               classif.multinom                      nnet\r\n56             classif.naiveBayes                     e1071\r\n57              classif.neuralnet                 neuralnet\r\n58                   classif.nnet                      nnet\r\n59                classif.nnTrain                   deepnet\r\n60            classif.nodeHarvest               nodeHarvest\r\n61                   classif.OneR                     RWeka\r\n62                   classif.pamr                      pamr\r\n63                   classif.PART                     RWeka\r\n64              classif.penalized                 penalized\r\n65                    classif.plr                   stepPlr\r\n66             classif.plsdaCaret                 caret,pls\r\n67                 classif.probit                     stats\r\n68                    classif.qda                      MASS\r\n69                  classif.quaDA               DiscriMiner\r\n70           classif.randomForest              randomForest\r\n71        classif.randomForestSRC           randomForestSRC\r\n72                 classif.ranger                    ranger\r\n73                    classif.rda                      klaR\r\n74                 classif.rFerns                    rFerns\r\n75                   classif.rknn                      rknn\r\n76         classif.rotationForest            rotationForest\r\n77                  classif.rpart                     rpart\r\n78                    classif.RRF                       RRF\r\n79                  classif.rrlda                     rrlda\r\n80                 classif.saeDNN                   deepnet\r\n81                    classif.sda                       sda\r\n82              classif.sparseLDA sparseLDA,MASS,elasticnet\r\n83                    classif.svm                     e1071\r\n84                classif.xgboost                   xgboost\r\n\r\n– ANALYTICS VIDHYA\r\nThe entire structure of this package relies on this premise:\r\n\\[\\text{Create a Task.   Make a Learner.   Train Them.}\\]\r\nCreating a task means loading data in the package (e.g., makeClassifTask).\r\nMaking a learner means choosing an algorithm (makeLearner) which learns from task (or data).\r\nFinally, train them (train).\r\nReferences\r\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R (1st ed. 2013. ed.). New York, NY: Springer New York.\r\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, Massachusetts: The MIT Press.\r\nHastie, T., Tibshirani, R., & Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. New York, NY: Springer New York.\r\nGéron, A. l. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems (First edition. ed.). Sebastopol, California: O’Reilly Media, Inc.\r\nRhys, H. (2020). Machine Learning with R, the tidyverse, and mlr (1st edition ed.): Manning Publications.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-17T14:33:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/",
    "title": "Beta Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Beta Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nMotivation\r\nDefinition\r\nIntuitive interpretation\r\nBeta function\r\nGamma function\r\nMain facts\r\nIn actions\r\nPlots in shiny\r\nFurther reading\r\n\r\nMotivation\r\n\r\nEven though I had learned the beta distribution from UIC’s Bayesian methods course and tutored it, such as setting up it as the prior distribution in conjugate distribution context. But it was easy to forget because of its dried content and too abstract. Here I try to combine the rigid theory (UC coursework’s content) and intuitive thought. By that way, I was able to ‘permenently stamp’ the concept to my brain.\r\n\r\nThe Beta distribution is a probability distribution on/of probabilities\r\nThe beta distribution describes a family of continuous probability distributions that are nonzero only on the interval (0 1).\r\nFor example, we can use it to model the probabilities: the Click-Through Rate of the advertisement, the batting averages, the 5-year survival chance for women with breast cancer, and so on.\r\nDefinition\r\nA continuous random variable \\(X_B \\sim Beta(\\alpha, \\beta)\\) has Beta distribution if its probability density function (PDF) is\r\n\\[ \r\nf_{X_B} (x; \\alpha, \\beta) = \\frac{1}{B(α,β)} x^{\\alpha − 1} (1−x)^{\\beta − 1}, \\ \\ \\text{for} \\ 0 < x < 1.\r\n\\]\r\nwhere \\(B(\\cdot)\\) is the Beta function and shape parameters \\(\\alpha, \\beta > 0\\).\r\nIntuitive interpretation\r\n\r\nPDF\r\nProbability as a …\r\nBinomial\r\n\\(f(x) = {n \\choose x} p^x (1-p)^{n-x}\\)\r\nparameter\r\n\r\n\\(\\rightarrow\\) the function of \\(x\\)\r\n\r\nBeta\r\n\\(f(p) = \\frac{1}{B(α,β)} p^{\\alpha − 1} (1−p)^{\\beta − 1}\\)\r\nrandom variable\r\n\r\n\\(\\rightarrow\\) the function of \\(p\\)\r\n\r\nThe beta distribution intuitively comes into play when we look at it in terms of numerator—\\(x/p\\) to the power of something multiplied by \\(1-x/1-p\\) to the power of something—from the lens of the binomial distribution.\r\nThe difference between the binomial and the beta is that the above models the number of successes (\\(x\\)), while the below models the probability (\\(p\\)) of success. In other words, the probability is a parameter in binomial; In the Beta, the probability is a random variable.\r\nIn this context, the shape parameters \\(\\alpha\\) and \\(\\beta\\) or \\(\\alpha-1\\) as the number of successes and \\(\\beta-1\\) as the number of failures\r\nWe can explore the beauty of beta distribution via the the calculator for Beta distribution—Dr. Bognar at the University of Iowa built it.\r\nBeta distribution is very flexible: bell-curve (The PDF of a beta distribution is approximately normal if \\(\\alpha + \\beta\\) is large enough and \\(\\alpha\\) & \\(\\beta\\) are approximately equal), U-shaped (when \\(\\alpha\\) < 1, \\(\\beta\\) < 1) and even straight line. Here’s an graph excerpt from wikipedia.\r\nThe very flexible of Beta distributionBeta function\r\nThe beta function is\r\n\\[ \r\nB(x,y) = \\int_0^1 t^{x−1} (1−t)^{y−1} dt = \\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)},\r\n\\]\r\nwhere \\(\\Gamma(\\cdot)\\) is the Gamma function.\r\nGamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\):\r\n\\[ \r\n\\Gamma (n) = (n−1)! = 1 \\times 2 \\times 3 \\times ... \\times (n−1)\r\n\\]\r\nThe gamma function is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\r\n\\Gamma (t) = \\int_0^{\\infty} x^{t-1} e^{-x} dx\r\n\\]\r\n\r\nSimplify the Beta function with the Gamma Function \\(\\Rightarrow\\) we saw the PDF of Beta written in terms of the Gamma function. The Beta function is the ratio of the product of the Gamma function of each parameter divided by the Gamma function of the sum of the parameters (proof refered the further reading topic).\r\n\r\nMain facts\r\n\\[\r\nE[X_B] = \\mu = \\frac{\\alpha}{\\alpha + \\beta}; \\ \\ V[X_B] = \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\r\n\\]\r\nThe standard uniform distribution \\(\\text{Unif} \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nThe mode is \\(\\omega = \\frac{\\alpha − 1}{\\alpha + \\beta − 2}\\) for \\(\\alpha, \\beta > 1\\).\r\nThe concentration is \\(\\kappa = \\alpha + \\beta\\).\r\nDefinitions of \\(\\mu, \\omega\\) and \\(\\kappa\\) can be inverted:\r\n\\[ \\alpha = \\mu\\kappa,  \\beta = (1 − \\mu)\\kappa \\]\r\n\\[ \\alpha = \\omega(\\kappa−2)+1,  \\beta = (1 − \\omega)(\\kappa−2)+1, \\ \\kappa > 2. \\]\r\nParameter \\(\\kappa\\) is a measure of number of observations needed to change our previous belief about \\(\\mu\\).\r\nIf \\(\\kappa\\) is small we need only a few new observations.\r\nExample. Concentration \\(\\kappa = 8\\) around \\(\\mu = 0.5\\) corresponds to \\(\\alpha = \\mu \\kappa = 4\\) and \\(\\beta = (1 − \\mu) \\kappa = 4\\).\r\nParameterization in terms of mean value and standard deviation is:\r\n\\[ \\alpha = \\mu [\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1]; \\ \\ \\beta = (1 - \\mu)[\\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1] \\]\r\nStandard deviation is typically smaller than standard deviation of uniform distribution on \\([0,1]\\), i.e. \\(0.28867\\).\r\nExamples.\r\nFor \\(\\mu = 0.5\\), \\(\\sigma = 0.28867\\) the shape parameters are \\(\\alpha = 1\\), \\(\\beta = 1\\).\r\nFind shape parameters of beta distribution with \\(\\mu = 0.5\\), \\(\\sigma = 0.1\\).\r\nThe standard uniform distribution \\(Unif \\ (0,1)\\) is a special case of the beta distribution \\(Beta \\ (1,1)\\), when \\(\\alpha = \\beta = 1\\).\r\nIn actions\r\nKeep parameter \\(\\beta\\) fixed. Move \\(\\alpha\\) up or down. Observe how the mass of the distribution moves\r\n\r\n\r\np <- seq(0,1,by=0.2)\r\n\r\ndf <- data.frame(p)\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=2), aes(colour = \"alpha=1,beta=2\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=2), aes(colour = \"alpha=4,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=2), aes(colour = \"alpha=6,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=8, shape2=2), aes(colour = \"alpha=8,beta=2\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDo the same as above, but keep \\(\\alpha\\) constant and move \\(\\beta\\) up or down\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=1), aes(colour = \"alpha=2,beta=1\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=5), aes(colour = \"alpha=2,beta=5\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=6), aes(colour = \"alpha=2,beta=6\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=8), aes(colour = \"alpha=2,beta=8\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nMake \\(\\alpha = \\beta = 1\\). What does the shape of the distribution tell you about your knowledge about random variable \\(\\theta\\)?\\(\\Rightarrow\\) The standard uniform distribution \\(Unif(0,1)\\) is a special case of the beta distribution \\(Beta (1,1)\\), when \\(\\alpha\\)=\\(\\beta\\)=1.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"green\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nKeep \\(\\alpha = \\beta\\) , but move both of them up or down. Interpret the shape of the distribution\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=0.5, shape2=0.5), aes(colour = \"alpha=0.5,beta=0.5\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=2, shape2=2), aes(colour = \"alpha=2,beta=2\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=4, shape2=4), aes(colour = \"alpha=4,beta=4\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=6, shape2=6), aes(colour = \"alpha=6,beta=6\")) +\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"orange\", \"olivedrab\", \"blue\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nVariance changes based on 2 shape parameters.\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=400, shape2=80), aes(colour = \"alpha=400,beta=80\")) + \r\n  stat_function(fun=dbeta, args=list(shape1=40, shape2=8), aes(colour = \"alpha=40,beta=8\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=30, shape2=70), aes(colour = \"alpha=30,beta=70\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=7), aes(colour = \"alpha=3,beta=7\")) +\r\n  scale_y_continuous(limits=c(0,25)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\", \"green\", \"orange\", \"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWhen beta distribution is used as a prior for parameter of binomial distribution, parameters \\(\\alpha\\) and \\(\\beta\\) can be interpreted as previously observed numbers of successes (\\(\\alpha\\)) or failures (\\(\\beta\\)). For example, if in 2 Bernoulli experiments there was 1 success and 1 failure you can express opinion about probability of success as \\(Beta(1,1)\\). What would you assume as prior if in 6 previously observed outcomes there were 3 successes and 3 failures? What is the likely value of the parameter? Do we have more or less information than in case of 1 success and 1 failure? \\(\\Rightarrow\\) Think of more\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=1, shape2=1), aes(colour = \"alpha=1,beta=1\")) +\r\n  stat_function(fun=dbeta, args=list(shape1=3, shape2=3), aes(colour = \"alpha=3,beta=3\")) +\r\n  stat_function(fun=dbinom, args=list(size=1, prob=0.5), aes(colour = \"Bernoulli w/ prob=0.5\")) + # bernoulli\r\n  scale_y_continuous(limits=c(0,3.6)) +\r\n  scale_colour_manual(\"\", values = c(\"red\",\"green\",\"black\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nDefine distribution with mode \\(\\omega\\)=.8 and concentration \\(\\kappa = 12\\). To do that find shape parameters \\(\\alpha = \\omega (\\kappa − 2) + 1 = 9\\) and \\(\\beta = (1 − \\omega)(\\kappa − 2) + 1 = 3\\).\r\n\r\n\r\nggplot(data=df, aes(x=p))+\r\n  stat_function(fun=dbeta, args=list(shape1=9, shape2=3), aes(colour = \"alpha=9,beta=3\")) +\r\n  scale_y_continuous(limits=c(0,3.4)) +\r\n  scale_colour_manual(\"\", values = c(\"blue\")) + \r\n  ylab(\"Density\") +\r\n  ggtitle(\"PDF of Beta Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the actions we notify that:\r\nThe special case \\(a=b=1\\) is the uniform distribution.\r\nThe distribution is roughly centered on \\(a/(a+b)\\). Actually, it turns out that the mean is exactly \\(a/(a+b)\\). Thus the mean of the distribution is determined by the relative values of \\(a\\) and \\(b\\).\r\nThe larger the values of \\(a\\) and \\(b\\), the smaller the variance of the distribution about the mean.\r\nFor moderately large values of \\(a\\) and \\(b\\) the distribution looks visually “kind of normal”, although unlike the normal distribution the Beta distribution is restricted to [0,1].\r\nPlots in shiny\r\nPlanning to build an shiny app to plot beta distribution on the specification of shape parameter (“still being in the process”).\r\nFurther reading\r\nBayesian Methods, UC’s lecture\r\nDavid Robinson (Principal Data Scientist at Heap, works in R and Python), Understanding the beta distribution (using baseball statistics), http://varianceexplained.org/statistics/beta_distribution_and_baseball/\r\nAerin Kim, Beta Distribution — Intuition, Examples, and Derivation, https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-beta-distribution-in-intuitive-explanation/Beta-Distribution-in-an-Intuitive-Explanation_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-04-15T00:29:44-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/",
    "title": "Gamma Distribution: an Intuitive Explanation",
    "description": "Intuitively explain the Gamma Distribution and its applications.",
    "author": [
      {
        "name": "Hai Nguyen",
        "url": {}
      }
    ],
    "date": "2021-04-11",
    "categories": [
      "Biostatistics",
      "Tutorial",
      "ggplot2",
      "Toolkit for Bayesian Methods"
    ],
    "contents": "\r\n\r\nContents\r\nFact\r\nGamma function\r\nDefinition\r\nParameters of Gamma: a shape with a scale or a rate\r\nPlots\r\nFurther reading\r\n\r\nFact\r\nWhy did we invent Gamma distribution? Answer: To predict the wait time until future events. Hmmm ok, but I thought that’s what the exponential distribution is for. Then, what’s the difference between exponential distribution and gamma distribution? The exponential distribution predicts the wait time until the very first event. The gamma distribution, on the other hand, predicts the wait time until the k-th event occurs.\r\n– Aerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nExamples:\r\nThe time I wait to receive an interview might follow an exponential. Now, I am waiting not for my first interview offer but my third interview offer. How long must I wait? This waiting time can be described by a gamma.\r\nI missed the first and second CTA train to go to the campus. Now, how long I am able to catch the third train?\r\nGamma function\r\nIn the lecture series of Statistics 110, Lecture 24: Gamma distribution and Poisson process | Statistics 110, Prof. Joe Blitzstein had connected the \\(n!\\) function to the Gamma function. Why?\r\nLet’s see the Gamma function\r\nThe Gamma function \\(\\Gamma\\) is an extension of the factorial function, with its argument shifted down by 1, to real and complex numbers.\r\nFor positive integer \\(n\\): \\[\\Gamma(n) = (n-1)! = 1 \\times 2 \\times 3 \\times ... \\times (n-1)\\]\r\nOne beautiful formula, Stirling formula to approximate the \\(n!\\), actually an extremely good approximation: \\[ n! \\approx \\sqrt{2\\pi n} \\Big( \\frac{n}{e}\\Big)^n\\]\r\n\r\n\r\nn <- c(1:6)\r\ny <- vector(mode = \"numeric\", length = length(n))\r\ny[1] <- 1\r\nfor(i in 2:length(n)) {\r\n  y[i] = y[i-1] * i\r\n}\r\ndta <- as.data.frame(cbind(n,y))\r\nlibrary(ggplot2)\r\nggplot(dta, aes(n, y)) + \r\n  geom_point() +\r\n  scale_x_discrete(limits=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\")) +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nThen how we connect the dots. There are many ways to do it, but there’s a philosophical way to do it by Gamma function, which is defined for all complex numbers except the non-positive integers by the integral:\r\n\\[\\Gamma(t) = \\int_0^{\\infty} x^t e^{−x} \\frac{dx}{x} \\]\r\nDefinition\r\nFrom the Gamma function, how we got the PDF of Gamma distribution. We would normalize the Gamma distribution, which means from:\r\n\\[ \\Gamma(k) = \\int_0^{\\infty} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nto,\r\n\\[ 1 = \\int_0^{\\infty} \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{dx}{x} \\]\r\nThen, \\(X = \\frac{1}{\\Gamma(k)} x^{k} e^{−x} \\frac{1}{x}\\) \\(\\sim\\) \\(Gamma(k, 1)\\) which has shape of \\(k\\) and scale of \\(1\\).\r\nHow we turn the scale of \\(1\\) to a general scale of \\(\\theta\\)?\r\nImagine that \\(Y \\sim \\frac{X}{\\theta}\\) where \\(X \\sim \\ Gamma(k,1)\\)\r\n\\(f_Y(y) = f_X(x) \\frac{dx}{dy} = \\frac{1}{\\Gamma(k)} (\\theta y)^{k} e^{−\\theta y} \\frac{1}{\\theta y} \\theta\\) where \\(\\frac{dx}{dy} = \\theta\\)\r\nThus, \\(f(y) = \\frac{1}{\\Gamma(k) \\theta^{k}} (y)^{k} e^{−\\theta y} \\frac{1}{y}\\)\r\nParameters of Gamma: a shape with a scale or a rate\r\n\r\n\r\nknitr::include_graphics(\"Gamma_scalevsrate_inwiki.png\") \r\n\r\n\r\n\r\n\r\n(#fig:model diagram)From https://en.wikipedia.org/wiki/Gamma_distribution\r\n\r\n\r\n\r\nFor (\\(\\alpha\\), \\(\\beta\\)) parameterization: Using our notation \\(k\\) (the # of events) & \\(\\lambda\\) (the rate of events), simply substitute \\(\\alpha\\) with \\(k\\), \\(\\beta\\) with \\(\\lambda\\). The PDF stays the same format as what we’ve derived.\r\nFor (\\(k\\), \\(\\theta\\)) parameterization: \\(\\theta\\) is a reciprocal of the event rate \\(\\lambda\\), which is the mean wait time (the average time between event arrivals).\r\nPlots\r\nI plotted the gamma distribution with the shape of \\(k\\), and constantly rate = \\(1\\)\r\n\r\n\r\nT <- seq(0,20,by=2.5)\r\n\r\ndf <- data.frame(T)\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=1, rate=1), aes(colour = \"k= 1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=5, rate=1), aes(colour = \"k= 5\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"k=10\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"palegreen\", \"yellowgreen\", \"olivedrab\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nI plotted the gamma distribution with the constantly shape of k = 10, and variant rate from 1 to 3.\r\n\r\n\r\nggplot(data=df, aes(x=T))+\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=1), aes(colour = \"r=1\")) + \r\n  stat_function(fun=dgamma, args=list(shape=10, rate=2), aes(colour = \"r=2\")) +\r\n  stat_function(fun=dgamma, args=list(shape=10, rate=3), aes(colour = \"r=3\")) +\r\n  scale_y_continuous(limits=c(0,0.40)) +\r\n  scale_colour_manual(\"\", values = c(\"gold\", \"burlywood\", \"darkorange\")) + \r\n  ylab(\"Probability Density\") +\r\n  ggtitle(\"PDF of Gamma Distribution (k=10)\") + \r\n  theme_bw() + \r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFurther reading\r\nLecture 24: Gamma distribution and Poisson process | Statistics 110\r\nAerin Kim, Gamma Distribution – Intuition, Derivation, and Examples\r\nWiki\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-gamma-distribution-in-intuitive-explanation/Gamma-distribution-in-an-intuitive-explanation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-23T11:53:12-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
